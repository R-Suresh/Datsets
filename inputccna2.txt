     


 Copyright      Other bestselling titles by Andrew S. Tanenbaum      About the Author      Preface      Chapter 1.  Introduction         Section 1.1.  Uses of Computer Networks         Section 1.2.  Network Hardware         Section 1.3.  Network Software         Section 1.4.  Reference Models         Section 1.5.  Example Networks         Section 1.6.  Network Standardization         Section 1.7.  Metric Units         Section 1.8.  Outline of the Rest of the Book         Section 1.9.  Summary       Chapter 2.  The Physical Layer         Section 2.1.  The Theoretical Basis for Data Communication         Section 2.2.  Guided Transmission Media         Section 2.3.  Wireless Transmission         Section 2.4.  Communication Satellites         Section 2.5.  The Public Switched Telephone Network         Section 2.6.  The Mobile Telephone System         Section 2.7.  Cable Television         Section 2.8.  Summary       Chapter 3.  The Data Link Layer         Section 3.1.  Data Link Layer Design Issues         Section 3.2.  Error Detection and Correction         Section 3.3.  Elementary Data Link Protocols         Section 3.4.  Sliding Window Protocols         Section 3.5.  Protocol Verification         Section 3.6.  Example Data Link Protocols         Section 3.7.  Summary       Chapter 4.  The Medium Access Control Sublayer         Section 4.1.  The Channel Allocation Problem         Section 4.2.  Multiple Access Protocols         Section 4.3.  Ethernet         Section 4.4.  Wireless LANs         Section 4.5.  Broadband Wireless         Section 4.6.  Bluetooth         Section 4.7.  Data Link Layer Switching         Section 4.8.  Summary       Chapter 5.  The Network Layer         Section 5.1.  Network Layer Design Issues         Section 5.2.  Routing Algorithms         Section 5.3.  Congestion Control Algorithms         Section 5.4.  Quality of Service         Section 5.5.  Internetworking         Section 5.6.  The Network Layer in the Internet         Section 5.7.  Summary       Chapter 6.  The Transport Layer         Section 6.1.  The Transport Service
Section 6.2.  Elements of Transport Protocols         
Section 6.3.  A Simple Transport Protocol         
Section 6.4.  The Internet Transport Protocols: UDP         
Section 6.5.  The Internet Transport Protocols: TCP         
Section 6.6.  Performance Issues         
Section 6.7.  Summary       
Chapter 7.  The Application Layer         
Section 7.1.  DNS—The Domain Name System         
Section 7.2.  Electronic Mail         
Section 7.3.  The World Wide Web         
Section 7.4.  Multimedia         
Section 7.5.  Summary       
Chapter 8.  Network Security         
Section 8.1.  Cryptography         
Section 8.2.  Symmetric-Key Algorithms         
Section 8.3.  Public-Key Algorithms         
Section 8.4.  Digital Signatures         
Section 8.5.  Management of Public Keys         
Section 8.6.  Communication Security         
Section 8.7.  Authentication Protocols         
Section 8.8.  E-Mail Security         
Section 8.9.  Web Security         
Section 8.10.  Social Issues         
Section 8.11.  Summary       
Chapter 9.  Reading List and Bibliography         Section 9.1.  Suggestions for Further Reading         Section 9.1.1.  Introduction and General Works         Section 9.2.  Alphabetical Bibliography 

3




Copyright 

This edition may be sold only in those countries to which it is consigned by Pearson Education International. It is not to be re-exported and it is not for sale in the U.S.A., Mexico, or Canada. 

Editorial/production supervision: 

Patti Guerrieri

Cover design director: 

Jerry Votta

Cover designer: 

Anthony Gemmellaro

Cover design: 

Andrew S. Tanenbaum

Art director: 

Gail Cocker-Bogusz

Interior Design: 

Andrew S. Tanenbaum

Interior graphics: 

Hadel Studio

Typesetting: 

Andrew S. Tanenbaum

Manufacturing buyer: 

Maura Zaldivar

Executive editor: 

Mary Franz

Editorial assistant: 

Noreen Regina

Marketing manager: 

Dan DePasquale

© 2003 Pearson Education, Inc. 

Publishing as Prentice Hall PTR 

Upper Saddle River, New Jersey 07458 

All products or services mentioned in this book are the trademarks or service marks of their respective companies or organizations. 

All rights reserved. No part of this book may be reproduced, in any form or by any means, without permission in writing from the publisher. Printed in the United States of America 

10 9 8 7 6 5 4 3 2 1 

Pearson Education LTD. 

Pearson Education Australia PTY, Limited 

Pearson Education Singapore, Pte. Ltd. 

Pearson Education North Asia Ltd. 

Pearson Education Canada, Ltd. 

4




Pearson Educación de Mexico, S.A. de C.V. 

Pearson Education — Japan 

Pearson Education Malaysia, Pte. Ltd. 

Pearson Education, Upper Saddle River, New Jersey 

Dedication 

To Suzanne, Barbara, Marvin, and the memory of Bram and Sweetie

 p 

 

5




Other bestselling titles by Andrew S. Tanenbaum 

Distributed Systems: Principles and Paradigms

This new book, co-authored with Maarten van Steen, covers both the principles and paradigms of modern distributed systems. In the first part, it covers the principles of communication, processes, naming, synchronization, consistency and replication, fault tolerance, and security in detail. Then in the second part, it goes into different paradigms used to build distributed systems, including object-based systems, distributed file systems, document-based systems, and coordination-based systems. Numerous examples are discussed at length. 

Modern Operating Systems, 2nd edition

This comprehensive text covers the principles of modern operating systems in detail and illustrates them with numerous real-world examples. After an introductory chapter, the next five chapters deal with the basic concepts: processes and threads, deadlocks, memory management, input/output, and file systems. The next six chapters deal with more advanced material, including multimedia systems, multiple processor systems, security. Finally, two detailed case studies are given: UNIX/Linux and Windows 2000. 

Structured Computer Organization, 4th edition

This widely-read classic, now in its fourth edition, provides the ideal introduction to computer architecture. It covers the topic in an easy-to-understand way, bottom up. There is a chapter on digital logic for beginners, followed by chapters on microarchitecture, the instruction set architecture level, operating systems, assembly language, and parallel computer architectures. 

Operating Systems: Design and Implementation, 2nd edition

This popular text on operating systems, co-authored with Albert S. Woodhull, is the only book covering both the principles of operating systems and their application to a real system. All the traditional operating systems topics are covered in detail. In addition, the principles are carefully illustrated with MINIX, a free POSIX-based UNIX-like operating system for personal computers. Each book contains a free CD-ROM containing the complete MINIX system, including all the source code. The source code is listed in an appendix to the book and explained in detail in the text. 

 


Chap. 1

 has the same introductory function as it did in the third edition, the contents have been revised and brought up to date. For example, introductions to the Internet, Ethernet, and wireless LANs are given there, along with some history and background. Home networking is also discussed briefly. 

Chapter 2

 has been reorganized somewhat. After a brief introduction to the principles of data communication, there are three major sections on transmission (guided media, wireless, and satellite), followed by three more on important examples (the public switched telephone system, the mobile telephone system, and cable television). Among the new topics covered in this chapter are ADSL, broadband wireless, wireless MANs, and Internet access over cable and DOCSIS. 

Chapter 3

 has always dealt with the fundamental principles of point-to-point protocols. These ideas are essentially timeless and have not changed for decades. Accordingly, the series of detailed example protocols presented in this chapter is largely unchanged from the third edition. 

In contrast, the MAC sublayer has been an area of great activity in recent years, so many changes are present in 

Chap. 4

. The section on Ethernet has been expanded to include gigabit Ethernet. Completely new are major sections on wireless LANs, broadband wireless, Bluetooth, and data link layer switching, including MPLS. 

Chapter 5

 has also been updated, with the removal of all the ATM material and the addition of additional material on the Internet. Quality of service is now also a major topic, including discussions of integrated services and differentiated services. Wireless networks are also present here, with a discussion of routing in ad hoc networks. Other new topics include NAT and peer-to-peer networks. 

Chap. 6

 is still about the transport layer, but here, too, some changes have occurred. Among these is an example of socket programming. A one-page client and a one-page server are given in C and discussed. These programs, available on the book's Web site, can be compiled and run. Together they provide a primitive remote file or Web server available for experimentation. Other new topics include remote procedure call, RTP, and transaction/TCP. 

8




Chap. 7

, on the application layer, has been more sharply focused. After a short introduction to DNS, the rest of the chapter deals with just three topics: e-mail, the Web, and multimedia. But each topic is treated in great detail. The discussion of how the Web works is now over 60 pages, covering a vast array of topics, including static and dynamic Web pages, HTTP, CGI scripts, content delivery networks, cookies, and Web caching. Material is also present on how modern Web pages are written, including brief introductions to XML, XSL, XHTML, PHP, and more, all with examples that can be tested. The wireless Web is also discussed, focusing on i-mode and WAP. The multimedia material now includes MP3, streaming audio, Internet radio, and voice over IP. 

Security has become so important that it has now been expanded to a complete chapter of over 100 pages. It covers both the principles of security (symmetric- and public-key algorithms, digital signatures, and X.509 certificates) and the applications of these principles (authentication, e-mail security, and Web security). The chapter is both broad (ranging from quantum cryptography to government censorship) and deep (e.g., how SHA-1 works in detail). 

Chapter 9

 contains an all-new list of suggested readings and a comprehensive bibliography of over 350 citations to the current literature. Over 200 of these are to papers and books written in 2000 or later. 

Computer books are full of acronyms. This one is no exception. By the time you are finished reading this one, the following should ring a bell: ADSL, AES, AMPS, AODV, ARP, ATM, BGP, CDMA, CDN, CGI, CIDR, DCF, DES, DHCP, DMCA, FDM, FHSS, GPRS, GSM, HDLC, HFC, HTML, HTTP, ICMP, IMAP, ISP, ITU, LAN, LMDS, MAC, MACA, MIME, MPEG, MPLS, MTU, NAP, NAT, NSA, NTSC, OFDM, OSPF, PCF, PCM, PGP, PHP, PKI, POTS, PPP, PSTN, QAM, QPSK, RED, RFC, RPC, RSA, RSVP, RTP, SSL, TCP, TDM, UDP, URL, UTP, VLAN, VPN, VSAT, WAN, WAP, WDMA, WEP, WWW, and XML But don't worry. Each will be carefully defined before it is used. 

To help instructors using this book as a text for a course, the author has prepared various teaching aids, including 

• A problem solutions manual. 

• Files containing the figures in multiple formats. 

• PowerPoint sheets for a course using the book. 

• A simulator (written in C) for the example protocols of 

Chap. 3

. 

• A Web page with links to many tutorials, organizations, FAQs, etc. 

The solutions manual is available directly from Prentice Hall (but 

only

 to instructors, not to students). All the other material is on the book's Web site: 

http://www.prenhall.com/tanenbaum

 

From there, click on the book's cover. 

Many people helped me during the course of the fourth edition. I would especially like to thank the following people: Ross Anderson, Elizabeth Belding-Royer, Steve Bellovin, Chatschik Bisdikian, Kees Bot, Scott Bradner, Jennifer Bray, Pat Cain, Ed Felten, Warwick Ford, Kevin Fu, Ron Fulle, Jim Geier, Mario Gerla, Natalie Giroux, Steve Hanna, Jeff Hayes, Amir Herzberg, Philip Homburg, Philipp Hoschka, David Green, Bart Jacobs, Frans Kaashoek, Steve Kent, Roger Kermode, Robert Kinicki, Shay Kutten, Rob Lanphier, Marcus Leech, Tom Maufer, Brent Miller, Shivakant Mishra, Thomas Nadeau, Shlomo Ovadia, Kaveh Pahlavan, Radia Perlman, Guillaume Pierre, Wayne Pleasant, Patrick Powell, Thomas Robertazzi, Medy Sanadidi, Christian Schmutzer, Henning Schulzrinne, Paul Sevinc, Mihail Sichitiu, Bernard Sklar, Ed Skoudis, Bob Strader, George Swallow, George Thiruvathukal, Peter Tomsu, Patrick Verkaik, Dave Vittali, Spyros Voulgaris, Jan-Mark Wams, Ruediger Weis, Bert Wijnen, Joseph Wilkes, Leendert van Doorn, and Maarten van Steen. 

9




Special thanks go to Trudy Levine for proving that grandmothers can do a fine job of reviewing technical material. Shivakant Mishra thought of many challenging end-of-chapter problems. Andy Dornan suggested additional readings for 

Chap. 9

. Jan Looyen provided essential hardware at a critical moment. Dr. F. de Nies did an expert cut-and-paste job right when it was needed. My editor at Prentice Hall, Mary Franz, provided me with more reading material than I had consumed in the previous 7 years and was helpful in numerous other ways as well. 

Finally, we come to the most important people: Suzanne, Barbara, and Marvin. To Suzanne for her love, patience, and picnic lunches. To Barbara and Marvin for being fun and cheery all the time (except when complaining about awful college textbooks, thus keeping me on my toes). Thank you. 

A

NDREW

 S. T

ANENBAUM

 

10




Chapter 1. Introduction 

Each of the past three centuries has been dominated by a single technology. The 18th century was the era of the great mechanical systems accompanying the Industrial Revolution. The 19th century was the age of the steam engine. During the 20th century, the key technology was information gathering, processing, and distribution. Among other developments, we saw the installation of worldwide telephone networks, the invention of radio and television, the birth and unprecedented growth of the computer industry, and the launching of communication satellites. 

As a result of rapid technological progress, these areas are rapidly converging and the differences between collecting, transporting, storing, and processing information are quickly disappearing. Organizations with hundreds of offices spread over a wide geographical area routinely expect to be able to examine the current status of even their most remote outpost at the push of a button. As our ability to gather, process, and distribute information grows, the demand for ever more sophisticated information processing grows even faster. 

Although the computer industry is still young compared to other industries (e.g., automobiles and air transportation), computers have made spectacular progress in a short time. During the first two decades of their existence, computer systems were highly centralized, usually within a single large room. Not infrequently, this room had glass walls, through which visitors could gawk at the great electronic wonder inside. A medium-sized company or university might have had one or two computers, while large institutions had at most a few dozen. The idea that within twenty years equally powerful computers smaller than postage stamps would be mass produced by the millions was pure science fiction. 

The merging of computers and communications has had a profound influence on the way computer systems are organized. The concept of the ''computer center'' as a room with a large computer to which users bring their work for processing is now totally obsolete. The old model of a single computer serving all of the organization's computational needs has been replaced by one in which a large number of separate but interconnected computers do the job. These systems are called 

computer networks

. The design and organization of these networks are the subjects of this book. 

Throughout the book we will use the term ''computer network'' to mean a collection of autonomous computers interconnected by a single technology. Two computers are said to be interconnected if they are able to exchange information. The connection need not be via a copper wire; fiber optics, microwaves, infrared, and communication satellites can also be used. Networks come in many sizes, shapes and forms, as we will see later. Although it may sound strange to some people, neither the Internet nor the World Wide Web is a computer network. By the end of this book, it should be clear why. The quick answer is: the Internet is not a single network but a network of networks and the Web is a distributed system that runs on top of the Internet. 

There is considerable confusion in the literature between a computer network and a 

distributed system

. The key distinction is that in a distributed system, a collection of independent computers appears to its users as a single coherent system. Usually, it has a single model or paradigm that it presents to the users. Often a layer of software on top of the operating system, called 

middleware

, is responsible for implementing this model. A well-known example of a distributed system is the 

World Wide Web

, in which everything looks like a document (Web page). 

In a computer network, this coherence, model, and software are absent. Users are exposed to the actual machines, without any attempt by the system to make the machines look and act in a coherent way. If the machines have different hardware and different operating systems, that is fully visible to the users. If a user wants to run a program on a remote machine, he 

[

]

 has to log onto that machine and run it there. 

[

]

 ''He'' should be read as ''he or she'' throughout this book. 

In effect, a distributed system is a software system built on top of a network. The software gives it a high degree of cohesiveness and transparency. Thus, the distinction between a network and a distributed system lies with the software (especially the operating system), rather than with the hardware. 

11




Nevertheless, there is considerable overlap between the two subjects. For example, both distributed systems and computer networks need to move files around. The difference lies in who invokes the movement, the system or the user. Although this book primarily focuses on networks, many of the topics are also important in distributed systems. For more information about distributed systems, see (Tanenbaum and Van Steen, 2002). 

1.1 Uses of Computer Networks 

Before we start to examine the technical issues in detail, it is worth devoting some time to pointing out why people are interested in computer networks and what they can be used for. After all, if nobody were interested in computer networks, few of them would be built. We will start with traditional uses at companies and for individuals and then move on to recent developments regarding mobile users and home networking. 

1.1.1 Business Applications 

Many companies have a substantial number of computers. For example, a company may have separate computers to monitor production, keep track of inventories, and do the payroll. Initially, each of these computers may have worked in isolation from the others, but at some point, management may have decided to connect them to be able to extract and correlate information about the entire company. 

Put in slightly more general form, the issue here is 

resource sharing

, and the goal is to make all programs, equipment, and especially data available to anyone on the network without regard to the physical location of the resource and the user. An obvious and widespread example is having a group of office workers share a common printer. None of the individuals really needs a private printer, and a high-volume networked printer is often cheaper, faster, and easier to maintain than a large collection of individual printers. 

However, probably even more important than sharing physical resources such as printers, scanners, and CD burners, is sharing information. Every large and medium-sized company and many small companies are vitally dependent on computerized information. Most companies have customer records, inventories, accounts receivable, financial statements, tax information, and much more online. If all of its computers went down, a bank could not last more than five minutes. A modern manufacturing plant, with a computer-controlled assembly line, would not last even that long. Even a small travel agency or three-person law firm is now highly dependent on computer networks for allowing employees to access relevant information and documents instantly. 

For smaller companies, all the computers are likely to be in a single office or perhaps a single building, but for larger ones, the computers and employees may be scattered over dozens of offices and plants in many countries. Nevertheless, a sales person in New York might sometimes need access to a product inventory database in Singapore. In other words, the mere fact that a user happens to be 15,000 km away from his data should not prevent him from using the data as though they were local. This goal may be summarized by saying that it is an attempt to end the ''tyranny of geography.'' 

In the simplest of terms, one can imagine a company's information system as consisting of one or more databases and some number of employees who need to access them remotely. In this model, the data are stored on powerful computers called 

servers

. Often these are centrally housed and maintained by a system administrator. In contrast, the employees have simpler machines, called 

clients

, on their desks, with which they access remote data, for example, to include in spreadsheets they are constructing. (Sometimes we will refer to the human user of the client machine as the ''client,'' but it should be clear from the context whether we mean the computer or its user.) The client and server machines are connected by a network, as illustrated in 

Fig. 1-1

. Note that we have shown the network as a simple oval, without any detail. We will use this form when we mean a network in the abstract sense. When more detail is required, it will be provided. 

Figure 1-1. A network with two clients and one server. 

12




 

This whole arrangement is called the 

client-server model

. It is widely used and forms the basis of much network usage. It is applicable when the client and server are both in the same building (e.g., belong to the same company), but also when they are far apart. For example, when a person at home accesses a page on the World Wide Web, the same model is employed, with the remote Web server being the server and the user's personal computer being the client. Under most conditions, one server can handle a large number of clients. 

If we look at the client-server model in detail, we see that two processes are involved, one on the client machine and one on the server machine. Communication takes the form of the client process sending a message over the network to the server process. The client process then waits for a reply message. When the server process gets the request, it performs the requested work or looks up the requested data and sends back a reply. These messages are shown in 

Fig. 1-2

. 

Figure 1-2. The client-server model involves requests and replies. 

 

A second goal of setting up a computer network has to do with people rather than information or even computers. A computer network can provide a powerful 

communication medium

 among employees. Virtually every company that has two or more computers now has 

e-mail

 (

electronic mail

), which employees generally use for a great deal of daily communication. In fact, a common gripe around the water cooler is how much e-mail everyone has to deal with, much of it meaningless because bosses have discovered that they can send the same (often content-free) message to all their subordinates at the push of a button. 

But e-mail is not the only form of improved communication made possible by computer networks. With a network, it is easy for two or more people who work far apart to write a report together. When one worker makes a change to an online document, the others can see the change immediately, instead of waiting several days for a letter. Such a speedup makes cooperation among far-flung groups of people easy where it previously had been impossible. 

Yet another form of computer-assisted communication is videoconferencing. Using this technology, employees at distant locations can hold a meeting, seeing and hearing each other and even writing on a shared virtual blackboard. Videoconferencing is a powerful tool for eliminating the cost and time previously devoted to travel. It is sometimes said that communication and transportation are having a race, and whichever wins will make the other obsolete. 

A third goal for increasingly many companies is doing business electronically with other companies, especially suppliers and customers. For example, manufacturers of automobiles, aircraft, and computers, among others, buy subsystems from a variety of suppliers and then assemble the parts. Using computer networks, manufacturers can place orders electronically as needed. Being able to place orders in real time (i.e., as needed) reduces the need for large inventories and enhances efficiency. 

13




A fourth goal that is starting to become more important is doing business with consumers over the Internet. Airlines, bookstores, and music vendors have discovered that many customers like the convenience of shopping from home. Consequently, many companies provide catalogs of their goods and services online and take orders on-line. This sector is expected to grow quickly in the future. It is called 

e-commerce

 (

electronic commerce

). 

1.1.2 Home Applications 

In 1977, Ken Olsen was president of the Digital Equipment Corporation, then the number two computer vendor in the world (after IBM). When asked why Digital was not going after the personal computer market in a big way, he said: ''There is no reason for any individual to have a computer in his home.'' History showed otherwise and Digital no longer exists. Why do people buy computers for home use? Initially, for word processing and games, but in recent years that picture has changed radically. Probably the biggest reason now is for Internet access. Some of the more popular uses of the Internet for home users are as follows: 

1. Access to remote information. 

2. Person-to-person communication. 

3. Interactive entertainment. 

4. Electronic commerce. 

Access to remote information comes in many forms. It can be surfing the World Wide Web for information or just for fun. Information available includes the arts, business, cooking, government, health, history, hobbies, recreation, science, sports, travel, and many others. Fun comes in too many ways to mention, plus some ways that are better left unmentioned. 

Many newspapers have gone on-line and can be personalized. For example, it is sometimes possible to tell a newspaper that you want everything about corrupt politicians, big fires, scandals involving celebrities, and epidemics, but no football, thank you. Sometimes it is even possible to have the selected articles downloaded to your hard disk while you sleep or printed on your printer just before breakfast. As this trend continues, it will cause massive unemployment among 12-year-old paperboys, but newspapers like it because distribution has always been the weakest link in the whole production chain. 

The next step beyond newspapers (plus magazines and scientific journals) is the on-line digital library. Many professional organizations, such as the ACM (

www.acm.org

) and the IEEE Computer Society (

www.computer.org

), already have many journals and conference proceedings on-line. Other groups are following rapidly. Depending on the cost, size, and weight of book-sized notebook computers, printed books may become obsolete. Skeptics should take note of the effect the printing press had on the medieval illuminated manuscript. 

All of the above applications involve interactions between a person and a remote database full of information. The second broad category of network use is person-to-person communication, basically the 21st century's answer to the 19th century's telephone. E-mail is already used on a daily basis by millions of people all over the world and its use is growing rapidly. It already routinely contains audio and video as well as text and pictures. Smell may take a while. 

Any teenager worth his or her salt is addicted to 

instant messaging

. This facility, derived from the UNIX 

talk

 program in use since around 1970, allows two people to type messages at each other in real time. A multiperson version of this idea is the 

chat room

, in which a group of people can type messages for all to see. 

Worldwide newsgroups, with discussions on every conceivable topic, are already commonplace among a select group of people, and this phenomenon will grow to include the population at large. These discussions, in which one person posts a message and all the other subscribers to the newsgroup can read it, run the gamut from humorous to impassioned. Unlike chat rooms, newsgroups are not real time and messages are saved so that when someone comes back from vacation, all messages that have been posted in the meanwhile are patiently waiting for reading. 

Another type of person-to-person communication often goes by the name of 

peer-to-peer

 communication, to distinguish it from the client-server model (Parameswaran et al., 2001). In this form, individuals who form a loose 

14




group can communicate with others in the group, as shown in 

Fig. 1-3

. Every person can, in principle, communicate with one or more other people; there is no fixed division into clients and servers. 

Figure 1-3. In a peer-to-peer system there are no fixed clients and servers. 

 

Peer-to-peer communication really hit the big time around 2000 with a service called Napster, which at its peak had over 50 million music fans swapping music, in what was probably the biggest copyright infringement in all of recorded history (Lam and Tan, 2001; and Macedonia, 2000). The idea was fairly simple. Members registered the music they had on their hard disks in a central database maintained on the Napster server. If a member wanted a song, he checked the database to see who had it and went directly there to get it. By not actually keeping any music on its machines, Napster argued that it was not infringing anyone's copyright. The courts did not agree and shut it down. 

However, the next generation of peer-to-peer systems eliminates the central database by having each user maintain his own database locally, as well as providing a list of other nearby people who are members of the system. A new user can then go to any existing member to see what he has and get a list of other members to inspect for more music and more names. This lookup process can be repeated indefinitely to build up a large local database of what is out there. It is an activity that would get tedious for people but is one at which computers excel. 

Legal applications for peer-to-peer communication also exist. For example, fans sharing public domain music or sample tracks that new bands have released for publicity purposes, families sharing photos, movies, and genealogical information, and teenagers playing multiperson on-line games. In fact, one of the most popular Internet applications of all, e-mail, is inherently peer-to-peer. This form of communication is expected to grow considerably in the future. 

Electronic crime is not restricted to copyright law. Another hot area is electronic gambling. Computers have been simulating things for decades. Why not simulate slot machines, roulette wheels, blackjack dealers, and more gambling equipment? Well, because it is illegal in a lot of places. The trouble is, gambling is legal in a lot of other places (England, for example) and casino owners there have grasped the potential for Internet gambling. What happens if the gambler and the casino are in different countries, with conflicting laws? Good question. 

Other communication-oriented applications include using the Internet to carry telephone calls, video phone, and Internet radio, three rapidly growing areas. Another application is telelearning, meaning attending 8 

A.M.

 classes without the inconvenience of having to get out of bed first. In the long run, the use of networks to enhance human-to-human communication may prove more important than any of the others. 

Our third category is entertainment, which is a huge and growing industry. The killer application here (the one that may drive all the rest) is video on demand. A decade or so hence, it may be possible to select any movie or television program ever made, in any country, and have it displayed on your screen instantly. New films may become interactive, where the user is occasionally prompted for the story direction (should Macbeth murder Duncan or just bide his time?) with alternative scenarios provided for all cases. Live television may also become interactive, with the audience participating in quiz shows, choosing among contestants, and so on. 

15




On the other hand, maybe the killer application will not be video on demand. Maybe it will be game playing. Already we have multiperson real-time simulation games, like hide-and-seek in a virtual dungeon, and flight simulators with the players on one team trying to shoot down the players on the opposing team. If games are played with goggles and three-dimensional real-time, photographic-quality moving images, we have a kind of worldwide shared virtual reality. 

Our fourth category is electronic commerce in the broadest sense of the term. Home shopping is already popular and enables users to inspect the on-line catalogs of thousands of companies. Some of these catalogs will soon provide the ability to get an instant video on any product by just clicking on the product's name. After the customer buys a product electronically but cannot figure out how to use it, on-line technical support may be consulted. 

Another area in which e-commerce is already happening is access to financial institutions. Many people already pay their bills, manage their bank accounts, and handle their investments electronically. This will surely grow as networks become more secure. 

One area that virtually nobody foresaw is electronic flea markets (e-flea?). On-line auctions of second-hand goods have become a massive industry. Unlike traditional e-commerce, which follows the client-server model, on-line auctions are more of a peer-to-peer system, sort of consumer-to-consumer. Some of these forms of e-commerce have acquired cute little tags based on the fact that ''to'' and ''2'' are pronounced the same. The most popular ones are listed in 

Fig. 1-4

. 

Figure 1-4. Some forms of e-commerce. 

 

No doubt the range of uses of computer networks will grow rapidly in the future, and probably in ways no one can now foresee. After all, how many people in 1990 predicted that teenagers tediously typing short text messages on mobile phones while riding buses would be an immense money maker for telephone companies in 10 years? But short message service is very profitable. 

Computer networks may become hugely important to people who are geographically challenged, giving them the same access to services as people living in the middle of a big city. Telelearning may radically affect education; universities may go national or international. Telemedicine is only now starting to catch on (e.g., remote patient monitoring) but may become much more important. But the killer application may be something mundane, like using the webcam in your refrigerator to see if you have to buy milk on the way home from work. 

1.1.3 Mobile Users 

Mobile computers, such as notebook computers and personal digital assistants (PDAs), are one of the fastest-growing segments of the computer industry. Many owners of these computers have desktop machines back at the office and want to be connected to their home base even when away from home or en route. Since having a wired connection is impossible in cars and airplanes, there is a lot of interest in wireless networks. In this section we will briefly look at some of the uses of wireless networks. 

Why would anyone want one? A common reason is the portable office. People on the road often want to use their portable electronic equipment to send and receive telephone calls, faxes, and electronic mail, surf the Web, access remote files, and log on to remote machines. And they want to do this from anywhere on land, sea, or air. For example, at computer conferences these days, the organizers often set up a wireless network in the conference area. Anyone with a notebook computer and a wireless modem can just turn the computer on and be connected to the Internet, as though the computer were plugged into a wired network. Similarly, some 

16




universities have installed wireless networks on campus so students can sit under the trees and consult the library's card catalog or read their e-mail. 

Wireless networks are of great value to fleets of trucks, taxis, delivery vehicles, and repairpersons for keeping in contact with home. For example, in many cities, taxi drivers are independent businessmen, rather than being employees of a taxi company. In some of these cities, the taxis have a display the driver can see. When a customer calls up, a central dispatcher types in the pickup and destination points. This information is displayed on the drivers' displays and a beep sounds. The first driver to hit a button on the display gets the call. 

Wireless networks are also important to the military. If you have to be able to fight a war anywhere on earth on short notice, counting on using the local networking infrastructure is probably not a good idea. It is better to bring your own. 

Although wireless networking and mobile computing are often related, they are not identical, as 

Fig. 1-5

 shows. Here we see a distinction between 

fixed wireless

 and 

mobile wireless

. Even notebook computers are sometimes wired. For example, if a traveler plugs a notebook computer into the telephone jack in a hotel room, he has mobility without a wireless network. 

Figure 1-5. Combinations of wireless networks and mobile computing. 

 

On the other hand, some wireless computers are not mobile. An important example is a company that owns an older building lacking network cabling, and which wants to connect its computers. Installing a wireless network may require little more than buying a small box with some electronics, unpacking it, and plugging it in. This solution may be far cheaper than having workmen put in cable ducts to wire the building. 

But of course, there are also the true mobile, wireless applications, ranging from the portable office to people walking around a store with a PDA doing inventory. At many busy airports, car rental return clerks work in the parking lot with wireless portable computers. They type in the license plate number of returning cars, and their portable, which has a built-in printer, calls the main computer, gets the rental information, and prints out the bill on the spot. 

As wireless technology becomes more widespread, numerous other applications are likely to emerge. Let us take a quick look at some of the possibilities. Wireless parking meters have advantages for both users and city governments. The meters could accept credit or debit cards with instant verification over the wireless link. When a meter expires, it could check for the presence of a car (by bouncing a signal off it) and report the expiration to the police. It has been estimated that city governments in the U.S. alone could collect an additional $10 billion this way (Harte et al., 2000). Furthermore, better parking enforcement would help the environment, as drivers who knew their illegal parking was sure to be caught might use public transport instead. 

Food, drink, and other vending machines are found everywhere. However, the food does not get into the machines by magic. Periodically, someone comes by with a truck to fill them. If the vending machines issued a wireless report once a day announcing their current inventories, the truck driver would know which machines needed servicing and how much of which product to bring. This information could lead to more efficient route planning. Of course, this information could be sent over a standard telephone line as well, but giving every vending machine a fixed telephone connection for one call a day is expensive on account of the fixed monthly charge. 

Another area in which wireless could save money is utility meter reading. If electricity, gas, water, and other meters in people's homes were to report usage over a wireless network, there would be no need to send out meter readers. Similarly, wireless smoke detectors could call the fire department instead of making a big noise 

17




(which has little value if no one is home). As the cost of both the radio devices and the air time drops, more and more measurement and reporting will be done with wireless networks. 

A whole different application area for wireless networks is the expected merger of cell phones and PDAs into tiny wireless computers. A first attempt was tiny wireless PDAs that could display stripped-down Web pages on their even tinier screens. This system, called 

WAP 1.0

 (

Wireless Application Protocol

) failed, mostly due to the microscopic screens, low bandwidth, and poor service. But newer devices and services will be better with WAP 2.0. 

One area in which these devices may excel is called 

m-commerce

 (

mobile-commerce

) (Senn, 2000). The driving force behind this phenomenon consists of an amalgam of wireless PDA manufacturers and network operators who are trying hard to figure out how to get a piece of the e-commerce pie. One of their hopes is to use wireless PDAs for banking and shopping. One idea is to use the wireless PDAs as a kind of electronic wallet, authorizing payments in stores, as a replacement for cash and credit cards. The charge then appears on the mobile phone bill. From the store's point of view, this scheme may save them most of the credit card company's fee, which can be several percent. Of course, this plan may backfire, since customers in a store might use their PDAs to check out competitors' prices before buying. Worse yet, telephone companies might offer PDAs with bar code readers that allow a customer to scan a product in a store and then instantaneously get a detailed report on where else it can be purchased and at what price. 

Since the network operator knows where the user is, some services are intentionally location dependent. For example, it may be possible to ask for a nearby bookstore or Chinese restaurant. Mobile maps are another candidate. So are very local weather forecasts (''When is it going to stop raining in my backyard?''). No doubt many other applications appear as these devices become more widespread. 

One huge thing that m-commerce has going for it is that mobile phone users are accustomed to paying for everything (in contrast to Internet users, who expect everything to be free). If an Internet Web site charged a fee to allow its customers to pay by credit card, there would be an immense howling noise from the users. If a mobile phone operator allowed people to pay for items in a store by using the phone and then tacked on a fee for this convenience, it would probably be accepted as normal. Time will tell. 

A little further out in time are personal area networks and wearable computers. IBM has developed a watch that runs Linux (including the X11 windowing system) and has wireless connectivity to the Internet for sending and receiving e-mail (Narayanaswami et al., 2002). In the future, people may exchange business cards just by exposing their watches to each other. Wearable wireless computers may give people access to secure rooms the same way magnetic stripe cards do now (possibly in combination with a PIN code or biometric measurement). These watches may also be able to retrieve information relevant to the user's current location (e.g., local restaurants). The possibilities are endless. 

Smart watches with radios have been part of our mental space since their appearance in the Dick Tracy comic strip in 1946. But smart dust? Researchers at Berkeley have packed a wireless computer into a cube 1 mm on edge (Warneke et al., 2001). Potential applications include tracking inventory, packages, and even small birds, rodents, and insects. 

1.1.4 Social Issues 

The widespread introduction of networking has introduced new social, ethical, and political problems. Let us just briefly mention a few of them; a thorough study would require a full book, at least. A popular feature of many networks are newsgroups or bulletin boards whereby people can exchange messages with like-minded individuals. As long as the subjects are restricted to technical topics or hobbies like gardening, not too many problems will arise. 

The trouble comes when newsgroups are set up on topics that people actually care about, like politics, religion, or sex. Views posted to such groups may be deeply offensive to some people. Worse yet, they may not be politically correct. Furthermore, messages need not be limited to text. High-resolution color photographs and even short video clips can now easily be transmitted over computer networks. Some people take a live-and-let-live view, but others feel that posting certain material (e.g., attacks on particular countries or religions, 

18




pornography, etc.) is simply unacceptable and must be censored. Different countries have different and conflicting laws in this area. Thus, the debate rages. 

People have sued network operators, claiming that they are responsible for the contents of what they carry, just as newspapers and magazines are. The inevitable response is that a network is like a telephone company or the post office and cannot be expected to police what its users say. Stronger yet, were network operators to censor messages, they would likely delete everything containing even the slightest possibility of them being sued, and thus violate their users' rights to free speech. It is probably safe to say that this debate will go on for a while. 

Another fun area is employee rights versus employer rights. Many people read and write e-mail at work. Many employers have claimed the right to read and possibly censor employee messages, including messages sent from a home computer after work. Not all employees agree with this. 

Even if employers have power over employees, does this relationship also govern universities and students? How about high schools and students? In 1994, Carnegie-Mellon University decided to turn off the incoming message stream for several newsgroups dealing with sex because the university felt the material was inappropriate for minors (i.e., those few students under 18). The fallout from this event took years to settle. 

Another key topic is government versus citizen. The FBI has installed a system at many Internet service providers to snoop on all incoming and outgoing e-mail for nuggets of interest to it (Blaze and Bellovin, 2000; Sobel, 2001; and Zacks, 2001). The system was originally called 

Carnivore

 but bad publicity caused it to be renamed to the more innocent-sounding DCS1000. But its goal is still to spy on millions of people in the hope of finding information about illegal activities. Unfortunately, the Fourth Amendment to the U.S. Constitution prohibits government searches without a search warrant. Whether these 54 words, written in the 18th century, still carry any weight in the 21st century is a matter that may keep the courts busy until the 22nd century. 

The government does not have a monopoly on threatening people's privacy. The private sector does its bit too. For example, small files called cookies that Web browsers store on users' computers allow companies to track users' activities in cyberspace and also may allow credit card numbers, social security numbers, and other confidential information to leak all over the Internet (Berghel, 2001). 

Computer networks offer the potential for sending anonymous messages. In some situations, this capability may be desirable. For example, it provides a way for students, soldiers, employees, and citizens to blow the whistle on illegal behavior on the part of professors, officers, superiors, and politicians without fear of reprisals. On the other hand, in the United States and most other democracies, the law specifically permits an accused person the right to confront and challenge his accuser in court. Anonymous accusations cannot be used as evidence. 

In short, computer networks, like the printing press 500 years ago, allow ordinary citizens to distribute their views in different ways and to different audiences than were previously possible. This new-found freedom brings with it many unsolved social, political, and moral issues. 

Along with the good comes the bad. Life seems to be like that. The Internet makes it possible to find information quickly, but a lot of it is ill-informed, misleading, or downright wrong. The medical advice you plucked from the Internet may have come from a Nobel Prize winner or from a high school dropout. Computer networks have also introduced new kinds of antisocial and criminal behavior. Electronic junk mail (spam) has become a part of life because people have collected millions of e-mail addresses and sell them on CD-ROMs to would-be marketeers. E-mail messages containing active content (basically programs or macros that execute on the receiver's machine) can contain viruses that wreak havoc. 

Identity theft is becoming a serious problem as thieves collect enough information about a victim to obtain get credit cards and other documents in the victim's name. Finally, being able to transmit music and video digitally has opened the door to massive copyright violations that are hard to catch and enforce. 

A lot of these problems could be solved if the computer industry took computer security seriously. If all messages were encrypted and authenticated, it would be harder to commit mischief. This technology is well established and we will study it in detail in 

Chap. 8

. The problem is that hardware and software vendors know that putting in security features costs money and their customers are not demanding such features. In addition, a substantial number of the problems are caused by buggy software, which occurs because vendors keep adding 

19




more and more features to their programs, which inevitably means more code and thus more bugs. A tax on new features might help, but that is probably a tough sell in some quarters. A refund for defective software might be nice, except it would bankrupt the entire software industry in the first year. 

1.2 Network Hardware 

It is now time to turn our attention from the applications and social aspects of networking (the fun stuff) to the technical issues involved in network design (the work stuff). There is no generally accepted taxonomy into which all computer networks fit, but two dimensions stand out as important: transmission technology and scale. We will now examine each of these in turn. 

Broadly speaking, there are two types of transmission technology that are in widespread use. They are as follows: 

1. Broadcast links. 

2. Point-to-point links. 

Broadcast networks

 have a single communication channel that is shared by all the machines on the network. Short messages, called 

packets

 in certain contexts, sent by any machine are received by all the others. An address field within the packet specifies the intended recipient. Upon receiving a packet, a machine checks the address field. If the packet is intended for the receiving machine, that machine processes the packet; if the packet is intended for some other machine, it is just ignored. 

As an analogy, consider someone standing at the end of a corridor with many rooms off it and shouting ''Watson, come here. I want you.'' Although the packet may actually be received (heard) by many people, only Watson responds. The others just ignore it. Another analogy is an airport announcement asking all flight 644 passengers to report to gate 12 for immediate boarding. 

Broadcast systems generally also allow the possibility of addressing a packet to 

all

 destinations by using a special code in the address field. When a packet with this code is transmitted, it is received and processed by every machine on the network. This mode of operation is called 

broadcasting

. Some broadcast systems also support transmission to a subset of the machines, something known as 

multicasting

. One possible scheme is to reserve one bit to indicate multicasting. The remaining 

n

 - 1 address bits can hold a group number. Each machine can ''subscribe'' to any or all of the groups. When a packet is sent to a certain group, it is delivered to all machines subscribing to that group. 

In contrast, 

point-to-point

 networks consist of many connections between individual pairs of machines. To go from the source to the destination, a packet on this type of network may have to first visit one or more intermediate machines. Often multiple routes, of different lengths, are possible, so finding good ones is important in point-to-point networks. As a general rule (although there are many exceptions), smaller, geographically localized networks tend to use broadcasting, whereas larger networks usually are point-to-point. Point-to-point transmission with one sender and one receiver is sometimes called 

unicasting

. 

An alternative criterion for classifying networks is their scale. In 

Fig. 1-6

 we classify multiple processor systems by their physical size. At the top are the 

personal area networks

, networks that are meant for one person. For example, a wireless network connecting a computer with its mouse, keyboard, and printer is a personal area network. Also, a PDA that controls the user's hearing aid or pacemaker fits in this category. Beyond the personal area networks come longer-range networks. These can be divided into local, metropolitan, and wide area networks. Finally, the connection of two or more networks is called an internetwork. The worldwide Internet is a well-known example of an internetwork. Distance is important as a classification metric because different techniques are used at different scales. In this book we will be concerned with networks at all these scales. Below we give a brief introduction to network hardware. 

Figure 1-6. Classification of interconnected processors by scale. 

20




 

1.2.1 Local Area Networks 

Local area networks

, generally called 

LANs

, are privately-owned networks within a single building or campus of up to a few kilometers in size. They are widely used to connect personal computers and workstations in company offices and factories to share resources (e.g., printers) and exchange information. LANs are distinguished from other kinds of networks by three characteristics: (1) their size, (2) their transmission technology, and (3) their topology. 

LANs are restricted in size, which means that the worst-case transmission time is bounded and known in advance. Knowing this bound makes it possible to use certain kinds of designs that would not otherwise be possible. It also simplifies network management. 

LANs may use a transmission technology consisting of a cable to which all the machines are attached, like the telephone company party lines once used in rural areas. Traditional LANs run at speeds of 10 Mbps to 100 Mbps, have low delay (microseconds or nanoseconds), and make very few errors. Newer LANs operate at up to 10 Gbps. In this book, we will adhere to tradition and measure line speeds in megabits/sec (1 Mbps is 1,000,000 bits/sec) and gigabits/sec (1 Gbps is 1,000,000,000 bits/sec). 

Various topologies are possible for broadcast LANs. 

Figure 1-7

 shows two of them. In a bus (i.e., a linear cable) network, at any instant at most one machine is the master and is allowed to transmit. All other machines are required to refrain from sending. An arbitration mechanism is needed to resolve conflicts when two or more machines want to transmit simultaneously. The arbitration mechanism may be centralized or distributed. IEEE 802.3, popularly called 

Ethernet

, for example, is a bus-based broadcast network with decentralized control, usually operating at 10 Mbps to 10 Gbps. Computers on an Ethernet can transmit whenever they want to; if two or more packets collide, each computer just waits a random time and tries again later. 

Figure 1-7. Two broadcast networks. (a) Bus. (b) Ring. 

 

A second type of broadcast system is the ring. In a ring, each bit propagates around on its own, not waiting for the rest of the packet to which it belongs. Typically, each bit circumnavigates the entire ring in the time it takes to 

21




transmit a few bits, often before the complete packet has even been transmitted. As with all other broadcast systems, some rule is needed for arbitrating simultaneous accesses to the ring. Various methods, such as having the machines take turns, are in use. IEEE 802.5 (the IBM token ring), is a ring-based LAN operating at 4 and 16 Mbps. FDDI is another example of a ring network. 

Broadcast networks can be further divided into static and dynamic, depending on how the channel is allocated. A typical static allocation would be to divide time into discrete intervals and use a round-robin algorithm, allowing each machine to broadcast only when its time slot comes up. Static allocation wastes channel capacity when a machine has nothing to say during its allocated slot, so most systems attempt to allocate the channel dynamically (i.e., on demand). 

Dynamic allocation methods for a common channel are either centralized or decentralized. In the centralized channel allocation method, there is a single entity, for example, a bus arbitration unit, which determines who goes next. It might do this by accepting requests and making a decision according to some internal algorithm. In the decentralized channel allocation method, there is no central entity; each machine must decide for itself whether to transmit. You might think that this always leads to chaos, but it does not. Later we will study many algorithms designed to bring order out of the potential chaos. 

1.2.2 Metropolitan Area Networks 

A 

metropolitan area network

, or 

MAN

, covers a city. The best-known example of a MAN is the cable television network available in many cities. This system grew from earlier community antenna systems used in areas with poor over-the-air television reception. In these early systems, a large antenna was placed on top of a nearby hill and signal was then piped to the subscribers' houses. 

At first, these were locally-designed, ad hoc systems. Then companies began jumping into the business, getting contracts from city governments to wire up an entire city. The next step was television programming and even entire channels designed for cable only. Often these channels were highly specialized, such as all news, all sports, all cooking, all gardening, and so on. But from their inception until the late 1990s, they were intended for television reception only. 

Starting when the Internet attracted a mass audience, the cable TV network operators began to realize that with some changes to the system, they could provide two-way Internet service in unused parts of the spectrum. At that point, the cable TV system began to morph from a way to distribute television to a metropolitan area network. To a first approximation, a MAN might look something like the system shown in 

Fig. 1-8

. In this figure we see both television signals and Internet being fed into the centralized 

head end

 for subsequent distribution to people's homes. We will come back to this subject in detail in 

Chap. 2

. 

Figure 1-8. A metropolitan area network based on cable TV. 

 

22




Cable television is not the only MAN. Recent developments in high-speed wireless Internet access resulted in another MAN, which has been standardized as IEEE 802.16. We will look at this area in 

Chap. 2

. 

1.2.3 Wide Area Networks 

A 

wide area network

, or 

WAN

, spans a large geographical area, often a country or continent. It contains a collection of machines intended for running user (i.e., application) programs. We will follow traditional usage and call these machines 

hosts

. The hosts are connected by a 

communication subnet

, or just 

subnet

 for short. The hosts are owned by the customers (e.g., people's personal computers), whereas the communication subnet is typically owned and operated by a telephone company or Internet service provider. The job of the subnet is to carry messages from host to host, just as the telephone system carries words from speaker to listener. Separation of the pure communication aspects of the network (the subnet) from the application aspects (the hosts), greatly simplifies the complete network design. 

In most wide area networks, the subnet consists of two distinct components: transmission lines and switching elements. 

Transmission lines

 move bits between machines. They can be made of copper wire, optical fiber, or even radio links. 

Switching elements

 are specialized computers that connect three or more transmission lines. When data arrive on an incoming line, the switching element must choose an outgoing line on which to forward them. These switching computers have been called by various names in the past; the name 

router

 is now most commonly used. Unfortunately, some people pronounce it ''rooter'' and others have it rhyme with ''doubter.'' Determining the correct pronunciation will be left as an exercise for the reader. (Note: the perceived correct answer may depend on where you live.) 

In this model, shown in 

Fig. 1-9

, each host is frequently connected to a LAN on which a router is present, although in some cases a host can be connected directly to a router. The collection of communication lines and routers (but not the hosts) form the subnet. 

Figure 1-9. Relation between hosts on LANs and the subnet. 

 

A short comment about the term ''subnet'' is in order here. Originally, its 

only

 meaning was the collection of routers and communication lines that moved packets from the source host to the destination host. However, some years later, it also acquired a second meaning in conjunction with network addressing (which we will discuss in 

Chap. 5

). Unfortunately, no widely-used alternative exists for its initial meaning, so with some hesitation we will use it in both senses. From the context, it will always be clear which is meant. 

In most WANs, the network contains numerous transmission lines, each one connecting a pair of routers. If two routers that do not share a transmission line wish to communicate, they must do this indirectly, via other routers. When a packet is sent from one router to another via one or more intermediate routers, the packet is received at each intermediate router in its entirety, stored there until the required output line is free, and then forwarded. A subnet organized according to this principle is called a 

store-and-forward

 or 

packet-switched

 subnet. Nearly all wide area networks (except those using satellites) have store-and-forward subnets. When the packets are small and all the same size, they are often called 

cells

. 

The principle of a packet-switched WAN is so important that it is worth devoting a few more words to it. Generally, when a process on some host has a message to be sent to a process on some other host, the sending host first cuts the message into packets, each one bearing its number in the sequence. These packets 

23




are then injected into the network one at a time in quick succession. The packets are transported individually over the network and deposited at the receiving host, where they are reassembled into the original message and delivered to the receiving process. A stream of packets resulting from some initial message is illustrated in 

Fig. 

1-10

. 

Figure 1-10. A stream of packets from sender to receiver. 

 

In this figure, all the packets follow the route 

ACE

, rather than 

ABDE

 or 

ACDE

. In some networks all packets from a given message 

must

 follow the same route; in others each packet is routed separately. Of course, if 

ACE

 is the best route, all packets may be sent along it, even if each packet is individually routed. 

Routing decisions are made locally. When a packet arrives at router 

A

,itis up to 

A

 to decide if this packet should be sent on the line to 

B

 or the line to 

C

. How 

A

 makes that decision is called the 

routing algorithm

. Many of them exist. We will study some of them in detail in 

Chap. 5

. 

Not all WANs are packet switched. A second possibility for a WAN is a satellite system. Each router has an antenna through which it can send and receive. All routers can hear the output 

from

 the satellite, and in some cases they can also hear the upward transmissions of their fellow routers 

to

 the satellite as well. Sometimes the routers are connected to a substantial point-to-point subnet, with only some of them having a satellite antenna. Satellite networks are inherently broadcast and are most useful when the broadcast property is important. 

1.2.4 Wireless Networks 

Digital wireless communication is not a new idea. As early as 1901, the Italian physicist Guglielmo Marconi demonstrated a ship-to-shore wireless telegraph, using Morse Code (dots and dashes are binary, after all). Modern digital wireless systems have better performance, but the basic idea is the same. 

To a first approximation, wireless networks can be divided into three main categories: 

1. System interconnection. 

2. Wireless LANs. 

3. Wireless WANs. 

System interconnection is all about interconnecting the components of a computer using short-range radio. Almost every computer has a monitor, keyboard, mouse, and printer connected to the main unit by cables. So many new users have a hard time plugging all the cables into the right little holes (even though they are usually color coded) that most computer vendors offer the option of sending a technician to the user's home to do it. Consequently, some companies got together to design a short-range wireless network called 

Bluetooth

 to connect these components without wires. Bluetooth also allows digital cameras, headsets, scanners, and other devices to connect to a computer by merely being brought within range. No cables, no driver installation, just put them down, turn them on, and they work. For many people, this ease of operation is a big plus. 

In the simplest form, system interconnection networks use the master-slave paradigm of 

Fig. 1-11(a)

. The system unit is normally the master, talking to the mouse, keyboard, etc., as slaves. The master tells the slaves what addresses to use, when they can broadcast, how long they can transmit, what frequencies they can use, and so on. We will discuss Bluetooth in more detail in 

Chap. 4

. 

24




Figure 1-11. (a) Bluetooth configuration. (b) Wireless LAN. 

 

The next step up in wireless networking are the wireless LANs. These are systems in which every computer has a radio modem and antenna with which it can communicate with other systems. Often there is an antenna on the ceiling that the machines talk to, as shown in 

Fig. 1-11(b)

. However, if the systems are close enough, they can communicate directly with one another in a peer-to-peer configuration. Wireless LANs are becoming increasingly common in small offices and homes, where installing Ethernet is considered too much trouble, as well as in older office buildings, company cafeterias, conference rooms, and other places. There is a standard for wireless LANs, called 

IEEE 802.11

, which most systems implement and which is becoming very widespread. We will discuss it in 

Chap. 4

. 

The third kind of wireless network is used in wide area systems. The radio network used for cellular telephones is an example of a low-bandwidth wireless system. This system has already gone through three generations. The first generation was analog and for voice only. The second generation was digital and for voice only. The third generation is digital and is for both voice and data. In a certain sense, cellular wireless networks are like wireless LANs, except that the distances involved are much greater and the bit rates much lower. Wireless LANs can operate at rates up to about 50 Mbps over distances of tens of meters. Cellular systems operate below 1 Mbps, but the distance between the base station and the computer or telephone is measured in kilometers rather than in meters. We will have a lot to say about these networks in 

Chap. 2

. 

In addition to these low-speed networks, high-bandwidth wide area wireless networks are also being developed. The initial focus is high-speed wireless Internet access from homes and businesses, bypassing the telephone system. This service is often called local multipoint distribution service. We will study it later in the book. A standard for it, called IEEE 802.16, has also been developed. We will examine the standard in 

Chap. 4

. 

Almost all wireless networks hook up to the wired network at some point to provide access to files, databases, and the Internet. There are many ways these connections can be realized, depending on the circumstances. For example, in 

Fig. 1-12(a)

, we depict an airplane with a number of people using modems and seat-back telephones to call the office. Each call is independent of the other ones. A much more efficient option, however, is the flying LAN of 

Fig. 1-12(b)

. Here each seat comes equipped with an Ethernet connector into which passengers can plug their computers. A single router on the aircraft maintains a radio link with some router on the ground, changing routers as it flies along. This configuration is just a traditional LAN, except that its connection to the outside world happens to be a radio link instead of a hardwired line. 

Figure 1-12. (a) Individual mobile computers. (b) A flying LAN. 

25




 

Many people believe wireless is the wave of the future (e.g., Bi et al., 2001; Leeper, 2001; Varshey and Vetter, 2000) but at least one dissenting voice has been heard. Bob Metcalfe, the inventor of Ethernet, has written: ''Mobile wireless computers are like mobile pipeless bathrooms—portapotties. They will be common on vehicles, and at construction sites, and rock concerts. My advice is to wire up your home and stay there'' (Metcalfe, 1995). History may record this remark in the same category as IBM's chairman T.J. Watson's 1945 explanation of why IBM was not getting into the computer business: ''Four or five computers should be enough for the entire world until the year 2000.'' 

1.2.5 Home Networks 

Home networking is on the horizon. The fundamental idea is that in the future most homes will be set up for networking. Every device in the home will be capable of communicating with every other device, and all of them will be accessible over the Internet. This is one of those visionary concepts that nobody asked for (like TV remote controls or mobile phones), but once they arrived nobody can imagine how they lived without them. 

Many devices are capable of being networked. Some of the more obvious categories (with examples) are as follows: 

1. Computers (desktop PC, notebook PC, PDA, shared peripherals). 

2. Entertainment (TV, DVD, VCR, camcorder, camera, stereo, MP3). 

3. Telecommunications (telephone, mobile telephone, intercom, fax). 

4. Appliances (microwave, refrigerator, clock, furnace, airco, lights). 

5. Telemetry (utility meter, smoke/burglar alarm, thermostat, babycam). 

Home computer networking is already here in a limited way. Many homes already have a device to connect multiple computers to a fast Internet connection. Networked entertainment is not quite here, but as more and more music and movies can be downloaded from the Internet, there will be a demand to connect stereos and televisions to it. Also, people will want to share their own videos with friends and family, so the connection will need to go both ways. Telecommunications gear is already connected to the outside world, but soon it will be digital and go over the Internet. The average home probably has a dozen clocks (e.g., in appliances), all of which have to be reset twice a year when daylight saving time (summer time) comes and goes. If all the clocks were on the Internet, that resetting could be done automatically. Finally, remote monitoring of the home and its contents is a likely winner. Probably many parents would be willing to spend some money to monitor their sleeping babies on their PDAs when they are eating out, even with a rented teenager in the house. While one can imagine a separate network for each application area, integrating all of them into a single network is probably a better idea. 

Home networking has some fundamentally different properties than other network types. First, the network and devices have to be easy to install. The author has installed numerous pieces of hardware and software on various computers over the years, with mixed results. A series of phone calls to the vendor's helpdesk typically resulted in answers like (1) Read the manual, (2) Reboot the computer, (3) Remove all hardware and software except ours and try again, (4) Download the newest driver from our Web site, and if all else fails, (5) Reformat the hard disk and then reinstall Windows from the CD-ROM. Telling the purchaser of an Internet refrigerator to download and install a new version of the refrigerator's operating system is not going to lead to happy customers. Computer users are accustomed to putting up with products that do not work; the car-, television-, and refrigerator-buying public is far less tolerant. They expect products to work for 100% from the word go. 

26




Second, the network and devices have to be foolproof in operation. Air conditioners used to have one knob with four settings: OFF, LOW, MEDIUM, and HIGH. Now they have 30-page manuals. Once they are networked, expect the chapter on security alone to be 30 pages. This will be beyond the comprehension of virtually all the users. 

Third, low price is essential for success. People will not pay a $50 premium for an Internet thermostat because few people regard monitoring their home temperature from work that important. For $5 extra, it might sell, though. 

Fourth, the main application is likely to involve multimedia, so the network needs sufficient capacity. There is no market for Internet-connected televisions that show shaky movies at 320 x 240 pixel resolution and 10 frames/sec. Fast Ethernet, the workhorse in most offices, is not good enough for multimedia. Consequently, home networks will need better performance than that of existing office networks and at lower prices before they become mass market items. 

Fifth, it must be possible to start out with one or two devices and expand the reach of the network gradually. This means no format wars. Telling consumers to buy peripherals with IEEE 1394 (FireWire) interfaces and a few years later retracting that and saying USB 2.0 is the interface-of-the-month is going to make consumers skittish. The network interface will have to remain stable for many years; the wiring (if any) will have to remain stable for decades. 

Sixth, security and reliability will be very important. Losing a few files to an e-mail virus is one thing; having a burglar disarm your security system from his PDA and then plunder your house is something quite different. 

An interesting question is whether home networks will be wired or wireless. Most homes already have six networks installed: electricity, telephone, cable television, water, gas, and sewer. Adding a seventh one during construction is not difficult, but retrofitting existing houses is expensive. Cost favors wireless networking, but security favors wired networking. The problem with wireless is that the radio waves they use are quite good at going through fences. Not everyone is overjoyed at the thought of having the neighbors piggybacking on their Internet connection and reading their e-mail on its way to the printer. In 

Chap. 8

 we will study how encryption can be used to provide security, but in the context of a home network, security has to be foolproof, even with inexperienced users. This is easier said than done, even with highly sophisticated users. 

In short, home networking offers many opportunities and challenges. Most of them relate to the need to be easy to manage, dependable, and secure, especially in the hands of nontechnical users, while at the same time delivering high performance at low cost. 

1.2.6 Internetworks 

Many networks exist in the world, often with different hardware and software. People connected to one network often want to communicate with people attached to a different one. The fulfillment of this desire requires that different, and frequently incompatible networks, be connected, sometimes by means of machines called 

gateways

 to make the connection and provide the necessary translation, both in terms of hardware and software. A collection of interconnected networks is called an 

internetwork

 or 

internet

. These terms will be used in a generic sense, in contrast to the worldwide Internet (which is one specific internet), which we will always capitalize. 

A common form of internet is a collection of LANs connected by a WAN. In fact, if we were to replace the label ''subnet'' in 

Fig. 1-9

 by ''WAN,'' nothing else in the figure would have to change. The only real technical distinction between a subnet and a WAN in this case is whether hosts are present. If the system within the gray area contains only routers, it is a subnet; if it contains both routers and hosts, it is a WAN. The real differences relate to ownership and use. 

Subnets, networks, and internetworks are often confused. Subnet makes the most sense in the context of a wide area network, where it refers to the collection of routers and communication lines owned by the network operator. As an analogy, the telephone system consists of telephone switching offices connected to one another by high-speed lines, and to houses and businesses by low-speed lines. These lines and equipment, owned and managed by the telephone company, form the subnet of the telephone system. The telephones themselves (the 

27




hosts in this analogy) are not part of the subnet. The combination of a subnet and its hosts forms a network. In the case of a LAN, the cable and the hosts form the network. There really is no subnet. 

An internetwork is formed when distinct networks are interconnected. In our view, connecting a LAN and a WAN or connecting two LANs forms an internetwork, but there is little agreement in the industry over terminology in this area. One rule of thumb is that if different organizations paid to construct different parts of the network and each maintains its part, we have an internetwork rather than a single network. Also, if the underlying technology is different in different parts (e.g., broadcast versus point-to-point), we probably have two networks. 

1.3 Network Software 

The first computer networks were designed with the hardware as the main concern and the software as an afterthought. This strategy no longer works. Network software is now highly structured. In the following sections we examine the software structuring technique in some detail. The method described here forms the keystone of the entire book and will occur repeatedly later on. 

1.3.1 Protocol Hierarchies 

To reduce their design complexity, most networks are organized as a stack of 

layers

 or 

levels

, each one built upon the one below it. The number of layers, the name of each layer, the contents of each layer, and the function of each layer differ from network to network. The purpose of each layer is to offer certain services to the higher layers, shielding those layers from the details of how the offered services are actually implemented. In a sense, each layer is a kind of virtual machine, offering certain services to the layer above it. 

This concept is actually a familiar one and used throughout computer science, where it is variously known as information hiding, abstract data types, data encapsulation, and object-oriented programming. The fundamental idea is that a particular piece of software (or hardware) provides a service to its users but keeps the details of its internal state and algorithms hidden from them. 

Layer 

n

 on one machine carries on a conversation with layer 

n

 on another machine. The rules and conventions used in this conversation are collectively known as the layer 

n

 protocol. Basically, a 

protocol

 is an agreement between the communicating parties on how communication is to proceed. As an analogy, when a woman is introduced to a man, she may choose to stick out her hand. He, in turn, may decide either to shake it or kiss it, depending, for example, on whether she is an American lawyer at a business meeting or a European princess at a formal ball. Violating the protocol will make communication more difficult, if not completely impossible. 

A five-layer network is illustrated in 

Fig. 1-13

. The entities comprising the corresponding layers on different machines are called 

peers

. The peers may be processes, hardware devices, or even human beings. In other words, it is the peers that communicate by using the protocol. 

Figure 1-13. Layers, protocols, and interfaces. 

28




 

In reality, no data are directly transferred from layer 

n

 on one machine to layer 

n

 on another machine. Instead, each layer passes data and control information to the layer immediately below it, until the lowest layer is reached. Below layer 1 is the 

physical medium

 through which actual communication occurs. In 

Fig. 1-13

, virtual communication is shown by dotted lines and physical communication by solid lines. 

Between each pair of adjacent layers is an 

interface

. The interface defines which primitive operations and services the lower layer makes available to the upper one. When network designers decide how many layers to include in a network and what each one should do, one of the most important considerations is defining clean interfaces between the layers. Doing so, in turn, requires that each layer perform a specific collection of well-understood functions. In addition to minimizing the amount of information that must be passed between layers, clear-cut interfaces also make it simpler to replace the implementation of one layer with a completely different implementation (e.g., all the telephone lines are replaced by satellite channels) because all that is required of the new implementation is that it offer exactly the same set of services to its upstairs neighbor as the old implementation did. In fact, it is common that different hosts use different implementations. 

A set of layers and protocols is called a 

network architecture

. The specification of an architecture must contain enough information to allow an implementer to write the program or build the hardware for each layer so that it will correctly obey the appropriate protocol. Neither the details of the implementation nor the specification of the interfaces is part of the architecture because these are hidden away inside the machines and not visible from the outside. It is not even necessary that the interfaces on all machines in a network be the same, provided that each machine can correctly use all the protocols. A list of protocols used by a certain system, one protocol per layer, is called a 

protocol stack

. The subjects of network architectures, protocol stacks, and the protocols themselves are the principal topics of this book. 

An analogy may help explain the idea of multilayer communication. Imagine two philosophers (peer processes in layer 3), one of whom speaks Urdu and English and one of whom speaks Chinese and French. Since they have no common language, they each engage a translator (peer processes at layer 2), each of whom in turn contacts a secretary (peer processes in layer 1). Philosopher 1 wishes to convey his affection for 

oryctolagus cuniculus

 to his peer. To do so, he passes a message (in English) across the 2/3 interface to his translator, saying ''I like rabbits,'' as illustrated in 

Fig. 1-14

. The translators have agreed on a neutral language known to both of them, Dutch, so the message is converted to ''Ik vind konijnen leuk.'' The choice of language is the layer 2 protocol and is up to the layer 2 peer processes. 

Figure 1-14. The philosopher-translator-secretary architecture. 

29




 

The translator then gives the message to a secretary for transmission, by, for example, fax (the layer 1 protocol). When the message arrives, it is translated into French and passed across the 2/3 interface to philosopher 2. Note that each protocol is completely independent of the other ones as long as the interfaces are not changed. The translators can switch from Dutch to say, Finnish, at will, provided that they both agree, and neither changes his interface with either layer 1 or layer 3. Similarly, the secretaries can switch from fax to e-mail or telephone without disturbing (or even informing) the other layers. Each process may add some information intended only for its peer. This information is not passed upward to the layer above. 

Now consider a more technical example: how to provide communication to the top layer of the five-layer network in 

Fig. 1-15

. A message, 

M

, is produced by an application process running in layer 5 and given to layer 4 for transmission. Layer 4 puts a 

header

 in front of the message to identify the message and passes the result to layer 3. The header includes control information, such as sequence numbers, to allow layer 4 on the destination machine to deliver messages in the right order if the lower layers do not maintain sequence. In some layers, headers can also contain sizes, times, and other control fields. 

Figure 1-15. Example information flow supporting virtual communication in layer 5. 

30




 

In many networks, there is no limit to the size of messages transmitted in the layer 4 protocol, but there is nearly always a limit imposed by the layer 3 protocol. Consequently, layer 3 must break up the incoming messages into smaller units, packets, prepending a layer 3 header to each packet. In this example, 

M

 is split into two parts, 

M

1

 and 

M

2

. 

Layer 3 decides which of the outgoing lines to use and passes the packets to layer 2. Layer 2 adds not only a header to each piece, but also a trailer, and gives the resulting unit to layer 1 for physical transmission. At the receiving machine the message moves upward, from layer to layer, with headers being stripped off as it progresses. None of the headers for layers below 

n

 are passed up to layer 

n.

The important thing to understand about 

Fig. 1-15

 is the relation between the virtual and actual communication and the difference between protocols and interfaces. The peer processes in layer 4, for example, conceptually think of their communication as being ''horizontal,'' using the layer 4 protocol. Each one is likely to have a procedure called something like 

SendToOtherSide

 and 

GetFromOtherSide

, even though these procedures actually communicate with lower layers across the 3/4 interface, not with the other side. 

The peer process abstraction is crucial to all network design. Using it, the unmanageable task of designing the complete network can be broken into several smaller, manageable design problems, namely, the design of the individual layers. 

Although 

Sec. 1.3

 is called ''

Network 1.3

,'' it is worth pointing out that the lower layers of a protocol hierarchy are frequently implemented in hardware or firmware. Nevertheless, complex protocol algorithms are involved, even if they are embedded (in whole or in part) in hardware. 

1.3.2 Design Issues for the Layers 

Some of the key design issues that occur in computer networks are present in several layers. Below, we will briefly mention some of the more important ones. 

Every layer needs a mechanism for identifying senders and receivers. Since a network normally has many computers, some of which have multiple processes, a means is needed for a process on one machine to specify with whom it wants to talk. As a consequence of having multiple destinations, some form of 

addressing

 is needed in order to specify a specific destination. 

Another set of design decisions concerns the rules for data transfer. In some systems, data only travel in one direction; in others, data can go both ways. The protocol must also determine how many logical channels the 

31




connection corresponds to and what their priorities are. Many networks provide at least two logical channels per connection, one for normal data and one for urgent data. 

Error control

 is an important issue because physical communication circuits are not perfect. Many error-detecting and error-correcting codes are known, but both ends of the connection must agree on which one is being used. In addition, the receiver must have some way of telling the sender which messages have been correctly received and which have not. 

Not all communication channels preserve the order of messages sent on them. To deal with a possible loss of sequencing, the protocol must make explicit provision for the receiver to allow the pieces to be reassembled properly. An obvious solution is to number the pieces, but this solution still leaves open the question of what should be done with pieces that arrive out of order. 

An issue that occurs at every level is how to keep a fast sender from swamping a slow receiver with data. Various solutions have been proposed and will be discussed later. Some of them involve some kind of feedback from the receiver to the sender, either directly or indirectly, about the receiver's current situation. Others limit the sender to an agreed-on transmission rate. This subject is called 

flow control

. 

Another problem that must be solved at several levels is the inability of all processes to accept arbitrarily long messages. This property leads to mechanisms for disassembling, transmitting, and then reassembling messages. A related issue is the problem of what to do when processes insist on transmitting data in units that are so small that sending each one separately is inefficient. Here the solution is to gather several small messages heading toward a common destination into a single large message and dismember the large message at the other side. 

When it is inconvenient or expensive to set up a separate connection for each pair of communicating processes, the underlying layer may decide to use the same connection for multiple, unrelated conversations. As long as this 

multiplexing

 and 

demultiplexing

 is done transparently, it can be used by any layer. Multiplexing is needed in the physical layer, for example, where all the traffic for all connections has to be sent over at most a few physical circuits. 

When there are multiple paths between source and destination, a route must be chosen. Sometimes this decision must be split over two or more layers. For example, to send data from London to Rome, a high-level decision might have to be made to transit France or Germany based on their respective privacy laws. Then a low-level decision might have to made to select one of the available circuits based on the current traffic load. This topic is called 

routing

. 

1.3.3 Connection-Oriented and Connectionless Services 

Layers can offer two different types of service to the layers above them: connection-oriented and connectionless. In this section we will look at these two types and examine the differences between them. 

Connection-oriented service

 is modeled after the telephone system. To talk to someone, you pick up the phone, dial the number, talk, and then hang up. Similarly, to use a connection-oriented network service, the service user first establishes a connection, uses the connection, and then releases the connection. The essential aspect of a connection is that it acts like a tube: the sender pushes objects (bits) in at one end, and the receiver takes them out at the other end. In most cases the order is preserved so that the bits arrive in the order they were sent. 

In some cases when a connection is established, the sender, receiver, and subnet conduct a 

negotiation

 about parameters to be used, such as maximum message size, quality of service required, and other issues. Typically, one side makes a proposal and the other side can accept it, reject it, or make a counterproposal. 

In contrast, 

connectionless service

 is modeled after the postal system. Each message (letter) carries the full destination address, and each one is routed through the system independent of all the others. Normally, when two messages are sent to the same destination, the first one sent will be the first one to arrive. However, it is possible that the first one sent can be delayed so that the second one arrives first. 

32




Each service can be characterized by a 

quality of service

. Some services are reliable in the sense that they never lose data. Usually, a reliable service is implemented by having the receiver acknowledge the receipt of each message so the sender is sure that it arrived. The acknowledgement process introduces overhead and delays, which are often worth it but are sometimes undesirable. 

A typical situation in which a reliable connection-oriented service is appropriate is file transfer. The owner of the file wants to be sure that all the bits arrive correctly and in the same order they were sent. Very few file transfer customers would prefer a service that occasionally scrambles or loses a few bits, even if it is much faster. 

Reliable connection-oriented service has two minor variations: message sequences and byte streams. In the former variant, the message boundaries are preserved. When two 1024-byte messages are sent, they arrive as two distinct 1024-byte messages, never as one 2048-byte message. In the latter, the connection is simply a stream of bytes, with no message boundaries. When 2048 bytes arrive at the receiver, there is no way to tell if they were sent as one 2048-byte message, two 1024-byte messages, or 2048 1-byte messages. If the pages of a book are sent over a network to a phototypesetter as separate messages, it might be important to preserve the message boundaries. On the other hand, when a user logs into a remote server, a byte stream from the user's computer to the server is all that is needed. Message boundaries are not relevant. 

As mentioned above, for some applications, the transit delays introduced by acknowledgements are unacceptable. One such application is digitized voice traffic. It is preferable for telephone users to hear a bit of noise on the line from time to time than to experience a delay waiting for acknowledgements. Similarly, when transmitting a video conference, having a few pixels wrong is no problem, but having the image jerk along as the flow stops to correct errors is irritating. 

Not all applications require connections. For example, as electronic mail becomes more common, electronic junk is becoming more common too. The electronic junk-mail sender probably does not want to go to the trouble of setting up and later tearing down a connection just to send one item. Nor is 100 percent reliable delivery essential, especially if it costs more. All that is needed is a way to send a single message that has a high probability of arrival, but no guarantee. Unreliable (meaning not acknowledged) connectionless service is often called 

datagram service

, in analogy with telegram service, which also does not return an acknowledgement to the sender. 

In other situations, the convenience of not having to establish a connection to send one short message is desired, but reliability is essential. The 

acknowledged datagram service

 can be provided for these applications. It is like sending a registered letter and requesting a return receipt. When the receipt comes back, the sender is absolutely sure that the letter was delivered to the intended party and not lost along the way. 

Still another service is the 

request-reply service

. In this service the sender transmits a single datagram containing a request; the reply contains the answer. For example, a query to the local library asking where Uighur is spoken falls into this category. Request-reply is commonly used to implement communication in the client-server model: the client issues a request and the server responds to it. 

Figure 1-16

 summarizes the types of services discussed above. 

Figure 1-16. Six different types of service. 

 

33




The concept of using unreliable communication may be confusing at first. After all, why would anyone actually prefer unreliable communication to reliable communication? First of all, reliable communication (in our sense, that is, acknowledged) may not be available. For example, Ethernet does not provide reliable communication. Packets can occasionally be damaged in transit. It is up to higher protocol levels to deal with this problem. Second, the delays inherent in providing a reliable service may be unacceptable, especially in real-time applications such as multimedia. For these reasons, both reliable and unreliable communication coexist. 

1.3.4 Service Primitives 

A service is formally specified by a set of 

primitives

 (operations) available to a user process to access the service. These primitives tell the service to perform some action or report on an action taken by a peer entity. If the protocol stack is located in the operating system, as it often is, the primitives are normally system calls. These calls cause a trap to kernel mode, which then turns control of the machine over to the operating system to send the necessary packets. 

The set of primitives available depends on the nature of the service being provided. The primitives for connection-oriented service are different from those of connectionless service. As a minimal example of the service primitives that might be provided to implement a reliable byte stream in a client-server environment, consider the primitives listed in 

Fig. 1-17

. 

Figure 1-17. Five service primitives for implementing a simple connection-oriented service. 

 

These primitives might be used as follows. First, the server executes LISTEN to indicate that it is prepared to accept incoming connections. A common way to implement LISTEN is to make it a blocking system call. After executing the primitive, the server process is blocked until a request for connection appears. 

Next, the client process executes CONNECT to establish a connection with the server. The CONNECT call needs to specify who to connect to, so it might have a parameter giving the server's address. The operating system then typically sends a packet to the peer asking it to connect, as shown by (1) in 

Fig. 1-18

. The client process is suspended until there is a response. When the packet arrives at the server, it is processed by the operating system there. When the system sees that the packet is requesting a connection, it checks to see if there is a listener. If so, it does two things: unblocks the listener and sends back an acknowledgement (2). The arrival of this acknowledgement then releases the client. At this point the client and server are both running and they have a connection established. It is important to note that the acknowledgement (2) is generated by the protocol code itself, not in response to a user-level primitive. If a connection request arrives and there is no listener, the result is undefined. In some systems the packet may be queued for a short time in anticipation of a LISTEN. 

Figure 1-18. Packets sent in a simple client-server interaction on a connection-oriented network. 

 

34




The obvious analogy between this protocol and real life is a customer (client) calling a company's customer service manager. The service manager starts out by being near the telephone in case it rings. Then the client places the call. When the manager picks up the phone, the connection is established. 

The next step is for the server to execute RECEIVE to prepare to accept the first request. Normally, the server does this immediately upon being released from the LISTEN, before the acknowledgement can get back to the client. The RECEIVE call blocks the server. 

Then the client executes SEND to transmit its request (3) followed by the execution of RECEIVE to get the reply. 

The arrival of the request packet at the server machine unblocks the server process so it can process the request. After it has done the work, it uses SEND to return the answer to the client (4). The arrival of this packet unblocks the client, which can now inspect the answer. If the client has additional requests, it can make them now. If it is done, it can use DISCONNECT to terminate the connection. Usually, an initial DISCONNECT is a blocking call, suspending the client and sending a packet to the server saying that the connection is no longer needed (5). When the server gets the packet, it also issues a DISCONNECT of its own, acknowledging the client and releasing the connection. When the server's packet (6) gets back to the client machine, the client process is released and the connection is broken. In a nutshell, this is how connection-oriented communication works. 

Of course, life is not so simple. Many things can go wrong here. The timing can be wrong (e.g., the CONNECT is done before the LISTEN), packets can get lost, and much more. We will look at these issues in great detail later, but for the moment, 

Fig. 1-18

 briefly summarizes how client-server communication might work over a connection-oriented network. 

Given that six packets are required to complete this protocol, one might wonder why a connectionless protocol is not used instead. The answer is that in a perfect world it could be, in which case only two packets would be needed: one for the request and one for the reply. However, in the face of large messages in either direction (e.g., a megabyte file), transmission errors, and lost packets, the situation changes. If the reply consisted of hundreds of packets, some of which could be lost during transmission, how would the client know if some pieces were missing? How would the client know whether the last packet actually received was really the last packet sent? Suppose that the client wanted a second file. How could it tell packet 1 from the second file from a lost packet 1 from the first file that suddenly found its way to the client? In short, in the real world, a simple request-reply protocol over an unreliable network is often inadequate. In 

Chap. 3

 we will study a variety of protocols in detail that overcome these and other problems. For the moment, suffice it to say that having a reliable, ordered byte stream between processes is sometimes very convenient. 

1.3.5 The Relationship of Services to Protocols 

Services and protocols are distinct concepts, although they are frequently confused. This distinction is so important, however, that we emphasize it again here. A 

service

 is a set of primitives (operations) that a layer provides to the layer above it. The service defines what operations the layer is prepared to perform on behalf of its users, but it says nothing at all about how these operations are implemented. A service relates to an interface between two layers, with the lower layer being the service provider and the upper layer being the service user. 

A 

protocol

, in contrast, is a set of rules governing the format and meaning of the packets, or messages that are exchanged by the peer entities within a layer. Entities use protocols to implement their service definitions. They are free to change their protocols at will, provided they do not change the service visible to their users. In this way, the service and the protocol are completely decoupled. 

In other words, services relate to the interfaces between layers, as illustrated in 

Fig. 1-19

. In contrast, protocols relate to the packets sent between peer entities on different machines. It is important not to confuse the two concepts. 

Figure 1-19. The relationship between a service and a protocol. 

35




 

An analogy with programming languages is worth making. A service is like an abstract data type or an object in an object-oriented language. It defines operations that can be performed on an object but does not specify how these operations are implemented. A protocol relates to the 

implementation

 of the service and as such is not visible to the user of the service. 

Many older protocols did not distinguish the service from the protocol. In effect, a typical layer might have had a service primitive SEND PACKET with the user providing a pointer to a fully assembled packet. This arrangement meant that all changes to the protocol were immediately visible to the users. Most network designers now regard such a design as a serious blunder. 

36




1.4 Reference Models 

Now that we have discussed layered networks in the abstract, it is time to look at some examples. In the nexttwo sections we will discuss two important network architectures, the OSI reference model and the TCP/IPreference model. Although the 

protocols

 associated with the OSI model are rarely used any more, the 

modelitself is actually quite general and still valid, and the features discussed at each layer are still very important. TheTCP/IP model has the opposite properties: the model itself is not of much use but the protocols are widely used. For this reason we will look at both of them in detail. Also, sometimes you can learn more from failures than fromsuccesses. 

1.4.1 The OSI Reference Model 

The OSI model (minus the physical medium) is shown in 

Fig. 1-20

. This model is based on a proposal developed by the International Standards Organization (ISO) as a first step toward international standardization of the protocols used in the various layers (Day and Zimmermann, 1983). It was revised in 1995 (Day, 1995). Themodel is called the 

ISO OSI (Open Systems Interconnection) Reference Model

because it deals with connecting open systems—that is, systems that are open for communication with other systems. We will just call it the OSImodel for short. 

Figure 1-20. The OSI reference model. 

 

The OSI model has seven layers. The principles that were applied to arrive at the seven layers can be brieflysummarized as follows: 

1. A layer should be created where a different abstraction is needed. 

2. Each layer should perform a well-defined function. 

3. The function of each layer should be chosen with an eye toward defining internationally standardized 

37




protocols. 

4. The layer boundaries should be chosen to minimize the information flow across the interfaces. 

5. The number of layers should be large enough that distinct functions need not be thrown together in the same layer out of necessity and small enough that the architecture does not become unwieldy. 

Below we will discuss each layer of the model in turn, starting at the bottom layer. Note that the OSI model itsel

f

is not a network architecture because it does not specify the exact services and protocols to be used in eachlayer. It just tells what each layer should do. However, ISO has also produced standards for all the layers,although these are not part of the reference model itself. Each one has been published as a separate international standard. 

The Physical Layer 

The 

physical layer

 is concerned with transmitting raw bits over a communication channel. The design issueshave to do with making sure that when one side sends a 1 bit, it is received by the other side as a 1 bit, not as a0 bit. Typical questions here are how many volts should be used to represent a 1 and how many for a 0, howmany nanoseconds a bit lasts, whether transmission may proceed simultaneously in both directions, how theinitial connection is established and how it is torn down when both sides are finished, and how many pins thenetwork connector has and what each pin is used for. The design issues here largely deal with mechanical,electrical, and timing interfaces, and the physical transmission medium, which lies below the physical layer. 

The Data Link Layer 

The main task of the 

data link layer

 is to transform a raw transmission facility into a line that appears free o

f

undetected transmission errors to the network layer. It accomplishes this task by having the sender break up the input data into 

data frames

 (typically a few hundred or a few thousand bytes) and transmit the framessequentially. If the service is reliable, the receiver confirms correct receipt of each frame by sending back an 

acknowledgement frame

. 

A

nother issue that arises in the data link layer (and most of the higher layers as well) is how to keep a fasttransmitter from drowning a slow receiver in data. Some traffic regulation mechanism is often needed to let thetransmitter know how much buffer space the receiver has at the moment. Frequently, this flow regulation and theerror handling are integrated. 

Broadcast networks have an additional issue in the data link layer: how to control access to the shared channel.A special sublayer of the data link layer, the medium access control sublayer, deals with this problem. 

The Network Layer 

The 

network layer

 controls the operation of the subnet. A key design issue is determining how packets arerouted from source to destination. Routes can be based on static tables that are ''wired into'' the network andrarely changed. They can also be determined at the start of each conversation, for example, a terminal session(e.g., a login to a remote machine). Finally, they can be highly dynamic, being determined anew for each packet, to reflect the current network load. 

If too many packets are present in the subnet at the same time, they will get in one another's way, formingbottlenecks. The control of such congestion also belongs to the network layer. More generally, the quality o

f

 service provided (delay, transit time, jitter, etc.) is also a network layer issue. 

When a packet has to travel from one network to another to get to its destination, many problems can arise. Theaddressing used by the second network may be different from the first one. The second one may not accept thepacket at all because it is too large. The protocols may differ, and so on. It is up to the network layer to overcomeall these problems to allow heterogeneous networks to be interconnected. 

In broadcast networks, the routing problem is simple, so the network layer is often thin or even nonexistent. 

The Transport Layer 

38




The basic function of the 

transport layer

 is to accept data from above, split it up into smaller units if need be, pass these to the network layer, and ensure that the pieces all arrive correctly at the other end. Furthermore, allthis must be done efficiently and in a way that isolates the upper layers from the inevitable changes in thehardware technology. 

The transport layer also determines what type of service to provide to the session layer, and, ultimately, to theusers of the network. The most popular type of transport connection is an error-free point-to-point channel that delivers messages or bytes in the order in which they were sent. However, other possible kinds of transpor

t

service are the transporting of isolated messages, with no guarantee about the order of delivery, and thebroadcasting of messages to multiple destinations. The typeof service is determined when the connection is established. (As an aside, an error-free channel is impossible to achieve; what people really mean by this term isthat the error rate is low enough to ignore in practice.) 

The transport layer is a true end-to-end layer, all the way from the source to the destination. In other words, aprogram on the source machine carries on a conversation with a similar program on the destination machine,using the message headers and control messages. In the lower layers, the protocols are between each machine and its immediate neighbors, and not between the ultimate source and destination machines, which may beseparated by many routers. The difference between layers 1 through 3, which are chained, and layers 4 through7, which are end-to-end, is illustrated in 

Fig. 1-20

. 

The Session Layer 

The session layer allows users on different machines to establish 

sessions

between them. Sessions offe

r

 various services, including 

dialog control

 (keeping track of whose turn it is to transmit), 

token management(preventing two parties from attempting the same critical operation at the same time), and 

synchronization(checkpointing long transmissions to allow them to continue from where they were after a crash). 

The Presentation Layer 

Unlike lower layers, which are mostly concerned with moving bits around, the 

presentation layer

is concerned with the syntax and semantics of the information transmitted. In order to make it possible for computers withdifferent data representations to communicate, the data structures to be exchanged can be defined in anabstract way, along with a standard encoding to be used ''on the wire.'' The presentation layer manages these abstract data structures and allows higher-level data structures (e.g., banking records), to be defined andexchanged. 

The Application Layer 

The 

application layer

 contains a variety of protocols that are commonly needed by users. One widely-used application protocol is 

HTTP

 (

HyperText Transfer Protocol

), which is the basis for the World Wide Web. When a browser wants a Web page, it sends the name of the page it wants to the server using HTTP. The server thensends the page back. Other application protocols are used for file transfer, electronic mail, and network news. 

1.4.2 The TCP/IP Reference Model 

Let us now turn from the OSI reference model to the reference model used in the grandparent of all wide areacomputer networks, the ARPANET, and its successor, the worldwide Internet. Although we will give a brie

f

history of the ARPANET later, it is useful to mention a few key aspects of it now. The ARPANET was a researchnetwork sponsored by the DoD (U.S. Department of Defense). It eventually connected hundreds of universitiesand government installations, using leased telephone lines. When satellite and radio networks were added later,the existing protocols had trouble interworking with them, so a new reference architecture was needed. Thus, the ability to connect multiple networks in a seamless way was one of the major design goals from the verybeginning. This architecture later became known as the 

TCP/IP Reference Model

, after its two primary protocols. It was first defined in (Cerf and Kahn, 1974). A later perspective is given in (Leiner et al., 1985). The designphilosophy behind the model is discussed in (Clark, 1988). 

Given the DoD's worry that some of its precious hosts, routers, and internetwork gateways might get blown to 

39




pieces at a moment's notice, another major goal was that the network be able to survive loss of subne

t

hardware, with existing conversations not being broken off. In other words, DoD wanted connections to remainintact as long as the source and destination machines were functioning, even if some of the machines o

r

transmission lines in between were suddenly put out of operation. Furthermore, a flexible architecture wasneeded since applications with divergent requirements were envisioned, ranging from transferring files to real-time speech transmission. 

The Internet Layer 

All these requirements led to the choice of a packet-switching network based on a connectionless internetworklayer. This layer, called the 

internet layer

, is the linchpin that holds the whole architecture together. Its job is to permit hosts to inject packets into any network and have them travel independently to the destination (potentiallyon a different network). They may even arrive in a different order than they were sent, in which case it is the job of higher layers to rearrange them, if in-order delivery is desired. Note that ''internet'' is used here in a genericsense, even though this layer is present in the Internet. 

The analogy here is with the (snail) mail system. A person can drop a sequence of international letters into amail box in one country, and with a little luck, most of them will be delivered to the correct address in thedestination country. Probably the letters will travel through one or more international mail gateways along the way, but this is transparent to the users. Furthermore, that each country (i.e., each network) has its own stamps,preferred envelope sizes, and delivery rules is hidden from the users. 

The internet layer defines an official packet format and protocol called 

IP

 (

Internet Protocol

). The job of the internet layer is to deliver IP packets where they are supposed to go. Packet routing is clearly the major issuehere, as is avoiding congestion. For these reasons, it is reasonable to say that the TCP/IP internet layer is similar in functionality to the OSI network layer. 

Figure 1-21

 shows this correspondence. 

Figure 1-21. The TCP/IP reference model. 

 

The Transport Layer 

The layer above the internet layer in the TCP/IP model is now usually called the 

transport layer

. It is designed to allow peer entities on the source and destination hosts to carry on a conversation, just as in the OSI transportlayer. Two end-to-end transport protocols have been defined here. The first one, 

TCP

 (

Transmission Control Protocol

), is a reliable connection-oriented protocol that allows abyte stream originating on one machine to be delivered without error on any other machine in the internet. It fragments the incoming byte stream into discretemessages and passes each one on to the internet layer. At the destination, the receiving TCP process reassembles the received messages into the output stream. TCP also handles flow control to make sure a fastsender cannot swamp a slow receiver with more messages than it can handle. 

The second protocol in this layer, 

UDP

 (

User Datagram Protocol

), is an unreliable, connectionless protocol fo

r

 applications that do not want TCP's sequencing or flow control and wish to provide their own. It is also widel

y

used for one-shot, client-server-type request-reply queries and applications in which prompt deliveryis more important than accurate delivery, such as transmitting speech or video. The relation of IP, TCP, and UDP is

40




shown in 

Fig. 1-22

. Since the model was developed, IP has been implemented on many other networks. 

Figure 1-22. Protocols and networks in the TCP/IP model initially. 

 

The Application Layer 

The TCP/IP model does not have session or presentation layers. No need for them was perceived, so they werenot included. Experience with the OSI model has proven this view correct: they are of little use to mostapplications. 

On top of the transport layer is the 

application layer

. It contains all the higher-level protocols. The early ones included virtual terminal (TELNET), file transfer (FTP), and electronic mail (SMTP), as shown in 

Fig. 1-22

. The virtual terminal protocol allows a user on one machine to log onto a distant machine and work there. The filetransfer protocol provides a way to move data efficiently from one machine to another. Electronic mail wasoriginally just a kind of file transfer, but later a specialized protocol (SMTP) was developed for it. Many othe

r

protocols have been added to these over the years: the Domain Name System (DNS) for mapping host namesonto their network addresses, NNTP, the protocol for moving USENET news articles around, and HTTP, theprotocol for fetching pages on the World Wide Web, and many others. 

The Host-to-Network Layer 

Below the internet layer is a great void. The TCP/IP reference model does not really say much about wha

t

 happens here, except to point out that the host has to connect to the network using some protocol so it can sendIP packets to it. This protocol is not defined and varies from host to host and network to network. Books andpapers about the TCP/IP model rarely discuss it. 

1.4.3 A Comparison of the OSI and TCP/IP Reference Models 

The OSI and TCP/IP reference models have much in common. Both are based on the concept of a stack o

f

independent protocols. Also, the functionality of the layers is roughly similar. For example, in both models the layers up through and including the transport layer are there to provide an end-to-end, network-independent transport service to processes wishing to communicate. These layers form the transport provider. Again in both models, the layers above transport are application-oriented users of the transport service. 

Despite these fundamental similarities, the two models also have many differences. In this section we will focuson the key differences between the two referencemodels. It is important to note that we are comparing the

reference models

 here, not the corresponding 

protocol stacks

. The protocols themselves will be discussed later. For an entire book comparing and contrasting TCP/IP and OSI, see (Piscitello and Chapin, 1993). 

Three concepts are central to the OSI model: 

1. Services. 

2. Interfaces. 

3. Protocols. 

41




Probably the biggest contribution of the OSI model is to make the distinction between these three conceptsexplicit. Each layer performs some services for the layer above it. The 

service

definition tells what the layer does, not how entities above it access it or how the layer works. It defines the layer's semantics. 

A layer's 

interface

 tells the processes above it how to access it. It specifies what the parameters are and wha

t

 results to expect. It, too, says nothing about how the layer works inside. 

Finally, the peer 

protocols

 used in a layer are the layer's own business. It can use any protocols it wants to, aslong as it gets the job done (i.e., provides the offered services). It can also change them at will without affectingsoftware in higher layers. 

These ideas fit very nicely with modern ideas about object-oriented programming. An object, like a layer, has a set of methods (operations) that processes outside the object can invoke. The semantics of these methods define the set of services that the object offers. The methods' parameters and results form the object's interface.The code internal to the object is its protocol and is not visible or of any concern outside the object. 

The TCP/IP model did not originally clearly distinguish between service, interface, and protocol, although peoplehave tried to retrofit it after the fact to make it more OSI-like. For example, the only real services offered by the internet layer are SEND IP PACKET and RECEIVE IP PACKET. 

A

s a consequence, the protocols in the OSI model are better hidden than in the TCP/IP model and can bereplaced relatively easily as the technology changes. Being able to make such changes is one of the mainpurposes of having layered protocols in the first place. 

The OSI reference model was devised 

before

the corresponding protocols were invented. This ordering meansthat the model was not biased toward one particular set of protocols, a fact that made it quite general. The downside of this ordering is that the designers did not have much experience with the subject and did not have agood idea of which functionality to put in which layer. 

For example, the data link layer originally dealt only with point-to-point networks. When broadcast networks came around, a new sublayer had to be hacked into the model. When people started to build real networks usingthe OSI model and existing protocols, it was discovered that these networks did not match the required service specifications (wonder of wonders), so convergence sublayers had to be grafted onto the model to provide aplace for papering over the differences. Finally, the committee originally expected that each country would haveone network, run by the government and using the OSI protocols, so no thought was given to internetworking.To make a long story short, things did not turn out that way. 

With TCP/IP the reverse was true: the protocols came first, and the model was really just a description of theexisting protocols. There was no problem with the protocols fitting the model. They fit perfectly. The only troublewas that the 

model

 did not fit any other protocol stacks. Consequently, it was not especially useful for describingother, non-TCP/IP networks. 

Turning from philosophical matters to more specific ones, an obvious difference between the two models is thenumber of layers: the OSI model has seven layers and the TCP/IP has four layers. Both have (inter)network,transport, and application layers, but the other layers are different. 

Another difference is in the area of connectionless versus connection-oriented communication. The OSI model supports both connectionless and connection-oriented communication in the network layer, but only connection-oriented communication in the transport layer, where it counts (because the transport service is visible to theusers). The TCP/IP model has only one mode in the network layer (connectionless) but supports both modes inthe transport layer, giving the users a choice. This choice is especially important for simple request-response protocols. 

1.4.4 A Critique of the OSI Model and Protocols 

Neither the OSI model and its protocols nor the TCP/IP model and its protocols are perfect. Quite a bit o

f

criticism can be, and has been, directed at both of them. In this section and the next one, we will look at some o

f

42




these criticisms. We will begin with OSI and examine TCP/IP afterward. 

At the time the second edition of this book was published (1989), it appeared to many experts in the field that the OSI model and its protocols were going to take over the world and push everything else out of their way. This didnot happen. Why? A look back at some of the lessons may be useful. These lessons can be summarized as: 

1. Bad timing. 

2. Bad technology. 

3. Bad implementations. 

4. Bad politics. 

Bad Timing 

First let us look at reason one: bad timing. The time at which a standard is established is absolutely critical to itssuccess. David Clark of M.I.T. has a theory of standards that he calls the 

apocalypse of the two elephants

, which is illustrated in 

Fig. 1-23

. 

Figure 1-23. The apocalypse of the two elephants. 

 

This figure shows the amount of activity surrounding a new subject. When the subject is first discovered, there isa burst of research activity in the form of discussions, papers, and meetings. After a while this activity subsides, corporations discover the subject, and the billion-dollar wave of investment hits. 

It is essential that the standards be written in the trough in between the two ''elephants.'' If the standards arewritten too early, before the research is finished, the subject may still be poorly understood; the result is badstandards. If they are written too late, so many companies may have already made major investments indifferent ways of doing things that the standards are effectively ignored. If the interval between the two elephants is very short (because everyone is in a hurry to get started), the people developing the standards may ge

t

crushed. 

It now appears that the standard OSI protocols got crushed. The competing TCP/IP protocols were already inwidespread use by research universities by the time the OSI protocols appeared. While the billion-dollar wave o

f

 investment had not yet hit, the academic market was large enough that many vendors had begun cautiouslyoffering TCP/IP products. When OSI came around, they did not want to support a second protocol stack untilthey were forced to, so there were no initial offerings. With every company waiting for every other company to gofirst, no company went first and OSI never happened. 

Bad Technology 

The second reason that OSI never caught on is that both the model and the protocols are flawed. The choice o

f

seven layers was more political than technical, and two of the layers (session and presentation) are nearlyempty, whereas two other ones (data link and network) are overfull. 

The OSI model, along with the associated service definitions and protocols, is extraordinarily complex. When

43




piled up, the printed standards occupy a significant fraction of a meter of paper. They are also difficult toimplement and inefficient in operation. In this context, a riddle posed by Paul Mockapetris and cited in (Rose,1993) comes to mind: 

 

Q1:

 

What do you get when you cross a mobster with an international standard? 

A1:  

Someone who makes you an offer you can't understand. 

 

In addition to being incomprehensible, another problem with OSI is that some functions, such as addressing, flow control, and error control, reappear again and again in each layer. Saltzer et al. (1984), for example, have pointed out that to be effective, error control must be done in the highest layer, so that repeating it over and over in each of the lower layers is often unnecessary and inefficient. 

Bad Implementations 

Given the enormous complexity of the model and the protocols, it will come as no surprise that the initial implementations were huge, unwieldy, and slow. Everyone who tried them got burned. It did not take long for people to associate ''OSI'' with ''poor quality.'' Although the products improved in the course of time, the image stuck. 

In contrast, one of the first implementations of TCP/IP was part of Berkeley UNIX and was quite good (not to mention, free). People began using it quickly, which led to a large user community, which led to improvements, which led to an even larger community. Here the spiral was upward instead of downward. 

Bad Politics 

On account of the initial implementation, many people, especially in academia, thought of TCP/IP as part of UNIX, and UNIX in the 1980s in academia was not unlike parenthood (then incorrectly called motherhood) and apple pie. 

OSI, on the other hand, was widely thought to be the creature of the European telecommunication ministries, the European Community, and later the U.S. Government. This belief was only partly true, but the very idea of a bunch of government bureaucrats trying to shove a technically inferior standard down the throats of the poor researchers and programmers down in the trenches actually developing computer networks did not help much. Some people viewed this development in the same light as IBM announcing in the 1960s that PL/I was the language of the future, or DoD correcting this later by announcing that it was actually Ada. 

1.4.5 A Critique of the TCP/IP Reference Model 

The TCP/IP model and protocols have their problems too. First, the model does not clearly distinguish the concepts of service, interface, and protocol. Good software engineering practice requires differentiating between the specification and the implementation, something that OSI does very carefully, and TCP/IP does not. Consequently, the TCP/IP model is not much of a guide for designing new networks using new technologies. 

Second, the TCP/IP model is not at all general and is poorly suited to describing any protocol stack other than TCP/IP. Trying to use the TCP/IP model to describe Bluetooth, for example, is completely impossible. 

Third, the host-to-network layer is not really a layer at all in the normal sense of the term as used in the context of layered protocols. It is an interface (between the network and data link layers). The distinction between an interface and a layer is crucial, and one should not be sloppy about it. 

44




Fourth, the TCP/IP model does not distinguish (or even mention) the physical and data link layers. These are completely different. The physical layer has to do with the transmission characteristics of copper wire, fiber optics, and wireless communication. The data link layer's job is to delimit the start and end of frames and get them from one side to the other with the desired degree of reliability. A proper model should include both as separate layers. The TCP/IP model does not do this. 

Finally, although the IP and TCP protocols were carefully thought out and well implemented, many of the other protocols were ad hoc, generally produced by a couple of graduate students hacking away until they got tired. The protocol implementations were then distributed free, which resulted in their becoming widely used, deeply entrenched, and thus hard to replace. Some of them are a bit of an embarrassment now. The virtual terminal protocol, TELNET, for example, was designed for a ten-character per second mechanical Teletype terminal. It knows nothing of graphical user interfaces and mice. Nevertheless, 25 years later, it is still in widespread use. 

In summary, despite its problems, the OSI 

model

 (minus the session and presentation layers) has proven to be exceptionally useful for discussing computer networks. In contrast, the OSI 

protocols

 have not become popular. The reverse is true of TCP/IP: the 

model

 is practically nonexistent, but the 

protocols

 are widely used. Since computer scientists like to have their cake and eat it, too, in this book we will use a modified OSI model but concentrate primarily on the TCP/IP and related protocols, as well as newer ones such as 802, SONET, and Bluetooth. In effect, we will use the hybrid model of 

Fig. 1-24

 as the framework for this book. 

Figure 1-24. The hybrid reference model to be used in this book. 

 

1.5 Example Networks 

The subject of computer networking covers many different kinds of networks, large and small, well known and less well known. They have different goals, scales, and technologies. In the following sections, we will look at some examples, to get an idea of the variety one finds in the area of computer networking. 

We will start with the Internet, probably the best known network, and look at its history, evolution, and technology. Then we will consider ATM, which is often used within the core of large (telephone) networks. Technically, it is quite different from the Internet, contrasting nicely with it. Next we will introduce Ethernet, the dominant local area network. Finally, we will look at IEEE 802.11, the standard for wireless LANs. 

1.5.1 The Internet 

The Internet is not a network at all, but a vast collection of different networks that use certain common protocols and provide certain common services. It is an unusual system in that it was not planned by anyone and is not controlled by anyone. To better understand it, let us start from the beginning and see how it has developed and why. For a wonderful history of the Internet, John Naughton's (2000) book is highly recommended. It is one of those rare books that is not only fun to read, but also has 20 pages of 

ibid.

's and 

op. cit.

's for the serious historian. Some of the material below is based on this book. 

Of course, countless technical books have been written about the Internet and its protocols as well. For more information, see, for example, (Maufer, 1999). 

The ARPANET 

The story begins in the late 1950s. At the height of the Cold War, the DoD wanted a command-and-control network that could survive a nuclear war. At that time, all military communications used the public telephone network, which was considered vulnerable. The reason for this belief can be gleaned from 

Fig. 1-25(a)

. Here the 

45




black dots represent telephone switching offices, each of which was connected to thousands of telephones. These switching offices were, in turn, connected to higher-level switching offices (toll offices), to form a national hierarchy with only a small amount of redundancy. The vulnerability of the system was that the destruction of a few key toll offices could fragment the system into many isolated islands. 

Figure 1-25. (a) Structure of the telephone system. (b) Baran's proposed distributed switching system. 

 

Around 1960, the DoD awarded a contract to the RAND Corporation to find a solution. One of its employees, Paul Baran, came up with the highly distributed and fault-tolerant design of 

Fig. 1-25(b)

. Since the paths between any two switching offices were now much longer than analog signals could travel without distortion, Baran proposed using digital packet-switching technology throughout the system. Baran wrote several reports for the DoD describing his ideas in detail. Officials at the Pentagon liked the concept and asked AT&T, then the U.S. national telephone monopoly, to build a prototype. AT&T dismissed Baran's ideas out of hand. The biggest and richest corporation in the world was not about to allow some young whippersnapper tell it how to build a telephone system. They said Baran's network could not be built and the idea was killed. 

Several years went by and still the DoD did not have a better command-and-control system. To understand what happened next, we have to go back to October 1957, when the Soviet Union beat the U.S. into space with the launch of the first artificial satellite, Sputnik. When President Eisenhower tried to find out who was asleep at the switch, he was appalled to find the Army, Navy, and Air Force squabbling over the Pentagon's research budget. His immediate response was to create a single defense research organization, 

ARPA

, the 

Advanced Research Projects Agency

. ARPA had no scientists or laboratories; in fact, it had nothing more than an office and a small (by Pentagon standards) budget. It did its work by issuing grants and contracts to universities and companies whose ideas looked promising to it. 

For the first few years, ARPA tried to figure out what its mission should be, but in 1967, the attention of ARPA's then director, Larry Roberts, turned to networking. He contacted various experts to decide what to do. One of them, Wesley Clark, suggested building a packet-switched subnet, giving each host its own router, as illustrated in 

Fig. 1-10

. 

After some initial skepticism, Roberts bought the idea and presented a somewhat vague paper about it at the ACM SIGOPS Symposium on Operating System Principles held in Gatlinburg, Tennessee in late 1967 (Roberts, 1967). Much to Roberts' surprise, another paper at the conference described a similar system that had not only been designed but actually implemented under the direction of Donald Davies at the National Physical Laboratory in England. The NPL system was not a national system (it just connected several computers on the NPL campus), but it demonstrated that packet switching could be made to work. Furthermore, it cited Baran's now discarded earlier work. Roberts came away from Gatlinburg determined to build what later became known as the 

ARPANET

. 

46




The subnet would consist of minicomputers called 

IMP

s (

Interface Message Processors

) connected by 56-kbps transmission lines. For high reliability, each IMP would be connected to at least two other IMPs. The subnet was to be a datagram subnet, so if some lines and IMPs were destroyed, messages could be automatically rerouted along alternative paths. 

Each node of the network was to consist of an IMP and a host, in the same room, connected by a short wire. A host could send messages of up to 8063 bits to its IMP, which would then break these up into packets of at most 1008 bits and forward them independently toward the destination. Each packet was received in its entirety before being forwarded, so the subnet was the first electronic store-and-forward packet-switching network. 

ARPA then put out a tender for building the subnet. Twelve companies bid for it. After evaluating all the proposals, ARPA selected BBN, a consulting firm in Cambridge, Massachusetts, and in December 1968, awarded it a contract to build the subnet and write the subnet software. BBN chose to use specially modified Honeywell DDP-316 minicomputers with 12K 16-bit words of core memory as the IMPs. The IMPs did not have disks, since moving parts were considered unreliable. The IMPs were interconnected by 56-kbps lines leased from telephone companies. Although 56 kbps is now the choice of teenagers who cannot afford ADSL or cable, it was then the best money could buy. 

The software was split into two parts: subnet and host. The subnet software consisted of the IMP end of the host-IMP connection, the IMP-IMP protocol, and a source IMP to destination IMP protocol designed to improve reliability. The original ARPANET design is shown in 

Fig. 1-26

. 

Figure 1-26. The original ARPANET design. 

 

Outside the subnet, software was also needed, namely, the host end of the host-IMP connection, the host-host protocol, and the application software. It soon became clear that BBN felt that when it had accepted a message on a host-IMP wire and placed it on the host-IMP wire at the destination, its job was done. 

Roberts had a problem: the hosts needed software too. To deal with it, he convened a meeting of network researchers, mostly graduate students, at Snowbird, Utah, in the summer of 1969. The graduate students expected some network expert to explain the grand design of the network and its software to them and then to assign each of them the job of writing part of it. They were astounded when there was no network expert and no grand design. They had to figure out what to do on their own. 

Nevertheless, somehow an experimental network went on the air in December 1969 with four nodes: at UCLA, UCSB, SRI, and the University of Utah. These four were chosen because all had a large number of ARPA contracts, and all had different and completely incompatible host computers (just to make it more fun). The network grew quickly as more IMPs were delivered and installed; it soon spanned the United States. 

Figure 1-27

 shows how rapidly the ARPANET grew in the first 3 years. 

Figure 1-27. Growth of the ARPANET. (a) December 1969. (b) July 1970. (c) March 1971. (d) April 1972. (e) September 1972. 

47




 

In addition to helping the fledgling ARPANET grow, ARPA also funded research on the use of satellite networks and mobile packet radio networks. In one now famous demonstration, a truck driving around in California used the packet radio network to send messages to SRI, which were then forwarded over the ARPANET to the East Coast, where they were shipped to University College in London over the satellite network. This allowed a researcher in the truck to use a computer in London while driving around in California. 

This experiment also demonstrated that the existing ARPANET protocols were not suitable for running over multiple networks. This observation led to more research on protocols, culminating with the invention of the TCP/IP model and protocols (Cerf and Kahn, 1974). TCP/IP was specifically designed to handle communication over internetworks, something becoming increasingly important as more and more networks were being hooked up to the ARPANET. 

To encourage adoption of these new protocols, ARPA awarded several contracts to BBN and the University of California at Berkeley to integrate them into Berkeley UNIX. Researchers at Berkeley developed a convenient program interface to the network (sockets) and wrote many application, utility, and management programs to make networking easier. 

The timing was perfect. Many universities had just acquired a second or third VAX computer and a LAN to connect them, but they had no networking software. When 4.2BSD came along, with TCP/IP, sockets, and many network utilities, the complete package was adopted immediately. Furthermore, with TCP/IP, it was easy for the LANs to connect to the ARPANET, and many did. 

During the 1980s, additional networks, especially LANs, were connected to the ARPANET. As the scale increased, finding hosts became increasingly expensive, so 

DNS

 (

Domain Name System

) was created to organize machines into domains and map host names onto IP addresses. Since then, DNS has become a generalized, distributed database system for storing a variety of information related to naming. We will study it in detail in 

Chap. 7

. 

NSFNET 

By the late 1970s, NSF (the U.S. National Science Foundation) saw the enormous impact the ARPANET was having on university research, allowing scientists across the country to share data and collaborate on research projects. However, to get on the ARPANET, a university had to have a research contract with the DoD, which many did not have. NSF's response was to design a successor to the ARPANET that would be open to all university research groups. To have something concrete to start with, NSF decided to build a backbone network 

48




to connect its six supercomputer centers, in San Diego, Boulder, Champaign, Pittsburgh, Ithaca, and Princeton. Each supercomputer was given a little brother, consisting of an LSI-11 microcomputer called a 

fuzzball

. The fuzzballs were connected with 56-kbps leased lines and formed the subnet, the same hardware technology as the ARPANET used. The software technology was different however: the fuzzballs spoke TCP/IP right from the start, making it the first TCP/IP WAN. 

NSF also funded some (eventually about 20) regional networks that connected to the backbone to allow users at thousands of universities, research labs, libraries, and museums to access any of the supercomputers and to communicate with one another. The complete network, including the backbone and the regional networks, was called 

NSFNET

. It connected to the ARPANET through a link between an IMP and a fuzzball in the Carnegie-Mellon machine room. The first NSFNET backbone is illustrated in 

Fig. 1-28

. 

Figure 1-28. The NSFNET backbone in 1988. 

 

NSFNET was an instantaneous success and was overloaded from the word go. NSF immediately began planning its successor and awarded a contract to the Michigan-based MERIT consortium to run it. Fiber optic channels at 448 kbps were leased from MCI (since merged with WorldCom) to provide the version 2 backbone. IBM PC-RTs were used as routers. This, too, was soon overwhelmed, and by 1990, the second backbone was upgraded to 1.5 Mbps. 

As growth continued, NSF realized that the government could not continue financing networking forever. Furthermore, commercial organizations wanted to join but were forbidden by NSF's charter from using networks NSF paid for. Consequently, NSF encouraged MERIT, MCI, and IBM to form a nonprofit corporation, 

ANS

 (

Advanced Networks and Services

), as the first step along the road to commercialization. In 1990, ANS took over NSFNET and upgraded the 1.5-Mbps links to 45 Mbps to form 

ANSNET

. This network operated for 5 years and was then sold to America Online. But by then, various companies were offering commercial IP service and it was clear the government should now get out of the networking business. 

To ease the transition and make sure every regional network could communicate with every other regional network, NSF awarded contracts to four different network operators to establish a 

NAP

 (

Network Access Point

). These operators were PacBell (San Francisco), Ameritech (Chicago), MFS (Washington, D.C.), and Sprint (New York City, where for NAP purposes, Pennsauken, New Jersey counts as New York City). Every network operator that wanted to provide backbone service to the NSF regional networks had to connect to all the NAPs. 

This arrangement meant that a packet originating on any regional network had a choice of backbone carriers to get from its NAP to the destination's NAP. Consequently, the backbone carriers were forced to compete for the regional networks' business on the basis of service and price, which was the idea, of course. As a result, the concept of a single default backbone was replaced by a commercially-driven competitive infrastructure. Many people like to criticize the Federal Government for not being innovative, but in the area of networking, it was DoD and NSF that created the infrastructure that formed the basis for the Internet and then handed it over to industry to operate. 

49




During the 1990s, many other countries and regions also built national research networks, often patterned on the ARPANET and NSFNET. These included EuropaNET and EBONE in Europe, which started out with 2-Mbps lines and then upgraded to 34-Mbps lines. Eventually, the network infrastructure in Europe was handed over to industry as well. 

Internet Usage 

The number of networks, machines, and users connected to the ARPANET grew rapidly after TCP/IP became the only official protocol on January 1, 1983. When NSFNET and the ARPANET were interconnected, the growth became exponential. Many regional networks joined up, and connections were made to networks in Canada, Europe, and the Pacific. 

Sometime in the mid-1980s, people began viewing the collection of networks as an internet, and later as the Internet, although there was no official dedication with some politician breaking a bottle of champagne over a fuzzball. 

The glue that holds the Internet together is the TCP/IP reference model and TCP/IP protocol stack. TCP/IP makes universal service possible and can be compared to the adoption of standard gauge by the railroads in the 19th century or the adoption of common signaling protocols by all the telephone companies. 

What does it actually mean to be on the Internet? Our definition is that a machine is on the Internet if it runs the TCP/IP protocol stack, has an IP address, and can send IP packets to all the other machines on the Internet. The mere ability to send and receive electronic mail is not enough, since e-mail is gatewayed to many networks outside the Internet. However, the issue is clouded somewhat by the fact that millions of personal computers can call up an Internet service provider using a modem, be assigned a temporary IP address, and send IP packets to other Internet hosts. It makes sense to regard such machines as being on the Internet for as long as they are connected to the service provider's router. 

Traditionally (meaning 1970 to about 1990), the Internet and its predecessors had four main applications: 

1. E-mail. The ability to compose, send, and receive electronic mail has been around since the early days of the ARPANET and is enormously popular. Many people get dozens of messages a day and consider it their primary way of interacting with the outside world, far outdistancing the telephone and snail mail. E-mail programs are available on virtually every kind of computer these days. 

2. News. Newsgroups are specialized forums in which users with a common interest can exchange messages. Thousands of newsgroups exist, devoted to technical and nontechnical topics, including computers, science, recreation, and politics. Each newsgroup has its own etiquette, style, and customs, and woe betide anyone violating them. 

3. Remote login. Using the telnet, rlogin, or ssh programs, users anywhere on the Internet can log on to any other machine on which they have an account. 

4. File transfer. Using the FTP program, users can copy files from one machine on the Internet to another. Vast numbers of articles, databases, and other information are available this way. 

Up until the early 1990s, the Internet was largely populated by academic, government, and industrial researchers. One new application, the 

WWW

 (

World Wide Web

) changed all that and brought millions of new, nonacademic users to the net. This application, invented by CERN physicist Tim Berners-Lee, did not change any of the underlying facilities but made them easier to use. Together with the Mosaic browser, written by Marc Andreessen at the National Center for Supercomputer Applications in Urbana, Illinois, the WWW made it possible for a site to set up a number of pages of information containing text, pictures, sound, and even video, with embedded links to other pages. By clicking on a link, the user is suddenly transported to the page pointed to by that link. For example, many companies have a home page with entries pointing to other pages for product information, price lists, sales, technical support, communication with employees, stockholder information, and more. 

Numerous other kinds of pages have come into existence in a very short time, including maps, stock market tables, library card catalogs, recorded radio programs, and even a page pointing to the complete text of many books whose copyrights have expired (Mark Twain, Charles Dickens, etc.). Many people also have personal pages (home pages). 

50




Much of this growth during the 1990s was fueled by companies called 

ISP

s (

Internet Service Providers

). These are companies that offer individual users at home the ability to call up one of their machines and connect to the Internet, thus gaining access to e-mail, the WWW, and other Internet services. These companies signed up tens of millions of new users a year during the late 1990s, completely changing the character of the network from an academic and military playground to a public utility, much like the telephone system. The number of Internet users now is unknown, but is certainly hundreds of millions worldwide and will probably hit 1 billion fairly soon. 

Architecture of the Internet 

In this section we will attempt to give a brief overview of the Internet today. Due to the many mergers between telephone companies (telcos) and ISPs, the waters have become muddied and it is often hard to tell who is doing what. Consequently, this description will be of necessity somewhat simpler than reality. The big picture is shown in 

Fig. 1-29

. Let us examine this figure piece by piece now. 

Figure 1-29. Overview of the Internet. 

 

A good place to start is with a client at home. Let us assume our client calls his or her ISP over a dial-up telephone line, as shown in 

Fig. 1-29

. The modem is a card within the PC that converts the digital signals the computer produces to analog signals that can pass unhindered over the telephone system. These signals are transferred to the ISP's 

POP

 (

Point of Presence

), where they are removed from the telephone system and injected into the ISP's regional network. From this point on, the system is fully digital and packet switched. If the ISP is the local telco, the POP will probably be located in the telephone switching office where the telephone wire from the client terminates. If the ISP is not the local telco, the POP may be a few switching offices down the road. 

The ISP's regional network consists of interconnected routers in the various cities the ISP serves. If the packet is destined for a host served directly by the ISP, the packet is delivered to the host. Otherwise, it is handed over to the ISP's backbone operator. 

At the top of the food chain are the major backbone operators, companies like AT&T and Sprint. They operate large international backbone networks, with thousands of routers connected by high-bandwidth fiber optics. Large corporations and hosting services that run server farms (machines that can serve thousands of Web pages per second) often connect directly to the backbone. Backbone operators encourage this direct connection by renting space in what are called 

carrier hotels

, basically equipment racks in the same room as the router to allow short, fast connections between server farms and the backbone. 

51




If a packet given to the backbone is destined for an ISP or company served by the backbone, it is sent to the closest router and handed off there. However, many backbones, of varying sizes, exist in the world, so a packet may have to go to a competing backbone. To allow packets to hop between backbones, all the major backbones connect at the NAPs discussed earlier. Basically, a NAP is a room full of routers, at least one per backbone. A LAN in the room connects all the routers, so packets can be forwarded from any backbone to any other backbone. In addition to being interconnected at NAPs, the larger backbones have numerous direct connections between their routers, a technique known as 

private peering

. One of the many paradoxes of the Internet is that ISPs who publicly compete with one another for customers often privately cooperate to do private peering (Metz, 2001). 

This ends our quick tour of the Internet. We will have a great deal to say about the individual components and their design, algorithms, and protocols in subsequent chapters. Also worth mentioning in passing is that some companies have interconnected all their existing internal networks, often using the same technology as the Internet. These 

intranets

 are typically accessible only within the company but otherwise work the same way as the Internet. 

1.5.2 Connection-Oriented Networks: X.25, Frame Relay, and ATM 

Since the beginning of networking, a war has been going on between the people who support connectionless (i.e., datagram) subnets and the people who support connection-oriented subnets. The main proponents of the connectionless subnets come from the ARPANET/Internet community. Remember that DoD's original desire in funding and building the ARPANET was to have a network that would continue functioning even after multiple direct hits by nuclear weapons wiped out numerous routers and transmission lines. Thus, fault tolerance was high on their priority list; billing customers was not. This approach led to a connectionless design in which every packet is routed independently of every other packet. As a consequence, if some routers go down during a session, no harm is done as long as the system can reconfigure itself dynamically so that subsequent packets can find some route to the destination, even if it is different from that which previous packets used. 

The connection-oriented camp comes from the world of telephone companies. In the telephone system, a caller must dial the called party's number and wait for a connection before talking or sending data. This connection setup establishes a route through the telephone system that is maintained until the call is terminated. All words or packets follow the same route. If a line or switch on the path goes down, the call is aborted. This property is precisely what the DoD did not like about it. 

Why do the telephone companies like it then? There are two reasons: 

1. Quality of service. 

2. Billing. 

By setting up a connection in advance, the subnet can reserve resources such as buffer space and router CPU capacity. If an attempt is made to set up a call and insufficient resources are available, the call is rejected and the caller gets a kind of busy signal. In this way, once a connection has been set up, the connection will get good service. With a connectionless network, if too many packets arrive at the same router at the same moment, the router will choke and probably lose packets. The sender will eventually notice this and resend them, but the quality of service will be jerky and unsuitable for audio or video unless the network is very lightly loaded. Needless to say, providing adequate audio quality is something telephone companies care about very much, hence their preference for connections. 

The second reason the telephone companies like connection-oriented service is that they are accustomed to charging for connect time. When you make a long distance call (or even a local call outside North America) you are charged by the minute. When networks came around, they just automatically gravitated toward a model in which charging by the minute was easy to do. If you have to set up a connection before sending data, that is when the billing clock starts running. If there is no connection, they cannot charge for it. 

Ironically, maintaining billing records is very expensive. If a telephone company were to adopt a flat monthly rate with unlimited calling and no billing or record keeping, it would probably save a huge amount of money, despite the increased calling this policy would generate. Political, regulatory, and other factors weigh against doing this, however. Interestingly enough, flat rate service exists in other sectors. For example, cable TV is billed at a flat 

52




rate per month, no matter how many programs you watch. It could have been designed with pay-per-view as the basic concept, but it was not, due in part to the expense of billing (and given the quality of most television, the embarrassment factor cannot be totally discounted either). Also, many theme parks charge a daily admission fee for unlimited rides, in contrast to traveling carnivals, which charge by the ride. 

That said, it should come as no surprise that all networks designed by the telephone industry have had connection-oriented subnets. What is perhaps surprising, is that the Internet is also drifting in that direction, in order to provide a better quality of service for audio and video, a subject we will return to in 

Chap. 5

. But now let us examine some connection-oriented networks. 

X.25 and Frame Relay 

Our first example of a connection-oriented network is 

X.25

, which was the first public data network. It was deployed in the 1970s at a time when telephone service was a monopoly everywhere and the telephone company in each country expected there to be one data network per country—theirs. To use X.25, a computer first established a connection to the remote computer, that is, placed a telephone call. This connection was given a connection number to be used in data transfer packets (because multiple connections could be open at the same time). Data packets were very simple, consisting of a 3-byte header and up to 128 bytes of data. The header consisted of a 12-bit connection number, a packet sequence number, an acknowledgement number, and a few miscellaneous bits. X.25 networks operated for about a decade with mixed success. 

In the 1980s, the X.25 networks were largely replaced by a new kind of network called 

frame relay

. The essence of frame relay is that it is a connection-oriented network with no error control and no flow control. Because it was connection-oriented, packets were delivered in order (if they were delivered at all). The properties of in-order delivery, no error control, and no flow control make frame relay akin to a wide area LAN. Its most important application is interconnecting LANs at multiple company offices. Frame relay enjoyed a modest success and is still in use in places today. 

Asynchronous Transfer Mode 

Yet another, and far more important, connection-oriented network is 

ATM

 (

Asynchronous Transfer Mode

). The reason for the somewhat strange name is that in the telephone system, most transmission is synchronous (closely tied to a clock), and ATM is not. 

ATM was designed in the early 1990s and launched amid truly incredible hype (Ginsburg, 1996; Goralski, 1995; Ibe, 1997; Kim et al., 1994; and Stallings, 2000). ATM was going to solve all the world's networking and telecommunications problems by merging voice, data, cable television, telex, telegraph, carrier pigeon, tin cans connected by strings, tom-toms, smoke signals, and everything else into a single integrated system that could do everything for everyone. It did not happen. In large part, the problems were similar to those we described earlier concerning OSI, that is, bad timing, technology, implementation, and politics. Having just beaten back the telephone companies in round 1, many in the Internet community saw ATM as Internet versus the Telcos: the Sequel. But it really was not, and this time around even diehard datagram fanatics were aware that the Internet's quality of service left a lot to be desired. To make a long story short, ATM was much more successful than OSI, and it is now widely used deep within the telephone system, often for moving IP packets. Because it is now mostly used by carriers for internal transport, users are often unaware of its existence, but it is definitely alive and well. 

ATM Virtual Circuits 

Since ATM networks are connection-oriented, sending data requires first sending a packet to set up the connection. As the setup packet wends its way through the subnet, all the routers on the path make an entry in their internal tables noting the existence of the connection and reserving whatever resources are needed for it. Connections are often called 

virtual circuits

, in analogy with the physical circuits used within the telephone system. Most ATM networks also support 

permanent virtual circuits

, which are permanent connections between two (distant) hosts. They are similar to leased lines in the telephone world. Each connection, temporary or permanent, has a unique connection identifier. A virtual circuit is illustrated in 

Fig. 1-30

. 

Figure 1-30. A virtual circuit. 

53




 

Once a connection has been established, either side can begin transmitting data. The basic idea behind ATM is to transmit all information in small, fixed-size packets called 

cells

. The cells are 53 bytes long, of which 5 bytes are header and 48 bytes are payload, as shown in 

Fig. 1-31

. Part of the header is the connection identifier, so the sending and receiving hosts and all the intermediate routers can tell which cells belong to which connections. This information allows each router to know how to route each incoming cell. Cell routing is done in hardware, at high speed. In fact, the main argument for having fixed-size cells is that it is easy to build hardware routers to handle short, fixed-length cells. Variable-length IP packets have to be routed by software, which is a slower process. Another plus of ATM is that the hardware can be set up to copy one incoming cell to multiple output lines, a property that is required for handling a television program that is being broadcast to many receivers. Finally, small cells do not block any line for very long, which makes guaranteeing quality of service easier. 

Figure 1-31. An ATM cell. 

 

All cells follow the same route to the destination. Cell delivery is not guaranteed, but their order is. If cells 1 and 2 are sent in that order, then if both arrive, they will arrive in that order, never first 2 then 1. But either or both of them can be lost along the way. It is up to higher protocol levels to recover from lost cells. Note that although this guarantee is not perfect, it is better than what the Internet provides. There packets can not only be lost, but delivered out of order as well. ATM, in contrast, guarantees never to deliver cells out of order. 

ATM networks are organized like traditional WANs, with lines and switches (routers). The most common speeds for ATM networks are 155 Mbps and 622 Mbps, although higher speeds are also supported. The 155-Mbps speed was chosen because this is about what is needed to transmit high definition television. The exact choice of 155.52 Mbps was made for compatibility with AT&T's SONET transmission system, something we will study in 

Chap. 2

. The 622 Mbps speed was chosen so that four 155-Mbps channels could be sent over it. 

The ATM Reference Model 

ATM has its own reference model, different from the OSI model and also different from the TCP/IP model. This model is shown in 

Fig. 1-32

. It consists of three layers, the physical, ATM, and ATM adaptation layers, plus whatever users want to put on top of that. 

Figure 1-32. The ATM reference model. 

54




 

The physical layer deals with the physical medium: voltages, bit timing, and various other issues. ATM does not prescribe a particular set of rules but instead says that ATM cells can be sent on a wire or fiber by themselves, but they can also be packaged inside the payload of other carrier systems. In other words, ATM has been designed to be independent of the transmission medium. 

The 

ATM layer

 deals with cells and cell transport. It defines the layout of a cell and tells what the header fields mean. It also deals with establishment and release of virtual circuits. Congestion control is also located here. 

Because most applications do not want to work directly with cells (although some may), a layer above the ATM layer has been defined to allow users to send packets larger than a cell. The ATM interface segments these packets, transmits the cells individually, and reassembles them at the other end. This layer is the 

AAL

 (

ATM Adaptation Layer

). 

Unlike the earlier two-dimensional reference models, the ATM model is defined as being three-dimensional, as shown in 

Fig. 1-32

. The 

user plane

 deals with data transport, flow control, error correction, and other user functions. In contrast, the 

control plane

 is concerned with connection management. The layer and plane management functions relate to resource management and interlayer coordination. 

The physical and AAL layers are each divided into two sublayers, one at the bottom that does the work and a convergence sublayer on top that provides the proper interface to the layer above it. The functions of the layers and sublayers are given in 

Fig. 1-33

. 

Figure 1-33. The ATM layers and sublayers, and their functions. 

 

55




The 

PMD

 (

Physical Medium Dependent

) sublayer interfaces to the actual cable. It moves the bits on and off and handles the bit timing. For different carriers and cables, this layer will be different. 

The other sublayer of the physical layer is the 

TC

 (

Transmission Convergence

) sublayer. When cells are transmitted, the TC layer sends them as a string of bits to the PMD layer. Doing this is easy. At the other end, the TC sublayer gets a pure incoming bit stream from the PMD sublayer. Its job is to convert this bit stream into a cell stream for the ATM layer. It handles all the issues related to telling where cells begin and end in the bit stream. In the ATM model, this functionality is in the physical layer. In the OSI model and in pretty much all other networks, the job of framing, that is, turning a raw bit stream into a sequence of frames or cells, is the data link layer's task. 

As we mentioned earlier, the ATM layer manages cells, including their generation and transport. Most of the interesting aspects of ATM are located here. It is a mixture of the OSI data link and network layers; it is not split into sublayers. 

The AAL layer is split into a 

SAR

 (

Segmentation And Reassembly

) sublayer and a 

CS

 (

Convergence Sublayer

). The lower sublayer breaks up packets into cells on the transmission side and puts them back together again at the destination. The upper sublayer makes it possible to have ATM systems offer different kinds of services to different applications (e.g., file transfer and video on demand have different requirements concerning error handling, timing, etc.). 

As it is probably mostly downhill for ATM from now on, we will not discuss it further in this book. Nevertheless, since it has a substantial installed base, it will probably be around for at least a few more years. For more information about ATM, see (Dobrowski and Grise, 2001; and Gadecki and Heckart, 1997). 

1.5.3 Ethernet 

Both the Internet and ATM were designed for wide area networking. However, many companies, universities, and other organizations have large numbers of computers that must be connected. This need gave rise to the local area network. In this section we will say a little bit about the most popular LAN, Ethernet. 

The story starts out in pristine Hawaii in the early 1970s. In this case, ''pristine'' can be interpreted as ''not having a working telephone system.'' While not being interrupted by the phone all day long makes life more pleasant for vacationers, it did not make life more pleasant for researcher Norman Abramson and his colleagues at the University of Hawaii who were trying to connect users on remote islands to the main computer in Honolulu. Stringing their own cables under the Pacific Ocean was not in the cards, so they looked for a different solution. 

The one they found was short-range radios. Each user terminal was equipped with a small radio having two frequencies: upstream (to the central computer) and downstream (from the central computer). When the user wanted to contact the computer, it just transmitted a packet containing the data in the upstream channel. If no one else was transmitting at that instant, the packet probably got through and was acknowledged on the downstream channel. If there was contention for the upstream channel, the terminal noticed the lack of acknowledgement and tried again. Since there was only one sender on the downstream channel (the central computer), there were never collisions there. This system, called ALOHANET, worked fairly well under conditions of low traffic but bogged down badly when the upstream traffic was heavy. 

About the same time, a student named Bob Metcalfe got his bachelor's degree at M.I.T. and then moved up the river to get his Ph.D. at Harvard. During his studies, he was exposed to Abramson's work. He became so interested in it that after graduating from Harvard, he decided to spend the summer in Hawaii working with Abramson before starting work at Xerox PARC (Palo Alto Research Center). When he got to PARC, he saw that the researchers there had designed and built what would later be called personal computers. But the machines were isolated. Using his knowledge of Abramson's work, he, together with his colleague David Boggs, designed and implemented the first local area network (Metcalfe and Boggs, 1976). 

They called the system 

Ethernet

 after the 

luminiferous ether

, through which electromagnetic radiation was once thought to propagate. (When the 19th century British physicist James Clerk Maxwell discovered that electromagnetic radiation could be described by a wave equation, scientists assumed that space must be filled 

56




with some ethereal medium in which the radiation was propagating. Only after the famous Michelson-Morley experiment in 1887 did physicists discover that electromagnetic radiation could propagate in a vacuum.) 

The transmission medium here was not a vacuum, but a thick coaxial cable (the ether) up to 2.5 km long (with repeaters every 500 meters). Up to 256 machines could be attached to the system via transceivers screwed onto the cable. A cable with multiple machines attached to it in parallel is called a 

multidrop cable

. The system ran at 2.94 Mbps. A sketch of its architecture is given in 

Fig. 1-34

. Ethernet had a major improvement over ALOHANET: before transmitting, a computer first listened to the cable to see if someone else was already transmitting. If so, the computer held back until the current transmission finished. Doing so avoided interfering with existing transmissions, giving a much higher efficiency. ALOHANET did not work like this because it was impossible for a terminal on one island to sense the transmission of a terminal on a distant island. With a single cable, this problem does not exist. 

Figure 1-34. Architecture of the original Ethernet. 

 

Despite the computer listening before transmitting, a problem still arises: what happens if two or more computers all wait until the current transmission completes and then all start at once? The solution is to have each computer listen during its own transmission and if it detects interference, jam the ether to alert all senders. Then back off and wait a random time before retrying. If a second collision happens, the random waiting time is doubled, and so on, to spread out the competing transmissions and give one of them a chance to go first. 

The Xerox Ethernet was so successful that DEC, Intel, and Xerox drew up a standard in 1978 for a 10-Mbps Ethernet, called the 

DIX standard

. With two minor changes, the DIX standard became the IEEE 802.3 standard in 1983. 

Unfortunately for Xerox, it already had a history of making seminal inventions (such as the personal computer) and then failing to commercialize on them, a story told in 

Fumbling the Future

 (Smith and Alexander, 1988). When Xerox showed little interest in doing anything with Ethernet other than helping standardize it, Metcalfe formed his own company, 3Com, to sell Ethernet adapters for PCs. It has sold over 100 million of them. 

Ethernet continued to develop and is still developing. New versions at 100 Mbps, 1000 Mbps, and still higher have come out. Also the cabling has improved, and switching and other features have been added. We will discuss Ethernet in detail in 

Chap. 4

. 

In passing, it is worth mentioning that Ethernet (IEEE 802.3) is not the only LAN standard. The committee also standardized a token bus (802.4) and a token ring (802.5). The need for three more-or-less incompatible standards has little to do with technology and everything to do with politics. At the time of standardization, General Motors was pushing a LAN in which the topology was the same as Ethernet (a linear cable) but computers took turns in transmitting by passing a short packet called a 

token

 from computer to computer. A computer could only send if it possessed the token, thus avoiding collisions. General Motors announced that this scheme was essential for manufacturing cars and was not prepared to budge from this position. This announcement notwithstanding, 802.4 has basically vanished from sight. 

Similarly, IBM had its own favorite: its proprietary token ring. The token was passed around the ring and whichever computer held the token was allowed to transmit before putting the token back on the ring. Unlike 802.4, this scheme, standardized as 802.5, is still in use at some IBM sites, but virtually nowhere outside of IBM sites. However, work is progressing on a gigabit version (802.5v), but it seems unlikely that it will ever catch up with Ethernet. In short, there was a war between Ethernet, token bus, and token ring, and Ethernet won, mostly because it was there first and the challengers were not as good. 

57




1.5.4 Wireless LANs: 802.11 

Almost as soon as notebook computers appeared, many people had a dream of walking into an office and magically having their notebook computer be connected to the Internet. Consequently, various groups began working on ways to accomplish this goal. The most practical approach is to equip both the office and the notebook computers with short-range radio transmitters and receivers to allow them to communicate. This work rapidly led to wireless LANs being marketed by a variety of companies. 

The trouble was that no two of them were compatible. This proliferation of standards meant that a computer equipped with a brand 

X

 radio would not work in a room equipped with a brand 

Y

 base station. Finally, the industry decided that a wireless LAN standard might be a good idea, so the IEEE committee that standardized the wired LANs was given the task of drawing up a wireless LAN standard. The standard it came up with was named 802.11. A common slang name for it is 

WiFi

. It is an important standard and deserves respect, so we will call it by its proper name, 802.11. 

The proposed standard had to work in two modes: 

1. In the presence of a base station. 

2. In the absence of a base station. 

In the former case, all communication was to go through the base station, called an 

access point

 in 802.11 terminology. In the latter case, the computers would just send to one another directly. This mode is now sometimes called 

ad hoc networking

. A typical example is two or more people sitting down together in a room not equipped with a wireless LAN and having their computers just communicate directly. The two modes are illustrated in 

Fig. 1-35

. 

Figure 1-35. (a) Wireless networking with a base station. (b) Ad hoc networking. 

 

The first decision was the easiest: what to call it. All the other LAN standards had numbers like 802.1, 802.2, 802.3, up to 802.10, so the wireless LAN standard was dubbed 802.11. The rest was harder. 

In particular, some of the many challenges that had to be met were: finding a suitable frequency band that was available, preferably worldwide; dealing with the fact that radio signals have a finite range; ensuring that users' privacy was maintained; taking limited battery life into account; worrying about human safety (do radio waves cause cancer?); understanding the implications of computer mobility; and finally, building a system with enough bandwidth to be economically viable. 

At the time the standardization process started (mid-1990s), Ethernet had already come to dominate local area networking, so the committee decided to make 802.11 compatible with Ethernet above the data link layer. In particular, it should be possible to send an IP packet over the wireless LAN the same way a wired computer sent an IP packet over Ethernet. Nevertheless, in the physical and data link layers, several inherent differences with Ethernet exist and had to be dealt with by the standard. 

First, a computer on Ethernet always listens to the ether before transmitting. Only if the ether is idle does the computer begin transmitting. With wireless LANs, that idea does not work so well. To see why, examine 

Fig. 1-

36

. Suppose that computer 

A

 is transmitting to computer 

B

, but the radio range of 

A

's transmitter is too short to 

58




reach computer 

C

. If 

C

 wants to transmit to 

B

 it can listen to the ether before starting, but the fact that it does not hear anything does not mean that its transmission will succeed. The 802.11 standard had to solve this problem. 

Figure 1-36. The range of a single radio may not cover the entire system. 

 

The second problem that had to be solved is that a radio signal can be reflected off solid objects, so it may be received multiple times (along multiple paths). This interference results in what is called 

multipath fading

. 

The third problem is that a great deal of software is not aware of mobility. For example, many word processors have a list of printers that users can choose from to print a file. When the computer on which the word processor runs is taken into a new environment, the built-in list of printers becomes invalid. 

The fourth problem is that if a notebook computer is moved away from the ceiling-mounted base station it is using and into the range of a different base station, some way of handing it off is needed. Although this problem occurs with cellular telephones, it does not occur with Ethernet and needed to be solved. In particular, the network envisioned consists of multiple cells, each with its own base station, but with the base stations connected by Ethernet, as shown in 

Fig. 1-37

. From the outside, the entire system should look like a single Ethernet. The connection between the 802.11 system and the outside world is called a 

portal

. 

Figure 1-37. A multicell 802.11 network. 

 

After some work, the committee came up with a standard in 1997 that addressed these and other concerns. The wireless LAN it described ran at either 1 Mbps or 2 Mbps. Almost immediately, people complained that it was too slow, so work began on faster standards. A split developed within the committee, resulting in two new standards in 1999. The 802.11a standard uses a wider frequency band and runs at speeds up to 54 Mbps. The 802.11b standard uses the same frequency band as 802.11, but uses a different modulation technique to achieve 11 Mbps. Some people see this as psychologically important since 11 Mbps is faster than the original wired Ethernet. It is likely that the original 1-Mbps 802.11 will die off quickly, but it is not yet clear which of the new standards will win out. 

To make matters even more complicated than they already were, the 802 committee has come up with yet another variant, 802.11g, which uses the modulation technique of 802.11a but the frequency band of 802.11b. We will come back to 802.11 in detail in 

Chap. 4

. 

59




That 802.11 is going to cause a revolution in computing and Internet access is now beyond any doubt. Airports, train stations, hotels, shopping malls, and universities are rapidly installing it. Even upscale coffee shops are installing 802.11 so that the assembled yuppies can surf the Web while drinking their lattes. It is likely that 802.11 will do to the Internet what notebook computers did to computing: make it mobile. 

1.6 Network Standardization 

Many network vendors and suppliers exist, each with its own ideas of how things should be done. Without coordination, there would be complete chaos, and users would get nothing done. The only way out is to agree on some network standards. 

Not only do standards allow different computers to communicate, but they also increase the market for products adhering to the standard. A larger market leads to mass production, economies of scale in manufacturing, VLSI implementations, and other benefits that decrease price and further increase acceptance. In the following sections we will take a quick look at the important, but little-known, world of international standardization. 

Standards fall into two categories: de facto and de jure. 

De facto

 (Latin for ''from the fact'') standards are those that have just happened, without any formal plan. The IBM PC and its successors are de facto standards for small-office and home computers because dozens of manufacturers chose to copy IBM's machines very closely. Similarly, UNIX is the de facto standard for operating systems in university computer science departments. 

De jure

 (Latin for ''by law'') standards, in contrast, are formal, legal standards adopted by some authorized standardization body. International standardization authorities are generally divided into two classes: those established by treaty among national governments, and those comprising voluntary, nontreaty organizations. In the area of computer network standards, there are several organizations of each type, which are discussed below. 

1.6.1 Who's Who in the Telecommunications World 

The legal status of the world's telephone companies varies considerably from country to country. At one extreme is the United States, which has 1500 separate, privately owned telephone companies. Before it was broken up in 1984, AT&T, at that time the world's largest corporation, completely dominated the scene. It provided telephone service to about 80 percent of America's telephones, spread throughout half of its geographical area, with all the other companies combined servicing the remaining (mostly rural) customers. Since the breakup, AT&T continues to provide long-distance service, although now in competition with other companies. The seven Regional Bell Operating Companies that were split off from AT&T and numerous independents provide local and cellular telephone service. Due to frequent mergers and other changes, the industry is in a constant state of flux. 

Companies in the United States that provide communication services to the public are called 

common carriers

. Their offerings and prices are described by a document called a 

tariff

, which must be approved by the Federal Communications Commission for the interstate and international traffic and by the state public utilities commissions for intrastate traffic. 

At the other extreme are countries in which the national government has a complete monopoly on all communication, including the mail, telegraph, telephone, and often, radio and television. Most of the world falls in this category. In some cases the telecommunication authority is a nationalized company, and in others it is simply a branch of the government, usually known as the 

PTT

 (

Post, Telegraph & Telephone

 administration). Worldwide, the trend is toward liberalization and competition and away from government monopoly. Most European countries have now (partially) privatized their PTTs, but elsewhere the process is still slowly gaining steam. 

With all these different suppliers of services, there is clearly a need to provide compatibility on a worldwide scale to ensure that people (and computers) in one country can call their counterparts in another one. Actually, this need has existed for a long time. In 1865, representatives from many European governments met to form the predecessor to today's 

ITU

 (

International Telecommunication Union

). Its job was standardizing international telecommunications, which in those days meant telegraphy. Even then it was clear that if half the countries used Morse code and the other half used some other code, there was going to be a problem. When the telephone 

60




was put into international service, ITU took over the job of standardizing telephony (pronounced te-LEF-ony) as well. In 1947, ITU became an agency of the United Nations. 

ITU has three main sectors: 

1. Radiocommunications Sector (ITU-R). 

2. Telecommunications Standardization Sector (ITU-T). 

3. Development Sector (ITU-D). 

ITU-R is concerned with allocating radio frequencies worldwide to the competing interest groups. We will focus primarily on ITU-T, which is concerned with telephone and data communication systems. From 1956 to 1993, ITU-T was known as 

CCITT

, an acronym for its French name: Comité Consultatif International Télégraphique et Téléphonique. On March 1, 1993, CCITT was reorganized to make it less bureaucratic and renamed to reflect its new role. Both ITU-T and CCITT issued recommendations in the area of telephone and data communications. One still frequently runs into CCITT recommendations, such as CCITT X.25, although since 1993 recommendations bear the ITU-T label. 

ITU-T has four classes of members: 

1. National governments. 

2. Sector members. 

3. Associate members. 

4. Regulatory agencies. 

ITU-T has about 200 governmental members, including almost every member of the United Nations. Since the United States does not have a PTT, somebody else had to represent it in ITU-T. This task fell to the State Department, probably on the grounds that ITU-T had to do with foreign countries, the State Department's specialty. There are approximately 500 sector members, including telephone companies (e.g., AT&T, Vodafone, WorldCom), telecom equipment manufacturers (e.g., Cisco, Nokia, Nortel), computer vendors (e.g., Compaq, Sun, Toshiba), chip manufacturers (e.g., Intel, Motorola, TI), media companies (e.g., AOL Time Warner, CBS, Sony), and other interested companies (e.g., Boeing, Samsung, Xerox). Various nonprofit scientific organizations and industry consortia are also sector members (e.g., IFIP and IATA). Associate members are smaller organizations that are interested in a particular Study Group. Regulatory agencies are the folks who watch over the telecom business, such as the U.S. Federal Communications Commission. 

ITU-T's task is to make technical recommendations about telephone, telegraph, and data communication interfaces. These often become internationally recognized standards, for example, V.24 (also known as EIA RS-232 in the United States), which specifies the placement and meaning of the various pins on the connector used by most asynchronous terminals and external modems. 

It should be noted that ITU-T recommendations are technically only suggestions that governments can adopt or ignore, as they wish (because governments are like 13-year-old boys—they do not take kindly to being given orders). In practice, a country that wishes to adopt a telephone standard different from that used by the rest of the world is free to do so, but at the price of cutting itself off from everyone else. This might work for North Korea, but elsewhere it would be a real problem. The fiction of calling ITU-T standards ''recommendations'' was and is necessary to keep nationalist forces in many countries placated. 

The real work of ITU-T is done in its 14 Study Groups, often as large as 400 people. There are currently 14 Study Groups, covering topics ranging from telephone billing to multimedia services. In order to make it possible to get anything at all done, the Study Groups are divided into Working Parties, which are in turn divided into Expert Teams, which are in turn divided into ad hoc groups. Once a bureaucracy, always a bureaucracy. 

Despite all this, ITU-T actually gets things done. Since its inception, it has produced close to 3000 recommendations occupying about 60,000 pages of paper. Many of these are widely used in practice. For example, the popular V.90 56-kbps modem standard is an ITU recommendation. 

61




As telecommunications completes the transition started in the 1980s from being entirely national to being entirely global, standards will become increasingly important, and more and more organizations will want to become involved in setting them. For more information about ITU, see (Irmer, 1994). 

1.6.2 Who's Who in the International Standards World 

International standards are produced and published by 

ISO

 (

International Standards Organization

 

[

]

), a voluntary nontreaty organization founded in 1946. Its members are the national standards organizations of the 89 member countries. These members include ANSI (U.S.), BSI (Great Britain), AFNOR (France), DIN (Germany), and 85 others. 

[

]

 For the purist, ISO's true name is the International Organization for Standardization. 

ISO issues standards on a truly vast number of subjects, ranging from nuts and bolts (literally) to telephone pole coatings [not to mention cocoa beans (ISO 2451), fishing nets (ISO 1530), women's underwear (ISO 4416) and quite a few other subjects one might not think were subject to standardization]. Over 13,000 standards have been issued, including the OSI standards. ISO has almost 200 Technical Committees, numbered in the order of their creation, each dealing with a specific subject. TC1 deals with the nuts and bolts (standardizing screw thread pitches). TC97 deals with computers and information processing. Each TC has subcommittees (SCs) divided into working groups (WGs). 

The real work is done largely in the WGs by over 100,000 volunteers worldwide. Many of these ''volunteers'' are assigned to work on ISO matters by their employers, whose products are being standardized. Others are government officials keen on having their country's way of doing things become the international standard. Academic experts also are active in many of the WGs. 

On issues of telecommunication standards, ISO and ITU-T often cooperate (ISO is a member of ITU-T) to avoid the irony of two official and mutually incompatible international standards. 

The U.S. representative in ISO is 

ANSI

 (

American National Standards Institute

), which despite its name, is a private, nongovernmental, nonprofit organization. Its members are manufacturers, common carriers, and other interested parties. ANSI standards are frequently adopted by ISO as international standards. 

The procedure used by ISO for adopting standards has been designed to achieve as broad a consensus as possible. The process begins when one of the national standards organizations feels the need for an international standard in some area. A working group is then formed to come up with a 

CD

 (

Committee Draft

). The CD is then circulated to all the member bodies, which get 6 months to criticize it. If a substantial majority approves, a revised document, called a 

DIS

 (

Draft International Standard

) is produced and circulated for comments and voting. Based on the results of this round, the final text of the 

IS

 (

International Standard

) is prepared, approved, and published. In areas of great controversy, a CD or DIS may have to go through several versions before acquiring enough votes, and the whole process can take years. 

NIST

 (

National Institute of Standards and Technology

) is part of the U.S. Department of Commerce. It used to be the National Bureau of Standards. It issues standards that are mandatory for purchases made by the U.S. Government, except for those of the Department of Defense, which has its own standards. 

Another major player in the standards world is 

IEEE

 (

Institute of Electrical and Electronics Engineers

), the largest professional organization in the world. In addition to publishing scores of journals and running hundreds of conferences each year, IEEE has a standardization group that develops standards in the area of electrical engineering and computing. IEEE's 802 committee has standardized many kinds of LANs. We will study some of its output later in this book. The actual work is done by a collection of working groups, which are listed in 

Fig. 1-

38

. The success rate of the various 802 working groups has been low; having an 802.x number is no guarantee of success. But the impact of the success stories (especially 802.3 and 802.11) has been enormous. 

Figure 1-38. The 802 working groups. The important ones are marked with *. The ones marked with 

are hibernating. The one marked with 

gave up and disbanded itself. 

62




 

1.6.3 Who's Who in the Internet Standards World 

The worldwide Internet has its own standardization mechanisms, very different from those of ITU-T and ISO. The difference can be crudely summed up by saying that the people who come to ITU or ISO standardization meetings wear suits. The people who come to Internet standardization meetings wear jeans (except when they meet in San Diego, when they wear shorts and T-shirts). 

ITU-T and ISO meetings are populated by corporate officials and government civil servants for whom standardization is their job. They regard standardization as a Good Thing and devote their lives to it. Internet people, on the other hand, prefer anarchy as a matter of principle. However, with hundreds of millions of people all doing their own thing, little communication can occur. Thus, standards, however regrettable, are sometimes needed. 

When the ARPANET was set up, DoD created an informal committee to oversee it. In 1983, the committee was renamed the 

IAB

 (

Internet Activities Board

) and was given a slighter broader mission, namely, to keep the researchers involved with the ARPANET and the Internet pointed more-or-less in the same direction, an activity not unlike herding cats. The meaning of the acronym ''IAB'' was later changed to 

Internet Architecture Board

. 

Each of the approximately ten members of the IAB headed a task force on some issue of importance. The IAB met several times a year to discuss results and to give feedback to the DoD and NSF, which were providing most of the funding at this time. When a standard was needed (e.g., a new routing algorithm), the IAB members would thrash it out and then announce the change so the graduate students who were the heart of the software effort could implement it. Communication was done by a series of technical reports called 

RFCs

 (

Request For Comments

). RFCs are stored on-line and can be fetched by anyone interested in them from 

www.ietf.org/rfc

. They are numbered in chronological order of creation. Over 3000 now exist. We will refer to many RFCs in this book. 

By 1989, the Internet had grown so large that this highly informal style no longer worked. Many vendors by then offered TCP/IP products and did not want to change them just because ten researchers had thought of a better idea. In the summer of 1989, the IAB was reorganized again. The researchers were moved to the 

IRTF

 (

Internet Research Task Force

), which was made subsidiary to IAB, along with the 

IETF

 (

Internet Engineering Task Force

). The IAB was repopulated with people representing a broader range of organizations than just the research community. It was initially a self-perpetuating group, with members serving for a 2-year term and new members being appointed by the old ones. Later, the 

Internet Society

 was created, populated by people 

63




interested in the Internet. The Internet Society is thus in a sense comparable to ACM or IEEE. It is governed by elected trustees who appoint the IAB members. 

The idea of this split was to have the IRTF concentrate on long-term research while the IETF dealt with short-term engineering issues. The IETF was divided up into working groups, each with a specific problem to solve. The chairmen of these working groups initially met as a steering committee to direct the engineering effort. The working group topics include new applications, user information, OSI integration, routing and addressing, security, network management, and standards. Eventually, so many working groups were formed (more than 70) that they were grouped into areas and the area chairmen met as the steering committee. 

In addition, a more formal standardization process was adopted, patterned after ISOs. To become a 

Proposed Standard

, the basic idea must be completely explained in an RFC and have sufficient interest in the community to warrant consideration. To advance to the 

Draft Standard

 stage, a working implementation must have been rigorously tested by at least two independent sites for at least 4 months. If the IAB is convinced that the idea is sound and the software works, it can declare the RFC to be an Internet Standard. Some Internet Standards have become DoD standards (MIL-STD), making them mandatory for DoD suppliers. David Clark once made a now-famous remark about Internet standardization consisting of ''rough consensus and running code.'' 

1.7 Metric Units 

To avoid any confusion, it is worth stating explicitly that in this book, as in computer science in general, metric units are used instead of traditional English units (the furlong-stone-fortnight system). The principal metric prefixes are listed in 

Fig. 1-39

. The prefixes are typically abbreviated by their first letters, with the units greater than 1 capitalized (KB, MB, etc.). One exception (for historical reasons) is kbps for kilobits/sec. Thus, a 1-Mbps communication line transmits 10

6

 bits/sec and a 100 psec (or 100 ps) clock ticks every 10

-10

 seconds. Since milli and micro both begin with the letter ''m,'' a choice had to be made. Normally, ''m'' is for milli and ''µ'' (the Greek letter mu) is for micro. 

Figure 1-39. The principal metric prefixes. 

 

It is also worth pointing out that for measuring memory, disk, file, and database sizes, in common industry practice, the units have slightly different meanings. There, kilo means 2

10

 (1024) rather than 10

3

 (1000) because memories are always a power of two. Thus, a 1-KB memory contains 1024 bytes, not 1000 bytes. Similarly, a 1-MB memory contains 2

20

 (1,048,576) bytes, a 1-GB memory contains 2

30

 (1,073,741,824) bytes, and a 1-TB database contains 2

40

 (1,099,511,627,776) bytes. However, a 1-kbps communication line transmits 1000 bits per second and a 10-Mbps LAN runs at 10,000,000 bits/sec because these speeds are not powers of two. Unfortunately, many people tend to mix up these two systems, especially for disk sizes. To avoid ambiguity, in this book, we will use the symbols KB, MB, and GB for 2

10

, 2

20

, and 2

30

 bytes, respectively, and the symbols kbps, Mbps, and Gbps for 10

3

, 10

6

, and 10

9

 bits/sec, respectively. 

1.8 Outline of the Rest of the Book 

This book discusses both the principles and practice of computer networking. Most chapters start with a discussion of the relevant principles, followed by a number of examples that illustrate these principles. These examples are usually taken from the Internet and wireless networks since these are both important and very different. Other examples will be given where relevant. 

64




The book is structured according to the hybrid model of 

Fig. 1-24

. Starting with 

Chap. 2

, we begin working our way up the protocol hierarchy beginning at the bottom. The second chapter provides some background in the field of data communication. It covers wired, wireless, and satellite transmission systems. This material is concerned with the physical layer, although we cover only the architectural rather than the hardware aspects. Several examples of the physical layer, such as the public switched telephone network, mobile telephones, and the cable television network are also discussed. 

Chapter 3

 discusses the data link layer and its protocols by means of a number of increasingly complex examples. The analysis of these protocols is also covered. After that, some important real-world protocols are discussed, including HDLC (used in low- and medium-speed networks) and PPP (used in the Internet). 

Chapter 4

 concerns the medium access sublayer, which is part of the data link layer. The basic question it deals with is how to determine who may use the network next when the network consists of a single shared channel, as in most LANs and some satellite networks. Many examples are given from the areas of wired LANs, wireless LANs (especially Ethernet), wireless MANs, Bluetooth, and satellite networks. Bridges and data link switches, which are used to connect LANs, are also discussed here. 

Chapter 5

 deals with the network layer, especially routing, with many routing algorithms, both static and dynamic, being covered. Even with good routing algorithms though, if more traffic is offered than the network can handle, congestion can develop, so we discuss congestion and how to prevent it. Even better than just preventing congestion is guaranteeing a certain quality of service. We will discuss that topic as well here. Connecting heterogeneous networks to form internetworks leads to numerous problems that are discussed here. The network layer in the Internet is given extensive coverage. 

Chapter 6

 deals with the transport layer. Much of the emphasis is on connection-oriented protocols, since many applications need these. An example transport service and its implementation are discussed in detail. The actual code is given for this simple example to show how it could be implemented. Both Internet transport protocols, UDP and TCP, are covered in detail, as are their performance issues. Issues concerning wireless networks are also covered. 

Chapter 7

 deals with the application layer, its protocols and applications. The first topic is DNS, which is the Internet's telephone book. Next comes e-mail, including a discussion of its protocols. Then we move onto the Web, with detailed discussions of the static content, dynamic content, what happens on the client side, what happens on the server side, protocols, performance, the wireless Web, and more. Finally, we examine networked multimedia, including streaming audio, Internet radio, and video on demand. 

Chapter 8

 is about network security. This topic has aspects that relate to all layers, so it is easiest to treat it after all the layers have been thoroughly explained. The chapter starts with an introduction to cryptography. Later, it shows how cryptography can be used to secure communication, e-mail, and the Web. The book ends with a discussion of some areas in which security hits privacy, freedom of speech, censorship, and other social issues collide head on. 

Chapter 9

 contains an annotated list of suggested readings arranged by chapter. It is intended to help those readers who would like to pursue their study of networking further. The chapter also has an alphabetical bibliography of all references cited in this book. 

The author's Web site at Prentice Hall: 

http://www.prenhall.com/tanenbaum

 

has a page with links to many tutorials, FAQs, companies, industry consortia, professional organizations, standards organizations, technologies, papers, and more. 

1.9 Summary 

Computer networks can be used for numerous services, both for companies and for individuals. For companies, networks of personal computers using shared servers often provide access to corporate information. Typically 

65




they follow the client-server model, with client workstations on employee desktops accessing powerful servers in the machine room. For individuals, networks offer access to a variety of information and entertainment resources. Individuals often access the Internet by calling up an ISP using a modem, although increasingly many people have a fixed connection at home. An up-and-coming area is wireless networking with new applications such as mobile e-mail access and m-commerce. 

Roughly speaking, networks can be divided up into LANs, MANs, WANs, and internetworks, with their own characteristics, technologies, speeds, and niches. LANs cover a building and operate at high speeds. MANs cover a city, for example, the cable television system, which is now used by many people to access the Internet. WANs cover a country or continent. LANs and MANs are unswitched (i.e., do not have routers); WANs are switched. Wireless networks are becoming extremely popular, especially wireless LANs. Networks can be interconnected to form internetworks. 

Network software consists of protocols, which are rules by which processes communicate. Protocols are either connectionless or connection-oriented. Most networks support protocol hierarchies, with each layer providing services to the layers above it and insulating them from the details of the protocols used in the lower layers. Protocol stacks are typically based either on the OSI model or on the TCP/IP model. Both have network, transport, and application layers, but they differ on the other layers. Design issues include multiplexing, flow control, error control, and others. Much of this book deals with protocols and their design. 

Networks provide services to their users. These services can be connection-oriented or connectionless. In some networks, connectionless service is provided in one layer and connection-oriented service is provided in the layer above it. 

Well-known networks include the Internet, ATM networks, Ethernet, and the IEEE 802.11 wireless LAN. The Internet evolved from the ARPANET, to which other networks were added to form an internetwork. The present Internet is actually a collection of many thousands of networks, rather than a single network. What characterizes it is the use of the TCP/IP protocol stack throughout. ATM is widely used inside the telephone system for long-haul data traffic. Ethernet is the most popular LAN and is present in most large companies and universities. Finally, wireless LANs at surprisingly high speeds (up to 54 Mbps) are beginning to be widely deployed. 

To have multiple computers talk to each other requires a large amount of standardization, both in the hardware and software. Organizations such as the ITU-T, ISO, IEEE, and IAB manage different parts of the standardization process. 

Problems 

1. Imagine that you have trained your St. Bernard, Bernie, to carry a box of three 8mm tapes instead of a flask of brandy. (When your disk fills up, you consider that an emergency.) These tapes each contain 7 gigabytes. The dog can travel to your side, wherever you may be, at 18 km/hour. For what range of distances does Bernie have a higher data rate than a transmission line whose data rate (excluding overhead) is 150 Mbps? 

2. An alternative to a LAN is simply a big timesharing system with terminals for all users. Give two advantages of a client-server system using a LAN. 

3. The performance of a client-server system is influenced by two network factors: the bandwidth of the network (how many bits/sec it can transport) and the latency (how many seconds it takes for the first bit to get from the client to the server). Give an example of a network that exhibits high bandwidth and high latency. Then give an example of one with low bandwidth and low latency. 

4. Besides bandwidth and latency, what other parameter is needed to give a good characterization of the quality of service offered by a network used for digitized voice traffic? 

5. A factor in the delay of a store-and-forward packet-switching system is how long it takes to store and forward a packet through a switch. If switching time is 10 µsec, is this likely to be a major factor in the response of a client-server system where the client is in New York and the server is in California? Assume the propagation speed in copper and fiber to be 2/3 the speed of light in vacuum. 

6. A client-server system uses a satellite network, with the satellite at a height of 40,000 km. What is the best-case delay in response to a request? 

7. In the future, when everyone has a home terminal connected to a computer network, instant public referendums on important pending legislation will become possible. Ultimately, existing legislatures 

66




could be eliminated, to let the will of the people be expressed directly. The positive aspects of such a direct democracy are fairly obvious; discuss some of the negative aspects. 

8. A collection of five routers is to be connected in a point-to-point subnet. Between each pair of routers, the designers may put a high-speed line, a medium-speed line, a low-speed line, or no line. If it takes 100 ms of computer time to generate and inspect each topology, how long will it take to inspect all of them? 

9. A group of 2

n

 - 1 routers are interconnected in a centralized binary tree, with a router at each tree node. Router 

i

 communicates with router 

j

 by sending a message to the root of the tree. The root then sends the message back down to 

j

. Derive an approximate expression for the mean number of hops per message for large 

n

, assuming that all router pairs are equally likely. 

10. A disadvantage of a broadcast subnet is the capacity wasted when multiple hosts attempt to access the channel at the same time. As a simplistic example, suppose that time is divided into discrete slots, with each of the 

n

 hosts attempting to use the channel with probability 

p

 during each slot. What fraction of the slots are wasted due to collisions? 

11. What are two reasons for using layered protocols? 

12. The president of the Specialty Paint Corp. gets the idea to work with a local beer brewer to produce an invisible beer can (as an anti-litter measure). The president tells her legal department to look into it, and they in turn ask engineering for help. As a result, the chief engineer calls his counterpart at the other company to discuss the technical aspects of the project. The engineers then report back to their respective legal departments, which then confer by telephone to arrange the legal aspects. Finally, the two corporate presidents discuss the financial side of the deal. Is this an example of a multilayer protocol in the sense of the OSI model? 

13. What is the principal difference between connectionless communication and connection-oriented communication? 

14. Two networks each provide reliable connection-oriented service. One of them offers a reliable byte stream and the other offers a reliable message stream. Are these identical? If so, why is the distinction made? If not, give an example of how they differ. 

15. What does ''negotiation'' mean when discussing network protocols? Give an example. 

16. In 

Fig. 1-19

, a service is shown. Are any other services implicit in this figure? If so, where? If not, why not? 

17. In some networks, the data link layer handles transmission errors by requesting damaged frames to be retransmitted. If the probability of a frame's being damaged is 

p

, what is the mean number of transmissions required to send a frame? Assume that acknowledgements are never lost. 

18. Which of the OSI layers handles each of the following: 

a. (a) Dividing the transmitted bit stream into frames. 

b. (b) Determining which route through the subnet to use. 

19. If the unit exchanged at the data link level is called a frame and the unit exchanged at the network level is called a packet, do frames encapsulate packets or do packets encapsulate frames? Explain your answer. 

20. A system has an 

n

-layer protocol hierarchy. Applications generate messages of length 

M

 bytes. At each of the layers, an 

h

-byte header is added. What fraction of the network bandwidth is filled with headers? 

21. List two ways in which the OSI reference model and the TCP/IP reference model are the same. Now list two ways in which they differ. 

22. What is the main difference between TCP and UDP? 

23. The subnet of 

Fig. 1-25(b)

 was designed to withstand a nuclear war. How many bombs would it take to partition the nodes into two disconnected sets? Assume that any bomb wipes out a node and all of the links connected to it. 

24. The Internet is roughly doubling in size every 18 months. Although no one really knows for sure, one estimate put the number of hosts on it at 100 million in 2001. Use these data to compute the expected number of Internet hosts in the year 2010. Do you believe this? Explain why or why not. 

25. When a file is transferred between two computers, two acknowledgement strategies are possible. In the first one, the file is chopped up into packets, which are individually acknowledged by the receiver, but the file transfer as a whole is not acknowledged. In the second one, the packets are not acknowledged individually, but the entire file is acknowledged when it arrives. Discuss these two approaches. 

26. Why does ATM use small, fixed-length cells? 

27. How long was a bit on the original 802.3 standard in meters? Use a transmission speed of 10 Mbps and assume the propagation speed in coax is 2/3 the speed of light in vacuum. 

28. An image is 1024 x 768 pixels with 3 bytes/pixel. Assume the image is uncompressed. How long does it take to transmit it over a 56-kbps modem channel? Over a 1-Mbps cable modem? Over a 10-Mbps Ethernet? Over 100-Mbps Ethernet? 

67




29. Ethernet and wireless networks have some similarities and some differences. One property of Ethernet is that only one frame at a time can be transmitted on an Ethernet. Does 802.11 share this property with Ethernet? Discuss your answer. 

30. Wireless networks are easy to install, which makes them inexpensive since installation costs usually far overshadow equipment costs. Nevertheless, they also have some disadvantages. Name two of them. 

31. List two advantages and two disadvantages of having international standards for network protocols. 

32. When a system has a permanent part and a removable part (such as a CD-ROM drive and the CD-ROM), it is important that the system be standardized, so that different companies can make both the permanent and removable parts and everything still works together. Give three examples outside the computer industry where such international standards exist. Now give three areas outside the computer industry where they do not exist. 

33. Make a list of activities that you do every day in which computer networks are used. How would your life be altered if these networks were suddenly switched off? 

34. Find out what networks are used at your school or place of work. Describe the network types, topologies, and switching methods used there. 

35. The 

ping

 program allows you to send a test packet to a given location and see how long it takes to get there and back. Try using 

ping

 to see how long it takes to get from your location to several known locations. From thes data, plot the one-way transit time over the Internet as a function of distance. It is best to use universities since the location of their servers is known very accurately. For example, 

berkeley.edu

 is in Berkeley, California, 

mit.edu

 is in Cambridge, Massachusetts, 

vu.nl

 is in Amsterdam, The Netherlands, 

www.usyd.edu.au

 is in Sydney, Australia, and 

www.uct.ac.za

 is in Cape Town, South Africa. 

36. Go to IETF's Web site, 

www.ietf.org

, to see what they are doing. Pick a project you like and write a half-page report on the problem and the proposed solution. 

37. Standardization is very important in the network world. ITU and ISO are the main official standardization organizations. Go to their Web sites, 

www.itu.org

 and 

www.iso.org

, respectively, and learn about their standardization work. Write a short report about the kinds of things they have standardized. 

38. The Internet is made up of a large number of networks. Their arrangement determines the topology of the Internet. A considerable amount of information about the Internet topology is available on line. Use a search engine to find out more about the Internet topology and write a short report summarizing your findings. 

 

68




Chapter 2. The Physical Layer 

In this chapter we will look at the lowest layer depicted in the hierarchy of 

Fig. 1-24

. It defines the mechanical, electrical, and timing interfaces to the network. We will begin with a theoretical analysis of data transmission, only to discover that Mother (Parent?) Nature puts some limits on what can be sent over a channel. 

Then we will cover three kinds of transmission media: guided (copper wire and fiber optics), wireless (terrestrial radio), and satellite. This material will provide background information on the key transmission technologies used in modern networks. 

The remainder of the chapter will be devoted to three examples of communication systems used in practice for wide area computer networks: the (fixed) telephone system, the mobile phone system, and the cable television system. All three use fiber optics in the backbone, but they are organized differently and use different technologies for the last mile. 

2.1 The Theoretical Basis for Data Communication 

Information can be transmitted on wires by varying some physical property such as voltage or current. By representing the value of this voltage or current as a single-valued function of time, 

f

(

t

), we can model the behavior of the signal and analyze it mathematically. This analysis is the subject of the following sections. 

2.1.1 Fourier Analysis 

In the early 19th century, the French mathematician Jean-Baptiste Fourier proved that any reasonably behaved periodic function, 

g

(

t

) with period 

T

 can be constructed as the sum of a (possibly infinite) number of sines and cosines: 

Equation 2 

 

 

where 

f

 = 1/

T

 is the fundamental frequency, 

a

n

 and 

b

n

 are the sine and cosine amplitudes of the 

n

th 

harmonics

 (terms), and 

c

 is a constant. Such a decomposition is called a 

Fourier series

. From the Fourier series, the function can be reconstructed; that is, if the period, 

T

, is known and the amplitudes are given, the original function of time can be found by performing the sums of 

Eq. (2-1)

. 

A data signal that has a finite duration (which all of them do) can be handled by just imagining that it repeats the entire pattern over and over forever (i.e., the interval from 

T

 to 

2T

 is the same as from 0 to 

T

, etc.). 

The 

a

n

 amplitudes can be computed for any given 

g

(

t

) by multiplying both sides of 

Eq. (2-1)

 by sin(2p

kft

) and then integrating from 0 to 

T

. Since 

 

 

only one term of the summation survives: 

a.

n

 The 

b

n

 summation vanishes completely. Similarly, by multiplying 

Eq. (2-1)

 by cos(2p

kft

) and integrating between 0 and 

T

, we can derive 

b.

n

 By just integrating both sides of the equation as it stands, we can find 

c

. The results of performing these operations are as follows: 

69




 

 

2.1.2 Bandwidth-Limited Signals 

To see what all this has to do with data communication, let us consider a specific example: the transmission of the ASCII character ''b'' encoded in an 8-bit byte. The bit pattern that is to be transmitted is 01100010. The left-hand part of 

Fig. 2-1(a)

 shows the voltage output by the transmitting computer. The Fourier analysis of this signal yields the coefficients: 

Figure 2-1. (a) A binary signal and its root-mean-square Fourier amplitudes. (b)-(e) Successive approximations to the original signal. 

 

70




 

 

The root-mean-square amplitudes, 

, for the first few terms are shown on the right-hand side of 

Fig. 2-

1(a)

. These values are of interest because their squares are proportional to the energy transmitted at the corresponding frequency. 

No transmission facility can transmit signals without losing some power in the process. If all the Fourier components were equally diminished, the resulting signal would be reduced in amplitude but not distorted [i.e., it would have the same nice squared-off shape as 

Fig. 2-1(a)

]. Unfortunately, all transmission facilities diminish different Fourier components by different amounts, thus introducing distortion. Usually, the amplitudes are transmitted undiminished from 0 up to some frequency 

f

c

 [measured in cycles/sec or Hertz (Hz)] with all frequencies above this cutoff frequency attenuated. The range of frequencies transmitted without being strongly attenuated is called the 

bandwidth

. In practice, the cutoff is not really sharp, so often the quoted bandwidth is from 0 to the frequency at which half the power gets through. 

The bandwidth is a physical property of the transmission medium and usually depends on the construction, thickness, and length of the medium. In some cases a filter is introduced into the circuit to limit the amount of bandwidth available to each customer. For example, a telephone wire may have a bandwidth of 1 MHz for short distances, but telephone companies add a filter restricting each customer to about 3100 Hz. This bandwidth is adequate for intelligible speech and improves system-wide efficiency by limiting resource usage by customers. 

Now let us consider how the signal of 

Fig. 2-1(a)

 would look if the bandwidth were so low that only the lowest frequencies were transmitted [i.e., if the function were being approximated by the first few terms of 

Eq. (2-1)

]. 

Figure 2-1(b)

 shows the signal that results from a channel that allows only the first harmonic (the fundamental, 

f

) to pass through. Similarly, 

Fig. 2-1(c)

-

(e)

 show the spectra and reconstructed functions for higher-bandwidth channels. 

Given a bit rate of 

b

 bits/sec, the time required to send 8 bits (for example) 1 bit at a time is 8

/b

 sec, so the frequency of the first harmonic is 

b/

8 Hz. An ordinary telephone line, often called a 

voice-grade line

, has an artificially-introduced cutoff frequency just above 3000 Hz. This restriction means that the number of the highest harmonic passed through is roughly 3000

/

(

b/

8) or 24,000

/b

, (the cutoff is not sharp). 

For some data rates, the numbers work out as shown in 

Fig. 2-2

. From these numbers, it is clear that trying to send at 9600 bps over a voice-grade telephone line will transform 

Fig. 2-1(a)

 into something looking like 

Fig. 2-

1(c)

, making accurate reception of the original binary bit stream tricky. It should be obvious that at data rates much higher than 38.4 kbps, there is no hope at all for 

binary

 signals, even if the transmission facility is completely noiseless. In other words, limiting the bandwidth limits the data rate, even for perfect channels. However, sophisticated coding schemes that make use of several voltage levels do exist and can achieve higher data rates. We will discuss these later in this chapter. 

Figure 2-2. Relation between data rate and harmonics. 

71




 

2.1.3 The Maximum Data Rate of a Channel 

As early as 1924, an AT&T engineer, Henry Nyquist, realized that even a perfect channel has a finite transmission capacity. He derived an equation expressing the maximum data rate for a finite bandwidth noiseless channel. In 1948, Claude Shannon carried Nyquist's work further and extended it to the case of a channel subject to random (that is, thermodynamic) noise (Shannon, 1948). We will just briefly summarize their now classical results here. 

Nyquist proved that if an arbitrary signal has been run through a low-pass filter of bandwidth 

H

, the filtered signal can be completely reconstructed by making only 2

H

 (exact) samples per second. Sampling the line faster than 2

H

 times per second is pointless because the higher frequency components that such sampling could recover have already been filtered out. If the signal consists of 

V

 discrete levels, Nyquist's theorem states: 

 

 

For example, a noiseless 3-kHz channel cannot transmit binary (i.e., two-level) signals at a rate exceeding 6000 bps. 

So far we have considered only noiseless channels. If random noise is present, the situation deteriorates rapidly. And there is always random (thermal) noise present due to the motion of the molecules in the system. The amount of thermal noise present is measured by the ratio of the signal power to the noise power, called the 

signal-to-noise ratio

. If we denote the signal power by 

S

 and the noise power by 

N

, the signal-to-noise ratio is 

S/N

. Usually, the ratio itself is not quoted; instead, the quantity 10 log

10

 

S/N

 is given. These units are called 

decibels

 (dB). An 

S/N

 ratio of 10 is 10 dB, a ratio of 100 is 20 dB, a ratio of 1000 is 30 dB, and so on. The manufacturers of stereo amplifiers often characterize the bandwidth (frequency range) over which their product is linear by giving the 3-dB frequency on each end. These are the points at which the amplification factor has been approximately halved (because log

10

3 

0.5). 

Shannon's major result is that the maximum data rate of a noisy channel whose bandwidth is 

H

 Hz, and whose signal-to-noise ratio is 

S/N

, is given by 

 

 

For example, a channel of 3000-Hz bandwidth with a signal to thermal noise ratio of 30 dB (typical parameters of the analog part of the telephone system) can never transmit much more than 30,000 bps, no matter how many or how few signal levels are used and no matter how often or how infrequently samples are taken. Shannon's result was derived from information-theory arguments and applies to any channel subject to thermal noise. Counterexamples should be treated in the same category as perpetual motion machines. It should be noted that this is only an upper bound and real systems rarely achieve it. 

72




2.2 Guided Transmission Media 

The purpose of the physical layer is to transport a raw bit stream from one machine to another. Various physical media can be used for the actual transmission. Each one has its own niche in terms of bandwidth, delay, cost, and ease of installation and maintenance. Media are roughly grouped into guided media, such as copper wire and fiber optics, and unguided media, such as radio and lasers through the air. We will look at all of these in the following sections. 

2.2.1 Magnetic Media 

One of the most common ways to transport data from one computer to another is to write them onto magnetic tape or removable media (e.g., recordable DVDs), physically transport the tape or disks to the destination machine, and read them back in again. Although this method is not as sophisticated as using a geosynchronous communication satellite, it is often more cost effective, especially for applications in which high bandwidth or cost per bit transported is the key factor. 

A simple calculation will make this point clear. An industry standard Ultrium tape can hold 200 gigabytes. A box 60 x 60 x 60 cm can hold about 1000 of these tapes, for a total capacity of 200 terabytes, or 1600 terabits (1.6 petabits). A box of tapes can be delivered anywhere in the United States in 24 hours by Federal Express and other companies. The effective bandwidth of this transmission is 1600 terabits/86,400 sec, or 19 Gbps. If the destination is only an hour away by road, the bandwidth is increased to over 400 Gbps. No computer network can even approach this. 

For a bank with many gigabytes of data to be backed up daily on a second machine (so the bank can continue to function even in the face of a major flood or earthquake), it is likely that no other transmission technology can even begin to approach magnetic tape for performance. Of course, networks are getting faster, but tape densities are increasing, too. 

If we now look at cost, we get a similar picture. The cost of an Ultrium tape is around $40 when bought in bulk. A tape can be reused at least ten times, so the tape cost is maybe $4000 per box per usage. Add to this another $1000 for shipping (probably much less), and we have a cost of roughly $5000 to ship 200 TB. This amounts to shipping a gigabyte for under 3 cents. No network can beat that. The moral of the story is: 

Never underestimate the bandwidth of a station wagon full of tapes hurtling down the highway

2.2.2 Twisted Pair 

Although the bandwidth characteristics of magnetic tape are excellent, the delay characteristics are poor. Transmission time is measured in minutes or hours, not milliseconds. For many applications an on-line connection is needed. One of the oldest and still most common transmission media is 

twisted pair

. A twisted pair consists of two insulated copper wires, typically about 1 mm thick. The wires are twisted together in a helical form, just like a DNA molecule. Twisting is done because two parallel wires constitute a fine antenna. When the wires are twisted, the waves from different twists cancel out, so the wire radiates less effectively. 

The most common application of the twisted pair is the telephone system. Nearly all telephones are connected to the telephone company (telco) office by a twisted pair. Twisted pairs can run several kilometers without amplification, but for longer distances, repeaters are needed. When many twisted pairs run in parallel for a substantial distance, such as all the wires coming from an apartment building to the telephone company office, they are bundled together and encased in a protective sheath. The pairs in these bundles would interfere with one another if it were not for the twisting. In parts of the world where telephone lines run on poles above ground, it is common to see bundles several centimeters in diameter. 

Twisted pairs can be used for transmitting either analog or digital signals. The bandwidth depends on the thickness of the wire and the distance traveled, but several megabits/sec can be achieved for a few kilometers in many cases. Due to their adequate performance and low cost, twisted pairs are widely used and are likely to remain so for years to come. 

73




Twisted pair cabling comes in several varieties, two of which are important for computer networks. 

Category 3

 twisted pairs consist of two insulated wires gently twisted together. Four such pairs are typically grouped in a plastic sheath to protect the wires and keep them together. Prior to about 1988, most office buildings had one category 3 cable running from a central 

wiring closet

 on each floor into each office. This scheme allowed up to four regular telephones or two multiline telephones in each office to connect to the telephone company equipment in the wiring closet. 

Starting around 1988, the more advanced 

category 5

 twisted pairs were introduced. They are similar to category 3 pairs, but with more twists per centimeter, which results in less crosstalk and a better-quality signal over longer distances, making them more suitable for high-speed computer communication. Up-and-coming categories are 6 and 7, which are capable of handling signals with bandwidths of 250 MHz and 600 MHz, respectively (versus a mere 16 MHz and 100 MHz for categories 3 and 5, respectively). 

All of these wiring types are often referred to as 

UTP

 (

Unshielded Twisted Pair

), to contrast them with the bulky, expensive, shielded twisted pair cables IBM introduced in the early 1980s, but which have not proven popular outside of IBM installations. Twisted pair cabling is illustrated in 

Fig. 2-3

. 

Figure 2-3. (a) Category 3 UTP. (b) Category 5 UTP. 

 

2.2.3 Coaxial Cable 

Another common transmission medium is the 

coaxial cable

 (known to its many friends as just ''coax'' and pronounced ''co-ax''). It has better shielding than twisted pairs, so it can span longer distances at higher speeds. Two kinds of coaxial cable are widely used. One kind, 50-ohm cable, is commonly used when it is intended for digital transmission from the start. The other kind, 75-ohm cable, is commonly used for analog transmission and cable television but is becoming more important with the advent of Internet over cable. This distinction is based on historical, rather than technical, factors (e.g., early dipole antennas had an impedance of 300 ohms, and it was easy to use existing 4:1 impedance matching transformers). 

A coaxial cable consists of a stiff copper wire as the core, surrounded by an insulating material. The insulator is encased by a cylindrical conductor, often as a closely-woven braided mesh. The outer conductor is covered in a protective plastic sheath. A cutaway view of a coaxial cable is shown in 

Fig. 2-4

. 

Figure 2-4. A coaxial cable. 

 

The construction and shielding of the coaxial cable give it a good combination of high bandwidth and excellent noise immunity. The bandwidth possible depends on the cable quality, length, and signal-to-noise ratio of the data signal. Modern cables have a bandwidth of close to 1 GHz. Coaxial cables used to be widely used within the telephone system for long-distance lines but have now largely been replaced by fiber optics on long-haul routes. Coax is still widely used for cable television and metropolitan area networks, however. 

74




2.2.4 Fiber Optics 

Many people in the computer industry take enormous pride in how fast computer technology is improving. The original (1981) IBM PC ran at a clock speed of 4.77 MHz. Twenty years later, PCs could run at 2 GHz, a gain of a factor of 20 per decade. Not too bad. 

In the same period, wide area data communication went from 56 kbps (the ARPANET) to 1 Gbps (modern optical communication), a gain of more than a factor of 125 per decade, while at the same time the error rate went from 10

-5

 per bit to almost zero. 

Furthermore, single CPUs are beginning to approach physical limits, such as speed of light and heat dissipation problems. In contrast, with 

current

 fiber technology, the achievable bandwidth is certainly in excess of 50,000 Gbps (50 Tbps) and many people are looking very hard for better technologies and materials. The current practical signaling limit of about 10 Gbps is due to our inability to convert between electrical and optical signals any faster, although in the laboratory, 100 Gbps has been achieved on a single fiber. 

In the race between computing and communication, communication won. The full implications of essentially infinite bandwidth (although not at zero cost) have not yet sunk in to a generation of computer scientists and engineers taught to think in terms of the low Nyquist and Shannon limits imposed by copper wire. The new conventional wisdom should be that all computers are hopelessly slow and that networks should try to avoid computation at all costs, no matter how much bandwidth that wastes. In this section we will study fiber optics to see how that transmission technology works. 

An optical transmission system has three key components: the light source, the transmission medium, and the detector. Conventionally, a pulse of light indicates a 1 bit and the absence of light indicates a 0 bit. The transmission medium is an ultra-thin fiber of glass. The detector generates an electrical pulse when light falls on it. By attaching a light source to one end of an optical fiber and a detector to the other, we have a unidirectional data transmission system that accepts an electrical signal, converts and transmits it by light pulses, and then reconverts the output to an electrical signal at the receiving end. 

This transmission system would leak light and be useless in practice except for an interesting principle of physics. When a light ray passes from one medium to another, for example, from fused silica to air, the ray is refracted (bent) at the silica/air boundary, as shown in 

Fig. 2-5(a)

. Here we see a light ray incident on the boundary at an angle a

1

 emerging at an angle b

1

.

 The amount of refraction depends on the properties of the two media (in particular, their indices of refraction). For angles of incidence above a certain critical value, the light is refracted back into the silica; none of it escapes into the air. Thus, a light ray incident at or above the critical angle is trapped inside the fiber, as shown in 

Fig. 2-5(b)

, and can propagate for many kilometers with virtually no loss. 

Figure 2-5. (a) Three examples of a light ray from inside a silica fiber impinging on the air/silica boundary at different angles. (b) Light trapped by total internal reflection. 

 

The sketch of 

Fig. 2-5(b)

 shows only one trapped ray, but since any light ray incident on the boundary above the critical angle will be reflected internally, many different rays will be bouncing around at different angles. Each ray is said to have a different 

mode

, so a fiber having this property is called a 

multimode fiber

. 

75




However, if the fiber's diameter is reduced to a few wavelengths of light, the fiber acts like a wave guide, and the light can propagate only in a straight line, without bouncing, yielding a 

single-mode fiber

. Single-mode fibers are more expensive but are widely used for longer distances. Currently available single-mode fibers can transmit data at 50 Gbps for 100 km without amplification. Even higher data rates have been achieved in the laboratory for shorter distances. 

Transmission of Light through Fiber 

Optical fibers are made of glass, which, in turn, is made from sand, an inexpensive raw material available in unlimited amounts. Glassmaking was known to the ancient Egyptians, but their glass had to be no more than 1 mm thick or the light could not shine through. Glass transparent enough to be useful for windows was developed during the Renaissance. The glass used for modern optical fibers is so transparent that if the oceans were full of it instead of water, the seabed would be as visible from the surface as the ground is from an airplane on a clear day. 

The attenuation of light through glass depends on the wavelength of the light (as well as on some physical properties of the glass). For the kind of glass used in fibers, the attenuation is shown in 

Fig. 2-6

 in decibels per linear kilometer of fiber. The attenuation in decibels is given by the formula 

Figure 2-6. Attenuation of light through fiber in the infrared region. 

 

 

 

For example, a factor of two loss gives an attenuation of 10 log

10

 2 = 3 dB. The figure shows the near infrared part of the spectrum, which is what is used in practice. Visible light has slightly shorter wavelengths, from 0.4 to 0.7 microns (1 micron is 10

-6

 meters). The true metric purist would refer to these wavelengths as 400 nm to 700 nm, but we will stick with traditional usage. 

Three wavelength bands are used for optical communication. They are centered at 0.85, 1.30, and 1.55 microns, respectively. The last two have good attenuation properties (less than 5 percent loss per kilometer). The 0.85 micron band has higher attenuation, but at that wavelength the lasers and electronics can be made from the same material (gallium arsenide). All three bands are 25,000 to 30,000 GHz wide. 

Light pulses sent down a fiber spread out in length as they propagate. This spreading is called 

chromatic dispersion

. The amount of it is wavelength dependent. One way to keep these spread-out pulses from overlapping is to increase the distance between them, but this can be done only by reducing the signaling rate. 

76




Fortunately, it has been discovered that by making the pulses in a special shape related to the reciprocal of the hyperbolic cosine, nearly all the dispersion effects cancel out, and it is possible to send pulses for thousands of kilometers without appreciable shape distortion. These pulses are called 

solitons

. A considerable amount of research is going on to take solitons out of the lab and into the field. 

Fiber Cables 

Fiber optic cables are similar to coax, except without the braid. 

Figure 2-7(a)

 shows a single fiber viewed from the side. At the center is the glass core through which the light propagates. In multimode fibers, the core is typically 50 microns in diameter, about the thickness of a human hair. In single-mode fibers, the core is 8 to 10 microns. 

Figure 2-7. (a) Side view of a single fiber. (b) End view of a sheath with three fibers. 

 

The core is surrounded by a glass cladding with a lower index of refraction than the core, to keep all the light in the core. Next comes a thin plastic jacket to protect the cladding. Fibers are typically grouped in bundles, protected by an outer sheath. 

Figure 2-7(b)

 shows a sheath with three fibers. 

Terrestrial fiber sheaths are normally laid in the ground within a meter of the surface, where they are occasionally subject to attacks by backhoes or gophers. Near the shore, transoceanic fiber sheaths are buried in trenches by a kind of seaplow. In deep water, they just lie on the bottom, where they can be snagged by fishing trawlers or attacked by giant squid. 

Fibers can be connected in three different ways. First, they can terminate in connectors and be plugged into fiber sockets. Connectors lose about 10 to 20 percent of the light, but they make it easy to reconfigure systems. 

Second, they can be spliced mechanically. Mechanical splices just lay the two carefully-cut ends next to each other in a special sleeve and clamp them in place. Alignment can be improved by passing light through the junction and then making small adjustments to maximize the signal. Mechanical splices take trained personnel about 5 minutes and result in a 10 percent light loss. 

Third, two pieces of fiber can be fused (melted) to form a solid connection. A fusion splice is almost as good as a single drawn fiber, but even here, a small amount of attenuation occurs. 

For all three kinds of splices, reflections can occur at the point of the splice, and the reflected energy can interfere with the signal. 

Two kinds of light sources are typically used to do the signaling, LEDs (Light Emitting Diodes) and semiconductor lasers. They have different properties, as shown in 

Fig. 2-8

. They can be tuned in wavelength by inserting Fabry-Perot or Mach-Zehnder interferometers between the source and the fiber. Fabry-Perot interferometers are simple resonant cavities consisting of two parallel mirrors. The light is incident perpendicular to the mirrors. The length of the cavity selects out those wavelengths that fit inside an integral number of times. Mach-Zehnder interferometers separate the light into two beams. The two beams travel slightly different distances. They are recombined at the end and are in phase for only certain wavelengths. 

Figure 2-8. A comparison of semiconductor diodes and LEDs as light sources. 

77




 

The receiving end of an optical fiber consists of a photodiode, which gives off an electrical pulse when struck by light. The typical response time of a photodiode is 1 nsec, which limits data rates to about 1 Gbps. Thermal noise is also an issue, so a pulse of light must carry enough energy to be detected. By making the pulses powerful enough, the error rate can be made arbitrarily small. 

Fiber Optic Networks 

Fiber optics can be used for LANs as well as for long-haul transmission, although tapping into it is more complex than connecting to an Ethernet. One way around the problem is to realize that a ring network is really just a collection of point-to-point links, as shown in 

Fig. 2-9

. The interface at each computer passes the light pulse stream through to the next link and also serves as a T junction to allow the computer to send and accept messages. 

Figure 2-9. A fiber optic ring with active repeaters. 

 

Two types of interfaces are used. A passive interface consists of two taps fused onto the main fiber. One tap has an LED or laser diode at the end of it (for transmitting), and the other has a photodiode (for receiving). The tap itself is completely passive and is thus extremely reliable because a broken LED or photodiode does not break the ring. It just takes one computer off-line. 

The other interface type, shown in 

Fig. 2-9

, is the 

active repeater

. The incoming light is converted to an electrical signal, regenerated to full strength if it has been weakened, and retransmitted as light. The interface with the computer is an ordinary copper wire that comes into the signal regenerator. Purely optical repeaters are now being used, too. These devices do not require the optical to electrical to optical conversions, which means they can operate at extremely high bandwidths. 

If an active repeater fails, the ring is broken and the network goes down. On the other hand, since the signal is regenerated at each interface, the individual computer-to-computer links can be kilometers long, with virtually no limit on the total size of the ring. The passive interfaces lose light at each junction, so the number of computers and total ring length are greatly restricted. 

A ring topology is not the only way to build a LAN using fiber optics. It is also possible to have hardware broadcasting by using the 

passive star

 construction of 

Fig. 2-10

. In this design, each interface has a fiber running from its transmitter to a silica cylinder, with the incoming fibers fused to one end of the cylinder. Similarly, fibers fused to the other end of the cylinder are run to each of the receivers. Whenever an interface emits a light pulse, it is diffused inside the passive star to illuminate all the receivers, thus achieving broadcast. 

78




In effect, the passive star combines all the incoming signals and transmits the merged result on all lines. Since the incoming energy is divided among all the outgoing lines, the number of nodes in the network is limited by the sensitivity of the photodiodes. 

Figure 2-10. A passive star connection in a fiber optics network. 

 

Comparison of Fiber Optics and Copper Wire 

It is instructive to compare fiber to copper. Fiber has many advantages. To start with, it can handle much higher bandwidths than copper. This alone would require its use in high-end networks. Due to the low attenuation, repeaters are needed only about every 50 km on long lines, versus about every 5 km for copper, a substantial cost saving. Fiber also has the advantage of not being affected by power surges, electromagnetic interference, or power failures. Nor is it affected by corrosive chemicals in the air, making it ideal for harsh factory environments. 

Oddly enough, telephone companies like fiber for a different reason: it is thin and lightweight. Many existing cable ducts are completely full, so there is no room to add new capacity. Removing all the copper and replacing it by fiber empties the ducts, and the copper has excellent resale value to copper refiners who see it as very high grade ore. Also, fiber is much lighter than copper. One thousand twisted pairs 1 km long weigh 8000 kg. Two fibers have more capacity and weigh only 100 kg, which greatly reduces the need for expensive mechanical support systems that must be maintained. For new routes, fiber wins hands down due to its much lower installation cost. 

Finally, fibers do not leak light and are quite difficult to tap. These properties gives fiber excellent security against potential wiretappers. 

On the downside, fiber is a less familiar technology requiring skills not all engineers have, and fibers can be damaged easily by being bent too much. Since optical transmission is inherently unidirectional, two-way communication requires either two fibers or two frequency bands on one fiber. Finally, fiber interfaces cost more than electrical interfaces. Nevertheless, the future of all fixed data communication for distances of more than a few meters is clearly with fiber. For a discussion of all aspects of fiber optics and their networks, see (Hecht, 2001). 

2.3 Wireless Transmission 

Our age has given rise to information junkies: people who need to be on-line all the time. For these mobile users, twisted pair, coax, and fiber optics are of no use. They need to get their hits of data for their laptop, 

79




notebook, shirt pocket, palmtop, or wristwatch computers without being tethered to the terrestrial communication infrastructure. For these users, wireless communication is the answer. In the following sections, we will look at wireless communication in general, as it has many other important applications besides providing connectivity to users who want to surf the Web from the beach. 

Some people believe that the future holds only two kinds of communication: fiber and wireless. All fixed (i.e., nonmobile) computers, telephones, faxes, and so on will use fiber, and all mobile ones will use wireless. 

Wireless has advantages for even fixed devices in some circumstances. For example, if running a fiber to a building is difficult due to the terrain (mountains, jungles, swamps, etc.), wireless may be better. It is noteworthy that modern wireless digital communication began in the Hawaiian Islands, where large chunks of Pacific Ocean separated the users and the telephone system was inadequate. 

2.3.1 The Electromagnetic Spectrum 

When electrons move, they create electromagnetic waves that can propagate through space (even in a vacuum). These waves were predicted by the British physicist James Clerk Maxwell in 1865 and first observed by the German physicist Heinrich Hertz in 1887. The number of oscillations per second of a wave is called its 

frequency

, 

f

, and is measured in 

Hz

 (in honor of Heinrich Hertz). The distance between two consecutive maxima (or minima) is called the 

wavelength

, which is universally designated by the Greek letter l (lambda). 

When an antenna of the appropriate size is attached to an electrical circuit, the electromagnetic waves can be broadcast efficiently and received by a receiver some distance away. All wireless communication is based on this principle. 

In vacuum, all electromagnetic waves travel at the same speed, no matter what their frequency. This speed, usually called the 

speed of light

, 

c

, is approximately 3 x 10

8

 m/sec, or about 1 foot (30 cm) per nanosecond. (A case could be made for redefining the foot as the distance light travels in a vacuum in 1 nsec rather than basing it on the shoe size of some long-dead king.) In copper or fiber the speed slows to about 2/3 of this value and becomes slightly frequency dependent. The speed of light is the ultimate speed limit. No object or signal can ever move faster than it. 

The fundamental relation between 

f

, l, and 

c

 (in vacuum) is 

Equation 2 

 

 

Since 

c

 is a constant, if we know 

f

, we can find l, and vice versa. As a rule of thumb, when l is in meters and 

f

 is in MHz, l

f

 

300. For example, 100-MHz waves are about 3 meters long, 1000-MHz waves are 0.3-meters long, and 0.1-meter waves have a frequency of 3000 MHz. 

The electromagnetic spectrum is shown in 

Fig. 2-11

. The radio, microwave, infrared, and visible light portions of the spectrum can all be used for transmitting information by modulating the amplitude, frequency, or phase of the waves. Ultraviolet light, X-rays, and gamma rays would be even better, due to their higher frequencies, but they are hard to produce and modulate, do not propagate well through buildings, and are dangerous to living things. The bands listed at the bottom of 

Fig. 2-11

 are the official ITU names and are based on the wavelengths, so the LF band goes from 1 km to 10 km (approximately 30 kHz to 300 kHz). The terms LF, MF, and HF refer to low, medium, and high frequency, respectively. Clearly, when the names were assigned, nobody expected to go above 10 MHz, so the higher bands were later named the Very, Ultra, Super, Extremely, and Tremendously High Frequency bands. Beyond that there are no names, but Incredibly, Astonishingly, and Prodigiously high frequency (IHF, AHF, and PHF) would sound nice. 

Figure 2-11. The electromagnetic spectrum and its uses for communication. 

80




 

The amount of information that an electromagnetic wave can carry is related to its bandwidth. With current technology, it is possible to encode a few bits per Hertz at low frequencies, but often as many as 8 at high frequencies, so a coaxial cable with a 750 MHz bandwidth can carry several gigabits/sec. From 

Fig. 2-11

 it should now be obvious why networking people like fiber optics so much. 

If we solve 

Eq. (2-2)

 for 

f

 and differentiate with respect to l, we get 

 

 

If we now go to finite differences instead of differentials and only look at absolute values, we get 

Equation 2 

 

 

Thus, given the width of a wavelength band, Dl, we can compute the corresponding frequency band, D

f

, and from that the data rate the band can produce. The wider the band, the higher the data rate. As an example, consider the 1.30-micron band of 

Fig. 2-6

. Here we have l=1.3 x 10

-6

 and Dl = 0.17 x 10

-6

,soD

f

 is about 30 THz. At, say, 8 bits/Hz, we get 240 Tbps. 

Most transmissions use a narrow frequency band (i.e., D

f/f

 

1) to get the best reception (many watts/Hz). However, in some cases, a wide band is used, with two variations. In 

frequency hopping spread spectrum

, the transmitter hops from frequency to frequency hundreds of times per second. It is popular for military communication because it makes transmissions hard to detect and next to impossible to jam. It also offers good resistance to multipath fading because the direct signal always arrives at the receiver first. Reflected signals follow a longer path and arrive later. By then the receiver may have changed frequency and no longer accepts signals on the previous frequency, thus eliminating interference between the direct and reflected signals. In recent years, this technique has also been applied commercially—both 802.11 and Bluetooth use it, for example. 

As a curious footnote, the technique was co-invented by the Austrian-born sex goddess Hedy Lamarr, the first woman to appear nude in a motion picture (the 1933 Czech film 

Extase

). Her first husband was an armaments manufacturer who told her how easy it was to block the radio signals then used to control torpedos. When she 

81




discovered that he was selling weapons to Hitler, she was horrified, disguised herself as a maid to escape him, and fled to Hollywood to continue her career as a movie actress. In her spare time, she invented frequency hopping to help the Allied war effort. Her scheme used 88 frequencies, the number of keys (and frequencies) on the piano. For their invention, she and her friend, the musical composer George Antheil, received U.S. patent 2,292,387. However, they were unable to convince the U.S. Navy that their invention had any practical use and never received any royalties. Only years after the patent expired did it become popular. 

The other form of spread spectrum, 

direct sequence spread spectrum

, which spreads the signal over a wide frequency band, is also gaining popularity in the commercial world. In particular, some second-generation mobile phones use it, and it will become dominant with the third generation, thanks to its good spectral efficiency, noise immunity, and other properties. Some wireless LANs also use it. We will come back to spread spectrum later in this chapter. For a fascinating and detailed history of spread spectrum communication, see (Scholtz, 1982). 

For the moment, we will assume that all transmissions use a narrow frequency band. We will now discuss how the various parts of the electromagnetic spectrum of 

Fig. 2-11

 are used, starting with radio. 

2.3.2 Radio Transmission 

Radio waves are easy to generate, can travel long distances, and can penetrate buildings easily, so they are widely used for communication, both indoors and outdoors. Radio waves also are omnidirectional, meaning that they travel in all directions from the source, so the transmitter and receiver do not have to be carefully aligned physically. 

Sometimes omnidirectional radio is good, but sometimes it is bad. In the 1970s, General Motors decided to equip all its new Cadillacs with computer-controlled antilock brakes. When the driver stepped on the brake pedal, the computer pulsed the brakes on and off instead of locking them on hard. One fine day an Ohio Highway Patrolman began using his new mobile radio to call headquarters, and suddenly the Cadillac next to him began behaving like a bucking bronco. When the officer pulled the car over, the driver claimed that he had done nothing and that the car had gone crazy. 

Eventually, a pattern began to emerge: Cadillacs would sometimes go berserk, but only on major highways in Ohio and then only when the Highway Patrol was watching. For a long, long time General Motors could not understand why Cadillacs worked fine in all the other states and also on minor roads in Ohio. Only after much searching did they discover that the Cadillac's wiring made a fine antenna for the frequency used by the Ohio Highway Patrol's new radio system. 

The properties of radio waves are frequency dependent. At low frequencies, radio waves pass through obstacles well, but the power falls off sharply with distance from the source, roughly as 1

/r

2

 in air. At high frequencies, radio waves tend to travel in straight lines and bounce off obstacles. They are also absorbed by rain. At all frequencies, radio waves are subject to interference from motors and other electrical equipment. 

Due to radio's ability to travel long distances, interference between users is a problem. For this reason, all governments tightly license the use of radio transmitters, with one exception, discussed below. 

In the VLF, LF, and MF bands, radio waves follow the ground, as illustrated in 

Fig. 2-12(a)

. These waves can be detected for perhaps 1000 km at the lower frequencies, less at the higher ones. AM radio broadcasting uses the MF band, which is why the ground waves from Boston AM radio stations cannot be heard easily in New York. Radio waves in these bands pass through buildings easily, which is why portable radios work indoors. The main problem with using these bands for data communication is their low bandwidth [see 

Eq. (2-3)

]. 

Figure 2-12. (a) In the VLF, LF, and MF bands, radio waves follow the curvature of the earth. (b) In the HF band, they bounce off the ionosphere. 

82




 

In the HF and VHF bands, the ground waves tend to be absorbed by the earth. However, the waves that reach the ionosphere, a layer of charged particles circling the earth at a height of 100 to 500 km, are refracted by it and sent back to earth, as shown in 

Fig. 2-12(b)

. Under certain atmospheric conditions, the signals can bounce several times. Amateur radio operators (hams) use these bands to talk long distance. The military also communicate in the HF and VHF bands. 

2.3.3 Microwave Transmission 

Above 100 MHz, the waves travel in nearly straight lines and can therefore be narrowly focused. Concentrating all the energy into a small beam by means of a parabolic antenna (like the familiar satellite TV dish) gives a much higher signal-to-noise ratio, but the transmitting and receiving antennas must be accurately aligned with each other. In addition, this directionality allows multiple transmitters lined up in a row to communicate with multiple receivers in a row without interference, provided some minimum spacing rules are observed. Before fiber optics, for decades these microwaves formed the heart of the long-distance telephone transmission system. In fact, MCI, one of AT&T's first competitors after it was deregulated, built its entire system with microwave communications going from tower to tower tens of kilometers apart. Even the company's name reflected this (MCI stood for Microwave Communications, Inc.). MCI has since gone over to fiber and merged with WorldCom. 

Since the microwaves travel in a straight line, if the towers are too far apart, the earth will get in the way (think about a San Francisco to Amsterdam link). Consequently, repeaters are needed periodically. The higher the towers are, the farther apart they can be. The distance between repeaters goes up very roughly with the square root of the tower height. For 100-meter-high towers, repeaters can be spaced 80 km apart. 

Unlike radio waves at lower frequencies, microwaves do not pass through buildings well. In addition, even though the beam may be well focused at the transmitter, there is still some divergence in space. Some waves may be refracted off low-lying atmospheric layers and may take slightly longer to arrive than the direct waves. The delayed waves may arrive out of phase with the direct wave and thus cancel the signal. This effect is called 

multipath fading

 and is often a serious problem. It is weather and frequency dependent. Some operators keep 10 percent of their channels idle as spares to switch on when multipath fading wipes out some frequency band temporarily. 

The demand for more and more spectrum drives operators to yet higher frequencies. Bands up to 10 GHz are now in routine use, but at about 4 GHz a new problem sets in: absorption by water. These waves are only a few centimeters long and are absorbed by rain. This effect would be fine if one were planning to build a huge outdoor microwave oven for roasting passing birds, but for communication, it is a severe problem. As with multipath fading, the only solution is to shut off links that are being rained on and route around them. 

In summary, microwave communication is so widely used for long-distance telephone communication, mobile phones, television distribution, and other uses that a severe shortage of spectrum has developed. It has several significant advantages over fiber. The main one is that no right of way is needed, and by buying a small plot of ground every 50 km and putting a microwave tower on it, one can bypass the telephone system and communicate directly. This is how MCI managed to get started as a new long-distance telephone company so quickly. (Sprint went a completely different route: it was formed by the Southern Pacific Railroad, which already owned a large amount of right of way and just buried fiber next to the tracks.) 

Microwave is also relatively inexpensive. Putting up two simple towers (may be just big poles with four guy wires) and putting antennas on each one may be cheaper than burying 50 km of fiber through a congested urban area or up over a mountain, and it may also be cheaper than leasing the telephone company's fiber, especially if the telephone company has not yet even fully paid for the copper it ripped out when it put in the fiber. 

83




The Politics of the Electromagnetic Spectrum 

To prevent total chaos, there are national and international agreements about who gets to use which frequencies. Since everyone wants a higher data rate, everyone wants more spectrum. National governments allocate spectrum for AM and FM radio, television, and mobile phones, as well as for telephone companies, police, maritime, navigation, military, government, and many other competing users. Worldwide, an agency of ITU-R (WARC) tries to coordinate this allocation so devices that work in multiple countries can be manufactured. However, countries are not bound by ITU-R's recommendations, and the FCC (Federal Communication Commission), which does the allocation for the United States, has occasionally rejected ITU-R's recommendations (usually because they required some politically-powerful group giving up some piece of the spectrum). 

Even when a piece of spectrum has been allocated to some use, such as mobile phones, there is the additional issue of which carrier is allowed to use which frequencies. Three algorithms were widely used in the past. The oldest algorithm, often called the 

beauty contest

, requires each carrier to explain why its proposal serves the public interest best. Government officials then decide which of the nice stories they enjoy most. Having some government official award property worth billions of dollars to his favorite company often leads to bribery, corruption, nepotism, and worse. Furthermore, even a scrupulously honest government official who thought that a foreign company could do a better job than any of the national companies would have a lot of explaining to do. 

This observation led to algorithm 2, holding a lottery among the interested companies. The problem with that idea is that companies with no interest in using the spectrum can enter the lottery. If, say, a fast food restaurant or shoe store chain wins, it can resell the spectrum to a carrier at a huge profit and with no risk. 

Bestowing huge windfalls on alert, but otherwise random, companies has been severely criticized by many, which led to algorithm 3: auctioning off the bandwidth to the highest bidder. When England auctioned off the frequencies needed for third-generation mobile systems in 2000, they expected to get about $4 billion. They actually received about $40 billion because the carriers got into a feeding frenzy, scared to death of missing the mobile boat. This event switched on nearby governments' greedy bits and inspired them to hold their own auctions. It worked, but it also left some of the carriers with so much debt that they are close to bankruptcy. Even in the best cases, it will take many years to recoup the licensing fee. 

A completely different approach to allocating frequencies is to not allocate them at all. Just let everyone transmit at will but regulate the power used so that stations have such a short range they do not interfere with each other. Accordingly, most governments have set aside some frequency bands, called the 

ISM

 (

Industrial, Scientific, Medical

) bands for unlicensed usage. Garage door openers, cordless phones, radio-controlled toys, wireless mice, and numerous other wireless household devices use the ISM bands. To minimize interference between these uncoordinated devices, the FCC mandates that all devices in the ISM bands use spread spectrum techniques. Similar rules apply in other countries 

The location of the ISM bands varies somewhat from country to country. In the United States, for example, devices whose power is under 1 watt can use the bands shown in 

Fig. 2-13

 without requiring a FCC license. The 900-MHz band works best, but it is crowded and not available worldwide. The 2.4-GHz band is available in most countries, but it is subject to interference from microwave ovens and radar installations. Bluetooth and some of the 802.11 wireless LANs operate in this band. The 5.7-GHz band is new and relatively undeveloped, so equipment for it is expensive, but since 802.11a uses it, it will quickly become more popular. 

Figure 2-13. The ISM bands in the United States. 

 

84




2.3.4 Infrared and Millimeter Waves 

Unguided infrared and millimeter waves are widely used for short-range communication. The remote controls used on televisions, VCRs, and stereos all use infrared communication. They are relatively directional, cheap, and easy to build but have a major drawback: they do not pass through solid objects (try standing between your remote control and your television and see if it still works). In general, as we go from long-wave radio toward visible light, the waves behave more and more like light and less and less like radio. 

On the other hand, the fact that infrared waves do not pass through solid walls well is also a plus. It means that an infrared system in one room of a building will not interfere with a similar system in adjacent rooms or buildings: you cannot control your neighbor's television with your remote control. Furthermore, security of infrared systems against eavesdropping is better than that of radio systems precisely for this reason. Therefore, no government license is needed to operate an infrared system, in contrast to radio systems, which must be licensed outside the ISM bands. Infrared communication has a limited use on the desktop, for example, connecting notebook computers and printers, but it is not a major player in the communication game. 

2.3.5 Lightwave Transmission 

Unguided optical signaling has been in use for centuries. Paul Revere used binary optical signaling from the Old North Church just prior to his famous ride. A more modern application is to connect the LANs in two buildings via lasers mounted on their rooftops. Coherent optical signaling using lasers is inherently unidirectional, so each building needs its own laser and its own photodetector. This scheme offers very high bandwidth and very low cost. It is also relatively easy to install and, unlike microwave, does not require an FCC license. 

The laser's strength, a very narrow beam, is also its weakness here. Aiming a laser beam 1-mm wide at a target the size of a pin head 500 meters away requires the marksmanship of a latter-day Annie Oakley. Usually, lenses are put into the system to defocus the beam slightly. 

A disadvantage is that laser beams cannot penetrate rain or thick fog, but they normally work well on sunny days. However, the author once attended a conference at a modern hotel in Europe at which the conference organizers thoughtfully provided a room full of terminals for the attendees to read their e-mail during boring presentations. Since the local PTT was unwilling to install a large number of telephone lines for just 3 days, the organizers put a laser on the roof and aimed it at their university's computer science building a few kilometers away. They tested it the night before the conference and it worked perfectly. At 9 a.m. the next morning, on a bright sunny day, the link failed completely and stayed down all day. That evening, the organizers tested it again very carefully, and once again it worked absolutely perfectly. The pattern repeated itself for two more days consistently. 

After the conference, the organizers discovered the problem. Heat from the sun during the daytime caused convection currents to rise up from the roof of the building, as shown in 

Fig. 2-14

. This turbulent air diverted the beam and made it dance around the detector. Atmospheric ''seeing'' like this makes the stars twinkle (which is why astronomers put their telescopes on the tops of mountains—to get above as much of the atmosphere as possible). It is also responsible for shimmering roads on a hot day and the wavy images seen when one looks out above a hot radiator. 

Figure 2-14. Convection currents can interfere with laser communication systems. A bidirectional system with two lasers is pictured here. 

85




 

2.4 Communication Satellites 

In the 1950s and early 1960s, people tried to set up communication systems by bouncing signals off metallized weather balloons. Unfortunately, the received signals were too weak to be of any practical use. Then the U.S. Navy noticed a kind of permanent weather balloon in the sky—the moon—and built an operational system for ship-to-shore communication by bouncing signals off it. 

Further progress in the celestial communication field had to wait until the first communication satellite was launched. The key difference between an artificial satellite and a real one is that the artificial one can amplify the signals before sending them back, turning a strange curiosity into a powerful communication system. 

Communication satellites have some interesting properties that make them attractive for many applications. In its simplest form, a communication satellite can be thought of as a big microwave repeater in the sky. It contains several 

transponders

, each of which listens to some portion of the spectrum, amplifies the incoming signal, and then rebroadcasts it at another frequency to avoid interference with the incoming signal. The downward beams can be broad, covering a substantial fraction of the earth's surface, or narrow, covering an area only hundreds of kilometers in diameter. This mode of operation is known as a 

bent pipe

. 

According to Kepler's law, the orbital period of a satellite varies as the radius of the orbit to the 3/2 power. The higher the satellite, the longer the period. Near the surface of the earth, the period is about 90 minutes. Consequently, low-orbit satellites pass out of view fairly quickly, so many of them are needed to provide continuous coverage. At an altitude of about 35,800 km, the period is 24 hours. At an altitude of 384,000 km, the period is about one month, as anyone who has observed the moon regularly can testify. 

A satellite's period is important, but it is not the only issue in determining where to place it. Another issue is the presence of the Van Allen belts, layers of highly charged particles trapped by the earth's magnetic field. Any satellite flying within them would be destroyed fairly quickly by the highly-energetic charged particles trapped there by the earth's magnetic field. These factors lead to three regions in which satellites can be placed safely. These regions and some of their properties are illustrated in 

Fig. 2-15

. Below we will briefly describe the satellites that inhabit each of these regions. 

Figure 2-15. Communication satellites and some of their properties, including altitude above the earth, round-trip delay time, and number of satellites needed for global coverage. 

86




 

2.4.1 Geostationary Satellites 

In 1945, the science fiction writer Arthur C. Clarke calculated that a satellite at an altitude of 35,800 km in a circular equatorial orbit would appear to remain motionless in the sky. so it would not need to be tracked (Clarke, 1945). He went on to describe a complete communication system that used these (manned) 

geostationary satellites

, including the orbits, solar panels, radio frequencies, and launch procedures. Unfortunately, he concluded that satellites were impractical due to the impossibility of putting power-hungry, fragile, vacuum tube amplifiers into orbit, so he never pursued this idea further, although he wrote some science fiction stories about it. 

The invention of the transistor changed all that, and the first artificial communication satellite, Telstar, was launched in July 1962. Since then, communication satellites have become a multibillion dollar business and the only aspect of outer space that has become highly profitable. These high-flying satellites are often called 

GEO

 (

Geostationary Earth Orbit

) satellites. 

With current technology, it is unwise to have geostationary satellites spaced much closer than 2 degrees in the 360-degree equatorial plane, to avoid interference. With a spacing of 2 degrees, there can only be 360/2 = 180 of these satellites in the sky at once. However, each transponder can use multiple frequencies and polarizations to increase the available bandwidth. 

To prevent total chaos in the sky, orbit slot allocation is done by ITU. This process is highly political, with countries barely out of the stone age demanding ''their'' orbit slots (for the purpose of leasing them to the highest bidder). Other countries, however, maintain that national property rights do not extend up to the moon and that no country has a legal right to the orbit slots above its territory. To add to the fight, commercial telecommunication is not the only application. Television broadcasters, governments, and the military also want a piece of the orbiting pie. 

Modern satellites can be quite large, weighing up to 4000 kg and consuming several kilowatts of electric power produced by the solar panels. The effects of solar, lunar, and planetary gravity tend to move them away from their assigned orbit slots and orientations, an effect countered by on-board rocket motors. This fine-tuning activity is called 

station keeping

. However, when the fuel for the motors has been exhausted, typically in about 10 years, the satellite drifts and tumbles helplessly, so it has to be turned off. Eventually, the orbit decays and the satellite reenters the atmosphere and burns up or occasionally crashes to earth. 

Orbit slots are not the only bone of contention. Frequencies are, too, because the downlink transmissions interfere with existing microwave users. Consequently, ITU has allocated certain frequency bands to satellite users. The main ones are listed in 

Fig. 2-16

. The C band was the first to be designated for commercial satellite traffic. Two frequency ranges are assigned in it, the lower one for downlink traffic (from the satellite) and the upper one for uplink traffic (to the satellite). To allow traffic to go both ways at the same time, two channels are required, one going each way. These bands are already overcrowded because they are also used by the common carriers for terrestrial microwave links. The L and S bands were added by international agreement in 2000. However, they are narrow and crowded. 

87




Figure 2-16. The principal satellite bands. 

 

The next highest band available to commercial telecommunication carriers is the Ku (K under) band. This band is not (yet) congested, and at these frequencies, satellites can be spaced as close as 1 degree. However, another problem exists: rain. Water is an excellent absorber of these short microwaves. Fortunately, heavy storms are usually localized, so using several widely separated ground stations instead of just one circumvents the problem but at the price of extra antennas, extra cables, and extra electronics to enable rapid switching between stations. Bandwidth has also been allocated in the Ka (K above) band for commercial satellite traffic, but the equipment needed to use it is still expensive. In addition to these commercial bands, many government and military bands also exist. 

A modern satellite has around 40 transponders, each with an 80-MHz bandwidth. Usually, each transponder operates as a bent pipe, but recent satellites have some on-board processing capacity, allowing more sophisticated operation. In the earliest satellites, the division of the transponders into channels was static: the bandwidth was simply split up into fixed frequency bands. Nowadays, each transponder beam is divided into time slots, with various users taking turns. We will study these two techniques (frequency division multiplexing and time division multiplexing) in detail later in this chapter. 

The first geostationary satellites had a single spatial beam that illuminated about 1/3 of the earth's surface, called its 

footprint

. With the enormous decline in the price, size, and power requirements of microelectronics, a much more sophisticated broadcasting strategy has become possible. Each satellite is equipped with multiple antennas and multiple transponders. Each downward beam can be focused on a small geographical area, so multiple upward and downward transmissions can take place simultaneously. Typically, these so-called 

spot beams

 are elliptically shaped, and can be as small as a few hundred km in diameter. A communication satellite for the United States typically has one wide beam for the contiguous 48 states, plus spot beams for Alaska and Hawaii. 

A new development in the communication satellite world is the development of low-cost microstations, sometimes called 

VSATs

 (

Very Small Aperture Terminals

) (Abramson, 2000). These tiny terminals have 1-meter or smaller antennas (versus 10 m for a standard GEO antenna) and can put out about 1 watt of power. The uplink is generally good for 19.2 kbps, but the downlink is more often 512 kbps or more. Direct broadcast satellite television uses this technology for one-way transmission. 

In many VSAT systems, the microstations do not have enough power to communicate directly with one another (via the satellite, of course). Instead, a special ground station, the 

hub

, with a large, high-gain antenna is needed to relay traffic between VSATs, as shown in 

Fig. 2-17

. In this mode of operation, either the sender or the receiver has a large antenna and a powerful amplifier. The trade-off is a longer delay in return for having cheaper end-user stations. 

Figure 2-17. VSATs using a hub. 

88




 

VSATs have great potential in rural areas. It is not widely appreciated, but over half the world's population lives over an hour's walk from the nearest telephone. Stringing telephone wires to thousands of small villages is far beyond the budgets of most Third World governments, but installing 1-meter VSAT dishes powered by solar cells is often feasible. VSATs provide the technology that will wire the world. 

Communication satellites have several properties that are radically different from terrestrial point-to-point links. To begin with, even though signals to and from a satellite travel at the speed of light (nearly 300,000 km/sec), the long round-trip distance introduces a substantial delay for GEO satellites. Depending on the distance between the user and the ground station, and the elevation of the satellite above the horizon, the end-to-end transit time is between 250 and 300 msec. A typical value is 270 msec (540 msec for a VSAT system with a hub). 

For comparison purposes, terrestrial microwave links have a propagation delay of roughly 3 µsec/km, and coaxial cable or fiber optic links have a delay of approximately 5 µsec/km. The latter is slower than the former because electromagnetic signals travel faster in air than in solid materials. 

Another important property of satellites is that they are inherently broadcast media. It does not cost more to send a message to thousands of stations within a transponder's footprint than it does to send to one. For some applications, this property is very useful. For example, one could imagine a satellite broadcasting popular Web pages to the caches of a large number of computers spread over a wide area. Even when broadcasting can be simulated with point-to-point lines, satellite broadcasting may be much cheaper. On the other hand, from a security and privacy point of view, satellites are a complete disaster: everybody can hear everything. Encryption is essential when security is required. 

Satellites also have the property that the cost of transmitting a message is independent of the distance traversed. A call across the ocean costs no more to service than a call across the street. Satellites also have excellent error rates and can be deployed almost instantly, a major consideration for military communication. 

2.4.2 Medium-Earth Orbit Satellites 

At much lower altitudes, between the two Van Allen belts, we find the 

MEO

 (

Medium-Earth Orbit

) satellites. As viewed from the earth, these drift slowly in longitude, taking something like 6 hours to circle the earth. Accordingly, they must be tracked as they move through the sky. Because they are lower than the GEOs, they have a smaller footprint on the ground and require less powerful transmitters to reach them. Currently they are not used for telecommunications, so we will not examine them further here. The 24 

GPS

 (

Global Positioning System

) satellites orbiting at about 18,000 km are examples of MEO satellites. 

89




2.4.3 Low-Earth Orbit Satellites 

Moving down in altitude, we come to the 

LEO

 (

Low-Earth Orbit

) satellites. Due to their rapid motion, large numbers of them are needed for a complete system. On the other hand, because the satellites are so close to the earth, the ground stations do not need much power, and the round-trip delay is only a few milliseconds. In this section we will examine three examples, two aimed at voice communication and one aimed at Internet service. 

Iridium 

As mentioned above, for the first 30 years of the satellite era, low-orbit satellites were rarely used because they zip into and out of view so quickly. In 1990, Motorola broke new ground by filing an application with the FCC asking for permission to launch 77 low-orbit satellites for the Iridium project (element 77 is iridium). The plan was later revised to use only 66 satellites, so the project should have been renamed Dysprosium (element 66), but that probably sounded too much like a disease. The idea was that as soon as one satellite went out of view, another would replace it. This proposal set off a feeding frenzy among other communication companies. All of a sudden, everyone wanted to launch a chain of low-orbit satellites. 

After seven years of cobbling together partners and financing, the partners launched the Iridium satellites in 1997. Communication service began in November 1998. Unfortunately, the commercial demand for large, heavy satellite telephones was negligible because the mobile phone network had grown spectacularly since 1990. As a consequence, Iridium was not profitable and was forced into bankruptcy in August 1999 in one of the most spectacular corporate fiascos in history. The satellites and other assets (worth $5 billion) were subsequently purchased by an investor for $25 million at a kind of extraterrestrial garage sale. The Iridium service was restarted in March 2001. 

Iridium's business was (and is) providing worldwide telecommunication service using hand-held devices that communicate directly with the Iridium satellites. It provides voice, data, paging, fax, and navigation service everywhere on land, sea, and air. Customers include the maritime, aviation, and oil exploration industries, as well as people traveling in parts of the world lacking a telecommunications infrastructure (e.g., deserts, mountains, jungles, and some Third World countries). 

The Iridium satellites are positioned at an altitude of 750 km, in circular polar orbits. They are arranged in north-south necklaces, with one satellite every 32 degrees of latitude. With six satellite necklaces, the entire earth is covered, as suggested by 

Fig. 2-18(a)

. People not knowing much about chemistry can think of this arrangement as a very, very big dysprosium atom, with the earth as the nucleus and the satellites as the electrons. 

Figure 2-18. (a) The Iridium satellites form six necklaces around the earth. (b) 1628 moving cells cover the earth. 

 

90




Each satellite has a maximum of 48 cells (spot beams), with a total of 1628 cells over the surface of the earth, as shown in 

Fig. 2-18(b)

. Each satellite has a capacity of 3840 channels, or 253,440 in all. Some of these are used for paging and navigation, while others are used for data and voice. 

An interesting property of Iridium is that communication between distant customers takes place in space, with one satellite relaying data to the next one, as illustrated in 

Fig. 2-19(a)

. Here we see a caller at the North Pole contacting a satellite directly overhead. The call is relayed via other satellites and finally sent down to the callee at the South Pole. 

Figure 2-19. (a) Relaying in space. (b) Relaying on the ground. 

 

Globalstar 

An alternative design to Iridium is Globalstar. It is based on 48 LEO satellites but uses a different switching scheme than that of Iridium. Whereas Iridium relays calls from satellite to satellite, which requires sophisticated switching equipment in the satellites, Globalstar uses a traditional bent-pipe design. The call originating at the North Pole in 

Fig. 2-19(b)

 is sent back to earth and picked up by the large ground station at Santa's Workshop. The call is then routed via a terrestrial network to the ground station nearest the callee and delivered by a bent-pipe connection as shown. The advantage of this scheme is that it puts much of the complexity on the ground, where it is easier to manage. Also, the use of large ground station antennas that can put out a powerful signal and receive a weak one means that lower-powered telephones can be used. After all, the telephone puts out only a few milliwatts of power, so the signal that gets back to the ground station is fairly weak, even after having been amplified by the satellite. 

Teledesic 

Iridium is targeted at telephone users located in odd places. Our next example, 

Teledesic

, is targeted at bandwidth-hungry Internet users all over the world. It was conceived in 1990 by mobile phone pioneer Craig McCaw and Microsoft founder Bill Gates, who was unhappy with the snail's pace at which the world's telephone companies were providing high bandwidth to computer users. The goal of the Teledesic system is to provide millions of concurrent Internet users with an uplink of as much as 100 Mbps and a downlink of up to 720 Mbps using a small, fixed, VSAT-type antenna, completely bypassing the telephone system. To telephone companies, this is pie-in-the-sky. 

The original design was for a system consisting of 288 small-footprint satellites arranged in 12 planes just below the lower Van Allen belt at an altitude of 1350 km. This was later changed to 30 satellites with larger footprints. Transmission occurs in the relatively uncrowded and high-bandwidth Ka band. The system is packet-switched in space, with each satellite capable of routing packets to its neighboring satellites. When a user needs bandwidth to send packets, it is requested and assigned dynamically in about 50 msec. The system is scheduled to go live in 2005 if all goes as planned. 

91




2.4.4 Satellites versus Fiber 

A comparison between satellite communication and terrestrial communication is instructive. As recently as 20 years ago, a case could be made that the future of communication lay with communication satellites. After all, the telephone system had changed little in the past 100 years and showed no signs of changing in the next 100 years. This glacial movement was caused in no small part by the regulatory environment in which the telephone companies were expected to provide good voice service at reasonable prices (which they did), and in return got a guaranteed profit on their investment. For people with data to transmit, 1200-bps modems were available. That was pretty much all there was. 

The introduction of competition in 1984 in the United States and somewhat later in Europe changed all that radically. Telephone companies began replacing their long-haul networks with fiber and introduced high-bandwidth services like ADSL (Asymmetric Digital Subscriber Line). They also stopped their long-time practice of charging artificially-high prices to long-distance users to subsidize local service. 

All of a sudden, terrestrial fiber connections looked like the long-term winner. Nevertheless, communication satellites have some major niche markets that fiber does not (and, sometimes, cannot) address. We will now look at a few of these. 

First, while a single fiber has, in principle, more potential bandwidth than all the satellites ever launched, this bandwidth is not available to most users. The fibers that are now being installed are used within the telephone system to handle many long distance calls at once, not to provide individual users with high bandwidth. With satellites, it is practical for a user to erect an antenna on the roof of the building and completely bypass the telephone system to get high bandwidth. Teledesic is based on this idea. 

A second niche is for mobile communication. Many people nowadays want to communicate while jogging, driving, sailing, and flying. Terrestrial fiber optic links are of no use to them, but satellite links potentially are. It is possible, however, that a combination of cellular radio and fiber will do an adequate job for most users (but probably not for those airborne or at sea). 

A third niche is for situations in which broadcasting is essential. A message sent by satellite can be received by thousands of ground stations at once. For example, an organization transmitting a stream of stock, bond, or commodity prices to thousands of dealers might find a satellite system to be much cheaper than simulating broadcasting on the ground. 

A fourth niche is for communication in places with hostile terrain or a poorly developed terrestrial infrastructure. Indonesia, for example, has its own satellite for domestic telephone traffic. Launching one satellite was cheaper than stringing thousands of undersea cables among the 13,677 islands in the archipelago. 

A fifth niche market for satellites is to cover areas where obtaining the right of way for laying fiber is difficult or unduly expensive. 

Sixth, when rapid deployment is critical, as in military communication systems in time of war, satellites win easily. 

In short, it looks like the mainstream communication of the future will be terrestrial fiber optics combined with cellular radio, but for some specialized uses, satellites are better. However, there is one caveat that applies to all of this: economics. Although fiber offers more bandwidth, it is certainly possible that terrestrial and satellite communication will compete aggressively on price. If advances in technology radically reduce the cost of deploying a satellite (e.g., some future space shuttle can toss out dozens of satellites on one launch) or low-orbit satellites catch on in a big way, it is not certain that fiber will win in all markets. 

2.5 The Public Switched Telephone Network 

When two computers owned by the same company or organization and located close to each other need to communicate, it is often easiest just to run a cable between them. LANs work this way. However, when the distances are large or there are many computers or the cables have to pass through a public road or other public right of way, the costs of running private cables are usually prohibitive. Furthermore, in just about every country 

92




in the world, stringing private transmission lines across (or underneath) public property is also illegal. Consequently, the network designers must rely on the existing telecommunication facilities. 

These facilities, especially the 

PSTN

 (

Public Switched Telephone Network

), were usually designed many years ago, with a completely different goal in mind: transmitting the human voice in a more-or-less recognizable form. Their suitability for use in computer-computer communication is often marginal at best, but the situation is rapidly changing with the introduction of fiber optics and digital technology. In any event, the telephone system is so tightly intertwined with (wide area) computer networks, that it is worth devoting some time to studying it. 

To see the order of magnitude of the problem, let us make a rough but illustrative comparison of the properties of a typical computer-computer connection via a local cable and via a dial-up telephone line. A cable running between two computers can transfer data at 10

9

 bps, maybe more. In contrast, a dial-up line has a maximum data rate of 56 kbps, a difference of a factor of almost 20,000. That is the difference between a duck waddling leisurely through the grass and a rocket to the moon. If the dial-up line is replaced by an ADSL connection, there is still a factor of 1000–2000 difference. 

The trouble, of course, is that computer systems designers are used to working with computer systems and when suddenly confronted with another system whose performance (from their point of view) is 3 or 4 orders of magnitude worse, they, not surprising, devoted much time and effort to trying to figure out how to use it efficiently. In the following sections we will describe the telephone system and show how it works. For additional information about the innards of the telephone system see (Bellamy, 2000). 

2.5.1 Structure of the Telephone System 

Soon after Alexander Graham Bell patented the telephone in 1876 (just a few hours ahead of his rival, Elisha Gray), there was an enormous demand for his new invention. The initial market was for the sale of telephones, which came in pairs. It was up to the customer to string a single wire between them. The electrons returned through the earth. If a telephone owner wanted to talk to 

n

 other telephone owners, separate wires had to be strung to all 

n

 houses. Within a year, the cities were covered with wires passing over houses and trees in a wild jumble. It became immediately obvious that the model of connecting every telephone to every other telephone, as shown in 

Fig. 2-20(a)

, was not going to work. 

Figure 2-20. (a) Fully-interconnected network. (b) Centralized switch. (c) Two-level hierarchy. 

 

To his credit, Bell saw this and formed the Bell Telephone Company, which opened its first switching office (in New Haven, Connecticut) in 1878. The company ran a wire to each customer's house or office. To make a call, the customer would crank the phone to make a ringing sound in the telephone company office to attract the attention of an operator, who would then manually connect the caller to the callee by using a jumper cable. The model of a single switching office is illustrated in 

Fig. 2-20(b)

. 

Pretty soon, Bell System switching offices were springing up everywhere and people wanted to make long-distance calls between cities, so the Bell system began to connect the switching offices. The original problem soon returned: to connect every switching office to every other switching office by means of a wire between them quickly became unmanageable, so second-level switching offices were invented. After a while, multiple second-level offices were needed, as illustrated in 

Fig. 2-20(c)

. Eventually, the hierarchy grew to five levels. 

93




By 1890, the three major parts of the telephone system were in place: the switching offices, the wires between the customers and the switching offices (by now balanced, insulated, twisted pairs instead of open wires with an earth return), and the long-distance connections between the switching offices. While there have been improvements in all three areas since then, the basic Bell System model has remained essentially intact for over 100 years. For a short technical history of the telephone system, see (Hawley, 1991). 

Prior to the 1984 breakup of AT&T, the telephone system was organized as a highly-redundant, multilevel hierarchy. The following description is highly simplified but gives the essential flavor nevertheless. Each telephone has two copper wires coming out of it that go directly to the telephone company's nearest 

end office

 (also called a 

local central office

). The distance is typically 1 to 10 km, being shorter in cities than in rural areas. In the United States alone there are about 22,000 end offices. The two-wire connections between each subscriber's telephone and the end office are known in the trade as the 

local loop

. If the world's local loops were stretched out end to end, they would extend to the moon and back 1000 times. 

At one time, 80 percent of AT&T's capital value was the copper in the local loops. AT&T was then, in effect, the world's largest copper mine. Fortunately, this fact was not widely known in the investment community. Had it been known, some corporate raider might have bought AT&T, terminated all telephone service in the United States, ripped out all the wire, and sold the wire to a copper refiner to get a quick payback. 

If a subscriber attached to a given end office calls another subscriber attached to the same end office, the switching mechanism within the office sets up a direct electrical connection between the two local loops. This connection remains intact for the duration of the call. 

If the called telephone is attached to another end office, a different procedure has to be used. Each end office has a number of outgoing lines to one or more nearby switching centers, called 

toll offices

 (or if they are within the same local area, 

tandem offices

). These lines are called 

toll connecting trunks

. If both the caller's and callee's end offices happen to have a toll connecting trunk to the same toll office (a likely occurrence if they are relatively close by), the connection may be established within the toll office. A telephone network consisting only of telephones (the small dots), end offices (the large dots), and toll offices (the squares) is shown in 

Fig. 2-20(c)

. 

If the caller and callee do not have a toll office in common, the path will have to be established somewhere higher up in the hierarchy. Primary, sectional, and regional offices form a network by which the toll offices are connected. The toll, primary, sectional, and regional exchanges communicate with each other via high-bandwidth 

intertoll trunks

 (also called 

interoffice trunks

). The number of different kinds of switching centers and their topology (e.g., can two sectional offices have a direct connection or must they go through a regional office?) varies from country to country depending on the country's telephone density. 

Figure 2-21

 shows how a medium-distance connection might be routed. 

Figure 2-21. A typical circuit route for a medium-distance call. 

 

A variety of transmission media are used for telecommunication. Local loops consist of category 3 twisted pairs nowadays, although in the early days of telephony, uninsulated wires spaced 25 cm apart on telephone poles were common. Between switching offices, coaxial cables, microwaves, and especially fiber optics are widely used. 

In the past, transmission throughout the telephone system was analog, with the actual voice signal being transmitted as an electrical voltage from source to destination. With the advent of fiber optics, digital electronics, and computers, all the trunks and switches are now digital, leaving the local loop as the last piece of analog 

94




technology in the system. Digital transmission is preferred because it is not necessary to accurately reproduce an analog waveform after it has passed through many amplifiers on a long call. Being able to correctly distinguish a 0 from a 1 is enough. This property makes digital transmission more reliable than analog. It is also cheaper and easier to maintain. 

In summary, the telephone system consists of three major components: 

1. Local loops (analog twisted pairs going into houses and businesses). 

2. Trunks (digital fiber optics connecting the switching offices). 

3. Switching offices (where calls are moved from one trunk to another). 

After a short digression on the politics of telephones, we will come back to each of these three components in some detail. The local loops provide everyone access to the whole system, so they are critical. Unfortunately, they are also the weakest link in the system. For the long-haul trunks, the main issue is how to collect multiple calls together and send them out over the same fiber. This subject is called multiplexing, and we will study three different ways to do it. Finally, there are two fundamentally different ways of doing switching; we will look at both. 

2.5.2 The Politics of Telephones 

For decades prior to 1984, the Bell System provided both local and long distance service throughout most of the United States. In the 1970s, the U.S. Federal Government came to believe that this was an illegal monopoly and sued to break it up. The government won, and on January 1, 1984, AT&T was broken up into AT&T Long Lines, 23 

BOC

s (

Bell Operating Companies

), and a few other pieces. The 23 BOCs were grouped into seven regional BOCs (RBOCs) to make them economically viable. The entire nature of telecommunication in the United States was changed overnight by court order (

not

 by an act of Congress). 

The exact details of the divestiture were described in the so-called 

MFJ

 (

Modified Final Judgment

, an oxymoron if ever there was one—if the judgment could be modified, it clearly was not final). This event led to increased competition, better service, and lower long distance prices to consumers and businesses. However, prices for local service rose as the cross subsidies from long-distance calling were eliminated and local service had to become self supporting. Many other countries have now introduced competition along similar lines. 

To make it clear who could do what, the United States was divided up into 164 

LATA

s (

Local Access and Transport Areas

). Very roughly, a LATA is about as big as the area covered by one area code. Within a LATA, there was one 

LEC

 (

Local Exchange Carrier

) that had a monopoly on traditional telephone service within its area. The most important LECs were the BOCs, although some LATAs contained one or more of the 1500 independent telephone companies operating as LECs. 

All inter-LATA traffic was handled by a different kind of company, an 

IXC

 (

IntereXchange Carrier

). Originally, AT&T Long Lines was the only serious IXC, but now WorldCom and Sprint are well-established competitors in the IXC business. One of the concerns at the breakup was to ensure that all the IXCs would be treated equally in terms of line quality, tariffs, and the number of digits their customers would have to dial to use them. The way this is handled is illustrated in 

Fig. 2-22

. Here we see three example LATAs, each with several end offices. LATAs 2 and 3 also have a small hierarchy with tandem offices (intra-LATA toll offices). 

Figure 2-22. The relationship of LATAs, LECs, and IXCs. All the circles are LEC switching offices. Each hexagon belongs to the IXC whose number is in it. 

95




 

Any IXC that wishes to handle calls originating in a LATA can build a switching office called a 

POP

 (

Point of Presence

) there. The LEC is required to connect each IXC to every end office, either directly, as in LATAs 1 and 3, or indirectly, as in LATA 2. Furthermore, the terms of the connection, both technical and financial, must be identical for all IXCs. In this way, a subscriber in, say, LATA 1, can choose which IXC to use for calling subscribers in LATA 3. 

As part of the MFJ, the IXCs were forbidden to offer local telephone service and the LECs were forbidden to offer inter-LATA telephone service, although both were free to enter any other business, such as operating fried chicken restaurants. In 1984, that was a fairly unambiguous statement. Unfortunately, technology has a funny way of making the law obsolete. Neither cable television nor mobile phones were covered by the agreement. As cable television went from one way to two way and mobile phones exploded in popularity, both LECs and IXCs began buying up or merging with cable and mobile operators. 

By 1995, Congress saw that trying to maintain a distinction between the various kinds of companies was no longer tenable and drafted a bill to allow cable TV companies, local telephone companies, long-distance carriers, and mobile operators to enter one another's businesses. The idea was that any company could then offer its customers a single integrated package containing cable TV, telephone, and information services and that different companies would compete on service and price. The bill was enacted into law in February 1996. As a result, some BOCs became IXCs and some other companies, such as cable television operators, began offering local telephone service in competition with the LECs. 

One interesting property of the 1996 law is the requirement that LECs implement local number portability. This means that a customer can change local telephone companies without having to get a new telephone number. This provision removes a huge hurdle for many people and makes them much more inclined to switch LECs, thus increasing competition. As a result, the U.S. telecommunications landscape is currently undergoing a radical restructuring. Again, many other countries are starting to follow suit. Often other countries wait to see how this kind of experiment works out in the U.S. If it works well, they do the same thing; if it works badly, they try something else. 

2.5.3 The Local Loop: Modems, ADSL, and Wireless 

It is now time to start our detailed study of how the telephone system works. The main parts of the system are illustrated in 

Fig. 2-23

. Here we see the local loops, the trunks, and the toll offices and end offices, both of which contain switching equipment that switches calls. An end office has up to 10,000 local loops (in the U.S. and other large countries). In fact, until recently, the area code + exchange indicated the end office, so (212) 601-xxxx was a specific end office with 10,000 subscribers, numbered 0000 through 9999. With the advent of competition for local service, this system was no longer tenable because multiple companies wanted to own the end office code. Also, the number of codes was basically used up, so complex mapping schemes had to be introduced. 

96




Figure 2-23. The use of both analog and digital transmission for a computer to computer call. Conversion is done by the modems and codecs. 

 

Let us begin with the part that most people are familiar with: the two-wire local loop coming from a telephone company end office into houses and small businesses. The local loop is also frequently referred to as the ''last mile,'' although the length can be up to several miles. It has used analog signaling for over 100 years and is likely to continue doing so for some years to come, due to the high cost of converting to digital. Nevertheless, even in this last bastion of analog transmission, change is taking place. In this section we will study the traditional local loop and the new developments taking place here, with particular emphasis on data communication from home computers. 

When a computer wishes to send digital data over an analog dial-up line, the data must first be converted to analog form for transmission over the local loop. This conversion is done by a device called a modem, something we will study shortly. At the telephone company end office the data are converted to digital form for transmission over the long-haul trunks. 

If the other end is a computer with a modem, the reverse conversion—digital to analog—is needed to traverse the local loop at the destination. This arrangement is shown in 

Fig. 2-23

 for ISP 1 (Internet Service Provider), which has a bank of modems, each connected to a different local loop. This ISP can handle as many connections as it has modems (assuming its server or servers have enough computing power). This arrangement was the normal one until 56-kbps modems appeared, for reasons that will become apparent shortly. 

Analog signaling consists of varying a voltage with time to represent an information stream. If transmission media were perfect, the receiver would receive exactly the same signal that the transmitter sent. Unfortunately, media are not perfect, so the received signal is not the same as the transmitted signal. For digital data, this difference can lead to errors. 

Transmission lines suffer from three major problems: attenuation, delay distortion, and noise. 

Attenuation

 is the loss of energy as the signal propagates outward. The loss is expressed in decibels per kilometer. The amount of energy lost depends on the frequency. To see the effect of this frequency dependence, imagine a signal not as a simple waveform, but as a series of Fourier components. Each component is attenuated by a different amount, which results in a different Fourier spectrum at the receiver. 

To make things worse, the different Fourier components also propagate at different speeds in the wire. This speed difference leads to 

distortion

 of the signal received at the other end. 

Another problem is 

noise

, which is unwanted energy from sources other than the transmitter. Thermal noise is caused by the random motion of the electrons in a wire and is unavoidable. Crosstalk is caused by inductive 

97




coupling between two wires that are close to each other. Sometimes when talking on the telephone, you can hear another conversation in the background. That is crosstalk. Finally, there is impulse noise, caused by spikes on the power line or other causes. For digital data, impulse noise can wipe out one or more bits. 

Modems 

Due to the problems just discussed, especially the fact that both attenuation and propagation speed are frequency dependent, it is undesirable to have a wide range of frequencies in the signal. Unfortunately, the square waves used in digital signals have a wide frequency spectrum and thus are subject to strong attenuation and delay distortion. These effects make baseband (DC) signaling unsuitable except at slow speeds and over short distances. 

To get around the problems associated with DC signaling, especially on telephone lines, AC signaling is used. A continuous tone in the 1000 to 2000-Hz range, called a 

sine wave carrier

, is introduced. Its amplitude, frequency, or phase can be modulated to transmit information. In 

amplitude modulation

, two different amplitudes are used to represent 0 and 1, respectively. In 

frequency modulation

, also known as 

frequency shift keying

, two (or more) different tones are used. (The term 

keying

 is also widely used in the industry as a synonym for modulation.) In the simplest form of 

phase modulation

, the carrier wave is systematically shifted 0 or 180 degrees at uniformly spaced intervals. A better scheme is to use shifts of 45, 135, 225, or 315 degrees to transmit 2 bits of information per time interval. Also, always requiring a phase shift at the end of every time interval, makes it is easier for the receiver to recognize the boundaries of the time intervals. 

Figure 2-24

 illustrates the three forms of modulation. In 

Fig. 2-24(a)

 one of the amplitudes is nonzero and one is zero. In 

Fig. 2-24(b)

 two frequencies are used. In 

Fig. 2-24(c)

 a phase shift is either present or absent at each bit boundary. A device that accepts a serial stream of bits as input and produces a carrier modulated by one (or more) of these methods (or vice versa) is called a 

modem

 (for modulator-demodulator). The modem is inserted between the (digital) computer and the (analog) telephone system. 

Figure 2-24. (a) A binary signal. (b) Amplitude modulation. (c) Frequency modulation. (d) Phase modulation. 

 

98




To go to higher and higher speeds, it is not possible to just keep increasing the sampling rate. The Nyquist theorem says that even with a perfect 3000-Hz line (which a dial-up telephone is decidedly not), there is no point in sampling faster than 6000 Hz. In practice, most modems sample 2400 times/sec and focus on getting more bits per sample. 

The number of samples per second is measured in 

baud

. During each baud, one 

symbol

 is sent. Thus, an 

n

-baud line transmits 

n

 symbols/sec. For example, a 2400-baud line sends one symbol about every 416.667 µsec. If the symbol consists of 0 volts for a logical 0 and 1 volt for a logical 1, the bit rate is 2400 bps. If, however, the voltages 0, 1, 2, and 3 volts are used, every symbol consists of 2 bits, so a 2400-baud line can transmit 2400 symbols/sec at a data rate of 4800 bps. Similarly, with four possible phase shifts, there are also 2 bits/symbol, so again here the bit rate is twice the baud rate. The latter technique is widely used and called 

QPSK

 (

Quadrature Phase Shift Keying

). 

The concepts of bandwidth, baud, symbol, and bit rate are commonly confused, so let us restate them here. The bandwidth of a medium is the range of frequencies that pass through it with minimum attenuation. It is a physical property of the medium (usually from 0 to some maximum frequency) and measured in Hz. The baud rate is the number of samples/sec made. Each sample sends one piece of information, that is, one symbol. The baud rate and symbol rate are thus the same. The modulation technique (e.g., QPSK) determines the number of bits/symbol. The bit rate is the amount of information sent over the channel and is equal to the number of symbols/sec times the number of bits/symbol. 

All advanced modems use a combination of modulation techniques to transmit multiple bits per baud. Often multiple amplitudes and multiple phase shifts are combined to transmit several bits/symbol. In 

Fig. 2-25(a)

, we see dots at 45, 135, 225, and 315 degrees with constant amplitude (distance from the origin). The phase of a dot is indicated by the angle a line from it to the origin makes with the positive x-axis. 

Fig. 2-25(a)

 has four valid combinations and can be used to transmit 2 bits per symbol. It is QPSK. 

Figure 2-25. (a) QPSK. (b) QAM-16. (c) QAM-64. 

 

In 

Fig. 2-25(b)

 we see a different modulation scheme, in which four amplitudes and four phases are used, for a total of 16 different combinations. This modulation scheme can be used to transmit 4 bits per symbol. It is called 

QAM-16

 (

Quadrature Amplitude Modulation

). Sometimes the term 

16-QAM

 is used instead. QAM-16 can be used, for example, to transmit 9600 bps over a 2400-baud line. 

Figure 2-25(c)

 is yet another modulation scheme involving amplitude and phase. It allows 64 different combinations, so 6 bits can be transmitted per symbol. It is called 

QAM-64

. Higher-order QAMs also are used. 

Diagrams such as those of 

Fig. 2-25

, which show the legal combinations of amplitude and phase, are called 

constellation diagrams

. Each high-speed modem standard has its own constellation pattern and can talk only to other modems that use the same one (although most modems can emulate all the slower ones). 

With many points in the constellation pattern, even a small amount of noise in the detected amplitude or phase can result in an error and, potentially, many bad bits. To reduce the chance of an error, standards for the higher speeds modems do error correction by adding extra bits to each sample. The schemes are known as 

TCM

 (

Trellis Coded Modulation

). Thus, for example, the V.32 modem standard uses 32 constellation points to transmit 4 data bits and 1 parity bit per symbol at 2400 baud to achieve 9600 bps with error correction. Its constellation 

99




pattern is shown in 

Fig. 2-26(a)

. The decision to ''rotate'' around the origin by 45 degrees was done for engineering reasons; the rotated and unrotated constellations have the same information capacity. 

Figure 2-26. (a) V.32 for 9600 bps. (b) V32 bis for 14,400 bps. 

 

The next step above 9600 bps is 14,400 bps. It is called 

V.32 bis

. This speed is achieved by transmitting 6 data bits and 1 parity bit per sample at 2400 baud. Its constellation pattern has 128 points when QAM-128 is used and is shown in 

Fig. 2-26(b)

. Fax modems use this speed to transmit pages that have been scanned in as bit maps. QAM-256 is not used in any standard telephone modems, but it is used on cable networks, as we shall see. 

The next telephone modem after V.32 bis is 

V.34

, which runs at 28,800 bps at 2400 baud with 12 data bits/symbol. The final modem in this series is 

V.34 bis

 which uses 14 data bits/symbol at 2400 baud to achieve 33,600 bps. 

To increase the effective data rate further, many modems compress the data before transmitting it, to get an effective data rate higher than 33,600 bps. On the other hand, nearly all modems test the line before starting to transmit user data, and if they find the quality lacking, cut back to a speed lower than the rated maximum. Thus, the 

effective

 modem speed observed by the user can be lower, equal to, or higher than the official rating. 

All modern modems allow traffic in both directions at the same time (by using different frequencies for different directions). A connection that allows traffic in both directions simultaneously is called 

full duplex

. A two-lane road is full duplex. A connection that allows traffic either way, but only one way at a time is called 

half duplex

. A single railroad track is half duplex. A connection that allows traffic only one way is called 

simplex

. A one-way street is simplex. Another example of a simplex connection is an optical fiber with a laser on one end and a light detector on the other end. 

The reason that standard modems stop at 33,600 is that the Shannon limit for the telephone system is about 35 kbps, so going faster than this would violate the laws of physics (department of thermodynamics). To find out whether 56-kbps modems are theoretically possible, stay tuned. 

But why is the theoretical limit 35 kbps? It has to do with the average length of the local loops and the quality of these lines. The 35 kbps is determined by the average length of the local loops. In 

Fig. 2-23

, a call originating at the computer on the left and terminating at ISP 1 goes over two local loops as an analog signal, once at the source and once at the destination. Each of these adds noise to the signal. If we could get rid of one of these local loops, the maximum rate would be doubled. 

ISP 2 does precisely that. It has a pure digital feed from the nearest end office. The digital signal used on the trunks is fed directly to ISP 2, eliminating the codecs, modems, and analog transmission on its end. Thus, when one end of the connection is purely digital, as it is with most ISPs now, the maximum data rate can be as high as 70 kbps. Between two home users with modems and analog lines, the maximum is 33.6 kbps. 

100




The reason that 56 kbps modems are in use has to do with the Nyquist theorem. The telephone channel is about 4000 Hz wide (including the guard bands). The maximum number of independent samples per second is thus 8000. The number of bits per sample in the U.S. is 8, one of which is used for control purposes, allowing 56,000 bit/sec of user data. In Europe, all 8 bits are available to users, so 64,000-bit/sec modems could have been used, but to get international agreement on a standard, 56,000 was chosen. 

This modem standard is called 

V.90

. It provides for a 33.6-kbps upstream channel (user to ISP), but a 56 kbps downstream channel (ISP to user) because there is usually more data transport from the ISP to the user than the other way (e.g., requesting a Web page takes only a few bytes, but the actual page could be megabytes). In theory, an upstream channel wider than 33.6 kbps would have been possible, but since many local loops are too noisy for even 33.6 kbps, it was decided to allocate more of the bandwidth to the downstream channel to increase the chances of it actually working at 56 kbps. 

The next step beyond V.90 is 

V.92

. These modems are capable of 48 kbps on the upstream channel if the line can handle it. They also determine the appropriate speed to use in about half of the usual 30 seconds required by older modems. Finally, they allow an incoming telephone call to interrupt an Internet session, provided that the line has call waiting service. 

Digital Subscriber Lines 

When the telephone industry finally got to 56 kbps, it patted itself on the back for a job well done. Meanwhile, the cable TV industry was offering speeds up to 10 Mbps on shared cables, and satellite companies were planning to offer upward of 50 Mbps. As Internet access became an increasingly important part of their business, the telephone companies (LECs) began to realize they needed a more competitive product. Their answer was to start offering new digital services over the local loop. Services with more bandwidth than standard telephone service are sometimes called 

broadband

, although the term really is more of a marketing concept than a specific technical concept. 

Initially, there were many overlapping offerings, all under the general name of 

xDSL

 (

Digital Subscriber Line

), for various 

x

. Below we will discuss these but primarily focus on what is probably going to become the most popular of these services, 

ADSL

 (

Asymmetric DSL

). Since ADSL is still being developed and not all the standards are fully in place, some of the details given below may change in time, but the basic picture should remain valid. For more information about ADSL, see (Summers, 1999; and Vetter et al., 2000). 

The reason that modems are so slow is that telephones were invented for carrying the human voice and the entire system has been carefully optimized for this purpose. Data have always been stepchildren. At the point where each local loop terminates in the end office, the wire runs through a filter that attenuates all frequencies below 300 Hz and above 3400 Hz. The cutoff is not sharp—300 Hz and 3400 Hz are the 3 dB points—so the bandwidth is usually quoted as 4000 Hz even though the distance between the 3 dB points is 3100 Hz. Data are thus also restricted to this narrow band. 

The trick that makes xDSL work is that when a customer subscribes to it, the incoming line is connected to a different kind of switch, one that does not have this filter, thus making the entire capacity of the local loop available. The limiting factor then becomes the physics of the local loop, not the artificial 3100 Hz bandwidth created by the filter. 

Unfortunately, the capacity of the local loop depends on several factors, including its length, thickness, and general quality. A plot of the potential bandwidth as a function of distance is given in 

Fig. 2-27

. This figure assumes that all the other factors are optimal (new wires, modest bundles, etc.). 

Figure 2-27. Bandwidth versus distance over category 3 UTP for DSL. 

101




 

The implication of this figure creates a problem for the telephone company. When it picks a speed to offer, it is simultaneously picking a radius from its end offices beyond which the service cannot be offered. This means that when distant customers try to sign up for the service, they may be told ''Thanks a lot for your interest, but you live 100 meters too far from the nearest end office to get the service. Could you please move?'' The lower the chosen speed, the larger the radius and the more customers covered. But the lower the speed, the less attractive the service and the fewer the people who will be willing to pay for it. This is where business meets technology. (One potential solution is building mini end offices out in the neighborhoods, but that is an expensive proposition.) 

The xDSL services have all been designed with certain goals in mind. First, the services must work over the existing category 3 twisted pair local loops. Second, they must not affect customers' existing telephones and fax machines. Third, they must be much faster than 56 kbps. Fourth, they should be always on, with just a monthly charge but no per-minute charge. 

The initial ADSL offering was from AT&T and worked by dividing the spectrum available on the local loop, which is about 1.1 MHz, into three frequency bands: 

POTS

 (

Plain Old Telephone Service

) upstream (user to end office) and downstream (end office to user). The technique of having multiple frequency bands is called frequency division multiplexing; we will study it in detail in a later section. Subsequent offerings from other providers have taken a different approach, and it appears this one is likely to win out, so we will describe it below. 

The alternative approach, called 

DMT

 (

Discrete MultiTone

), is illustrated in 

Fig. 2-28

. In effect, what it does is divide the available 1.1 MHz spectrum on the local loop into 256 independent channels of 4312.5 Hz each. Channel 0 is used for POTS. Channels 1–5 are not used, to keep the voice signal and data signals from interfering with each other. Of the remaining 250 channels, one is used for upstream control and one is used for downstream control. The rest are available for user data. 

Figure 2-28. Operation of ADSL using discrete multitone modulation. 

 

In principle, each of the remaining channels can be used for a full-duplex data stream, but harmonics, crosstalk, and other effects keep practical systems well below the theoretical limit. It is up to the provider to determine how many channels are used for upstream and how many for downstream. A 50–50 mix of upstream and downstream is technically possible, but most providers allocate something like 80%–90% of the bandwidth to the downstream channel since most users download more data than they upload. This choice gives rise to the ''A'' in 

102




ADSL. A common split is 32 channels for upstream and the rest downstream. It is also possible to have a few of the highest upstream channels be bidirectional for increased bandwidth, although making this optimization requires adding a special circuit to cancel echoes. 

The ADSL standard (ANSI T1.413 and ITU G.992.1) allows speeds of as much as 8 Mbps downstream and 1 Mbps upstream. However, few providers offer this speed. Typically, providers offer 512 kbps downstream and 64 kbps upstream (standard service) and 1 Mbps downstream and 256 kbps upstream (premium service). 

Within each channel, a modulation scheme similar to V.34 is used, although the sampling rate is 4000 baud instead of 2400 baud. The line quality in each channel is constantly monitored and the data rate adjusted continuously as needed, so different channels may have different data rates. The actual data are sent with QAM modulation, with up to 15 bits per baud, using a constellation diagram analogous to that of 

Fig. 2-25(b)

. With, for example, 224 downstream channels and 15 bits/baud at 4000 baud, the downstream bandwidth is 13.44 Mbps. In practice, the signal-to-noise ratio is never good enough to achieve this rate, but 8 Mbps is possible on short runs over high-quality loops, which is why the standard goes up this far. 

A typical ADSL arrangement is shown in 

Fig. 2-29

. In this scheme, a telephone company technician must install a 

NID

 (

Network Interface Device

) on the customer's premises. This small plastic box marks the end of the telephone company's property and the start of the customer's property. Close to the NID (or sometimes combined with it) is a 

splitter

, an analog filter that separates the 0-4000 Hz band used by POTS from the data. The POTS signal is routed to the existing telephone or fax machine, and the data signal is routed to an ADSL modem. The ADSL modem is actually a digital signal processor that has been set up to act as 250 QAM modems operating in parallel at different frequencies. Since most current ADSL modems are external, the computer must be connected to it at high speed. Usually, this is done by putting an Ethernet card in the computer and operating a very short two-node Ethernet containing only the computer and ADSL modem. Occasionally the USB port is used instead of Ethernet. In the future, internal ADSL modem cards will no doubt become available. 

Figure 2-29. A typical ADSL equipment configuration. 

 

At the other end of the wire, on the end office side, a corresponding splitter is installed. Here the voice portion of the signal is filtered out and sent to the normal voice switch. The signal above 26 kHz is routed to a new kind of device called a 

DSLAM

 (

Digital Subscriber Line Access Multiplexer

), which contains the same kind of digital signal processor as the ADSL modem. Once the digital signal has been recovered into a bit stream, packets are formed and sent off to the ISP. 

This complete separation between the voice system and ADSL makes it relatively easy for a telephone company to deploy ADSL. All that is needed is buying a DSLAM and splitter and attaching the ADSL subscribers to the 

103




splitter. Other high-bandwidth services (e.g., ISDN) require much greater changes to the existing switching equipment. 

One disadvantage of the design of 

Fig. 2-29

 is the presence of the NID and splitter on the customer premises. Installing these can only be done by a telephone company technician, necessitating an expensive ''truck roll'' (i.e., sending a technician to the customer's premises). Therefore, an alternative splitterless design has also been standardized. It is informally called G.lite but the ITU standard number is G.992.2. It is the same as 

Fig. 2-

29

 but without the splitter. The existing telephone line is used as is. The only difference is that a microfilter has to be inserted into each telephone jack between the telephone or ADSL modem and the wire. The microfilter for the telephone is a low-pass filter eliminating frequencies above 3400 Hz; the microfilter for the ADSL modem is a high-pass filter eliminating frequencies below 26 kHz. However this system is not as reliable as having a splitter, so G.lite can be used only up to 1.5 Mbps (versus 8 Mbps for ADSL with a splitter). G.lite still requires a splitter in the end office, however, but that installation does not require thousands of truck rolls. 

ADSL is just a physical layer standard. What runs on top of it depends on the carrier. Often the choice is ATM due to ATM's ability to manage quality of service and the fact that many telephone companies run ATM in the core network. 

Wireless Local Loops 

Since 1996 in the U.S. and a bit later in other countries, companies that wish to compete with the entrenched local telephone company (the former monopolist), called an 

ILEC

 (

Incumbent LEC

), are free to do so. The most likely candidates are long-distance telephone companies (IXCs). Any IXC wishing to get into the local phone business in some city must do the following things. First, it must buy or lease a building for its first end office in that city. Second, it must fill the end office with telephone switches and other equipment, all of which are available as off-the-shelf products from various vendors. Third, it must run a fiber between the end office and its nearest toll office so the new local customers will have access to its national network. Fourth, it must acquire customers, typically by advertising better service or lower prices than those of the ILEC. 

Then the hard part begins. Suppose that some customers actually show up. How is the new local phone company, called a 

CLEC

 (

Competitive LEC

) going to connect customer telephones and computers to its shiny new end office? Buying the necessary rights of way and stringing wires or fibers is prohibitively expensive. Many CLECs have discovered a cheaper alternative to the traditional twisted-pair local loop: the 

WLL

 (

Wireless Local Loop

). 

In a certain sense, a fixed telephone using a wireless local loop is a bit like a mobile phone, but there are three crucial technical differences. First, the wireless local loop customer often wants high-speed Internet connectivity, often at speeds at least equal to ADSL. Second, the new customer probably does not mind having a CLEC technician install a large directional antenna on his roof pointed at the CLEC's end office. Third, the user does not move, eliminating all the problems with mobility and cell handoff that we will study later in this chapter. And thus a new industry is born: 

fixed wireless

 (local telephone and Internet service run by CLECs over wireless local loops). 

Although WLLs began serious operation in 1998, we first have to go back to 1969 to see the origin. In that year the FCC allocated two television channels (at 6 MHz each) for instructional television at 2.1 GHz. In subsequent years, 31 more channels were added at 2.5 GHz for a total of 198 MHz. 

Instructional television never took off and in 1998, the FCC took the frequencies back and allocated them to two-way radio. They were immediately seized upon for wireless local loops. At these frequencies, the microwaves are 10–12 cm long. They have a range of about 50 km and can penetrate vegetation and rain moderately well. The 198 MHz of new spectrum was immediately put to use for wireless local loops as a service called 

MMDS

 (

Multichannel Multipoint Distribution Service

). MMDS can be regarded as a MAN (Metropolitan Area Network), as can its cousin LMDS (discussed below). 

The big advantage of this service is that the technology is well established and the equipment is readily available. The disadvantage is that the total bandwidth available is modest and must be shared by many users over a fairly large geographic area. 

104




The low bandwidth of MMDS led to interest in millimeter waves as an alternative. At frequencies of 28–31 GHz in the U.S. and 40 GHz in Europe, no frequencies were allocated because it is difficult to build silicon integrated circuits that operate so fast. That problem was solved with the invention of gallium arsenide integrated circuits, opening up millimeter bands for radio communication. The FCC responded to the demand by allocating 1.3 GHz to a new wireless local loop service called 

LMDS

 (

Local Multipoint Distribution Service

). This allocation is the single largest chunk of bandwidth ever allocated by the FCC for any one use. A similar chunk is being allocated in Europe, but at 40 GHz. 

The operation of LMDS is shown in 

Fig. 2-30

. Here a tower is shown with multiple antennas on it, each pointing in a different direction. Since millimeter waves are highly directional, each antenna defines a sector, independent of the other ones. At this frequency, the range is 2–5 km, which means that many towers are needed to cover a city. 

Figure 2-30. Architecture of an LMDS system. 

 

Like ADSL, LMDS uses an asymmetric bandwidth allocation favoring the downstream channel. With current technology, each sector can have 36 Gbps downstream and 1 Mbps upstream, shared among all the users in that sector. If each active user downloads three 5-KB pages per minute, the user is occupying an average of 2000 bps of spectrum, which allows a maximum of 18,000 active users per sector. To keep the delay reasonable, no more than 9000 active users should be supported, though. With four sectors, as shown in 

Fig. 2-

30

, an active user population of 36,000 could be supported. Assuming that one in three customers is on line during peak periods, a single tower with four antennas could serve 100,000 people within a 5-km radius of the tower. These calculations have been done by many potential CLECs, some of whom have concluded that for a modest investment in millimeter-wave towers, they can get into the local telephone and Internet business and offer users data rates comparable to cable TV and at a lower price. 

LMDS has a few problems, however. For one thing, millimeter waves propagate in straight lines, so there must be a clear line of sight between the roof top antennas and the tower. For another, leaves absorb these waves well, so the tower must be high enough to avoid having trees in the line of sight. And what may have looked like a clear line of sight in December may not be clear in July when the trees are full of leaves. Rain also absorbs these waves. To some extent, errors introduced by rain can be compensated for with error correcting codes or turning up the power when it is raining. Nevertheless, LMDS service is more likely to be rolled out first in dry climates, say, in Arizona rather than in Seattle. 

Wireless local loops are not likely to catch on unless there are standards, to encourage equipment vendors to produce products and to ensure that customers can change CLECs without having to buy new equipment. To provide this standardization, IEEE set up a committee called 802.16 to draw up a standard for LMDS. The 802.16 standard was published in April 2002. IEEE calls 802.16 a 

wireless MAN

. 

105




IEEE 802.16 was designed for digital telephony, Internet access, connection of two remote LANs, television and radio broadcasting, and other uses. We will look at it in more detail in 

Chap. 4

. 

2.5.4 Trunks and Multiplexing 

Economies of scale play an important role in the telephone system. It costs essentially the same amount of money to install and maintain a high-bandwidth trunk as a low-bandwidth trunk between two switching offices (i.e., the costs come from having to dig the trench and not from the copper wire or optical fiber). Consequently, telephone companies have developed elaborate schemes for multiplexing many conversations over a single physical trunk. These multiplexing schemes can be divided into two basic categories: 

FDM

 (

Frequency Division Multiplexing

) and 

TDM

 (

Time Division Multiplexing

). In FDM, the frequency spectrum is divided into frequency bands, with each user having exclusive possession of some band. In TDM, the users take turns (in a round-robin fashion), each one periodically getting the entire bandwidth for a little burst of time. 

AM radio broadcasting provides illustrations of both kinds of multiplexing. The allocated spectrum is about 1 MHz, roughly 500 to 1500 kHz. Different frequencies are allocated to different logical channels (stations), each operating in a portion of the spectrum, with the interchannel separation great enough to prevent interference. This system is an example of frequency division multiplexing. In addition (in some countries), the individual stations have two logical subchannels: music and advertising. These two alternate in time on the same frequency, first a burst of music, then a burst of advertising, then more music, and so on. This situation is time division multiplexing. 

Below we will examine frequency division multiplexing. After that we will see how FDM can be applied to fiber optics (wavelength division multiplexing). Then we will turn to TDM, and end with an advanced TDM system used for fiber optics (SONET). 

Frequency Division Multiplexing 

Figure 2-31

 shows how three voice-grade telephone channels are multiplexed using FDM. Filters limit the usable bandwidth to about 3100 Hz per voice-grade channel. When many channels are multiplexed together, 4000 Hz is allocated to each channel to keep them well separated. First the voice channels are raised in frequency, each by a different amount. Then they can be combined because no two channels now occupy the same portion of the spectrum. Notice that even though there are gaps (guard bands) between the channels, there is some overlap between adjacent channels because the filters do not have sharp edges. This overlap means that a strong spike at the edge of one channel will be felt in the adjacent one as nonthermal noise. 

Figure 2-31. Frequency division multiplexing. (a) The original bandwidths. (b) The bandwidths raised in frequency. (c) The multiplexed channel. 

 

106




The FDM schemes used around the world are to some degree standardized. A widespread standard is twelve 4000-Hz voice channels multiplexed into the 60 to 108 kHz band. This unit is called a 

group.

 The 12-kHz to 60-kHz band is sometimes used for another group. Many carriers offer a 48- to 56-kbps leased line service to customers, based on the group. Five groups (60 voice channels) can be multiplexed to form a 

supergroup

. The next unit is the 

mastergroup

, which is five supergroups (CCITT standard) or ten supergroups (Bell system). Other standards of up to 230,000 voice channels also exist. 

Wavelength Division Multiplexing 

For fiber optic channels, a variation of frequency division multiplexing is used. It is called 

WDM

 (

Wavelength Division Multiplexing

). The basic principle of WDM on fibers is depicted in 

Fig. 2-32

. Here four fibers come together at an optical combiner, each with its energy present at a different wavelength. The four beams are combined onto a single shared fiber for transmission to a distant destination. At the far end, the beam is split up over as many fibers as there were on the input side. Each output fiber contains a short, specially-constructed core that filters out all but one wavelength. The resulting signals can be routed to their destination or recombined in different ways for additional multiplexed transport. 

Figure 2-32. Wavelength division multiplexing. 

 

There is really nothing new here. This is just frequency division multiplexing at very high frequencies. As long as each channel has its own frequency (i.e., wavelength) range and all the ranges are disjoint, they can be multiplexed together on the long-haul fiber. The only difference with electrical FDM is that an optical system using a diffraction grating is completely passive and thus highly reliable. 

WDM technology has been progressing at a rate that puts computer technology to shame. WDM was invented around 1990. The first commercial systems had eight channels of 2.5 Gbps per channel. By 1998, systems with 40 channels of 2.5 Gbps were on the market. By 2001, there were products with 96 channels of 10 Gbps, for a total of 960 Gbps. This is enough bandwidth to transmit 30 full-length movies per second (in MPEG-2). Systems with 200 channels are already working in the laboratory. When the number of channels is very large and the wavelengths are spaced close together, for example, 0.1 nm, the system is often referred to as 

DWDM

 (

Dense WDM

). 

It should be noted that the reason WDM is popular is that the energy on a single fiber is typically only a few gigahertz wide because it is currently impossible to convert between electrical and optical media any faster. By running many channels in parallel on different wavelengths, the aggregate bandwidth is increased linearly with the number of channels. Since the bandwidth of a single fiber band is about 25,000 GHz (see 

Fig. 2-6

), there is theoretically room for 2500 10-Gbps channels even at 1 bit/Hz (and higher rates are also possible). 

Another new development is all optical amplifiers. Previously, every 100 km it was necessary to split up all the channels and convert each one to an electrical signal for amplification separately before reconverting to optical 

107




and combining them. Nowadays, all optical amplifiers can regenerate the entire signal once every 1000 km without the need for multiple opto-electrical conversions. 

In the example of 

Fig. 2-32

, we have a fixed wavelength system. Bits from input fiber 1 go to output fiber 3, bits from input fiber 2 go to output fiber 1, etc. However, it is also possible to build WDM systems that are switched. In such a device, the output filters are tunable using Fabry-Perot or Mach-Zehnder interferometers. For more information about WDM and its application to Internet packet switching, see (Elmirghani and Mouftah, 2000; Hunter and Andonovic, 2000; and Listani et al., 2001). 

Time Division Multiplexing 

WDM technology is wonderful, but there is still a lot of copper wire in the telephone system, so let us turn back to it for a while. Although FDM is still used over copper wires or microwave channels, it requires analog circuitry and is not amenable to being done by a computer. In contrast, TDM can be handled entirely by digital electronics, so it has become far more widespread in recent years. Unfortunately, it can only be used for digital data. Since the local loops produce analog signals, a conversion is needed from analog to digital in the end office, where all the individual local loops come together to be combined onto outgoing trunks. 

We will now look at how multiple analog voice signals are digitized and combined onto a single outgoing digital trunk. Computer data sent over a modem are also analog, so the following description also applies to them. The analog signals are digitized in the end office by a device called a 

codec

 (coder-decoder), producing a series of 8-bit numbers. The codec makes 8000 samples per second (125 µsec/sample) because the Nyquist theorem says that this is sufficient to capture all the information from the 4-kHz telephone channel bandwidth. At a lower sampling rate, information would be lost; at a higher one, no extra information would be gained. This technique is called 

PCM

 (

Pulse Code Modulation

). PCM forms the heart of the modern telephone system. As a consequence, virtually all time intervals within the telephone system are multiples of 125 µsec. 

When digital transmission began emerging as a feasible technology, CCITT was unable to reach agreement on an international standard for PCM. Consequently, a variety of incompatible schemes are now in use in different countries around the world. 

The method used in North America and Japan is the 

T1

 carrier, depicted in 

Fig. 2-33

. (Technically speaking, the format is called DS1 and the carrier is called T1, but following widespread industry tradition, we will not make that subtle distinction here.) The T1 carrier consists of 24 voice channels multiplexed together. Usually, the analog signals are sampled on a round-robin basis with the resulting analog stream being fed to the codec rather than having 24 separate codecs and then merging the digital output. Each of the 24 channels, in turn, gets to insert 8 bits into the output stream. Seven bits are data and one is for control, yielding 7 x 8000 = 56,000 bps of data, and 1 x 8000 = 8000 bps of signaling information per channel. 

Figure 2-33. The T1 carrier (1.544 Mbps). 

 

108




A frame consists of 24 x 8 = 192 bits plus one extra bit for framing, yielding 193 bits every 125 µsec

.

 This gives a gross data rate of 1.544 Mbps. The 193rd bit is used for frame synchronization. It takes on the pattern 0101010101 . . . . Normally, the receiver keeps checking this bit to make sure that it has not lost synchronization. If it does get out of sync, the receiver can scan for this pattern to get resynchronized. Analog customers cannot generate the bit pattern at all because it corresponds to a sine wave at 4000 Hz, which would be filtered out. Digital customers can, of course, generate this pattern, but the odds are against its being present when the frame slips. When a T1 system is being used entirely for data, only 23 of the channels are used for data. The 24th one is used for a special synchronization pattern, to allow faster recovery in the event that the frame slips. 

When CCITT finally did reach agreement, they felt that 8000 bps of signaling information was far too much, so its 1.544-Mbps standard is based on an 8- rather than a 7-bit data item; that is, the analog signal is quantized into 256 rather than 128 discrete levels. Two (incompatible) variations are provided. In 

common-channel signaling

, the extra bit (which is attached onto the rear rather than the front of the 193-bit frame) takes on the values 10101010 . . . in the odd frames and contains signaling information for all the channels in the even frames. 

In the other variation, 

channel-associated signaling

, each channel has its own private signaling subchannel. A private subchannel is arranged by allocating one of the eight user bits in every sixth frame for signaling purposes, so five out of six samples are 8 bits wide, and the other one is only 7 bits wide. CCITT also recommended a PCM carrier at 2.048 Mbps called 

E1

. This carrier has 32 8-bit data samples packed into the basic 125

-

µsec frame. Thirty of the channels are used for information and two are used for signaling. Each group of four frames provides 64 signaling bits, half of which are used for channel-associated signaling and half of which are used for frame synchronization or are reserved for each country to use as it wishes. Outside North America and Japan, the 2.048-Mbps E1 carrier is used instead of T1. 

Once the voice signal has been digitized, it is tempting to try to use statistical techniques to reduce the number of bits needed per channel. These techniques are appropriate not only for encoding speech, but for the digitization of any analog signal. All of the compaction methods are based on the principle that the signal changes relatively slowly compared to the sampling frequency, so that much of the information in the 7- or 8-bit digital level is redundant. 

One method, called 

differential pulse code modulation

, consists of outputting not the digitized amplitude, but the difference between the current value and the previous one. Since jumps of ±16 or more on a scale of 128 are unlikely, 5 bits should suffice instead of 7. If the signal does occasionally jump wildly, the encoding logic may require several sampling periods to ''catch up.'' For speech, the error introduced can be ignored. 

A variation of this compaction method requires each sampled value to differ from its predecessor by either +1 or -1. Under these conditions, a single bit can be transmitted, telling whether the new sample is above or below the previous one. This technique, called 

delta modulation

, is illustrated in 

Fig. 2-34

. Like all compaction techniques that assume small level changes between consecutive samples, delta encoding can get into trouble if the signal changes too fast, as shown in the figure. When this happens, information is lost. 

Figure 2-34. Delta modulation. 

109




 

An improvement to differential PCM is to extrapolate the previous few values to predict the next value and then to encode the difference between the actual signal and the predicted one. The transmitter and receiver must use the same prediction algorithm, of course. Such schemes are called 

predictive encoding

. They are useful because they reduce the size of the numbers to be encoded, hence the number of bits to be sent. 

Time division multiplexing allows multiple T1 carriers to be multiplexed into higher-order carriers. 

Figure 2-35

 shows how this can be done. At the left we see four T1 channels being multiplexed onto one T2 channel. The multiplexing at T2 and above is done bit for bit, rather than byte for byte with the 24 voice channels that make up a T1 frame. Four T1 streams at 1.544 Mbps should generate 6.176 Mbps, but T2 is actually 6.312 Mbps. The extra bits are used for framing and recovery in case the carrier slips. T1 and T3 are widely used by customers, whereas T2 and T4 are only used within the telephone system itself, so they are not well known. 

Figure 2-35. Multiplexing T1 streams onto higher carriers. 

 

At the next level, seven T2 streams are combined bitwise to form a T3 stream. Then six T3 streams are joined to form a T4 stream. At each step a small amount of overhead is added for framing and recovery in case the synchronization between sender and receiver is lost. 

Just as there is little agreement on the basic carrier between the United States and the rest of the world, there is equally little agreement on how it is to be multiplexed into higher-bandwidth carriers. The U.S. scheme of stepping up by 4, 7, and 6 did not strike everyone else as the way to go, so the CCITT standard calls for multiplexing four streams onto one stream at each level. Also, the framing and recovery data are different between the U.S. and CCITT standards. The CCITT hierarchy for 32, 128, 512, 2048, and 8192 channels runs at speeds of 2.048, 8.848, 34.304, 139.264, and 565.148 Mbps. 

SONET/SDH 

In the early days of fiber optics, every telephone company had its own proprietary optical TDM system. After AT&T was broken up in 1984, local telephone companies had to connect to multiple long-distance carriers, all with different optical TDM systems, so the need for standardization became obvious. In 1985, Bellcore, the RBOCs research arm, began working on a standard, called 

SONET

 (

Synchronous Optical NETwork

). Later, 

110




CCITT joined the effort, which resulted in a SONET standard and a set of parallel CCITT recommendations (G.707, G.708, and G.709) in 1989. The CCITT recommendations are called 

SDH

 (

Synchronous Digital Hierarchy

) but differ from SONET only in minor ways. Virtually all the long-distance telephone traffic in the United States, and much of it elsewhere, now uses trunks running SONET in the physical layer. For additional information about SONET, see (Bellamy, 2000; Goralski, 2000; and Shepard, 2001). 

The SONET design had four major goals. First and foremost, SONET had to make it possible for different carriers to interwork. Achieving this goal required defining a common signaling standard with respect to wavelength, timing, framing structure, and other issues. 

Second, some means was needed to unify the U.S., European, and Japanese digital systems, all of which were based on 64-kbps PCM channels, but all of which combined them in different (and incompatible) ways. 

Third, SONET had to provide a way to multiplex multiple digital channels. At the time SONET was devised, the highest-speed digital carrier actually used widely in the United States was T3, at 44.736 Mbps. T4 was defined, but not used much, and nothing was even defined above T4 speed. Part of SONET's mission was to continue the hierarchy to gigabits/sec and beyond. A standard way to multiplex slower channels into one SONET channel was also needed. 

Fourth, SONET had to provide support for operations, administration, and maintenance (OAM). Previous systems did not do this very well. 

An early decision was to make SONET a traditional TDM system, with the entire bandwidth of the fiber devoted to one channel containing time slots for the various subchannels. As such, SONET is a synchronous system. It is controlled by a master clock with an accuracy of about 1 part in 10

9

. Bits on a SONET line are sent out at extremely precise intervals, controlled by the master clock. When cell switching was later proposed to be the basis of ATM, the fact that it permitted irregular cell arrivals got it labeled as 

Asynchronous

 Transfer Mode to contrast it to the synchronous operation of SONET. With SONET, the sender and receiver are tied to a common clock; with ATM they are not. 

The basic SONET frame is a block of 810 bytes put out every 125 µsec. Since SONET is synchronous, frames are emitted whether or not there are any useful data to send. Having 8000 frames/sec exactly matches the sampling rate of the PCM channels used in all digital telephony systems. 

The 810-byte SONET frames are best described as a rectangle of bytes, 90 columns wide by 9 rows high. Thus, 8 x 810 = 6480 bits are transmitted 8000 times per second, for a gross data rate of 51.84 Mbps. This is the basic SONET channel, called 

STS-1

 (

Synchronous Transport Signal-1

). All SONET trunks are a multiple of STS-1. 

The first three columns of each frame are reserved for system management information, as illustrated in 

Fig. 2-

36

. The first three rows contain the section overhead; the next six contain the line overhead. The section overhead is generated and checked at the start and end of each section, whereas the line overhead is generated and checked at the start and end of each line. 

Figure 2-36. Two back-to-back SONET frames. 

111




 

A SONET transmitter sends back-to-back 810-byte frames, without gaps between them, even when there are no data (in which case it sends dummy data). From the receiver's point of view, all it sees is a continuous bit stream, so how does it know where each frame begins? The answer is that the first two bytes of each frame contain a fixed pattern that the receiver searches for. If it finds this pattern in the same place in a large number of consecutive frames, it assumes that it is in sync with the sender. In theory, a user could insert this pattern into the payload in a regular way, but in practice it cannot be done due to the multiplexing of multiple users into the same frame and other reasons. 

The remaining 87 columns hold 87 x 9 x 8 x 8000 = 50.112 Mbps of user data. However, the user data, called the 

SPE

 (

Synchronous Payload Envelope

), do not always begin in row 1, column 4. The SPE can begin anywhere within the frame. A pointer to the first byte is contained in the first row of the line overhead. The first column of the SPE is the path overhead (i.e., header for the end-to-end path sublayer protocol). 

The ability to allow the SPE to begin anywhere within the SONET frame and even to span two frames, as shown in 

Fig. 2-36

, gives added flexibility to the system. For example, if a payload arrives at the source while a dummy SONET frame is being constructed, it can be inserted into the current frame instead of being held until the start of the next one. 

The SONET multiplexing hierarchy is shown in 

Fig. 2-37

. Rates from STS-1 to STS-192 have been defined. The optical carrier corresponding to STS-

n

 is called OC-

n

 but is bit for bit the same except for a certain bit reordering needed for synchronization. The SDH names are different, and they start at OC-3 because CCITT-based systems do not have a rate near 51.84 Mbps. The OC-9 carrier is present because it closely matches the speed of a major high-speed trunk used in Japan. OC-18 and OC-36 are used in Japan. The gross data rate includes all the overhead. The SPE data rate excludes the line and section overhead. The user data rate excludes all overhead and counts only the 86 payload columns. 

Figure 2-37. SONET and SDH multiplex rates. 

 

112




As an aside, when a carrier, such as OC-3, is not multiplexed, but carries the data from only a single source, the letter 

c

 (for concatenated) is appended to the designation, so OC-3 indicates a 155.52-Mbps carrier consisting of three separate OC-1 carriers, but OC-3c indicates a data stream from a single source at 155.52 Mbps. The three OC-1 streams within an OC-3c stream are interleaved by column, first column 1 from stream 1, then column 1 from stream 2, then column 1 from stream 3, followed by column 2 from stream 1, and so on, leading to a frame 270 columns wide and 9 rows deep. 

2.5.5 Switching 

From the point of view of the average telephone engineer, the phone system is divided into two principal parts: outside plant (the local loops and trunks, since they are physically outside the switching offices) and inside plant (the switches), which are inside the switching offices. We have just looked at the outside plant. Now it is time to examine the inside plant. 

Two different switching techniques are used nowadays: circuit switching and packet switching. We will give a brief introduction to each of them below. Then we will go into circuit switching in detail because that is how the telephone system works. We will study packet switching in detail in subsequent chapters. 

Circuit Switching 

When you or your computer places a telephone call, the switching equipment within the telephone system seeks out a physical path all the way from your telephone to the receiver's telephone. This technique is called 

circuit switching

 and is shown schematically in 

Fig. 2-38(a)

. Each of the six rectangles represents a carrier switching office (end office, toll office, etc.). In this example, each office has three incoming lines and three outgoing lines. When a call passes through a switching office, a physical connection is (conceptually) established between the line on which the call came in and one of the output lines, as shown by the dotted lines. 

Figure 2-38. (a) Circuit switching. (b) Packet switching. 

 

In the early days of the telephone, the connection was made by the operator plugging a jumper cable into the input and output sockets. In fact, a surprising little story is associated with the invention of automatic circuit switching equipment. It was invented by a 19th century Missouri undertaker named Almon B. Strowger. Shortly after the telephone was invented, when someone died, one of the survivors would call the town operator and say ''Please connect me to an undertaker.'' Unfortunately for Mr. Strowger, there were two undertakers in his town, 

113




and the other one's wife was the town telephone operator. He quickly saw that either he was going to have to invent automatic telephone switching equipment or he was going to go out of business. He chose the first option. For nearly 100 years, the circuit-switching equipment used worldwide was known as Strowger gear. (History does not record whether the now-unemployed switchboard operator got a job as an information operator, answering questions such as ''What is the phone number of an undertaker?'') 

The model shown in 

Fig. 2-39(a)

 is highly simplified, of course, because parts of the physical path between the two telephones may, in fact, be microwave or fiber links onto which thousands of calls are multiplexed. Nevertheless, the basic idea is valid: once a call has been set up, a dedicated path between both ends exists and will continue to exist until the call is finished. 

Figure 2-39. Timing of events in (a) circuit switching, (b) message switching, (c) packet switching. 

 

The alternative to circuit switching is packet switching, shown in 

Fig. 2-38(b)

. With this technology, individual packets are sent as need be, with no dedicated path being set up in advance. It is up to each packet to find its way to the destination on its own. 

An important property of circuit switching is the need to set up an end-to-end path 

before

 any data can be sent. The elapsed time between the end of dialing and the start of ringing can easily be 10 sec, more on long-distance or international calls. During this time interval, the telephone system is hunting for a path, as shown in 

Fig. 2-

39(a)

. Note that before data transmission can even begin, the call request signal must propagate all the way to the destination and be acknowledged. For many computer applications (e.g., point-of-sale credit verification), long setup times are undesirable. 

As a consequence of the reserved path between the calling parties, once the setup has been completed, the only delay for data is the propagation time for the electromagnetic signal, about 5 msec per 1000 km. Also as a consequence of the established path, there is no danger of congestion—that is, once the call has been put through, you never get busy signals. Of course, you might get one before the connection has been established due to lack of switching or trunk capacity. 

114




Message Switching 

An alternative switching strategy is 

message switching

, illustrated in 

Fig. 2-39(b)

. When this form of switching is used, no physical path is established in advance between sender and receiver. Instead, when the sender has a block of data to be sent, it is stored in the first switching office (i.e., router) and then forwarded later, one hop at a time. Each block is received in its entirety, inspected for errors, and then retransmitted. A network using this technique is called a 

store-and-forward

 network, as mentioned in 

Chap. 1

. 

The first electromechanical telecommunication systems used message switching, namely, for telegrams. The message was punched on paper tape (off-line) at the sending office, and then read in and transmitted over a communication line to the next office along the way, where it was punched out on paper tape. An operator there tore the tape off and read it in on one of the many tape readers, one reader per outgoing trunk. Such a switching office was called a 

torn tape office

. Paper tape is long gone and message switching is not used any more, so we will not discuss it further in this book. 

Packet Switching 

With message switching, there is no limit at all on block size, which means that routers (in a modern system) must have disks to buffer long blocks. It also means that a single block can tie up a router-router line for minutes, rendering message switching useless for interactive traffic. To get around these problems, 

packet switching

 was invented, as described in 

Chap. 1

. Packet-switching networks place a tight upper limit on block size, allowing packets to be buffered in router main memory instead of on disk. By making sure that no user can monopolize any transmission line very long (milliseconds), packet-switching networks are well suited for handling interactive traffic. A further advantage of packet switching over message switching is shown in 

Fig. 2-39(b)

 and 

(c)

: the first packet of a multipacket message can be forwarded before the second one has fully arrived, reducing delay and improving throughput. For these reasons, computer networks are usually packet switched, occasionally circuit switched, but never message switched. 

Circuit switching and packet switching differ in many respects. To start with, circuit switching requires that a circuit be set up end to end before communication begins. Packet switching does not require any advance setup. The first packet can just be sent as soon as it is available. 

The result of the connection setup with circuit switching is the reservation of bandwidth all the way from the sender to the receiver. All packets follow this path. Among other properties, having all packets follow the same path means that they cannot arrive out of order. With packet switching there is no path, so different packets can follow different paths, depending on network conditions at the time they are sent. They may arrive out of order. 

Packet switching is more fault tolerant than circuit switching. In fact, that is why it was invented. If a switch goes down, all of the circuits using it are terminated and no more traffic can be sent on any of them. With packet switching, packets can be routed around dead switches. 

Setting up a path in advance also opens up the possibility of reserving bandwidth in advance. If bandwidth is reserved, then when a packet arrives, it can be sent out immediately over the reserved bandwidth. With packet switching, no bandwidth is reserved, so packets may have to wait their turn to be forwarded. 

Having bandwidth reserved in advance means that no congestion can occur when a packet shows up (unless more packets show up than expected). On the other hand, when an attempt is made to establish a circuit, the attempt can fail due to congestion. Thus, congestion can occur at different times with circuit switching (at setup time) and packet switching (when packets are sent). 

If a circuit has been reserved for a particular user and there is no traffic to send, the bandwidth of that circuit is wasted. It cannot be used for other traffic. Packet switching does not waste bandwidth and thus is more efficient from a system-wide perspective. Understanding this trade-off is crucial for comprehending the difference between circuit switching and packet switching. The trade-off is between guaranteed service and wasting resources versus not guaranteeing service and not wasting resources. 

115




Packet switching uses store-and-forward transmission. A packet is accumulated in a router's memory, then sent on to the next router. With circuit switching, the bits just flow through the wire continuously. The store-and-forward technique adds delay. 

Another difference is that circuit switching is completely transparent. The sender and receiver can use any bit rate, format, or framing method they want to. The carrier does not know or care. With packet switching, the carrier determines the basic parameters. A rough analogy is a road versus a railroad. In the former, the user determines the size, speed, and nature of the vehicle; in the latter, the carrier does. It is this transparency that allows voice, data, and fax to coexist within the phone system. 

A final difference between circuit and packet switching is the charging algorithm. With circuit switching, charging has historically been based on distance and time. For mobile phones, distance usually does not play a role, except for international calls, and time plays only a minor role (e.g., a calling plan with 2000 free minutes costs more than one with 1000 free minutes and sometimes night or weekend calls are cheaper than normal). With packet switching, connect time is not an issue, but the volume of traffic sometimes is. For home users, ISPs usually charge a flat monthly rate because it is less work for them and their customers can understand this model easily, but backbone carriers charge regional networks based on the volume of their traffic. The differences are summarized in 

Fig. 2-40

. 

Figure 2-40. A comparison of circuit-switched and packet-switched networks. 

 

Both circuit switching and packet switching are important enough that we will come back to them shortly and describe the various technologies used in detail. 

2.6 The Mobile Telephone System 

The traditional telephone system (even if it some day gets multigigabit end-to-end fiber) will still not be able to satisfy a growing group of users: people on the go. People now expect to make phone calls from airplanes, cars, swimming pools, and while jogging in the park. Within a few years they will also expect to send e-mail and surf the Web from all these locations and more. Consequently, there is a tremendous amount of interest in wireless telephony. In the following sections we will study this topic in some detail. 

Wireless telephones come in two basic varieties: cordless phones and mobile phones (sometimes called 

cell phones

). 

Cordless phones

 are devices consisting of a base station and a handset sold as a set for use within the home. These are never used for networking, so we will not examine them further. Instead we will concentrate on the mobile system, which is used for wide area voice and data communication. 

Mobile phones

 have gone through three distinct generations, with different technologies: 

1. Analog voice. 

2. Digital voice. 

116




3. Digital voice and data (Internet, e-mail, etc.). 

Although most of our discussion will be about the technology of these systems, it is interesting to note how political and tiny marketing decisions can have a huge impact. The first mobile system was devised in the U.S. by AT&T and mandated for the whole country by the FCC. As a result, the entire U.S. had a single (analog) system and a mobile phone purchased in California also worked in New York. In contrast, when mobile came to Europe, every country devised its own system, which resulted in a fiasco. 

Europe learned from its mistake and when digital came around, the government-run PTTs got together and standardized on a single system (GSM), so any European mobile phone will work anywhere in Europe. By then, the U.S. had decided that government should not be in the standardization business, so it left digital to the marketplace. This decision resulted in different equipment manufacturers producing different kinds of mobile phones. As a consequence, the U.S. now has two major incompatible digital mobile phone systems in operation (plus one minor one). 

Despite an initial lead by the U.S., mobile phone ownership and usage in Europe is now far greater than in the U.S. Having a single system for all of Europe is part of the reason, but there is more. A second area where the U.S. and Europe differed is in the humble matter of phone numbers. In the U.S. mobile phones are mixed in with regular (fixed) telephones. Thus, there is no way for a caller to see if, say, (212) 234-5678 is a fixed telephone (cheap or free call) or a mobile phone (expensive call). To keep people from getting nervous about using the telephone, the telephone companies decided to make the mobile phone owner pay for incoming calls. As a consequence, many people hesitated to buy a mobile phone for fear of running up a big bill by just receiving calls. In Europe, mobile phones have a special area code (analogous to 800 and 900 numbers) so they are instantly recognizable. Consequently, the usual rule of ''caller pays'' also applies to mobile phones in Europe (except for international calls where costs are split). 

A third issue that has had a large impact on adoption is the widespread use of prepaid mobile phones in Europe (up to 75% in some areas). These can be purchased in many stores with no more formality than buying a radio. You pay and you go. They are preloaded with, for example, 20 or 50 euro and can be recharged (using a secret PIN code) when the balance drops to zero. As a consequence, practically every teenager and many small children in Europe have (usually prepaid) mobile phones so their parents can locate them, without the danger of the child running up a huge bill. If the mobile phone is used only occasionally, its use is essentially free since there is no monthly charge or charge for incoming calls. 

2.6.1 First-Generation Mobile Phones: Analog Voice 

Enough about the politics and marketing aspects of mobile phones. Now let us look at the technology, starting with the earliest system. Mobile radiotelephones were used sporadically for maritime and military communication during the early decades of the 20th century. In 1946, the first system for car-based telephones was set up in St. Louis. This system used a single large transmitter on top of a tall building and had a single channel, used for both sending and receiving. To talk, the user had to push a button that enabled the transmitter and disabled the receiver. Such systems, known as 

push-to-talk systems

, were installed in several cities beginning in the late 1950s. CB-radio, taxis, and police cars on television programs often use this technology. 

In the 1960s, 

IMTS

 (

Improved Mobile Telephone System

) was installed. It, too, used a high-powered (200-watt) transmitter, on top of a hill, but now had two frequencies, one for sending and one for receiving, so the push-to-talk button was no longer needed. Since all communication from the mobile telephones went inbound on a different channel than the outbound signals, the mobile users could not hear each other (unlike the push-to-talk system used in taxis). 

IMTS supported 23 channels spread out from 150 MHz to 450 MHz. Due to the small number of channels, users often had to wait a long time before getting a dial tone. Also, due to the large power of the hilltop transmitter, adjacent systems had to be several hundred kilometers apart to avoid interference. All in all, the limited capacity made the system impractical. 

Advanced Mobile Phone System 

117




All that changed with 

AMPS

 (

Advanced Mobile Phone System

), invented by Bell Labs and first installed in the United States in 1982. It was also used in England, where it was called TACS, and in Japan, where it was called MCS-L1. Although no longer state of the art, we will look at it in some detail because many of its fundamental properties have been directly inherited by its digital successor, D-AMPS, in order to achieve backward compatibility. 

In all mobile phone systems, a geographic region is divided up into 

cells

, which is why the devices are sometimes called cell phones. In AMPS, the cells are typically 10 to 20 km across; in digital systems, the cells are smaller. Each cell uses some set of frequencies not used by any of its neighbors. The key idea that gives cellular systems far more capacity than previous systems is the use of relatively small cells and the reuse of transmission frequencies in nearby (but not adjacent) cells. Whereas an IMTS system 100 km across can have one call on each frequency, an AMPS system might have 100 10-km cells in the same area and be able to have 10 to 15 calls on each frequency, in widely separated cells. Thus, the cellular design increases the system capacity by at least an order of magnitude, more as the cells get smaller. Furthermore, smaller cells mean that less power is needed, which leads to smaller and cheaper transmitters and handsets. Hand-held telephones put out 0.6 watts; transmitters in cars are 3 watts, the maximum allowed by the FCC. 

The idea of frequency reuse is illustrated in 

Fig. 2-41(a)

. The cells are normally roughly circular, but they are easier to model as hexagons. In 

Fig. 2-41(a)

, the cells are all the same size. They are grouped in units of seven cells. Each letter indicates a group of frequencies. Notice that for each frequency set, there is a buffer about two cells wide where that frequency is not reused, providing for good separation and low interference. 

Figure 2-41. (a) Frequencies are not reused in adjacent cells. (b) To add more users, smaller cells can be used. 

 

Finding locations high in the air to place base station antennas is a major issue. This problem has led some telecommunication carriers to forge alliances with the Roman Catholic Church, since the latter owns a substantial number of exalted potential antenna sites worldwide, all conveniently under a single management. 

In an area where the number of users has grown to the point that the system is overloaded, the power is reduced, and the overloaded cells are split into smaller 

microcells

 to permit more frequency reuse, as shown in 

Fig. 2-41(b)

. Telephone companies sometimes create temporary microcells, using portable towers with satellite links at sporting events, rock concerts, and other places where large numbers of mobile users congregate for a few hours. How big the cells should be is a complex matter, which is treated in (Hac, 1995). 

At the center of each cell is a base station to which all the telephones in the cell transmit. The base station consists of a computer and transmitter/receiver connected to an antenna. In a small system, all the base stations are connected to a single device called an 

MTSO

 (

Mobile Telephone Switching Office

) or 

MSC

 (

Mobile Switching Center

). In a larger one, several MTSOs may be needed, all of which are connected to a second-level MTSO, and so on. The MTSOs are essentially end offices as in the telephone system, and are, in fact, connected to at least one telephone system end office. The MTSOs communicate with the base stations, each other, and the PSTN using a packet-switching network. 

118




At any instant, each mobile telephone is logically in one specific cell and under the control of that cell's base station. When a mobile telephone physically leaves a cell, its base station notices the telephone's signal fading away and asks all the surrounding base stations how much power they are getting from it. The base station then transfers ownership to the cell getting the strongest signal, that is, the cell where the telephone is now located. The telephone is then informed of its new boss, and if a call is in progress, it will be asked to switch to a new channel (because the old one is not reused in any of the adjacent cells). This process, called 

handoff

, takes about 300 msec. Channel assignment is done by the MTSO, the nerve center of the system. The base stations are really just radio relays. 

Handoffs can be done in two ways. In a 

soft handoff

, the telephone is acquired by the new base station before the previous one signs off. In this way there is no loss of continuity. The downside here is that the telephone needs to be able to tune to two frequencies at the same time (the old one and the new one). Neither first nor second generation devices can do this. 

In a 

hard handoff

, the old base station drops the telephone before the new one acquires it. If the new one is unable to acquire it (e.g., because there is no available frequency), the call is disconnected abruptly. Users tend to notice this, but it is inevitable occasionally with the current design. 

Channels 

The AMPS system uses 832 full-duplex channels, each consisting of a pair of simplex channels. There are 832 simplex transmission channels from 824 to 849 MHz and 832 simplex receive channels from 869 to 894 MHz. Each of these simplex channels is 30 kHz wide. Thus, AMPS uses FDM to separate the channels. 

In the 800-MHz band, radio waves are about 40 cm long and travel in straight lines. They are absorbed by trees and plants and bounce off the ground and buildings. It is possible that a signal sent by a mobile telephone will reach the base station by the direct path, but also slightly later after bouncing off the ground or a building. This may lead to an echo or signal distortion (multipath fading). Sometimes, it is even possible to hear a distant conversation that has bounced several times. 

The 832 channels are divided into four categories: 

1. Control (base to mobile) to manage the system. 

2. Paging (base to mobile) to alert mobile users to calls for them. 

3. Access (bidirectional) for call setup and channel assignment. 

4. Data (bidirectional) for voice, fax, or data. 

Twenty-one of the channels are reserved for control, and these are wired into a PROM in each telephone. Since the same frequencies cannot be reused in nearby cells, the actual number of voice channels available per cell is much smaller than 832, typically about 45. 

Call Management 

Each mobile telephone in AMPS has a 32-bit serial number and a 10-digit telephone number in its PROM. The telephone number is represented as a 3-digit area code in 10 bits, and a 7-digit subscriber number in 24 bits. When a phone is switched on, it scans a preprogrammed list of 21 control channels to find the most powerful signal. 

The phone then broadcasts its 32-bit serial number and 34-bit telephone number. Like all the control information in AMPS, this packet is sent in digital form, multiple times, and with an error-correcting code, even though the voice channels themselves are analog. 

When the base station hears the announcement, it tells the MTSO, which records the existence of its new customer and also informs the customer's home MTSO of his current location. During normal operation, the mobile telephone reregisters about once every 15 minutes. 

119




To make a call, a mobile user switches on the phone, enters the number to be called on the keypad, and hits the SEND button. The phone then transmits the number to be called and its own identity on the access channel. If a collision occurs there, it tries again later. When the base station gets the request, it informs the MTSO. If the caller is a customer of the MTSO's company (or one of its partners), the MTSO looks for an idle channel for the call. If one is found, the channel number is sent back on the control channel. The mobile phone then automatically switches to the selected voice channel and waits until the called party picks up the phone. 

Incoming calls work differently. To start with, all idle phones continuously listen to the paging channel to detect messages directed at them. When a call is placed to a mobile phone (either from a fixed phone or another mobile phone), a packet is sent to the callee's home MTSO to find out where it is. A packet is then sent to the base station in its current cell, which then sends a broadcast on the paging channel of the form ''Unit 14, are you there?'' The called phone then responds with ''Yes'' on the access channel. The base then says something like: ''Unit 14, call for you on channel 3.'' At this point, the called phone switches to channel 3 and starts making ringing sounds (or playing some melody the owner was given as a birthday present). 

2.6.2 Second-Generation Mobile Phones: Digital Voice 

The first generation of mobile phones was analog; the second generation was digital. Just as there was no worldwide standardization during the first generation, there was also no standardization during the second, either. Four systems are in use now: D-AMPS, GSM, CDMA, and PDC. Below we will discuss the first three. PDC is used only in Japan and is basically D-AMPS modified for backward compatibility with the first-generation Japanese analog system. The name 

PCS

 (

Personal Communications Services

) is sometimes used in the marketing literature to indicate a second-generation (i.e., digital) system. Originally it meant a mobile phone using the 1900 MHz band, but that distinction is rarely made now. 

D-AMPS—The Digital Advanced Mobile Phone System 

The second generation of the AMPS systems is 

D-AMPS

 and is fully digital. It is described in International Standard IS-54 and its successor IS-136. D-AMPS was carefully designed to co-exist with AMPS so that both first- and second-generation mobile phones could operate simultaneously in the same cell. In particular, D-AMPS uses the same 30 kHz channels as AMPS and at the same frequencies so that one channel can be analog and the adjacent ones can be digital. Depending on the mix of phones in a cell, the cell's MTSO determines which channels are analog and which are digital, and it can change channel types dynamically as the mix of phones in a cell changes. 

When D-AMPS was introduced as a service, a new frequency band was made available to handle the expected increased load. The upstream channels were in the 1850–1910 MHz range, and the corresponding downstream channels were in the 1930–1990 MHz range, again in pairs, as in AMPS. In this band, the waves are 16 cm long, so a standard ¼-wave antenna is only 4 cm long, leading to smaller phones. However, many D-AMPS phones can use both the 850-MHz and 1900-MHz bands to get a wider range of available channels. 

On a D-AMPS mobile phone, the voice signal picked up by the microphone is digitized and compressed using a model that is more sophisticated than the delta modulation and predictive encoding schemes we studied earlier. Compression takes into account detailed properties of the human vocal system to get the bandwidth from the standard 56-kbps PCM encoding to 8 kbps or less. The compression is done by a circuit called a 

vocoder

 (Bellamy, 2000). The compression is done in the telephone, rather than in the base station or end office, to reduce the number of bits sent over the air link. With fixed telephony, there is no benefit to having compression done in the telephone, since reducing the traffic over the local loop does not increase system capacity at all. 

With mobile telephony there is a huge gain from doing digitization and compression in the handset, so much so that in D-AMPS, three users can share a single frequency pair using time division multiplexing. Each frequency pair supports 25 frames/sec of 40 msec each. Each frame is divided into six time slots of 6.67 msec each, as illustrated in 

Fig. 2-42(a)

 for the lowest frequency pair. 

Figure 2-42. (a) A D-AMPS channel with three users. (b) A D-AMPS channel with six users. 

120




 

Each frame holds three users who take turns using the upstream and downstream links. During slot 1 of 

Fig. 2-

42(a)

, for example, user 1 may transmit to the base station and user 3 is receiving from the base station. Each slot is 324 bits long, of which 64 bits are used for guard times, synchronization, and control purposes, leaving 260 bits for the user payload. Of the payload bits, 101 are used for error correction over the noisy air link, so ultimately only 159 bits are left for compressed speech. With 50 slots/sec, the bandwidth available for compressed speech is just under 8 kbps, 1/7 of the standard PCM bandwidth. 

Using better compression algorithms, it is possible to get the speech down to 4 kbps, in which case six users can be stuffed into a frame, as illustrated in 

Fig. 2-42(b)

. From the operator's perspective, being able to squeeze three to six times as many D-AMPS users into the same spectrum as one AMPS user is a huge win and explains much of the popularity of PCS. Of course, the quality of speech at 4 kbps is not comparable to what can be achieved at 56 kbps, but few PCS operators advertise their hi-fi sound quality. It should also be clear that for data, an 8 kbps channel is not even as good as an ancient 9600-bps modem. 

The control structure of D-AMPS is fairly complicated. Briefly summarized, groups of 16 frames form a superframe, with certain control information present in each superframe a limited number of times. Six main control channels are used: system configuration, real-time and nonreal-time control, paging, access response, and short messages. But conceptually, it works like AMPS. When a mobile is switched on, it makes contact with the base station to announce itself and then listens on a control channel for incoming calls. Having picked up a new mobile, the MTSO informs the user's home base where he is, so calls can be routed correctly. 

One difference between AMPS and D-AMPS is how handoff is handled. In AMPS, the MTSO manages it completely without help from the mobile devices. As can be seen from 

Fig. 2-42

, in D-AMPS, 1/3 of the time a mobile is neither sending nor receiving. It uses these idle slots to measure the line quality. When it discovers that the signal is waning, it complains to the MTSO, which can then break the connection, at which time the mobile can try to tune to a stronger signal from another base station. As in AMPS, it still takes about 300 msec to do the handoff. This technique is called 

MAHO

 (

Mobile Assisted HandOff

). 

GSM—The Global System for Mobile Communications 

D-AMPS is widely used in the U.S. and (in modified form) in Japan. Virtually everywhere else in the world, a system called 

GSM

 (

Global System for Mobile communications

) is used, and it is even starting to be used in the U.S. on a limited scale. To a first approximation, GSM is similar to D-AMPS. Both are cellular systems. In both systems, frequency division multiplexing is used, with each mobile transmitting on one frequency and receiving on a higher frequency (80 MHz higher for D-AMPS, 55 MHz higher for GSM). Also in both systems, a single frequency pair is split by time-division multiplexing into time slots shared by multiple mobiles. However, the GSM channels are much wider than the AMPS channels (200 kHz versus 30 kHz) and hold relatively few additional users (8 versus 3), giving GSM a much higher data rate per user than D-AMPS. 

Below we will briefly discuss some of the main properties of GSM. However, the printed GSM standard is over 5000 [sic] pages long. A large fraction of this material relates to engineering aspects of the system, especially the design of receivers to handle multipath signal propagation, and synchronizing transmitters and receivers. None of this will be even mentioned below. 

121




Each frequency band is 200 kHz wide, as shown in 

Fig. 2-43

. A GSM system has 124 pairs of simplex channels. Each simplex channel is 200 kHz wide and supports eight separate connections on it, using time division multiplexing. Each currently active station is assigned one time slot on one channel pair. Theoretically, 992 channels can be supported in each cell, but many of them are not available, to avoid frequency conflicts with neighboring cells. In 

Fig. 2-43

, the eight shaded time slots all belong to the same connection, four of them in each direction. Transmitting and receiving does not happen in the same time slot because the GSM radios cannot transmit and receive at the same time and it takes time to switch from one to the other. If the mobile station assigned to 890.4/935.4 MHz and time slot 2 wanted to transmit to the base station, it would use the lower four shaded slots (and the ones following them in time), putting some data in each slot until all the data had been sent. 

Figure 2-43. GSM uses 124 frequency channels, each of which uses an eight-slot TDM system. 

 

The TDM slots shown in 

Fig. 2-43

 are part of a complex framing hierarchy. Each TDM slot has a specific structure, and groups of TDM slots form multiframes, also with a specific structure. A simplified version of this hierarchy is shown in 

Fig. 2-44

. Here we can see that each TDM slot consists of a 148-bit data frame that occupies the channel for 577 µsec (including a 30-µsec guard time after each slot). Each data frame starts and ends with three 0 bits, for frame delineation purposes. It also contains two 57-bit 

Information

 fields, each one having a control bit that indicates whether the following 

Information

 field is for voice or data. Between the 

Information

 fields is a 26-bit 

Sync

 (training) field that is used by the receiver to synchronize to the sender's frame boundaries. 

Figure 2-44. A portion of the GSM framing structure. 

 

122




A data frame is transmitted in 547 µsec, but a transmitter is only allowed to send one data frame every 4.615 msec, since it is sharing the channel with seven other stations. The gross rate of each channel is 270,833 bps, divided among eight users. This gives 33.854 kbps gross, more than double D-AMPS' 324 bits 50 times per second for 16.2 kbps. However, as with AMPS, the overhead eats up a large fraction of the bandwidth, ultimately leaving 24.7 kbps worth of payload per user before error correction. After error correction, 13 kbps is left for speech, giving substantially better voice quality than D-AMPS (at the cost of using correspondingly more bandwidth). 

As can be seen from 

Fig. 2-44

, eight data frames make up a TDM frame and 26 TDM frames make up a 120-msec multiframe. Of the 26 TDM frames in a multiframe, slot 12 is used for control and slot 25 is reserved for future use, so only 24 are available for user traffic. 

However, in addition to the 26-slot multiframe shown in 

Fig. 2-44

, a 51-slot multiframe (not shown) is also used. Some of these slots are used to hold several control channels used to manage the system. The 

broadcast control channel

 is a continuous stream of output from the base station containing the base station's identity and the channel status. All mobile stations monitor their signal strength to see when they have moved into a new cell. 

The 

dedicated control channel

 is used for location updating, registration, and call setup. In particular, each base station maintains a database of mobile stations currently under its jurisdiction. Information needed to maintain this database is sent on the dedicated control channel. 

Finally, there is the 

common control channel

, which is split up into three logical subchannels. The first of these subchannels is the 

paging channel

, which the base station uses to announce incoming calls. Each mobile station monitors it continuously to watch for calls it should answer. The second is the 

random access channel

, which allows users to request a slot on the dedicated control channel. If two requests collide, they are garbled and have to be retried later. Using the dedicated control channel slot, the station can set up a call. The assigned slot is announced on the third subchannel, the 

access grant channel

. 

CDMA—Code Division Multiple Access 

D-AMPS and GSM are fairly conventional systems. They use both FDM and TDM to divide the spectrum into channels and the channels into time slots. However, there is a third kid on the block, 

CDMA

 (

Code Division Multiple Access

), which works completely differently. When CDMA was first proposed, the industry gave it approximately the same reaction that Columbus first got from Queen Isabella when he proposed reaching India by sailing in the wrong direction. However, through the persistence of a single company, Qualcomm, CDMA has matured to the point where it is not only acceptable, it is now viewed as the best technical solution around and the basis for the third-generation mobile systems. It is also widely used in the U.S. in second-generation mobile systems, competing head-on with D-AMPS. For example, Sprint PCS uses CDMA, whereas AT&T Wireless uses D-AMPS. CDMA is described in International Standard IS-95 and is sometimes referred to by that name. The brand name 

cdmaOne

 is also used. 

CDMA is completely different from AMPS, D-AMPS, and GSM. Instead of dividing the allowed frequency range into a few hundred narrow channels, CDMA allows each station to transmit over the entire frequency spectrum all the time. Multiple simultaneous transmissions are separated using coding theory. CDMA also relaxes the assumption that colliding frames are totally garbled. Instead, it assumes that multiple signals add linearly. 

Before getting into the algorithm, let us consider an analogy: an airport lounge with many pairs of people conversing. TDM is comparable to all the people being in the middle of the room but taking turns speaking. FDM is comparable to the people being in widely separated clumps, each clump holding its own conversation at the same time as, but still independent of, the others. CDMA is comparable to everybody being in the middle of the room talking at once, but with each pair in a different language. The French-speaking couple just hones in on the French, rejecting everything that is not French as noise. Thus, the key to CDMA is to be able to extract the desired signal while rejecting everything else as random noise. A somewhat simplified description of CDMA follows. 

In CDMA, each bit time is subdivided into 

m

 short intervals called 

chips

. Typically, there are 64 or 128 chips per bit, but in the example given below we will use 8 chips/bit for simplicity. 

123




Each station is assigned a unique 

m

-bit code called a 

chip sequence

. To transmit a 1 bit, a station sends its chip sequence. To transmit a 0 bit, it sends the one's complement of its chip sequence. No other patterns are permitted. Thus, for 

m

 = 8, if station 

A

 is assigned the chip sequence 00011011, it sends a 1 bit by sending 00011011 and a 0 bit by sending 11100100. 

Increasing the amount of information to be sent from 

b

 bits/sec to 

mb

 chips/sec can only be done if the bandwidth available is increased by a factor of 

m

, making CDMA a form of spread spectrum communication (assuming no changes in the modulation or encoding techniques). If we have a 1-MHz band available for 100 stations, with FDM each one would have 10 kHz and could send at 10 kbps (assuming 1 bit per Hz). With CDMA, each station uses the full 1 MHz, so the chip rate is 1 megachip per second. With fewer than 100 chips per bit, the effective bandwidth per station is higher for CDMA than FDM, and the channel allocation problem is also solved. 

For pedagogical purposes, it is more convenient to use a bipolar notation, with binary 0 being -1 and binary 1 being +1. We will show chip sequences in parentheses, so a 1 bit for station 

A

 now becomes (-1 -1 -1 +1 +1 -1 +1 +1). In 

Fig. 2-45(a)

 we show the binary chip sequences assigned to four example stations. In 

Fig. 2-45(b)

 we show them in our bipolar notation. 

Figure 2-45. (a) Binary chip sequences for four stations. (b) Bipolar chip sequences. (c) Six examples of transmissions. (d) Recovery of station C's signal. 

 

Each station has its own unique chip sequence. Let us use the symbol 

S

 to indicate the 

m

-chip vector for station 

S

, and 

for its negation. All chip sequences are pairwise 

orthogonal

, by which we mean that the normalized inner product of any two distinct chip sequences, 

S

 and 

T

 (written as 

S

•

T

), is 0. It is known how to generate such orthogonal chip sequences using a method known as 

Walsh codes

. In mathematical terms, orthogonality of the chip sequences can be expressed as follows: 

Equation 2 

 

 

124




In plain English, as many pairs are the same as are different. This orthogonality property will prove crucial later on. Note that if 

S

•

T

 = 0, then 

is also 0. The normalized inner product of any chip sequence with itself is 1: 

 

 

This follows because each of the 

m

 terms in the inner product is 1, so the sum is 

m

. Also note that 

. 

During each bit time, a station can transmit a 1 by sending its chip sequence, it can transmit a 0 by sending the negative of its chip sequence, or it can be silent and transmit nothing. For the moment, we assume that all stations are synchronized in time, so all chip sequences begin at the same instant. 

When two or more stations transmit simultaneously, their bipolar signals add linearly. For example, if in one chip period three stations output +1 and one station outputs -1, the result is +2. One can think of this as adding voltages: three stations outputting +1 volts and 1 station outputting -1 volts gives 2 volts. 

In 

Fig. 2-45(c)

 we see six examples of one or more stations transmitting at the same time. In the first example, 

C

 transmits a 1 bit, so we just get 

C

's chip sequence. In the second example, both 

B

 and 

C

 transmit 1 bits, so we get the sum of their bipolar chip sequences, namely: 

 

 

In the third example, station 

A

 sends a 1 and station 

B

 sends a 0. The others are silent. In the fourth example, 

A

 and 

C

 send a 1 bit while 

B

 sends a 0 bit. In the fifth example, all four stations send a 1 bit. Finally, in the last example, 

A

, 

B

, and 

D

 send a 1 bit, while 

C

 sends a 0 bit. Note that each of the six sequences 

S

 

1

 through 

S

 

6

 given in 

Fig. 2-45(c)

 represents only one bit time. 

To recover the bit stream of an individual station, the receiver must know that station's chip sequence in advance. It does the recovery by computing the normalized inner product of the received chip sequence (the linear sum of all the stations that transmitted) and the chip sequence of the station whose bit stream it is trying to recover. If the received chip sequence is 

S

 and the receiver is trying to listen to a station whose chip sequence is 

C

, it just computes the normalized inner product, 

S

•

C

. 

To see why this works, just imagine that two stations, 

A

 and 

C

, both transmit a 1 bit at the same time that 

B

 transmits a 0 bit. The receiver sees the sum, 

and computes 

 

 

The first two terms vanish because all pairs of chip sequences have been carefully chosen to be orthogonal, as shown in 

Eq. (2-4)

. Now it should be clear why this property must be imposed on the chip sequences. 

An alternative way of thinking about this situation is to imagine that the three chip sequences all came in separately, rather than summed. Then, the receiver would compute the inner product with each one separately and add the results. Due to the orthogonality property, all the inner products except 

C

•

C

 would be 0. Adding them and then doing the inner product is in fact the same as doing the inner products and then adding those. 

125




To make the decoding process more concrete, let us consider the six examples of 

Fig. 2-45(c)

 again as illustrated in 

Fig. 2-45(d)

. Suppose that the receiver is interested in extracting the bit sent by station 

C

 from each of the six sums 

S

1

 through 

S

6

. It calculates the bit by summing the pairwise products of the received 

S

 and the 

C

 vector of 

Fig. 2-45(b)

 and then taking 1/8 of the result (since 

m

 = 8 here). As shown, the correct bit is decoded each time. It is just like speaking French. 

In an ideal, noiseless CDMA system, the capacity (i.e., number of stations) can be made arbitrarily large, just as the capacity of a noiseless Nyquist channel can be made arbitrarily large by using more and more bits per sample. In practice, physical limitations reduce the capacity considerably. First, we have assumed that all the chips are synchronized in time. In reality, such synchronization is impossible. What can be done is that the sender and receiver synchronize by having the sender transmit a predefined chip sequence that is long enough for the receiver to lock onto. All the other (unsynchronized) transmissions are then seen as random noise. If there are not too many of them, however, the basic decoding algorithm still works fairly well. A large body of theory exists relating the superposition of chip sequences to noise level (Pickholtz et al., 1982). As one might expect, the longer the chip sequence, the higher the probability of detecting it correctly in the presence of noise. For extra reliability, the bit sequence can use an error-correcting code. Chip sequences never use error-correcting codes. 

An implicit assumption in our discussion is that the power levels of all stations are the same as perceived by the receiver. CDMA is typically used for wireless systems with a fixed base station and many mobile stations at varying distances from it. The power levels received at the base station depend on how far away the transmitters are. A good heuristic here is for each mobile station to transmit to the base station at the inverse of the power level it receives from the base station. In other words, a mobile station receiving a weak signal from the will use more power than one getting a strong signal. The base station can also give explicit commands to the mobile stations to increase or decrease their transmission power. 

We have also assumed that the receiver knows who the sender is. In principle, given enough computing capacity, the receiver can listen to all the senders at once by running the decoding algorithm for each of them in parallel. In real life, suffice it to say that this is easier said than done. CDMA also has many other complicating factors that have been glossed over in this brief introduction. Nevertheless, CDMA is a clever scheme that is being rapidly introduced for wireless mobile communication. It normally operates in a band of 1.25 MHz (versus 30 kHz for D-AMPS and 200 kHz for GSM), but it supports many more users in that band than either of the other systems. In practice, the bandwidth available to each user is at least as good as GSM and often much better. 

Engineers who want to gain a very deep understanding of CDMA should read (Lee and Miller, 1998). An alternative spreading scheme, in which the spreading is over time rather than frequency, is described in (Crespo et al., 1995). Yet another scheme is described in (Sari et al., 2000). All of these references require quite a bit of background in communication engineering. 

2.6.3 Third-Generation Mobile Phones: Digital Voice and Data 

What is the future of mobile telephony? Let us take a quick look. A number of factors are driving the industry. First, data traffic already exceeds voice traffic on the fixed network and is growing exponentially, whereas voice traffic is essentially flat. Many industry experts expect data traffic to dominate voice on mobile devices as well soon. Second, the telephone, entertainment, and computer industries have all gone digital and are rapidly converging. Many people are drooling over a lightweight, portable device that acts as a telephone, CD player, DVD player, e-mail terminal, Web interface, gaming machine, word processor, and more, all with worldwide wireless connectivity to the Internet at high bandwidth. This device and how to connect it is what third generation mobile telephony is all about. For more information, see (Huber et al., 2000; and Sarikaya, 2000). 

Back in 1992, ITU tried to get a bit more specific about this dream and issued a blueprint for getting there called 

IMT-2000

, where IMT stood for 

International Mobile Telecommunications

. The number 2000 stood for three things: (1) the year it was supposed to go into service, (2) the frequency it was supposed to operate at (in MHz), and (3) the bandwidth the service should have (in kHz). 

It did not make it on any of the three counts. Nothing was implemented by 2000. ITU recommended that all governments reserve spectrum at 2 GHz so devices could roam seamlessly from country to country. China reserved the required bandwidth but nobody else did. Finally, it was recognized that 2 Mbps is not currently 

126




feasible for users who are 

too

 mobile (due to the difficulty of performing handoffs quickly enough). More realistic is 2 Mbps for stationary indoor users (which will compete head-on with ADSL), 384 kbps for people walking, and 144 kbps for connections in cars. Nevertheless, the whole area of 

3G

,asitis called, is one great cauldron of activity. The third generation may be a bit less than originally hoped for and a bit late, but it will surely happen. 

The basic services that the IMT-2000 network is supposed to provide to its users are: 

1. High-quality voice transmission. 

2. Messaging (replacing e-mail, fax, SMS, chat, etc.). 

3. Multimedia (playing music, viewing videos, films, television, etc.). 

4. Internet access (Web surfing, including pages with audio and video). 

Additional services might be video conferencing, telepresence, group game playing, and m-commerce (waving your telephone at the cashier to pay in a store). Furthermore, all these services are supposed to be available worldwide (with automatic connection via a satellite when no terrestrial network can be located), instantly (always on), and with quality-of-service guarantees. 

ITU envisioned a single worldwide technology for IMT-2000, so that manufacturers could build a single device that could be sold and used anywhere in the world (like CD players and computers and unlike mobile phones and televisions). Having a single technology would also make life much simpler for network operators and would encourage more people to use the services. Format wars, such as the Betamax versus VHS battle when videorecorders first came out, are not good for business. 

Several proposals were made, and after some winnowing, it came down to two main ones. The first one, 

W-CDMA

 (

Wideband CDMA

), was proposed by Ericsson. This system uses direct sequence spread spectrum of the type we described above. It runs in a 5 MHz bandwidth and has been designed to interwork with GSM networks although it is not backward compatible with GSM. It does, however, have the property that a caller can leave a W-CDMA cell and enter a GSM cell without losing the call. This system was pushed hard by the European Union, which called it 

UMTS

 (

Universal Mobile Telecommunications System

). 

The other contender was 

CDMA2000

, proposed by Qualcomm. It, too, is a direct sequence spread spectrum design, basically an extension of IS-95 and backward compatible with it. It also uses a 5-MHz bandwidth, but it has not been designed to interwork with GSM and cannot hand off calls to a GSM cell (or a D-AMPS cell, for that matter). Other technical differences with W-CDMA include a different chip rate, different frame time, different spectrum used, and a different way to do time synchronization. 

If the Ericsson and Qualcomm engineers were put in a room and told to come to a common design, they probably could. After all, the basic principle behind both systems is CDMA in a 5 MHz channel and nobody is willing to die for his preferred chip rate. The trouble is that the real problem is not engineering, but politics (as usual). Europe wanted a system that interworked with GSM; the U.S. wanted a system that was compatible with one already widely deployed in the U.S. (IS-95). Each side also supported its local company (Ericsson is based in Sweden; Qualcomm is in California). Finally, Ericsson and Qualcomm were involved in numerous lawsuits over their respective CDMA patents. 

In March 1999, the two companies settled the lawsuits when Ericsson agreed to buy Qualcomm's infrastructure. They also agreed to a single 3G standard, but one with multiple incompatible options, which to a large extent just papers over the technical differences. These disputes notwithstanding, 3G devices and services are likely to start appearing in the coming years. 

Much has been written about 3G systems, most of it praising it as the greatest thing since sliced bread. Some references are (Collins and Smith, 2001; De Vriendt et al., 2002; Harte et al., 2002; Lu, 2002; and Sarikaya, 2000). However, some dissenters think that the industry is pointed in the wrong direction (Garber, 2002; and Goodman, 2000). 

While waiting for the fighting over 3G to stop, some operators are gingerly taking a cautious small step in the direction of 3G by going to what is sometimes called 

2.5G

, although 2.1G might be more accurate. One such system is 

EDGE

 (

Enhanced Data rates for GSM Evolution

), which is just GSM with more bits per baud. The trouble is, more bits per baud also means more errors per baud, so EDGE has nine different schemes for 

127




modulation and error correction, differing on how much of the bandwidth is devoted to fixing the errors introduced by the higher speed. 

Another 2.5G scheme is 

GPRS

 (

General Packet Radio Service

), which is an overlay packet network on top of D-AMPS or GSM. It allows mobile stations to send and receive IP packets in a cell running a voice system. When GPRS is in operation, some time slots on some frequencies are reserved for packet traffic. The number and location of the time slots can be dynamically managed by the base station, depending on the ratio of voice to data traffic in the cell. 

The available time slots are divided into several logical channels, used for different purposes. The base station determines which logical channels are mapped onto which time slots. One logical channel is for downloading packets from the base station to some mobile station, with each packet indicating who it is destined for. To send an IP packet, a mobile station requests one or more time slots by sending a request to the base station. If the request arrives without damage, the base station announces the frequency and time slots allocated to the mobile for sending the packet. Once the packet has arrived at the base station, it is transferred to the Internet by a wired connection. Since GPRS is just an overlay over the existing voice system, it is at best a stop-gap measure until 3G arrives. 

Even though 3G networks are not fully deployed yet, some researchers regard 3G as a done deal and thus not interesting any more. These people are already working on 4G systems (Berezdivin et al., 2002; Guo and Chaskar, 2002; Huang and Zhuang, 2002; Kellerer et al., 2002; and Misra et al., 2002). Some of the proposed features of 4G systems include high bandwidth, ubiquity (connectivity everywhere), seamless integration with wired networks and especially IP, adaptive resource and spectrum management, software radios, and high quality of service for multimedia. 

Then on the other hand, so many 802.11 wireless LAN access points are being set up all over the place, that some people think 3G is not only not a done deal, it is doomed. In this vision, people will just wander from one 802.11 access point to another to stay connected. To say the industry is in a state of enormous flux is a huge understatement. Check back in about 5 years to see what happens. 

2.7 Cable Television 

We have now studied both the fixed and wireless telephone systems in a fair amount of detail. Both will clearly play a major role in future networks. However, an alternative available for fixed networking is now becoming a major player: cable television networks. Many people already get their telephone and Internet service over the cable, and the cable operators are actively working to increase their market share. In the following sections we will look at cable television as a networking system in more detail and contrast it with the telephone systems we have just studied. For more information about cable, see (Laubach et al., 2001; Louis, 2002; Ovadia, 2001; and Smith, 2002). 

2.7.1 Community Antenna Television 

Cable television was conceived in the late 1940s as a way to provide better reception to people living in rural or mountainous areas. The system initially consisted of a big antenna on top of a hill to pluck the television signal out of the air, an amplifier, called the 

head end

, to strengthen it, and a coaxial cable to deliver it to people's houses, as illustrated in 

Fig. 2-46

. 

Figure 2-46. An early cable television system. 

128




 

In the early years, cable television was called 

Community Antenna Television

. It was very much a mom-and-pop operation; anyone handy with electronics could set up a service for his town, and the users would chip in to pay the costs. As the number of subscribers grew, additional cables were spliced onto the original cable and amplifiers were added as needed. Transmission was one way, from the headend to the users. By 1970, thousands of independent systems existed. 

In 1974, Time, Inc., started a new channel, Home Box Office, with new content (movies) and distributed only on cable. Other cable-only channels followed with news, sports, cooking, and many other topics. This development gave rise to two changes in the industry. First, large corporations began buying up existing cable systems and laying new cable to acquire new subscribers. Second, there was now a need to connect multiple systems, often in distant cities, in order to distribute the new cable channels. The cable companies began to lay cable between their cities to connect them all into a single system. This pattern was analogous to what happened in the telephone industry 80 years earlier with the connection of previously isolated end offices to make long distance calling possible. 

2.7.2 Internet over Cable 

Over the course of the years the cable system grew and the cables between the various cities were replaced by high-bandwidth fiber, similar to what was happening in the telephone system. A system with fiber for the long-haul runs and coaxial cable to the houses is called an 

HFC

 (

Hybrid Fiber Coax

) system. The electro-optical converters that interface between the optical and electrical parts of the system are called 

fiber nodes

. Because the bandwidth of fiber is so much more than that of coax, a fiber node can feed multiple coaxial cables. Part of a modern HFC system is shown in 

Fig. 2-47(a)

. 

Figure 2-47. (a) Cable television. (b) The fixed telephone system. 

129




 

In recent years, many cable operators have decided to get into the Internet access business, and often the telephony business as well. However, technical differences between the cable plant and telephone plant have an effect on what has to be done to achieve these goals. For one thing, all the one-way amplifiers in the system have to be replaced by two-way amplifiers. 

However, there is another difference between the HFC system of 

Fig. 2-47(a)

 and the telephone system of 

Fig. 

2-47(b)

 that is much harder to remove. Down in the neighborhoods, a single cable is shared by many houses, whereas in the telephone system, every house has its own private local loop. When used for television broadcasting, this sharing does not play a role. All the programs are broadcast on the cable and it does not matter whether there are 10 viewers or 10,000 viewers. When the same cable is used for Internet access, it matters a lot if there are 10 users or 10,000. If one user decides to download a very large file, that bandwidth is potentially being taken away from other users. The more users, the more competition for bandwidth. The telephone system does not have this particular property: downloading a large file over an ADSL line does not reduce your neighbor's bandwidth. On the other hand, the bandwidth of coax is much higher than that of twisted pairs. 

The way the cable industry has tackled this problem is to split up long cables and connect each one directly to a fiber node. The bandwidth from the headend to each fiber node is effectively infinite, so as long as there are not too many subscribers on each cable segment, the amount of traffic is manageable. Typical cables nowadays have 500–2000 houses, but as more and more people subscribe to Internet over cable, the load may become too much, requiring more splitting and more fiber nodes. 

130




2.7.3 Spectrum Allocation 

Throwing off all the TV channels and using the cable infrastructure strictly for Internet access would probably generate a fair number of irate customers, so cable companies are hesitant to do this. Furthermore, most cities heavily regulate what is on the cable, so the cable operators would not be allowed to do this even if they really wanted to. As a consequence, they needed to find a way to have television and Internet coexist on the same cable. 

Cable television channels in North America normally occupy the 54–550 MHz region (except for FM radio from 88 to 108 MHz). These channels are 6 MHz wide, including guard bands. In Europe the low end is usually 65 MHz and the channels are 6–8 MHz wide for the higher resolution required by PAL and SECAM but otherwise the allocation scheme is similar. The low part of the band is not used. Modern cables can also operate well above 550 MHz, often to 750 MHz or more. The solution chosen was to introduce upstream channels in the 5–42 MHz band (slightly higher in Europe) and use the frequencies at the high end for the downstream. The cable spectrum is illustrated in 

Fig. 2-48

. 

Figure 2-48. Frequency allocation in a typical cable TV system used for Internet access. 

 

Note that since the television signals are all downstream, it is possible to use upstream amplifiers that work only in the 5–42 MHz region and downstream amplifiers that work only at 54 MHz and up, as shown in the figure. Thus, we get an asymmetry in the upstream and downstream bandwidths because more spectrum is available above television than below it. On the other hand, most of the traffic is likely to be downstream, so cable operators are not unhappy with this fact of life. As we saw earlier, telephone companies usually offer an asymmetric DSL service, even though they have no technical reason for doing so. 

Long coaxial cables are not any better for transmitting digital signals than are long local loops, so analog modulation is needed here, too. The usual scheme is to take each 6 MHz or 8 MHz downstream channel and modulate it with QAM-64 or, if the cable quality is exceptionally good, QAM-256. With a 6 MHz channel and QAM-64, we get about 36 Mbps. When the overhead is subtracted, the net payload is about 27 Mbps. With QAM-256, the net payload is about 39 Mbps. The European values are 1/3 larger. 

For upstream, even QAM-64 does not work well. There is too much noise from terrestrial microwaves, CB radios, and other sources, so a more conservative scheme—QPSK—is used. This method (shown in 

Fig. 2-25

) yields 2 bits per baud instead of the 6 or 8 bits QAM provides on the downstream channels. Consequently, the asymmetry between upstream bandwidth and downstream bandwidth is much more than suggested by 

Fig. 2-

48

. 

In addition to upgrading the amplifiers, the operator has to upgrade the headend, too, from a dumb amplifier to an intelligent digital computer system with a high-bandwidth fiber interface to an ISP. Often the name gets upgraded as well, from ''headend'' to 

CMTS

 (

Cable Modem Termination System

). In the following text, we will refrain from doing a name upgrade and stick with the traditional ''headend.'' 

2.7.4 Cable Modems 

Internet access requires a cable modem, a device that has two interfaces on it: one to the computer and one to the cable network. In the early years of cable Internet, each operator had a proprietary cable modem, which was installed by a cable company technician. However, it soon became apparent that an open standard would create 

131




a competitive cable modem market and drive down prices, thus encouraging use of the service. Furthermore, having the customers buy cable modems in stores and install them themselves (as they do with V.9x telephone modems) would eliminate the dreaded truck rolls. 

Consequently, the larger cable operators teamed up with a company called CableLabs to produce a cable modem standard and to test products for compliance. This standard, called 

DOCSIS

 (

Data Over Cable Service Interface Specification

) is just starting to replace proprietary modems. The European version is called 

EuroDOCSIS

. Not all cable operators like the idea of a standard, however, since many of them were making good money leasing their modems to their captive customers. An open standard with dozens of manufacturers selling cable modems in stores ends this lucrative practice. 

The modem-to-computer interface is straightforward. It is normally 10-Mbps Ethernet (or occasionally USB) at present. In the future, the entire modem might be a small card plugged into the computer, just as with V.9x internal modems. 

The other end is more complicated. A large part of the standard deals with radio engineering, a subject that is far beyond the scope of this book. The only part worth mentioning here is that cable modems, like ADSL modems, are always on. They make a connection when turned on and maintain that connection as long as they are powered up because cable operators do not charge for connect time. 

To better understand how they work, let us see what happens when a cable modem is plugged in and powered up. The modem scans the downstream channels looking for a special packet periodically put out by the headend to provide system parameters to modems that have just come on-line. Upon finding this packet, the new modem announces its presence on one of the upstream channels. The headend responds by assigning the modem to its upstream and downstream channels. These assignments can be changed later if the headend deems it necessary to balance the load. 

The modem then determines its distance from the headend by sending it a special packet and seeing how long it takes to get the response. This process is called 

ranging

. It is important for the modem to know its distance to accommodate the way the upstream channels operate and to get the timing right. They are divided in time in 

minislots

. Each upstream packet must fit in one or more consecutive minislots. The headend announces the start of a new round of minislots periodically, but the starting gun is not heard at all modems simultaneously due to the propagation time down the cable. By knowing how far it is from the headend, each modem can compute how long ago the first minislot really started. Minislot length is network dependent. A typical payload is 8 bytes. 

During initialization, the headend also assigns each modem to a minislot to use for requesting upstream bandwidth. As a rule, multiple modems will be assigned the same minislot, which leads to contention. When a computer wants to send a packet, it transfers the packet to the modem, which then requests the necessary number of minislots for it. If the request is accepted, the headend puts an acknowledgement on the downstream channel telling the modem which minislots have been reserved for its packet. The packet is then sent, starting in the minislot allocated to it. Additional packets can be requested using a field in the header. 

On the other hand, if there is contention for the request minislot, there will be no acknowledgement and the modem just waits a random time and tries again. After each successive failure, the randomization time is doubled. (For readers already somewhat familiar with networking, this algorithm is just slotted ALOHA with binary exponential backoff. Ethernet cannot be used on cable because stations cannot sense the medium. We will come back to these issues in 

Chap. 4

.) 

The downstream channels are managed differently from the upstream channels. For one thing, there is only one sender (the headend) so there is no contention and no need for minislots, which is actually just time division statistical multiplexing. For another, the traffic downstream is usually much larger than upstream, so a fixed packet size of 204 bytes is used. Part of that is a Reed-Solomon error-correcting code and some other overhead, leaving a user payload of 184 bytes. These numbers were chosen for compatibility with digital television using MPEG-2, so the TV and downstream data channels are formatted the same way. Logically, the connections are as depicted in 

Fig. 2-49

. 

Figure 2-49. Typical details of the upstream and downstream channels in North America. 

132




 

Getting back to modem initialization, once the modem has completed ranging and gotten its upstream channel, downstream channel, and minislot assignments, it is free to start sending packets. The first packet it sends is one to the ISP requesting an IP address, which is dynamically assigned using a protocol called DHCP, which we will study in 

Chap. 5

. It also requests and gets an accurate time of day from the headend. 

The next step involves security. Since cable is a shared medium, anybody who wants to go to the trouble to do so can read all the traffic going past him. To prevent everyone from snooping on their neighbors (literally), all traffic is encrypted in both directions. Part of the initialization procedure involves establishing encryption keys. At first one might think that having two strangers, the headend and the modem, establish a secret key in broad daylight with thousands of people watching would be impossible. Turns out it is not, but we have to wait until 

Chap. 8

 to explain how (the short answer: use the Diffie-Hellman algorithm). 

Finally, the modem has to log in and provide its unique identifier over the secure channel. At this point the initialization is complete. The user can now log in to the ISP and get to work. 

There is much more to be said about cable modems. Some relevant references are (Adams and Dulchinos, 2001; Donaldson and Jones, 2001; and Dutta-Roy, 2001). 

2.7.5 ADSL versus Cable 

Which is better, ADSL or cable? That is like asking which operating system is better. Or which language is better. Or which religion. Which answer you get depends on whom you ask. Let us compare ADSL and cable on a few points. Both use fiber in the backbone, but they differ on the edge. Cable uses coax; ADSL uses twisted pair. The theoretical carrying capacity of coax is hundreds of times more than twisted pair. However, the full capacity of the cable is not available for data users because much of the cable's bandwidth is wasted on useless stuff such as television programs. 

In practice, it is hard to generalize about effective capacity. ADSL providers give specific statements about the bandwidth (e.g., 1 Mbps downstream, 256 kbps upstream) and generally achieve about 80% of it consistently. Cable providers do not make any claims because the effective capacity depends on how many people are currently active on the user's cable segment. Sometimes it may be better than ADSL and sometimes it may be worse. What can be annoying, though, is the unpredictability. Having great service one minute does not guarantee great service the next minute since the biggest bandwidth hog in town may have just turned on his computer. 

As an ADSL system acquires more users, their increasing numbers have little effect on existing users, since each user has a dedicated connection. With cable, as more subscribers sign up for Internet service, performance for existing users will drop. The only cure is for the cable operator to split busy cables and connect each one to a fiber node directly. Doing so costs time and money, so their are business pressures to avoid it. 

As an aside, we have already studied another system with a shared channel like cable: the mobile telephone system. Here, too, a group of users, we could call them cellmates, share a fixed amount of bandwidth. Normally, it is rigidly divided in fixed chunks among the active users by FDM and TDM because voice traffic is fairly smooth. But for data traffic, this rigid division is very inefficient because data users are frequently idle, in which case their reserved bandwidth is wasted. Nevertheless, in this respect, cable access is more like the mobile phone system than it is like the fixed system. 

133




Availability is an issue on which ADSL and cable differ. Everyone has a telephone, but not all users are close enough to their end office to get ADSL. On the other hand, not everyone has cable, but if you do have cable and the company provides Internet access, you can get it. Distance to the fiber node or headend is not an issue. It is also worth noting that since cable started out as a television distribution medium, few businesses have it. 

Being a point-to-point medium, ADSL is inherently more secure than cable. Any cable user can easily read all the packets going down the cable. For this reason, any decent cable provider will encrypt all traffic in both directions. Nevertheless, having your neighbor get your encrypted messages is still less secure than having him not get anything at all. 

The telephone system is generally more reliable than cable. For example, it has backup power and continues to work normally even during a power outage. With cable, if the power to any amplifier along the chain fails, all downstream users are cut off instantly. 

Finally, most ADSL providers offer a choice of ISPs. Sometimes they are even required to do so by law. This is not always the case with cable operators. 

The conclusion is that ADSL and cable are much more alike than they are different. They offer comparable service and, as competition between them heats up, probably comparable prices. 

2.8 Summary 

The physical layer is the basis of all networks. Nature imposes two fundamental limits on all channels, and these determine their bandwidth. These limits are the Nyquist limit, which deals with noiseless channels, and the Shannon limit, which deals with noisy channels. 

Transmission media can be guided or unguided. The principal guided media are twisted pair, coaxial cable, and fiber optics. Unguided media include radio, microwaves, infrared, and lasers through the air. An up-and-coming transmission system is satellite communication, especially LEO systems. 

A key element in most wide area networks is the telephone system. Its main components are the local loops, trunks, and switches. Local loops are analog, twisted pair circuits, which require modems for transmitting digital data. ADSL offers speeds up to 50 Mbps by dividing the local loop into many virtual channels and modulating each one separately. Wireless local loops are another new development to watch, especially LMDS. 

Trunks are digital, and can be multiplexed in several ways, including FDM, TDM, and WDM. Both circuit switching and packet switching are important. 

For mobile applications, the fixed telephone system is not suitable. Mobile phones are currently in widespread use for voice and will soon be in widespread use for data. The first generation was analog, dominated by AMPS. The second generation was digital, with D-AMPS, GSM, and CDMA the major options. The third generation will be digital and based on broadband CDMA. 

An alternative system for network access is the cable television system, which has gradually evolved from a community antenna to hybrid fiber coax. Potentially, it offers very high bandwidth, but the actual bandwidth available in practice depends heavily on the number of other users currently active and what they are doing. 

Problems 

1. Compute the Fourier coefficients for the function 

f

(

t

) = 

t

 (0 

t

 

1)

.

 

2. A noiseless 4-kHz channel is sampled every 1 msec. What is the maximum data rate? 

3. Television channels are 6 MHz wide. How many bits/sec can be sent if four-level digital signals are used? Assume a noiseless channel. 

4. If a binary signal is sent over a 3-kHz channel whose signal-to-noise ratio is 20 dB, what is the maximum achievable data rate? 

5. What signal-to-noise ratio is needed to put a T1 carrier on a 50-kHz line? 

134




6. What is the difference between a passive star and an active repeater in a fiber network? 

7. How much bandwidth is there in 0.1 micron of spectrum at a wavelength of 1 micron? 

8. It is desired to send a sequence of computer screen images over an optical fiber. The screen is 480 x 640 pixels, each pixel being 24 bits. There are 60 screen images per second. How much bandwidth is needed, and how many microns of wavelength are needed for this band at 1.30 microns? 

9. Is the Nyquist theorem true for optical fiber or only for copper wire? 

10. In 

Fig. 2-6

 the lefthand band is narrower than the others. Why? 

11. Radio antennas often work best when the diameter of the antenna is equal to the wavelength of the radio wave. Reasonable antennas range from 1 cm to 5 meters in diameter. What frequency range does this cover? 

12. Multipath fading is maximized when the two beams arrive 180 degrees out of phase. How much of a path difference is required to maximize the fading for a 50-km-long 1-GHz microwave link? 

13. A laser beam 1 mm wide is aimed at a detector 1 mm wide 100 m away on the roof of a building. How much of an angular diversion (in degrees) does the laser have to have before it misses the detector? 

14. The 66 low-orbit satellites in the Iridium project are divided into six necklaces around the earth. At the altitude they are using, the period is 90 minutes. What is the average interval for handoffs for a stationary transmitter? 

15. Consider a satellite at the altitude of geostationary satellites but whose orbital plane is inclined to the equatorial plane by an angle 

. To a stationary user on the earth's surface at north latitude 

, does this satellite appear motionless in the sky? If not, describe its motion. 

16. How many end office codes were there pre-1984, when each end office was named by its three-digit area code and the first three digits of the local number? Area codes started with a digit in the range 2–9, had a 0 or 1 as the second digit, and ended with any digit. The first two digits of a local number were always in the range 2–9. The third digit could be any digit. 

17. Using 

only

 the data given in the text, what is the maximum number of telephones that the existing U.S. system can support without changing the numbering plan or adding additional equipment? Could this number of telephones actually be achieved? For purposes of this problem, a computer or fax machine counts as a telephone. Assume there is only one device per subscriber line. 

18. A simple telephone system consists of two end offices and a single toll office to which each end office is connected by a 1-MHz full-duplex trunk. The average telephone is used to make four calls per 8-hour workday. The mean call duration is 6 min. Ten percent of the calls are long-distance (i.e., pass through the toll office). What is the maximum number of telephones an end office can support? (Assume 4 kHz per circuit.) 

19. A regional telephone company has 10 million subscribers. Each of their telephones is connected to a central office by a copper twisted pair. The average length of these twisted pairs is 10 km. How much is the copper in the local loops worth? Assume that the cross section of each strand is a circle 1 mm in diameter, the density of copper is 9.0 grams/cm

3

, and that copper sells for 3 dollars per kilogram. 

20. Is an oil pipeline a simplex system, a half-duplex system, a full-duplex system, or none of the above? 

21. The cost of a fast microprocessor has dropped to the point where it is now possible to put one in each modem. How does that affect the handling of telephone line errors? 

22. A modem constellation diagram similar to 

Fig. 2-25

 has data points at the following coordinates: (1, 1), (1, -1), (-1, 1), and (-1, -1). How many bps can a modem with these parameters achieve at 1200 baud? 

23. A modem constellation diagram similar to 

Fig. 2-25

 has data points at (0, 1) and (0, 2). Does the modem use phase modulation or amplitude modulation? 

24. In a constellation diagram, all the points lie on a circle centered on the origin. What kind of modulation is being used? 

25. How many frequencies does a full-duplex QAM-64 modem use? 

26. An ADSL system using DMT allocates 3/4 of the available data channels to the downstream link. It uses QAM-64 modulation on each channel. What is the capacity of the downstream link? 

27. In the four-sector LMDS example of 

Fig. 2-30

, each sector has its own 36-Mbps channel. According to queueing theory, if the channel is 50% loaded, the queueing time will be equal to the download time. Under these conditions, how long does it take to download a 5-KB Web page? How long does it take to download the page over a 1-Mbps ADSL line? Over a 56-kbps modem? 

28. Ten signals, each requiring 4000 Hz, are multiplexed on to a single channel using FDM. How much minimum bandwidth is required for the multiplexed channel? Assume that the guard bands are 400 Hz wide. 

29. Why has the PCM sampling time been set at 125 µsec? 

30. What is the percent overhead on a T1 carrier; that is, what percent of the 1.544 Mbps are not delivered to the end user? 

31. Compare the maximum data rate of a noiseless 4-kHz channel using 

135




a. (a) Analog encoding (e.g., QPSK) with 2 bits per sample. 

b. (b) The T1 PCM system. 

32. If a T1 carrier system slips and loses track of where it is, it tries to resynchronize using the 1st bit in each frame. How many frames will have to be inspected on average to resynchronize with a probability of 0.001 of being wrong? 

33. What is the difference, if any, between the demodulator part of a modem and the coder part of a codec? (After all, both convert analog signals to digital ones.) 

34. A signal is transmitted digitally over a 4-kHz noiseless channel with one sample every 125 µsec

.

 How many bits per second are actually sent for each of these encoding methods? 

a. (a) CCITT 2.048 Mbps standard. 

b. (b) DPCM with a 4-bit relative signal value. 

c. (c) Delta modulation. 

35. A pure sine wave of amplitude 

A

 is encoded using delta modulation, with 

x

 samples/sec. An output of +1 corresponds to a signal change of +

A/

8, and an output signal of -1 corresponds to a signal change of -

A/

8

.

 What is the highest frequency that can be tracked without cumulative error? 

36. SONET clocks have a drift rate of about 1 part in 10

9

. How long does it take for the drift to equal the width of 1 bit? What are the implications of this calculation? 

37. In 

Fig. 2-37

, the user data rate for OC-3 is stated to be 148.608 Mbps. Show how this number can be derived from the SONET OC-3 parameters. 

38. To accommodate lower data rates than STS-1, SONET has a system of virtual tributaries (VT). A VT is a partial payload that can be inserted into an STS-1 frame and combined with other partial payloads to fill the data frame. VT1.5 uses 3 columns, VT2 uses 4 columns, VT3 uses 6 columns, and VT6 uses 12 columns of an STS-1 frame. Which VT can accommodate 

a. (a) A DS-1 service (1.544 Mbps)? 

b. (b) European CEPT-1 service (2.048 Mbps)? 

c. (c) A DS-2 service (6.312 Mbps)? 

39. What is the essential difference between message switching and packet switching? 

40. What is the available user bandwidth in an OC-12c connection? 

41. Three packet-switching networks each contain 

n

 nodes. The first network has a star topology with a central switch, the second is a (bidirectional) ring, and the third is fully interconnected, with a wire from every node to every other node. What are the best-, average-, and-worst case transmission paths in hops? 

42. Compare the delay in sending an 

x

-bit message over a 

k

-hop path in a circuit-switched network and in a (lightly loaded) packet-switched network. The circuit setup time is 

s

 sec, the propagation delay is 

d

 sec per hop, the packet size is 

p

 bits, and the data rate is 

b

 bps. Under what conditions does the packet network have a lower delay? 

43. Suppose that 

x

 bits of user data are to be transmitted over a 

k

-hop path in a packet-switched network as a series of packets, each containing 

p

 data bits and 

h

 header bits, with 

x

 

p

 + 

h.

 The bit rate of the lines is 

b

 bps and the propagation delay is negligible. What value of 

p

 minimizes the total delay? 

44. In a typical mobile phone system with hexagonal cells, it is forbidden to reuse a frequency band in an adjacent cell. If 840 frequencies are available, how many can be used in a given cell? 

45. The actual layout of cells is seldom as regular that as shown in 

Fig. 2-41

. Even the shapes of individual cells are typically irregular. Give a possible reason why this might be. 

46. Make a rough estimate of the number of PCS microcells 100 m in diameter it would take to cover San Francisco (120 square km). 

47. Sometimes when a mobile user crosses the boundary from one cell to another, the current call is abruptly terminated, even though all transmitters and receivers are functioning perfectly. Why? 

48. D-AMPS has appreciably worse speech quality than GSM. Is this due to the requirement that D-AMPS be backward compatible with AMPS, whereas GSM had no such constraint? If not, what is the cause? 

49. Calculate the maximum number of users that D-AMPS can support simultaneously within a single cell. Do the same calculation for GSM. Explain the difference. 

50. Suppose that 

A

, 

B

, and 

C

 are simultaneously transmitting 0 bits, using a CDMA system with the chip sequences of 

Fig. 2-45(b)

. What is the resulting chip sequence? 

51. In the discussion about orthogonality of CDMA chip sequences, it was stated that if 

S

•

T

 = 0 then 

is also 0. Prove this. 

52. Consider a different way of looking at the orthogonality property of CDMA chip sequences. Each bit in a pair of sequences can match or not match. Express the orthogonality property in terms of matches and mismatches. 

136




53. A CDMA receiver gets the following chips: (-1 +1 -3 +1 -1 -3 +1 +1). Assuming the chip sequences defined in 

Fig. 2-45(b)

, which stations transmitted, and which bits did each one send? 

54. At the low end, the telephone system is star shaped, with all the local loops in a neighborhood converging on an end office. In contrast, cable television consists of a single long cable snaking its way past all the houses in the same neighborhood. Suppose that a future TV cable were 10 Gbps fiber instead of copper. Could it be used to simulate the telephone model of everybody having their own private line to the end office? If so, how many one-telephone houses could be hooked up to a single fiber? 

55. A cable TV system has 100 commercial channels, all of them alternating programs with advertising. Is this more like TDM or like FDM? 

56. A cable company decides to provide Internet access over cable in a neighborhood consisting of 5000 houses. The company uses a coaxial cable and spectrum allocation allowing 100 Mbps downstream bandwidth per cable. To attract customers, the company decides to guarantee at least 2 Mbps downstream bandwidth to each house at any time. Describe what the cable company needs to do to provide this guarantee. 

57. Using the spectral allocation shown in 

Fig. 2-48

 and the information given in the text, how many Mbps does a cable system allocate to upstream and how many to downstream? 

58. How fast can a cable user receive data if the network is otherwise idle? 

59. Multiplexing STS-1 multiple data streams, called tributaries, plays an important role in SONET. A 3:1 multiplexer multiplexes three input STS-1 tributaries onto one output STS-3 stream. This multiplexing is done byte for byte, that is, the first three output bytes are the first bytes of tributaries 1, 2, and 3, respectively. The next three output bytes are the second bytes of tributaries 1, 2, and 3, respectively, and so on. Write a program that simulates this 3:1 multiplexer. Your program should consist of five processes. The main process creates four processes, one each for the three STS-1 tributaries and one for the multiplexer. Each tributary process reads in an STS-1 frame from an input file as a sequence of 810 bytes. They send their frames (byte by byte) to the multiplexer process. The multiplexer process receives these bytes and outputs an STS-3 frame (byte by byte) by writing it on standard output. Use pipes for communication among processes. 

 

137




Chapter 3. The Data Link Layer 

In this chapter we will study the design principles for layer 2, the data link layer. This study deals with the algorithms for achieving reliable, efficient communication between two adjacent machines at the data link layer. By adjacent, we mean that the two machines are connected by a communication channel that acts conceptually like a wire (e.g., a coaxial cable, telephone line, or point-to-point wireless channel). The essential property of a channel that makes it ''wirelike'' is that the bits are delivered in exactly the same order in which they are sent. 

At first you might think this problem is so trivial that there is no software to study—machine 

A

 just puts the bits on the wire, and machine 

B

 just takes them off. Unfortunately, communication circuits make errors occasionally. Furthermore, they have only a finite data rate, and there is a nonzero propagation delay between the time a bit is sent and the time it is received. These limitations have important implications for the efficiency of the data transfer. The protocols used for communications must take all these factors into consideration. These protocols are the subject of this chapter. 

After an introduction to the key design issues present in the data link layer, we will start our study of its protocols by looking at the nature of errors, their causes, and how they can be detected and corrected. Then we will study a series of increasingly complex protocols, each one solving more and more of the problems present in this layer. Finally, we will conclude with an examination of protocol modeling and correctness and give some examples of data link protocols. 

3.1 Data Link Layer Design Issues 

The data link layer has a number of specific functions it can carry out. These functions include 

1. Providing a well-defined service interface to the network layer. 

2. Dealing with transmission errors. 

3. Regulating the flow of data so that slow receivers are not swamped by fast senders. 

To accomplish these goals, the data link layer takes the packets it gets from the network layer and encapsulates them into 

frames

 for transmission. Each frame contains a frame header, a payload field for holding the packet, and a frame trailer, as illustrated in 

Fig. 3-1

. Frame management forms the heart of what the data link layer does. In the following sections we will examine all the above-mentioned issues in detail. 

Figure 3-1. Relationship between packets and frames. 

 

Although this chapter is explicitly about the data link layer and the data link protocols, many of the principles we will study here, such as error control and flow control, are found in transport and other protocols as well. In fact, in many networks, these functions are found only in the upper layers and not in the data link layer. However, no matter where they are found, the principles are pretty much the same, so it does not really matter where we study them. In the data link layer they often show up in their simplest and purest forms, making this a good place to examine them in detail. 

138




3.1.1 Services Provided to the Network Layer 

The function of the data link layer is to provide services to the network layer. The principal service is transferring data from the network layer on the source machine to the network layer on the destination machine. On the source machine is an entity, call it a process, in the network layer that hands some bits to the data link layer for transmission to the destination. The job of the data link layer is to transmit the bits to the destination machine so they can be handed over to the network layer there, as shown in 

Fig. 3-2(a)

. The actual transmission follows the path of 

Fig. 3-2(b)

, but it is easier to think in terms of two data link layer processes communicating using a data link protocol. For this reason, we will implicitly use the model of 

Fig. 3-2(a)

 throughout this chapter. 

Figure 3-2. (a) Virtual communication. (b) Actual communication. 

 

The data link layer can be designed to offer various services. The actual services offered can vary from system to system. Three reasonable possibilities that are commonly provided are 

1. Unacknowledged connectionless service. 

2. Acknowledged connectionless service. 

3. Acknowledged connection-oriented service. 

Let us consider each of these in turn. 

Unacknowledged connectionless service consists of having the source machine send independent frames to the destination machine without having the destination machine acknowledge them. No logical connection is established beforehand or released afterward. If a frame is lost due to noise on the line, no attempt is made to detect the loss or recover from it in the data link layer. This class of service is appropriate when the error rate is very low so that recovery is left to higher layers. It is also appropriate for real-time traffic, such as voice, in which late data are worse than bad data. Most LANs use unacknowledged connectionless service in the data link layer. 

The next step up in terms of reliability is acknowledged connectionless service. When this service is offered, there are still no logical connections used, but each frame sent is individually acknowledged. In this way, the sender knows whether a frame has arrived correctly. If it has not arrived within a specified time interval, it can be sent again. This service is useful over unreliable channels, such as wireless systems. 

It is perhaps worth emphasizing that providing acknowledgements in the data link layer is just an optimization, never a requirement. The network layer can always send a packet and wait for it to be acknowledged. If the acknowledgement is not forthcoming before the timer expires, the sender can just send the entire message again. The trouble with this strategy is that frames usually have a strict maximum length imposed by the hardware and network layer packets do not. If the average packet is broken up into, say, 10 frames, and 20 

139




percent of all frames are lost, it may take a very long time for the packet to get through. If individual frames are acknowledged and retransmitted, entire packets get through much faster. On reliable channels, such as fiber, the overhead of a heavyweight data link protocol may be unnecessary, but on wireless channels, with their inherent unreliability, it is well worth the cost. 

Getting back to our services, the most sophisticated service the data link layer can provide to the network layer is connection-oriented service. With this service, the source and destination machines establish a connection before any data are transferred. Each frame sent over the connection is numbered, and the data link layer guarantees that each frame sent is indeed received. Furthermore, it guarantees that each frame is received exactly once and that all frames are received in the right order. With connectionless service, in contrast, it is conceivable that a lost acknowledgement causes a packet to be sent several times and thus received several times. Connection-oriented service, in contrast, provides the network layer processes with the equivalent of a reliable bit stream. 

When connection-oriented service is used, transfers go through three distinct phases. In the first phase, the connection is established by having both sides initialize variables and counters needed to keep track of which frames have been received and which ones have not. In the second phase, one or more frames are actually transmitted. In the third and final phase, the connection is released, freeing up the variables, buffers, and other resources used to maintain the connection. 

Consider a typical example: a WAN subnet consisting of routers connected by point-to-point leased telephone lines. When a frame arrives at a router, the hardware checks it for errors (using techniques we will study late in this chapter), then passes the frame to the data link layer software (which might be embedded in a chip on the network interface board). The data link layer software checks to see if this is the frame expected, and if so, gives the packet contained in the payload field to the routing software. The routing software then chooses the appropriate outgoing line and passes the packet back down to the data link layer software, which then transmits it. The flow over two routers is shown in 

Fig. 3-3

. 

Figure 3-3. Placement of the data link protocol. 

 

The routing code frequently wants the job done right, that is, with reliable, sequenced connections on each of the point-to-point lines. It does not want to be bothered too often with packets that got lost on the way. It is up to the data link protocol, shown in the dotted rectangle, to make unreliable communication lines look perfect or, at least, fairly good. As an aside, although we have shown multiple copies of the data link layer software in each router, in fact, one copy handles all the lines, with different tables and data structures for each one. 

3.1.2 Framing 

To provide service to the network layer, the data link layer must use the service provided to it by the physical layer. What the physical layer does is accept a raw bit stream and attempt to deliver it to the destination. This bit stream is not guaranteed to be error free. The number of bits received may be less than, equal to, or more than 

140




the number of bits transmitted, and they may have different values. It is up to the data link layer to detect and, if necessary, correct errors. 

The usual approach is for the data link layer to break the bit stream up into discrete frames and compute the checksum for each frame. (Checksum algorithms will be discussed later in this chapter.) When a frame arrives at the destination, the checksum is recomputed. If the newly-computed checksum is different from the one contained in the frame, the data link layer knows that an error has occurred and takes steps to deal with it (e.g., discarding the bad frame and possibly also sending back an error report). 

Breaking the bit stream up into frames is more difficult than it at first appears. One way to achieve this framing is to insert time gaps between frames, much like the spaces between words in ordinary text. However, networks rarely make any guarantees about timing, so it is possible these gaps might be squeezed out or other gaps might be inserted during transmission. 

Since it is too risky to count on timing to mark the start and end of each frame, other methods have been devised. In this section we will look at four methods: 

1. Character count. 

2. Flag bytes with byte stuffing. 

3. Starting and ending flags, with bit stuffing. 

4. Physical layer coding violations. 

The first framing method uses a field in the header to specify the number of characters in the frame. When the data link layer at the destination sees the character count, it knows how many characters follow and hence where the end of the frame is. This technique is shown in 

Fig. 3-4(a)

 for four frames of sizes 5, 5, 8, and 8 characters, respectively. 

Figure 3-4. A character stream. (a) Without errors. (b) With one error. 

 

The trouble with this algorithm is that the count can be garbled by a transmission error. For example, if the character count of 5 in the second frame of 

Fig. 3-4(b)

 becomes a 7, the destination will get out of synchronization and will be unable to locate the start of the next frame. Even if the checksum is incorrect so the destination knows that the frame is bad, it still has no way of telling where the next frame starts. Sending a frame back to the source asking for a retransmission does not help either, since the destination does not know how many characters to skip over to get to the start of the retransmission. For this reason, the character count method is rarely used anymore. 

The second framing method gets around the problem of resynchronization after an error by having each frame start and end with special bytes. In the past, the starting and ending bytes were different, but in recent years most protocols have used the same byte, called a 

flag byte

, as both the starting and ending delimiter, as shown in 

Fig. 3-5(a)

 as FLAG. In this way, if the receiver ever loses synchronization, it can just search for the flag byte to find the end of the current frame. Two consecutive flag bytes indicate the end of one frame and start of the next one. 

141




Figure 3-5. (a) A frame delimited by flag bytes. (b) Four examples of byte sequences before and after byte stuffing. 

 

A serious problem occurs with this method when binary data, such as object programs or floating-point numbers, are being transmitted. It may easily happen that the flag byte's bit pattern occurs in the data. This situation will usually interfere with the framing. One way to solve this problem is to have the sender's data link layer insert a special escape byte (ESC) just before each ''accidental'' flag byte in the data. The data link layer on the receiving end removes the escape byte before the data are given to the network layer. This technique is called 

byte stuffing

 or 

character stuffing

. Thus, a framing flag byte can be distinguished from one in the data by the absence or presence of an escape byte before it. 

Of course, the next question is: What happens if an escape byte occurs in the middle of the data? The answer is that it, too, is stuffed with an escape byte. Thus, any single escape byte is part of an escape sequence, whereas a doubled one indicates that a single escape occurred naturally in the data. Some examples are shown in 

Fig. 3-

5(b)

. In all cases, the byte sequence delivered after destuffing is exactly the same as the original byte sequence. 

The byte-stuffing scheme depicted in 

Fig. 3-5

 is a slight simplification of the one used in the PPP protocol that most home computers use to communicate with their Internet service provider. We will discuss PPP later in this chapter. 

A major disadvantage of using this framing method is that it is closely tied to the use of 8-bit characters. Not all character codes use 8-bit characters. For example. UNICODE uses 16-bit characters, As networks developed, the disadvantages of embedding the character code length in the framing mechanism became more and more obvious, so a new technique had to be developed to allow arbitrary sized characters. 

The new technique allows data frames to contain an arbitrary number of bits and allows character codes with an arbitrary number of bits per character. It works like this. Each frame begins and ends with a special bit pattern, 01111110 (in fact, a flag byte). Whenever the sender's data link layer encounters five consecutive 1s in the data, it automatically stuffs a 0 bit into the outgoing bit stream. This 

bit stuffing

 is analogous to byte stuffing, in which an escape byte is stuffed into the outgoing character stream before a flag byte in the data. 

When the receiver sees five consecutive incoming 1 bits, followed by a 0 bit, it automatically destuffs (i.e., deletes) the 0 bit. Just as byte stuffing is completely transparent to the network layer in both computers, so is bit stuffing. If the user data contain the flag pattern, 01111110, this flag is transmitted as 011111010 but stored in the receiver's memory as 01111110. 

Figure 3-6

 gives an example of bit stuffing. 

Figure 3-6. Bit stuffing. (a) The original data. (b) The data as they appear on the line. (c) The data as they are stored in the receiver's memory after destuffing. 

142




 

With bit stuffing, the boundary between two frames can be unambiguously recognized by the flag pattern. Thus, if the receiver loses track of where it is, all it has to do is scan the input for flag sequences, since they can only occur at frame boundaries and never within the data. 

The last method of framing is only applicable to networks in which the encoding on the physical medium contains some redundancy. For example, some LANs encode 1 bit of data by using 2 physical bits. Normally, a 1 bit is a high-low pair and a 0 bit is a low-high pair. The scheme means that every data bit has a transition in the middle, making it easy for the receiver to locate the bit boundaries. The combinations high-high and low-low are not used for data but are used for delimiting frames in some protocols. 

As a final note on framing, many data link protocols use a combination of a character count with one of the other methods for extra safety. When a frame arrives, the count field is used to locate the end of the frame. Only if the appropriate delimiter is present at that position and the checksum is correct is the frame accepted as valid. Otherwise, the input stream is scanned for the next delimiter. 

3.1.3 Error Control 

Having solved the problem of marking the start and end of each frame, we come to the next problem: how to make sure all frames are eventually delivered to the network layer at the destination and in the proper order. Suppose that the sender just kept outputting frames without regard to whether they were arriving properly. This might be fine for unacknowledged connectionless service, but would most certainly not be fine for reliable, connection-oriented service. 

The usual way to ensure reliable delivery is to provide the sender with some feedback about what is happening at the other end of the line. Typically, the protocol calls for the receiver to send back special control frames bearing positive or negative acknowledgements about the incoming frames. If the sender receives a positive acknowledgement about a frame, it knows the frame has arrived safely. On the other hand, a negative acknowledgement means that something has gone wrong, and the frame must be transmitted again. 

An additional complication comes from the possibility that hardware troubles may cause a frame to vanish completely (e.g., in a noise burst). In this case, the receiver will not react at all, since it has no reason to react. It should be clear that a protocol in which the sender transmits a frame and then waits for an acknowledgement, positive or negative, will hang forever if a frame is ever lost due to, for example, malfunctioning hardware. 

This possibility is dealt with by introducing timers into the data link layer. When the sender transmits a frame, it generally also starts a timer. The timer is set to expire after an interval long enough for the frame to reach the destination, be processed there, and have the acknowledgement propagate back to the sender. Normally, the frame will be correctly received and the acknowledgement will get back before the timer runs out, in which case the timer will be canceled. 

However, if either the frame or the acknowledgement is lost, the timer will go off, alerting the sender to a potential problem. The obvious solution is to just transmit the frame again. However, when frames may be transmitted multiple times there is a danger that the receiver will accept the same frame two or more times and pass it to the network layer more than once. To prevent this from happening, it is generally necessary to assign sequence numbers to outgoing frames, so that the receiver can distinguish retransmissions from originals. 

The whole issue of managing the timers and sequence numbers so as to ensure that each frame is ultimately passed to the network layer at the destination exactly once, no more and no less, is an important part of the data link layer's duties. Later in this chapter, we will look at a series of increasingly sophisticated examples to see how this management is done. 

143




3.1.4 Flow Control 

Another important design issue that occurs in the data link layer (and higher layers as well) is what to do with a sender that systematically wants to transmit frames faster than the receiver can accept them. This situation can easily occur when the sender is running on a fast (or lightly loaded) computer and the receiver is running on a slow (or heavily loaded) machine. The sender keeps pumping the frames out at a high rate until the receiver is completely swamped. Even if the transmission is error free, at a certain point the receiver will simply be unable to handle the frames as they arrive and will start to lose some. Clearly, something has to be done to prevent this situation. 

Two approaches are commonly used. In the first one, 

feedback-based flow control

, the receiver sends back information to the sender giving it permission to send more data or at least telling the sender how the receiver is doing. In the second one, 

rate-based flow control

, the protocol has a built-in mechanism that limits the rate at which senders may transmit data, without using feedback from the receiver. In this chapter we will study feedback-based flow control schemes because rate-based schemes are never used in the data link layer. We will look at rate-based schemes in 

Chap. 5

. 

Various feedback-based flow control schemes are known, but most of them use the same basic principle. The protocol contains well-defined rules about when a sender may transmit the next frame. These rules often prohibit frames from being sent until the receiver has granted permission, either implicitly or explicitly. For example, when a connection is set up, the receiver might say: ''You may send me 

n

 frames now, but after they have been sent, do not send any more until I have told you to continue.'' We will examine the details shortly. 

3.2 Error Detection and Correction 

As we saw in 

Chap. 2

, the telephone system has three parts: the switches, the interoffice trunks, and the local loops. The first two are now almost entirely digital in most developed countries. The local loops are still analog twisted copper pairs and will continue to be so for years due to the enormous expense of replacing them. While errors are rare on the digital part, they are still common on the local loops. Furthermore, wireless communication is becoming more common, and the error rates here are orders of magnitude worse than on the interoffice fiber trunks. The conclusion is: transmission errors are going to be with us for many years to come. We have to learn how to deal with them. 

As a result of the physical processes that generate them, errors on some media (e.g., radio) tend to come in bursts rather than singly. Having the errors come in bursts has both advantages and disadvantages over isolated single-bit errors. On the advantage side, computer data are always sent in blocks of bits. Suppose that the block size is 1000 bits and the error rate is 0.001 per bit. If errors were independent, most blocks would contain an error. If the errors came in bursts of 100 however, only one or two blocks in 100 would be affected, on average. The disadvantage of burst errors is that they are much harder to correct than are isolated errors. 

3.2.1 Error-Correcting Codes 

Network designers have developed two basic strategies for dealing with errors. One way is to include enough redundant information along with each block of data sent, to enable the receiver to deduce what the transmitted data must have been. The other way is to include only enough redundancy to allow the receiver to deduce that an error occurred, but not which error, and have it request a retransmission. The former strategy uses 

error-correcting codes

 and the latter uses 

error-detecting codes

. The use of error-correcting codes is often referred to as 

forward error correction

. 

Each of these techniques occupies a different ecological niche. On channels that are highly reliable, such as fiber, it is cheaper to use an error detecting code and just retransmit the occasional block found to be faulty. However, on channels such as wireless links that make many errors, it is better to add enough redundancy to each block for the receiver to be able to figure out what the original block was, rather than relying on a retransmission, which itself may be in error. 

To understand how errors can be handled, it is necessary to look closely at what an error really is. Normally, a frame consists of 

m

 data (i.e., message) bits and 

r

 redundant, or check, bits. Let the total length be 

n

 (i.e., 

n

 = 

m

 + 

r

). An 

n

-bit unit containing data and check bits is often referred to as an 

n

-bit 

codeword

. 

144




Given any two codewords, say, 10001001 and 10110001, it is possible to determine how many corresponding bits differ. In this case, 3 bits differ. To determine how many bits differ, just exclusive OR the two codewords and count the number of 1 bits in the result, for example: 

 

 

The number of bit positions in which two codewords differ is called the 

Hamming distance

 (Hamming, 1950). Its significance is that if two codewords are a Hamming distance 

d

 apart, it will require 

d

 single-bit errors to convert one into the other. 

In most data transmission applications, all 2

m

 possible data messages are legal, but due to the way the check bits are computed, not all of the 2

n

 possible codewords are used. Given the algorithm for computing the check bits, it is possible to construct a complete list of the legal codewords, and from this list find the two codewords whose Hamming distance is minimum. This distance is the Hamming distance of the complete code. 

The error-detecting and error-correcting properties of a code depend on its Hamming distance. To detect 

d

 errors, you need a distance 

d

 + 1 code because with such a code there is no way that 

d

 single-bit errors can change a valid codeword into another valid codeword. When the receiver sees an invalid codeword, it can tell that a transmission error has occurred. Similarly, to correct 

d

 errors, you need a distance 2

d

 + 1 code because that way the legal codewords are so far apart that even with 

d

 changes, the original codeword is still closer than any other codeword, so it can be uniquely determined. 

As a simple example of an error-detecting code, consider a code in which a single 

parity bit

 is appended to the data. The parity bit is chosen so that the number of 1 bits in the codeword is even (or odd). For example, when 1011010 is sent in even parity, a bit is added to the end to make it 10110100. With odd parity 1011010 becomes 10110101. A code with a single parity bit has a distance 2, since any single-bit error produces a codeword with the wrong parity. It can be used to detect single errors. 

As a simple example of an error-correcting code, consider a code with only four valid codewords: 

0000000000, 0000011111, 1111100000, and 1111111111 

This code has a distance 5, which means that it can correct double errors. If the codeword 0000000111 arrives, the receiver knows that the original must have been 0000011111. If, however, a triple error changes 0000000000 into 0000000111, the error will not be corrected properly. 

Imagine that we want to design a code with 

m

 message bits and 

r

 check bits that will allow all single errors to be corrected. Each of the 2

m

 legal messages has 

n

 illegal codewords at a distance 1 from it. These are formed by systematically inverting each of the 

n

 bits in the 

n

-bit codeword formed from it. Thus, each of the 2

m

 legal messages requires 

n

 + 1 bit patterns dedicated to it. Since the total number of bit patterns is 2

n

, we must have (

n

 + 1)2

m

 

2

n

.

 Using 

n

 = 

m

 + 

r

, this requirement becomes (

m

 + 

r

 + 1) 

2

r

.

 Given 

m

, this puts a lower limit on the number of check bits needed to correct single errors. 

This theoretical lower limit can, in fact, be achieved using a method due to Hamming (1950). The bits of the codeword are numbered consecutively, starting with bit 1 at the left end, bit 2 to its immediate right, and so on. The bits that are powers of 2 (1, 2, 4, 8, 16, etc.) are check bits. The rest (3, 5, 6, 7, 9, etc.) are filled up with the 

m

 data bits. Each check bit forces the parity of some collection of bits, including itself, to be even (or odd). A bit may be included in several parity computations. To see which check bits the data bit in position 

k

 contributes to, rewrite 

k

 as a sum of powers of 2. For example, 11 = 1 + 2 + 8 and 29 = 1 + 4 + 8 + 16. A bit is checked by just those check bits occurring in its expansion (e.g., bit 11 is checked by bits 1, 2, and 8). 

145




When a codeword arrives, the receiver initializes a counter to zero. It then examines each check bit, 

k

 (

k

 = 1, 2, 4, 8, ...), to see if it has the correct parity. If not, the receiver adds 

k

 to the counter. If the counter is zero after all the check bits have been examined (i.e., if they were all correct), the codeword is accepted as valid. If the counter is nonzero, it contains the number of the incorrect bit. For example, if check bits 1, 2, and 8 are in error, the inverted bit is 11, because it is the only one checked by bits 1, 2, and 8. 

Figure 3-7

 shows some 7-bit ASCII characters encoded as 11-bit codewords using a Hamming code. Remember that the data are found in bit positions 3, 5, 6, 7, 9, 10, and 11. 

Figure 3-7. Use of a Hamming code to correct burst errors. 

 

Hamming codes can only correct single errors. However, there is a trick that can be used to permit Hamming codes to correct burst errors. A sequence of 

k

 consecutive codewords are arranged as a matrix, one codeword per row. Normally, the data would be transmitted one codeword at a time, from left to right. To correct burst errors, the data should be transmitted one column at a time, starting with the leftmost column. When all 

k

 bits have been sent, the second column is sent, and so on, as indicated in 

Fig. 3-7

. When the frame arrives at the receiver, the matrix is reconstructed, one column at a time. If a burst error of length 

k

 occurs, at most 1 bit in each of the 

k

 codewords will have been affected, but the Hamming code can correct one error per codeword, so the entire block can be restored. This method uses 

kr

 check bits to make blocks of 

km

 data bits immune to a single burst error of length 

k

 or less. 

3.2.2 Error-Detecting Codes 

Error-correcting codes are widely used on wireless links, which are notoriously noisy and error prone when compared to copper wire or optical fibers. Without error-correcting codes, it would be hard to get anything through. However, over copper wire or fiber, the error rate is much lower, so error detection and retransmission is usually more efficient there for dealing with the occasional error. 

As a simple example, consider a channel on which errors are isolated and the error rate is 10

-6

 per bit. Let the block size be 1000 bits. To provide error correction for 1000-bit blocks, 10 check bits are needed; a megabit of data would require 10,000 check bits. To merely detect a block with a single 1-bit error, one parity bit per block will suffice. Once every 1000 blocks, an extra block (1001 bits) will have to be transmitted. The total overhead for the error detection + retransmission method is only 2001 bits per megabit of data, versus 10,000 bits for a Hamming code. 

If a single parity bit is added to a block and the block is badly garbled by a long burst error, the probability that the error will be detected is only 0.5, which is hardly acceptable. The odds can be improved considerably if each block to be sent is regarded as a rectangular matrix 

n

 bits wide and 

k

 bits high, as described above. A parity bit is computed separately for each column and affixed to the matrix as the last row. The matrix is then transmitted one row at a time. When the block arrives, the receiver checks all the parity bits. If any one of them is wrong, the receiver requests a retransmission of the block. Additional retransmissions are requested as needed until an entire block is received without any parity errors. 

146




This method can detect a single burst of length 

n

, since only 1 bit per column will be changed. A burst of length 

n

 + 1 will pass undetected, however, if the first bit is inverted, the last bit is inverted, and all the other bits are correct. (A burst error does not imply that all the bits are wrong; it just implies that at least the first and last are wrong.) If the block is badly garbled by a long burst or by multiple shorter bursts, the probability that any of the 

n

 columns will have the correct parity, by accident, is 0.5, so the probability of a bad block being accepted when it should not be is 2

-

n.

Although the above scheme may sometimes be adequate, in practice, another method is in widespread use: the 

polynomial code

, also known as a 

CRC

 (

Cyclic Redundancy Check

). Polynomial codes are based upon treating bit strings as representations of polynomials with coefficients of 0 and 1 only. A 

k

-bit frame is regarded as the coefficient list for a polynomial with 

k

 terms, ranging from 

x

k

 

- 1

 to 

x

0

.

 Such a polynomial is said to be of degree 

k

 - 1

.

 The high-order (leftmost) bit is the coefficient of 

x

k

 

- 1

; the next bit is the coefficient of 

x

k

 

- 2

, and so on. For example, 110001 has 6 bits and thus represents a six-term polynomial with coefficients 1, 1, 0, 0, 0, and 1: 

x

5

 + 

x

4

 + 

x

0

.

Polynomial arithmetic is done modulo 2, according to the rules of algebraic field theory. There are no carries for addition or borrows for subtraction. Both addition and subtraction are identical to exclusive OR. For example: 

 

 

Long division is carried out the same way as it is in binary except that the subtraction is done modulo 2, as above. A divisor is said ''to go into'' a dividend if the dividend has as many bits as the divisor. 

When the polynomial code method is employed, the sender and receiver must agree upon a 

generator polynomial

, 

G

(

x

), in advance. Both the high- and low-order bits of the generator must be 1. To compute the 

checksum

 for some frame with 

m

 bits, corresponding to the polynomial 

M

(

x

), the frame must be longer than the generator polynomial. The idea is to append a checksum to the end of the frame in such a way that the polynomial represented by the checksummed frame is divisible by 

G

(

x

)

.

 When the receiver gets the checksummed frame, it tries dividing it by 

G

(

x

)

.

 If there is a remainder, there has been a transmission error. 

The algorithm for computing the checksum is as follows: 

1. Let 

r

 be the degree of 

G

(

x

)

.

 Append 

r

 zero bits to the low-order end of the frame so it now contains 

m

 + 

r

 bits and corresponds to the polynomial 

xM

r

(

x

)

.

 

2. Divide the bit string corresponding to 

G

(

x

) into the bit string corresponding to 

xM

r

(

x

), using modulo 2 division. 

3. Subtract the remainder (which is always 

r

 or fewer bits) from the bit string corresponding to 

xM

r

(

x

) using modulo 2 subtraction. The result is the checksummed frame to be transmitted. Call its polynomial 

T

(

x

)

.

 

Figure 3-8

 illustrates the calculation for a frame 1101011011 using the generator 

G

(

x

) = 

x

4

 + 

x

 + 1

.

Figure 3-8. Calculation of the polynomial code checksum. 

147




 

It should be clear that 

T

(

x

) is divisible (modulo 2) by 

G

(

x

)

.

 In any division problem, if you diminish the dividend by the remainder, what is left over is divisible by the divisor. For example, in base 10, if you divide 210,278 by 10,941, the remainder is 2399. By subtracting 2399 from 210,278, what is left over (207,879) is divisible by 10,941. 

Now let us analyze the power of this method. What kinds of errors will be detected? Imagine that a transmission error occurs, so that instead of the bit string for 

T

(

x

) arriving, 

T

(

x

) + 

E

(

x

) arrives. Each 1 bit in 

E

(

x

) corresponds to a bit that has been inverted. If there are 

k

 1 bits in 

E

(

x

), 

k

 single-bit errors have occurred. A single burst error is characterized by an initial 1, a mixture of 0s and 1s, and a final 1, with all other bits being 0. 

Upon receiving the checksummed frame, the receiver divides it by 

G

(

x

); that is, it computes [

T

(

x

) + 

E

(

x

)]

/G

(

x

). 

T

(

x

)

/G

(

x

) is 0, so the result of the computation is simply 

E

(

x

)

/G

(

x

)

.

 Those errors that happen to correspond to polynomials containing 

G

(

x

) as a factor will slip by; all other errors will be caught. 

If there has been a single-bit error, 

E

(

x

) = 

x

i

, where 

i

 determines which bit is in error. If 

G

(

x

) contains two or more terms, it will never divide 

E

(

x

), so all single-bit errors will be detected. 

If there have been two isolated single-bit errors, 

E

(

x

) = 

x

i

 + 

x

j

, where 

i > j.

 Alternatively, this can be written as 

E

(

x

) = 

x

j

(

x

i

 

-

 

j

 + 1)

.

 If we assume that 

G

(

x

) is not divisible by 

x

, a sufficient condition for all double errors to be detected is that 

G

(

x

) does not divide 

x

k

 + 1 for any 

k

 up to the maximum value of 

i

 - 

j

 (i.e., up to the maximum frame length). Simple, low-degree polynomials that give protection to long frames are known. For example, 

x

15

 + 

x

14

 + 1 will not divide 

x

k

 + 1 for any value of 

k

 below 32,768. 

148




If there are an odd number of bits in error, 

E

(

X

) contains an odd number of terms (e.g., 

x

5

 + 

x

2

 + 1, but not 

x

2

 + 1). Interestingly, no polynomial with an odd number of terms has 

x

 + 1 as a factor in the modulo 2 system. By making 

x

 + 1a factor of 

G

(

x

), we can catch all errors consisting of an odd number of inverted bits. 

To see that no polynomial with an odd number of terms is divisible by 

x

 + 1, assume that 

E

(

x

) has an odd number of terms and is divisible by 

x

 + 1

.

 Factor 

E

(

x

) into (

x

 + 1) 

Q

(

x

)

.

 Now evaluate 

E

(1) = (1 + 1)

Q

(1)

.

 Since 1 + 1 = 0 (modulo 2), 

E

(1) must be zero. If 

E

(

x

) has an odd number of terms, substituting 1 for 

x

 everywhere will always yield 1 as the result. Thus, no polynomial with an odd number of terms is divisible by 

x

 + 1

.

Finally, and most importantly, a polynomial code with 

r

 check bits will detect all burst errors of length 

r.

 A burst error of length 

k

 can be represented by 

x

i

(

x

k

 

- 1

 + 

...

 + 1), where 

i

 determines how far from the right-hand end of the received frame the burst is located. If 

G

(

x

) contains an 

x

0

 term, it will not have 

x

i

 as a factor, so if the degree of the parenthesized expression is less than the degree of 

G

(

x

), the remainder can never be zero. 

If the burst length is 

r

 + 1, the remainder of the division by 

G

(

x

) will be zero if and only if the burst is identical to 

G

(

x

)

.

 By definition of a burst, the first and last bits must be 1, so whether it matches depends on the 

r

 - 1 intermediate bits. If all combinations are regarded as equally likely, the probability of such an incorrect frame being accepted as valid is 

½

r

 

- 1

.

It can also be shown that when an error burst longer than 

r

 + 1 bits occurs or when several shorter bursts occur, the probability of a bad frame getting through unnoticed is 

½

r

, assuming that all bit patterns are equally likely. 

Certain polynomials have become international standards. The one used in IEEE 802 is 

 

 

Among other desirable properties, it has the property that it detects all bursts of length 32 or less and all bursts affecting an odd number of bits. 

Although the calculation required to compute the checksum may seem complicated, Peterson and Brown (1961) have shown that a simple shift register circuit can be constructed to compute and verify the checksums in hardware. In practice, this hardware is nearly always used. Virtually all LANs use it and point-to-point lines do, too, in some cases. 

For decades, it has been assumed that frames to be checksummed contain random bits. All analyses of checksum algorithms have been made under this assumption. Inspection of real data has shown this assumption to be quite wrong. As a consequence, under some circumstances, undetected errors are much more common than had been previously thought (Partridge et al., 1995). 

3.3 Elementary Data Link Protocols 

To introduce the subject of protocols, we will begin by looking at three protocols of increasing complexity. For interested readers, a simulator for these and subsequent protocols is available via the Web (see the preface). Before we look at the protocols, it is useful to make explicit some of the assumptions underlying the model of communication. To start with, we assume that in the physical layer, data link layer, and network layer are independent processes that communicate by passing messages back and forth. In many cases, the physical and data link layer processes will be running on a processor inside a special network I/O chip and the network layer code will be running on the main CPU. However, other implementations are also possible (e.g., three processes inside a single I/O chip; or the physical and data link layers as procedures called by the network layer process). In any event, treating the three layers as separate processes makes the discussion conceptually cleaner and also serves to emphasize the independence of the layers. 

Another key assumption is that machine 

A

 wants to send a long stream of data to machine 

B

, using a reliable, connection-oriented service. Later, we will consider the case where 

B

 also wants to send data to 

A

 

149




simultaneously. 

A

 is assumed to have an infinite supply of data ready to send and never has to wait for data to be produced. Instead, when 

A

's data link layer asks for data, the network layer is always able to comply immediately. (This restriction, too, will be dropped later.) 

We also assume that machines do not crash. That is, these protocols deal with communication errors, but not the problems caused by computers crashing and rebooting. 

As far as the data link layer is concerned, the packet passed across the interface to it from the network layer is pure data, whose every bit is to be delivered to the destination's network layer. The fact that the destination's network layer may interpret part of the packet as a header is of no concern to the data link layer. 

When the data link layer accepts a packet, it encapsulates the packet in a frame by adding a data link header and trailer to it (see 

Fig. 3-1

). Thus, a frame consists of an embedded packet, some control information (in the header), and a checksum (in the trailer). The frame is then transmitted to the data link layer on the other machine. We will assume that there exist suitable library procedures 

to

_

physical

_

layer

 to send a frame and 

from

_

physical

_

layer

 to receive a frame. The transmitting hardware computes and appends the checksum (thus creating the trailer), so that the datalink layer software need not worry about it. The polynomial algorithm discussed earlier in this chapter might be used, for example. 

Initially, the receiver has nothing to do. It just sits around waiting for something to happen. In the example protocols of this chapter we will indicate that the data link layer is waiting for something to happen by the procedure call 

wait

_

for

_

event

(&

event

). This procedure only returns when something has happened (e.g., a frame has arrived). Upon return, the variable 

event

 tells what happened. The set of possible events differs for the various protocols to be described and will be defined separately for each protocol. Note that in a more realistic situation, the data link layer will not sit in a tight loop waiting for an event, as we have suggested, but will receive an interrupt, which will cause it to stop whatever it was doing and go handle the incoming frame. Nevertheless, for simplicity we will ignore all the details of parallel activity within the data link layer and assume that it is dedicated full time to handling just our one channel. 

When a frame arrives at the receiver, the hardware computes the checksum. If the checksum is incorrect (i.e., there was a transmission error), the data link layer is so informed (

event

 = 

cksum

_

err

)

.

 If the inbound frame arrived undamaged, the data link layer is also informed (

event

 = 

frame

_

arrival

) so that it can acquire the frame for inspection using 

from

_

physical

_

layer

. As soon as the receiving data link layer has acquired an undamaged frame, it checks the control information in the header, and if everything is all right, passes the packet portion to the network layer. Under no circumstances is a frame header ever given to a network layer. 

There is a good reason why the network layer must never be given any part of the frame header: to keep the network and data link protocols completely separate. As long as the network layer knows nothing at all about the data link protocol or the frame format, these things can be changed without requiring changes to the network layer's software. Providing a rigid interface between network layer and data link layer greatly simplifies the software design because communication protocols in different layers can evolve independently. 

Figure 3-9

 shows some declarations (in C) common to many of the protocols to be discussed later. Five data structures are defined there: 

boolean

, 

seq

_

nr

, 

packet

, 

frame

_

kind

, and 

frame

. A 

boolean

 is an enumerated type and can take on the values 

true

 and 

false

. A 

seq

_

nr

 is a small integer used to number the frames so that we can tell them apart. These sequence numbers run from 0 up to and including 

MAX

_

SEQ

, which is defined in each protocol needing it. A 

packet

 is the unit of information exchanged between the network layer and the data link layer on the same machine, or between network layer peers. In our model it always contains 

MAX

_

PKT

 bytes, but more realistically it would be of variable length. 

Figure 3-9. Some definitions needed in the protocols to follow. These definitions are located in the file 

protocol.h

. 

150




 

A 

frame

 is composed of four fields: 

kind

, 

seq

, 

ack

, and 

info

, the first three of which contain control information and the last of which may contain actual data to be transferred. These control fields are collectively called the 

frame header

. 

The 

kind

 field tells whether there are any data in the frame, because some of the protocols distinguish frames containing only control information from those containing data as well. The 

seq

 and 

ack

 fields are used for sequence numbers and acknowledgements, respectively; their use will be described in more detail later. The 

info

 field of a data frame contains a single packet; the 

info

 field of a control frame is not used. A more realistic implementation would use a variable-length 

info

 field, omitting it altogether for control frames. 

Again, it is important to realize the relationship between a packet and a frame. The network layer builds a packet by taking a message from the transport layer and adding the network layer header to it. This packet is passed to the data link layer for inclusion in the 

info

 field of an outgoing frame. When the frame arrives at the destination, the data link layer extracts the packet from the frame and passes the packet to the network layer. In this manner, the network layer can act as though machines can exchange packets directly. 

151




A number of procedures are also listed in 

Fig. 3-9

. These are library routines whose details are implementation dependent and whose inner workings will not concern us further here. The procedure 

wait

_

for

_

event

 sits in a tight loop waiting for something to happen, as mentioned earlier. The procedures 

to

_

network

_

layer

 and 

from

_

network

_

layer

 are used by the data link layer to pass packets to the network layer and accept packets from the network layer, respectively. Note that 

from

_

physical

_

layer

 and 

to

_

physical

_

layer

 pass frames between the data link layer and physical layer. On the other hand, the procedures 

to

_

network

_

layer

 and 

from

_

network

_

layer

 pass packets between the data link layer and network layer. In other words, 

to

_

network

_

layer

 and 

from

_

network

_

layer

 deal with the interface between layers 2 and 3, whereas 

from

_

physical

_

layer

 and 

to

_

physical

_

layer

 deal with the interface between layers 1 and 2. 

In most of the protocols, we assume that the channel is unreliable and loses entire frames upon occasion. To be able to recover from such calamities, the sending data link layer must start an internal timer or clock whenever it sends a frame. If no reply has been received within a certain predetermined time interval, the clock times out and the data link layer receives an interrupt signal. 

In our protocols this is handled by allowing the procedure 

wait

_

for

_

event

 to return 

event

 = 

timeout.

 The procedures 

start

_

timer

 and 

stop

_

timer

 turn the timer on and off, respectively. Timeouts are possible only when the timer is running. It is explicitly permitted to call 

start

_

timer

 while the timer is running; such a call simply resets the clock to cause the next timeout after a full timer interval has elapsed (unless it is reset or turned off in the meanwhile). 

The procedures 

start

_

ack

_

timer

 and 

stop

_

ack

_

timer

 control an auxiliary timer used to generate acknowledgements under certain conditions. 

The procedures 

enable

_

network

_

layer

 and 

disable

_

network

_

layer

 are used in the more sophisticated protocols, where we no longer assume that the network layer always has packets to send. When the data link layer enables the network layer, the network layer is then permitted to interrupt when it has a packet to be sent. We indicate this with 

event

 = 

network

_

layer

_

ready.

 When a network layer is disabled, it may not cause such events. By being careful about when it enables and disables its network layer, the data link layer can prevent the network layer from swamping it with packets for which it has no buffer space. 

Frame sequence numbers are always in the range 0 to 

MAX

_

SEQ

 (inclusive), where 

MAX

_

SEQ

 is different for the different protocols. It is frequently necessary to advance a sequence number by 1 circularly (i.e., 

MAX

_

SEQ

 is followed by 0). The macro 

inc

 performs this incrementing. It has been defined as a macro because it is used in-line within the critical path. As we will see later, the factor limiting network performance is often protocol processing, so defining simple operations like this as macros does not affect the readability of the code but does improve performance. Also, since 

MAX

_

SEQ

 will have different values in different protocols, by making it a macro, it becomes possible to include all the protocols in the same binary without conflict. This ability is useful for the simulator. 

The declarations of 

Fig. 3-9

 are part of each of the protocols to follow. To save space and to provide a convenient reference, they have been extracted and listed together, but conceptually they should be merged with the protocols themselves. In C, this merging is done by putting the definitions in a special header file, in this case 

protocol.h

, and using the #include facility of the C preprocessor to include them in the protocol files. 

3.3.1 An Unrestricted Simplex Protocol 

As an initial example we will consider a protocol that is as simple as it can be. Data are transmitted in one direction only. Both the transmitting and receiving network layers are always ready. Processing time can be ignored. Infinite buffer space is available. And best of all, the communication channel between the data link layers never damages or loses frames. This thoroughly unrealistic protocol, which we will nickname ''utopia,'' is shown in 

Fig. 3-10

. 

Figure 3-10. An unrestricted simplex protocol. 

152




 

The protocol consists of two distinct procedures, a sender and a receiver. The sender runs in the data link layer of the source machine, and the receiver runs in the data link layer of the destination machine. No sequence numbers or acknowledgements are used here, so 

MAX

_

SEQ

 is not needed. The only event type possible is 

frame

_

arrival

 (i.e., the arrival of an undamaged frame). 

The sender is in an infinite 

while

 loop just pumping data out onto the line as fast as it can. The body of the loop consists of three actions: go fetch a packet from the (always obliging) network layer, construct an outbound frame using the variable 

s

, and send the frame on its way. Only the 

info

 field of the frame is used by this protocol, because the other fields have to do with error and flow control and there are no errors or flow control restrictions here. 

The receiver is equally simple. Initially, it waits for something to happen, the only possibility being the arrival of an undamaged frame. Eventually, the frame arrives and the procedure 

wait

_

for

_

event

 returns, with 

event

 set to 

frame

_

arrival

 (which is ignored anyway). The call to 

from

_

physical

_

layer

 removes the newly arrived frame from the hardware buffer and puts it in the variable 

r

, where the receiver code can get at it. Finally, the data portion is passed on to the network layer, and the data link layer settles back to wait for the next frame, effectively suspending itself until the frame arrives. 

153




3.3.2 A Simplex Stop-and-Wait Protocol 

Now we will drop the most unrealistic restriction used in protocol 1: the ability of the receiving network layer to process incoming data infinitely quickly (or equivalently, the presence in the receiving data link layer of an infinite amount of buffer space in which to store all incoming frames while they are waiting their respective turns). The communication channel is still assumed to be error free however, and the data traffic is still simplex. 

The main problem we have to deal with here is how to prevent the sender from flooding the receiver with data faster than the latter is able to process them. In essence, if the receiver requires a time 

t

 to execute 

from

_

physical

_

layer

 plus 

to

_

network

_

layer

, the sender must transmit at an average rate less than one frame per time 

t

. Moreover, if we assume that no automatic buffering and queueing are done within the receiver's hardware, the sender must never transmit a new frame until the old one has been fetched by 

from

_

physical

_

layer

, lest the new one overwrite the old one. 

In certain restricted circumstances (e.g., synchronous transmission and a receiving data link layer fully dedicated to processing the one input line), it might be possible for the sender to simply insert a delay into protocol 1 to slow it down sufficiently to keep from swamping the receiver. However, more usually, each data link layer will have several lines to attend to, and the time interval between a frame arriving and its being processed may vary considerably. If the network designers can calculate the worst-case behavior of the receiver, they can program the sender to transmit so slowly that even if every frame suffers the maximum delay, there will be no overruns. The trouble with this approach is that it is too conservative. It leads to a bandwidth utilization that is far below the optimum, unless the best and worst cases are almost the same (i.e., the variation in the data link layer's reaction time is small). 

A more general solution to this dilemma is to have the receiver provide feedback to the sender. After having passed a packet to its network layer, the receiver sends a little dummy frame back to the sender which, in effect, gives the sender permission to transmit the next frame. After having sent a frame, the sender is required by the protocol to bide its time until the little dummy (i.e., acknowledgement) frame arrives. Using feedback from the receiver to let the sender know when it may send more data is an example of the flow control mentioned earlier. 

Protocols in which the sender sends one frame and then waits for an acknowledgement before proceeding are called 

stop-and-wait

. 

Figure 3-11

 gives an example of a simplex stop-and-wait protocol. 

Figure 3-11. A simplex stop-and-wait protocol. 

154




 

Although data traffic in this example is simplex, going only from the sender to the receiver, frames do travel in both directions. Consequently, the communication channel between the two data link layers needs to be capable of bidirectional information transfer. However, this protocol entails a strict alternation of flow: first the sender sends a frame, then the receiver sends a frame, then the sender sends another frame, then the receiver sends another one, and so on. A half- duplex physical channel would suffice here. 

As in protocol 1, the sender starts out by fetching a packet from the network layer, using it to construct a frame, and sending it on its way. But now, unlike in protocol 1, the sender must wait until an acknowledgement frame arrives before looping back and fetching the next packet from the network layer. The sending data link layer need not even inspect the incoming frame: there is only one possibility. The incoming frame is always an acknowledgement. 

The only difference between 

receiver1

 and 

receiver2

 is that after delivering a packet to the network layer, 

receiver2

 sends an acknowledgement frame back to the sender before entering the wait loop again. Because only the arrival of the frame back at the sender is important, not its contents, the receiver need not put any particular information in it. 

3.3.3 A Simplex Protocol for a Noisy Channel 

Now let us consider the normal situation of a communication channel that makes errors. Frames may be either damaged or lost completely. However, we assume that if a frame is damaged in transit, the receiver hardware will detect this when it computes the checksum. If the frame is damaged in such a way that the checksum is nevertheless correct, an unlikely occurrence, this protocol (and all other protocols) can fail (i.e., deliver an incorrect packet to the network layer). 

155




At first glance it might seem that a variation of protocol 2 would work: adding a timer. The sender could send a frame, but the receiver would only send an acknowledgement frame if the data were correctly received. If a damaged frame arrived at the receiver, it would be discarded. After a while the sender would time out and send the frame again. This process would be repeated until the frame finally arrived intact. 

The above scheme has a fatal flaw in it. Think about the problem and try to discover what might go wrong before reading further. 

To see what might go wrong, remember that it is the task of the data link layer processes to provide error-free, transparent communication between network layer processes. The network layer on machine 

A

 gives a series of packets to its data link layer, which must ensure that an identical series of packets are delivered to the network layer on machine 

B

 by its data link layer. In particular, the network layer on 

B

 has no way of knowing that a packet has been lost or duplicated, so the data link layer must guarantee that no combination of transmission errors, however unlikely, can cause a duplicate packet to be delivered to a network layer. 

Consider the following scenario: 

1. The network layer on 

A

 gives packet 1 to its data link layer. The packet is correctly received at 

B

 and passed to the network layer on 

B

. 

B

 sends an acknowledgement frame back to 

A

. 

2. The acknowledgement frame gets lost completely. It just never arrives at all. Life would be a great deal simpler if the channel mangled and lost only data frames and not control frames, but sad to say, the channel is not very discriminating. 

3. The data link layer on 

A

 eventually times out. Not having received an acknowledgement, it (incorrectly) assumes that its data frame was lost or damaged and sends the frame containing packet 1 again. 

4. The duplicate frame also arrives at the data link layer on 

B

 perfectly and is unwittingly passed to the network layer there. If 

A

 is sending a file to 

B

, part of the file will be duplicated (i.e., the copy of the file made by 

B

 will be incorrect and the error will not have been detected). In other words, the protocol will fail. 

Clearly, what is needed is some way for the receiver to be able to distinguish a frame that it is seeing for the first time from a retransmission. The obvious way to achieve this is to have the sender put a sequence number in the header of each frame it sends. Then the receiver can check the sequence number of each arriving frame to see if it is a new frame or a duplicate to be discarded. 

Since a small frame header is desirable, the question arises: What is the minimum number of bits needed for the sequence number? The only ambiguity in this protocol is between a frame, 

m

, and its direct successor, 

m

 + 1

.

 If frame 

m

 is lost or damaged, the receiver will not acknowledge it, so the sender will keep trying to send it. Once it has been correctly received, the receiver will send an acknowledgement to the sender. It is here that the potential trouble crops up. Depending upon whether the acknowledgement frame gets back to the sender correctly or not, the sender may try to send 

m

 or 

m

 + 1. 

The event that triggers the sender to start sending frame 

m

 + 2 is the arrival of an acknowledgement for frame 

m

 + 1

.

 But this implies that 

m

 has been correctly received, and furthermore that its acknowledgement has also been correctly received by the sender (otherwise, the sender would not have begun with 

m

 + 1, let alone 

m

 + 2)

.

 As a consequence, the only ambiguity is between a frame and its immediate predecessor or successor, not between the predecessor and successor themselves. 

A 1-bit sequence number (0 or 1) is therefore sufficient. At each instant of time, the receiver expects a particular sequence number next. Any arriving frame containing the wrong sequence number is rejected as a duplicate. When a frame containing the correct sequence number arrives, it is accepted and passed to the network layer. Then the expected sequence number is incremented modulo 2 (i.e., 0 becomes 1 and 1 becomes 0). 

An example of this kind of protocol is shown in 

Fig. 3-12

. Protocols in which the sender waits for a positive acknowledgement before advancing to the next data item are often called 

PAR

 (

Positive Acknowledgement with Retransmission

) or 

ARQ

 (

Automatic Repeat reQuest

). Like protocol 2, this one also transmits data only in one direction. 

Figure 3-12. A positive acknowledgement with retransmission protocol. 

156




 

Protocol 3 differs from its predecessors in that both sender and receiver have a variable whose value is remembered while the data link layer is in the wait state. The sender remembers the sequence number of the next frame to send in 

next

_

frame

_

to

_

send

; the receiver remembers the sequence number of the next frame expected in 

frame

_

expected

. Each protocol has a short initialization phase before entering the infinite loop. 

After transmitting a frame, the sender starts the timer running. If it was already running, it will be reset to allow another full timer interval. The time interval should be chosen to allow enough time for the frame to get to the receiver, for the receiver to process it in the worst case, and for the acknowledgement frame to propagate back to the sender. Only when that time interval has elapsed is it safe to assume that either the transmitted frame or its acknowledgement has been lost, and to send a duplicate. If the timeout interval is set too short, the sender will transmit unnecessary frames. While these extra frames will not affect the correctness of the protocol, they will hurt performance. 

157




After transmitting a frame and starting the timer, the sender waits for something exciting to happen. Only three possibilities exist: an acknowledgement frame arrives undamaged, a damaged acknowledgement frame staggers in, or the timer expires. If a valid acknowledgement comes in, the sender fetches the next packet from its network layer and puts it in the buffer, overwriting the previous packet. It also advances the sequence number. If a damaged frame arrives or no frame at all arrives, neither the buffer nor the sequence number is changed so that a duplicate can be sent. 

When a valid frame arrives at the receiver, its sequence number is checked to see if it is a duplicate. If not, it is accepted, passed to the network layer, and an acknowledgement is generated. Duplicates and damaged frames are not passed to the network layer. 

3.4 Sliding Window Protocols 

In the previous protocols, data frames were transmitted in one direction only. In most practical situations, there is a need for transmitting data in both directions. One way of achieving full-duplex data transmission is to have two separate communication channels and use each one for simplex data traffic (in different directions). If this is done, we have two separate physical circuits, each with a ''forward'' channel (for data) and a ''reverse'' channel (for acknowledgements). In both cases the bandwidth of the reverse channel is almost entirely wasted. In effect, the user is paying for two circuits but using only the capacity of one. 

A better idea is to use the same circuit for data in both directions. After all, in protocols 2 and 3 it was already being used to transmit frames both ways, and the reverse channel has the same capacity as the forward channel. In this model the data frames from 

A

 to 

B

 are intermixed with the acknowledgement frames from 

A

 to 

B

. By looking at the 

kind

 field in the header of an incoming frame, the receiver can tell whether the frame is data or acknowledgement. 

Although interleaving data and control frames on the same circuit is an improvement over having two separate physical circuits, yet another improvement is possible. When a data frame arrives, instead of immediately sending a separate control frame, the receiver restrains itself and waits until the network layer passes it the next packet. The acknowledgement is attached to the outgoing data frame (using the 

ack

 field in the frame header). In effect, the acknowledgement gets a free ride on the next outgoing data frame. The technique of temporarily delaying outgoing acknowledgements so that they can be hooked onto the next outgoing data frame is known as 

piggybacking

. 

The principal advantage of using piggybacking over having distinct acknowledgement frames is a better use of the available channel bandwidth. The 

ack

 field in the frame header costs only a few bits, whereas a separate frame would need a header, the acknowledgement, and a checksum. In addition, fewer frames sent means fewer ''frame arrival'' interrupts, and perhaps fewer buffers in the receiver, depending on how the receiver's software is organized. In the next protocol to be examined, the piggyback field costs only 1 bit in the frame header. It rarely costs more than a few bits. 

However, piggybacking introduces a complication not present with separate acknowledgements. How long should the data link layer wait for a packet onto which to piggyback the acknowledgement? If the data link layer waits longer than the sender's timeout period, the frame will be retransmitted, defeating the whole purpose of having acknowledgements. If the data link layer were an oracle and could foretell the future, it would know when the next network layer packet was going to come in and could decide either to wait for it or send a separate acknowledgement immediately, depending on how long the projected wait was going to be. Of course, the data link layer cannot foretell the future, so it must resort to some ad hoc scheme, such as waiting a fixed number of milliseconds. If a new packet arrives quickly, the acknowledgement is piggybacked onto it; otherwise, if no new packet has arrived by the end of this time period, the data link layer just sends a separate acknowledgement frame. 

The next three protocols are bidirectional protocols that belong to a class called 

sliding window

 protocols. The three differ among themselves in terms of efficiency, complexity, and buffer requirements, as discussed later. In these, as in all sliding window protocols, each outbound frame contains a sequence number, ranging from 0 up to some maximum. The maximum is usually 2

n

 - 1 so the sequence number fits exactly in an 

n

-bit field. The stop-and-wait sliding window protocol uses 

n

 = 1, restricting the sequence numbers to 0 and 1, but more sophisticated versions can use arbitrary 

n

. 

158




The essence of all sliding window protocols is that at any instant of time, the sender maintains a set of sequence numbers corresponding to frames it is permitted to send. These frames are said to fall within the 

sending window

. Similarly, the receiver also maintains a 

receiving window

 corresponding to the set of frames it is permitted to accept. The sender's window and the receiver's window need not have the same lower and upper limits or even have the same size. In some protocols they are fixed in size, but in others they can grow or shrink over the course of time as frames are sent and received. 

Although these protocols give the data link layer more freedom about the order in which it may send and receive frames, we have definitely not dropped the requirement that the protocol must deliver packets to the destination network layer in the same order they were passed to the data link layer on the sending machine. Nor have we changed the requirement that the physical communication channel is ''wire-like,'' that is, it must deliver all frames in the order sent. 

The sequence numbers within the sender's window represent frames that have been sent or can be sent but are as yet not acknowledged. Whenever a new packet arrives from the network layer, it is given the next highest sequence number, and the upper edge of the window is advanced by one. When an acknowledgement comes in, the lower edge is advanced by one. In this way the window continuously maintains a list of unacknowledged frames. 

Figure 3-13

 shows an example. 

Figure 3-13. A sliding window of size 1, with a 3-bit sequence number. (a) Initially. (b) After the first frame has been sent. (c) After the first frame has been received. (d) After the first acknowledgement has been received. 

 

Since frames currently within the sender's window may ultimately be lost or damaged in transit, the sender must keep all these frames in its memory for possible retransmission. Thus, if the maximum window size is 

n

, the sender needs 

n

 buffers to hold the unacknowledged frames. If the window ever grows to its maximum size, the sending data link layer must forcibly shut off the network layer until another buffer becomes free. 

The receiving data link layer's window corresponds to the frames it may accept. Any frame falling outside the window is discarded without comment. When a frame whose sequence number is equal to the lower edge of the window is received, it is passed to the network layer, an acknowledgement is generated, and the window is rotated by one. Unlike the sender's window, the receiver's window always remains at its initial size. Note that a window size of 1 means that the data link layer only accepts frames in order, but for larger windows this is not so. The network layer, in contrast, is always fed data in the proper order, regardless of the data link layer's window size. 

Figure 3-13

 shows an example with a maximum window size of 1. Initially, no frames are outstanding, so the lower and upper edges of the sender's window are equal, but as time goes on, the situation progresses as shown. 

159




3.4.1 A One-Bit Sliding Window Protocol 

Before tackling the general case, let us first examine a sliding window protocol with a maximum window size of 1. Such a protocol uses stop-and-wait since the sender transmits a frame and waits for its acknowledgement before sending the next one. 

Figure 3-14

 depicts such a protocol. Like the others, it starts out by defining some variables. 

Next

_

frame

_

to

_

send

 tells which frame the sender is trying to send. Similarly, 

frame

_

expected

 tells which frame the receiver is expecting. In both cases, 0 and 1 are the only possibilities. 

Figure 3-14. A 1-bit sliding window protocol. 

 

Under normal circumstances, one of the two data link layers goes first and transmits the first frame. In other words, only one of the data link layer programs should contain the 

to

_

physical

_

layer

 and 

start

_

timer

 procedure calls outside the main loop. In the event that both data link layers start off simultaneously, a peculiar situation arises, as discussed later. The starting machine fetches the first packet from its network layer, builds a frame from it, and sends it. When this (or any) frame arrives, the receiving data link layer checks to see if it is a 

160




duplicate, just as in protocol 3. If the frame is the one expected, it is passed to the network layer and the receiver's window is slid up. 

The acknowledgement field contains the number of the last frame received without error. If this number agrees with the sequence number of the frame the sender is trying to send, the sender knows it is done with the frame stored in 

buffer

 and can fetch the next packet from its network layer. If the sequence number disagrees, it must continue trying to send the same frame. Whenever a frame is received, a frame is also sent back. 

Now let us examine protocol 4 to see how resilient it is to pathological scenarios. Assume that computer 

A

 is trying to send its frame 0 to computer 

B

 and that 

B

 is trying to send its frame 0 to 

A

. Suppose that 

A

 sends a frame to 

B

, but 

A

's timeout interval is a little too short. Consequently, 

A

 may time out repeatedly, sending a series of identical frames, all with 

seq

 = 0 and 

ack

 = 1

.

When the first valid frame arrives at computer 

B

, it will be accepted and 

frame

_

expected

 will be set to 1. All the subsequent frames will be rejected because 

B

 is now expecting frames with sequence number 1, not 0. Furthermore, since all the duplicates have 

ack

 = 1 and 

B

 is still waiting for an acknowledgement of 0, 

B

 will not fetch a new packet from its network layer. 

After every rejected duplicate comes in, 

B

 sends 

A

 a frame containing 

seq

 = 0 and 

ack

 = 0. Eventually, one of these arrives correctly at 

A

, causing 

A

 to begin sending the next packet. No combination of lost frames or premature timeouts can cause the protocol to deliver duplicate packets to either network layer, to skip a packet, or to deadlock. 

However, a peculiar situation arises if both sides simultaneously send an initial packet. This synchronization difficulty is illustrated by 

Fig. 3-15

. In part (a), the normal operation of the protocol is shown. In (b) the peculiarity is illustrated. If 

B

 waits for 

A

's first frame before sending one of its own, the sequence is as shown in (a), and every frame is accepted. However, if 

A

 and 

B

 simultaneously initiate communication, their first frames cross, and the data link layers then get into situation (b). In (a) each frame arrival brings a new packet for the network layer; there are no duplicates. In (b) half of the frames contain duplicates, even though there are no transmission errors. Similar situations can occur as a result of premature timeouts, even when one side clearly starts first. In fact, if multiple premature timeouts occur, frames may be sent three or more times. 

Figure 3-15. Two scenarios for protocol 4. (a) Normal case. (b) Abnormal case. The notation is (seq, ack, packet number). An asterisk indicates where a network layer accepts a packet. 

 

3.4.2 A Protocol Using Go Back N 

Until now we have made the tacit assumption that the transmission time required for a frame to arrive at the receiver plus the transmission time for the acknowledgement to come back is negligible. Sometimes this assumption is clearly false. In these situations the long round-trip time can have important implications for the efficiency of the bandwidth utilization. As an example, consider a 50-kbps satellite channel with a 500-msec 

161




round-trip propagation delay. Let us imagine trying to use protocol 4 to send 1000-bit frames via the satellite. At 

t

 = 0 the sender starts sending the first frame. At 

t

 = 20 msec the frame has been completely sent. Not until 

t

 = 270 msec has the frame fully arrived at the receiver, and not until 

t

 = 520 msec has the acknowledgement arrived back at the sender, under the best of circumstances (no waiting in the receiver and a short acknowledgement frame). This means that the sender was blocked during 500/520 or 96 percent of the time. In other words, only 4 percent of the available bandwidth was used. Clearly, the combination of a long transit time, high bandwidth, and short frame length is disastrous in terms of efficiency. 

The problem described above can be viewed as a consequence of the rule requiring a sender to wait for an acknowledgement before sending another frame. If we relax that restriction, much better efficiency can be achieved. Basically, the solution lies in allowing the sender to transmit up to 

w

 frames before blocking, instead of just 1. With an appropriate choice of 

w

 the sender will be able to continuously transmit frames for a time equal to the round-trip transit time without filling up the window. In the example above, 

w

 should be at least 26. The sender begins sending frame 0 as before. By the time it has finished sending 26 frames, at 

t

 = 520, the acknowledgement for frame 0 will have just arrived. Thereafter, acknowledgements arrive every 20 msec, so the sender always gets permission to continue just when it needs it. At all times, 25 or 26 unacknowledged frames are outstanding. Put in other terms, the sender's maximum window size is 26. 

The need for a large window on the sending side occurs whenever the product of bandwidth x round-trip-delay is large. If the bandwidth is high, even for a moderate delay, the sender will exhaust its window quickly unless it has a large window. If the delay is high (e.g., on a geostationary satellite channel), the sender will exhaust its window even for a moderate bandwidth. The product of these two factors basically tells what the capacity of the pipe is, and the sender needs the ability to fill it without stopping in order to operate at peak efficiency. 

This technique is known as 

pipelining

. If the channel capacity is 

b

 bits/sec, the frame size 

l

 bits, and the round-trip propagation time 

R

 sec, the time required to transmit a single frame is 

l/b

 sec. After the last bit of a data frame has been sent, there is a delay of 

R/

2 before that bit arrives at the receiver and another delay of at least 

R/

2 for the acknowledgement to come back, for a total delay of 

R

. In stop-and-wait the line is busy for 

l/b

and idle for 

R

, giving 

 

 

If 

l < bR

, the efficiency will be less than 50 percent. Since there is always a nonzero delay for the acknowledgement to propagate back, pipelining can, in principle, be used to keep the line busy during this interval, but if the interval is small, the additional complexity is not worth the trouble. 

Pipelining frames over an unreliable communication channel raises some serious issues. First, what happens if a frame in the middle of a long stream is damaged or lost? Large numbers of succeeding frames will arrive at the receiver before the sender even finds out that anything is wrong. When a damaged frame arrives at the receiver, it obviously should be discarded, but what should the receiver do with all the correct frames following it? Remember that the receiving data link layer is obligated to hand packets to the network layer in sequence. In 

Fig. 3-16

 we see the effects of pipelining on error recovery. We will now examine it in some detail. 

Figure 3-16. Pipelining and error recovery. Effect of an error when (a) receiver's window size is 1 and (b) receiver's window size is large. 

162




 

Two basic approaches are available for dealing with errors in the presence of pipelining. One way, called 

go back n

, is for the receiver simply to discard all subsequent frames, sending no acknowledgements for the discarded frames. This strategy corresponds to a receive window of size 1. In other words, the data link layer refuses to accept any frame except the next one it must give to the network layer. If the sender's window fills up before the timer runs out, the pipeline will begin to empty. Eventually, the sender will time out and retransmit all unacknowledged frames in order, starting with the damaged or lost one. This approach can waste a lot of bandwidth if the error rate is high. 

In 

Fig. 3-16(a)

 we see go back n for the case in which the receiver's window is large. Frames 0 and 1 are correctly received and acknowledged. Frame 2, however, is damaged or lost. The sender, unaware of this problem, continues to send frames until the timer for frame 2 expires. Then it backs up to frame 2 and starts all over with it, sending 2, 3, 4, etc. all over again. 

The other general strategy for handling errors when frames are pipelined is called 

selective repeat

. When it is used, a bad frame that is received is discarded, but good frames received after it are buffered. When the sender times out, only the oldest unacknowledged frame is retransmitted. If that frame arrives correctly, the receiver can deliver to the network layer, in sequence, all the frames it has buffered. Selective repeat is often combined with having the receiver send a negative acknowledgement (NAK) when it detects an error, for example, when it receives a checksum error or a frame out of sequence. NAKs stimulate retransmission before the corresponding timer expires and thus improve performance. 

In 

Fig. 3-16(b)

, frames 0 and 1 are again correctly received and acknowledged and frame 2 is lost. When frame 3 arrives at the receiver, the data link layer there notices that is has missed a frame, so it sends back a NAK for 2 but buffers 3. When frames 4 and 5 arrive, they, too, are buffered by the data link layer instead of being passed to the network layer. Eventually, the NAK 2 gets back to the sender, which immediately resends frame 2. When that arrives, the data link layer now has 2, 3, 4, and 5 and can pass all of them to the network layer in the correct order. It can also acknowledge all frames up to and including 5, as shown in the figure. If the NAK should get lost, eventually the sender will time out for frame 2 and send it (and only it) of its own accord, but that may be a quite a while later. In effect, the NAK speeds up the retransmission of one specific frame. 

163




Selective repeat corresponds to a receiver window larger than 1. Any frame within the window may be accepted and buffered until all the preceding ones have been passed to the network layer. This approach can require large amounts of data link layer memory if the window is large. 

These two alternative approaches are trade-offs between bandwidth and data link layer buffer space. Depending on which resource is scarcer, one or the other can be used. 

Figure 3-17

 shows a pipelining protocol in which the receiving data link layer only accepts frames in order; frames following an error are discarded. In this protocol, for the first time we have dropped the assumption that the network layer always has an infinite supply of packets to send. When the network layer has a packet it wants to send, it can cause a 

network

_

layer

_

ready

 event to happen. However, to enforce the flow control rule of no more than 

MAX_SEQ

 unacknowledged frames outstanding at any time, the data link layer must be able to keep the network layer from bothering it with more work. The library procedures 

enable

_

network

_

layer

 and 

disable

_

network

_

layer

 do this job. 

Figure 3-17. A sliding window protocol using go back n. 

164




165




Note that a maximum of 

MAX

_

SEQ

 frames and not 

MAX

_

SEQ

 + 1 frames may be outstanding at any instant, even though there are 

MAX

_

SEQ

 + 1 distinct sequence numbers: 0, 1, 2, 

...

, 

MAX

_

SEQ.

 To see why this restriction is required, consider the following scenario with 

MAX

_

SEQ

 = 7

.

1. The sender sends frames 0 through 7. 

2. A piggybacked acknowledgement for frame 7 eventually comes back to the sender. 

3. The sender sends another eight frames, again with sequence numbers 0 through 7. 

4. Now another piggybacked acknowledgement for frame 7 comes in. 

The question is this: Did all eight frames belonging to the second batch arrive successfully, or did all eight get lost (counting discards following an error as lost)? In both cases the receiver would be sending frame 7 as the acknowledgement. The sender has no way of telling. For this reason the maximum number of outstanding frames must be restricted to 

MAX

_

SEQ

. 

Although protocol 5 does not buffer the frames arriving after an error, it does not escape the problem of buffering altogether. Since a sender may have to retransmit all the unacknowledged frames at a future time, it must hang on to all transmitted frames until it knows for sure that they have been accepted by the receiver. When an acknowledgement comes in for frame 

n

, frames 

n

 - 1, 

n

 - 2, and so on are also automatically acknowledged. This property is especially important when some of the previous acknowledgement-bearing frames were lost or garbled. Whenever any acknowledgement comes in, the data link layer checks to see if any buffers can now be released. If buffers can be released (i.e., there is some room available in the window), a previously blocked network layer can now be allowed to cause more 

network

_

layer

_

ready

 events. 

For this protocol, we assume that there is always reverse traffic on which to piggyback acknowledgements. If there is not, no acknowledgements can be sent. Protocol 4 does not need this assumption since it sends back one frame every time it receives a frame, even if it has just already sent that frame. In the next protocol we will solve the problem of one-way traffic in an elegant way. 

Because protocol 5 has multiple outstanding frames, it logically needs multiple timers, one per outstanding frame. Each frame times out independently of all the other ones. All of these timers can easily be simulated in software, using a single hardware clock that causes interrupts periodically. The pending timeouts form a linked list, with each node of the list telling the number of clock ticks until the timer expires, the frame being timed, and a pointer to the next node. 

As an illustration of how the timers could be implemented, consider the example of 

Fig. 3-18(a)

. Assume that the clock ticks once every 100 msec. Initially, the real time is 10:00:00.0; three timeouts are pending, at 10:00:00.5, 10:00:01.3, and 10:00:01.9. Every time the hardware clock ticks, the real time is updated and the tick counter at the head of the list is decremented. When the tick counter becomes zero, a timeout is caused and the node is removed from the list, as shown in 

Fig. 3-18(b)

. Although this organization requires the list to be scanned when 

start

_

timer

 or 

stop

_

timer

 is called, it does not require much work per tick. In protocol 5, both of these routines have been given a parameter, indicating which frame is to be timed. 

Figure 3-18. Simulation of multiple timers in software. 

 

166




3.4.3 A Protocol Using Selective Repeat 

Protocol 5 works well if errors are rare, but if the line is poor, it wastes a lot of bandwidth on retransmitted frames. An alternative strategy for handling errors is to allow the receiver to accept and buffer the frames following a damaged or lost one. Such a protocol does not discard frames merely because an earlier frame was damaged or lost. 

In this protocol, both sender and receiver maintain a window of acceptable sequence numbers. The sender's window size starts out at 0 and grows to some predefined maximum, 

MAX

_

SEQ

. The receiver's window, in contrast, is always fixed in size and equal to 

MAX

_

SEQ

. The receiver has a buffer reserved for each sequence number within its fixed window. Associated with each buffer is a bit (

arrived

) telling whether the buffer is full or empty. Whenever a frame arrives, its sequence number is checked by the function 

between

 to see if it falls within the window. If so and if it has not already been received, it is accepted and stored. This action is taken without regard to whether or not it contains the next packet expected by the network layer. Of course, it must be kept within the data link layer and not passed to the network layer until all the lower-numbered frames have already been delivered to the network layer in the correct order. A protocol using this algorithm is given in 

Fig. 3-

19

. 

Figure 3-19. A sliding window protocol using selective repeat. 

167




168




Nonsequential receive introduces certain problems not present in protocols in which frames are only accepted in order. We can illustrate the trouble most easily with an example. Suppose that we have a 3-bit sequence number, so that the sender is permitted to transmit up to seven frames before being required to wait for an acknowledgement. Initially, the sender's and receiver's windows are as shown in 

Fig. 3-20(a)

. The sender now transmits frames 0 through 6. The receiver's window allows it to accept any frame with sequence number between 0 and 6 inclusive. All seven frames arrive correctly, so the receiver acknowledges them and advances its window to allow receipt of 7, 0, 1, 2, 3, 4, or 5, as shown in 

Fig. 3-20(b)

. All seven buffers are marked empty. 

Figure 3-20. (a) Initial situation with a window of size seven. (b) After seven frames have been sent and received but not acknowledged. (c) Initial situation with a window size of four. (d) After four frames have been sent and received but not acknowledged. 

 

It is at this point that disaster strikes in the form of a lightning bolt hitting the telephone pole and wiping out all the acknowledgements. The sender eventually times out and retransmits frame 0. When this frame arrives at the receiver, a check is made to see if it falls within the receiver's window. Unfortunately, in 

Fig. 3-20(b)

 frame 0 is within the new window, so it will be accepted. The receiver sends a piggybacked acknowledgement for frame 6, since 0 through 6 have been received. 

The sender is happy to learn that all its transmitted frames did actually arrive correctly, so it advances its window and immediately sends frames 7, 0, 1, 2, 3, 4, and 5. Frame 7 will be accepted by the receiver and its packet will be passed directly to the network layer. Immediately thereafter, the receiving data link layer checks to see if it has a valid frame 0 already, discovers that it does, and passes the embedded packet to the network layer. Consequently, the network layer gets an incorrect packet, and the protocol fails. 

The essence of the problem is that after the receiver advanced its window, the new range of valid sequence numbers overlapped the old one. Consequently, the following batch of frames might be either duplicates (if all the acknowledgements were lost) or new ones (if all the acknowledgements were received). The poor receiver has no way of distinguishing these two cases. 

The way out of this dilemma lies in making sure that after the receiver has advanced its window, there is no overlap with the original window. To ensure that there is no overlap, the maximum window size should be at most half the range of the sequence numbers, as is done in 

Fig. 3-20(c)

 and 

Fig. 3-20(d)

. For example, if 4 bits are used for sequence numbers, these will range from 0 to 15. Only eight unacknowledged frames should be outstanding at any instant. That way, if the receiver has just accepted frames 0 through 7 and advanced its window to permit acceptance of frames 8 through 15, it can unambiguously tell if subsequent frames are retransmissions (0 through 7) or new ones (8 through 15). In general, the window size for protocol 6 will be (

MAX

_

SEQ

 + 1)

/

2. Thus, for 3-bit sequence numbers, the window size is four. 

An interesting question is: How many buffers must the receiver have? Under no conditions will it ever accept frames whose sequence numbers are below the lower edge of the window or frames whose sequence numbers are above the upper edge of the window. Consequently, the number of buffers needed is equal to the window size, not to the range of sequence numbers. In the above example of a 4-bit sequence number, eight buffers, numbered 0 through 7, are needed. When frame 

i

 arrives, it is put in buffer 

i

 mod 8

.

 Notice that although 

i

 and (

i

 + 8) mod 8 are ''competing'' for the same buffer, they are never within the window at the same time, because that would imply a window size of at least 9. 

169




For the same reason, the number of timers needed is equal to the number of buffers, not to the size of the sequence space. Effectively, a timer is associated with each buffer. When the timer runs out, the contents of the buffer are retransmitted. 

In protocol 5, there is an implicit assumption that the channel is heavily loaded. When a frame arrives, no acknowledgement is sent immediately. Instead, the acknowledgement is piggybacked onto the next outgoing data frame. If the reverse traffic is light, the acknowledgement will be held up for a long period of time. If there is a lot of traffic in one direction and no traffic in the other direction, only 

MAX

_

SEQ

 packets are sent, and then the protocol blocks, which is why we had to assume there was always some reverse traffic. 

In protocol 6 this problem is fixed. After an in-sequence data frame arrives, an auxiliary timer is started by 

start

_

ack

_

timer

. If no reverse traffic has presented itself before this timer expires, a separate acknowledgement frame is sent. An interrupt due to the auxiliary timer is called an 

ack

_

timeout

 event. With this arrangement, one-directional traffic flow is now possible because the lack of reverse data frames onto which acknowledgements can be piggybacked is no longer an obstacle. Only one auxiliary timer exists, and if 

start

_

ack

_

timer

 is called while the timer is running, it is reset to a full acknowledgement timeout interval. 

It is essential that the timeout associated with the auxiliary timer be appreciably shorter than the timer used for timing out data frames. This condition is required to make sure a correctly received frame is acknowledged early enough that the frame's retransmission timer does not expire and retransmit the frame. 

Protocol 6 uses a more efficient strategy than protocol 5 for dealing with errors. Whenever the receiver has reason to suspect that an error has occurred, it sends a negative acknowledgement (NAK) frame back to the sender. Such a frame is a request for retransmission of the frame specified in the NAK. There are two cases when the receiver should be suspicious: a damaged frame has arrived or a frame other than the expected one arrived (potential lost frame). To avoid making multiple requests for retransmission of the same lost frame, the receiver should keep track of whether a NAK has already been sent for a given frame. The variable 

no

_

nak

 in protocol 6 is true if no NAK has been sent yet for 

frame

_

expected

. If the NAK gets mangled or lost, no real harm is done, since the sender will eventually time out and retransmit the missing frame anyway. If the wrong frame arrives after a NAK has been sent and lost, 

no

_

nak

 will be true and the auxiliary timer will be started. When it expires, an ACK will be sent to resynchronize the sender to the receiver's current status. 

In some situations, the time required for a frame to propagate to the destination, be processed there, and have the acknowledgement come back is (nearly) constant. In these situations, the sender can adjust its timer to be just slightly larger than the normal time interval expected between sending a frame and receiving its acknowledgement. However, if this time is highly variable, the sender is faced with the choice of either setting the interval to a small value (and risking unnecessary retransmissions), or setting it to a large value (and going idle for a long period after an error). 

Both choices waste bandwidth. If the reverse traffic is sporadic, the time before acknowledgement will be irregular, being shorter when there is reverse traffic and longer when there is not. Variable processing time within the receiver can also be a problem here. In general, whenever the standard deviation of the acknowledgement interval is small compared to the interval itself, the timer can be set ''tight'' and NAKs are not useful. Otherwise the timer must be set ''loose,'' to avoid unnecessary retransmissions, but NAKs can appreciably speed up retransmission of lost or damaged frames. 

Closely related to the matter of timeouts and NAKs is the question of determining which frame caused a timeout. In protocol 5, it is always 

ack

_

expected

, because it is always the oldest. In protocol 6, there is no trivial way to determine who timed out. Suppose that frames 0 through 4 have been transmitted, meaning that the list of outstanding frames is 01234, in order from oldest to youngest. Now imagine that 0 times out, 5 (a new frame) is transmitted, 1 times out, 2 times out, and 6 (another new frame) is transmitted. At this point the list of outstanding frames is 3405126, from oldest to youngest. If all inbound traffic (i.e., acknowledgement-bearing frames) is lost for a while, the seven outstanding frames will time out in that order. 

To keep the example from getting even more complicated than it already is, we have not shown the timer administration. Instead, we just assume that the variable 

oldest

_

frame

 is set upon timeout to indicate which frame timed out. 

170




3.5 Protocol Verification 

Realistic protocols and the programs that implement them are often quite complicated. Consequently, much research has been done trying to find formal, mathematical techniques for specifying and verifying protocols. In the following sections we will look at some models and techniques. Although we are looking at them in the context of the data link layer, they are also applicable to other layers. 

3.5.1 Finite State Machine Models 

A key concept used in many protocol models is the 

finite state machine

. With this technique, each 

protocol machine

 (i.e., sender or receiver) is always in a specific state at every instant of time. Its state consists of all the values of its variables, including the program counter. 

In most cases, a large number of states can be grouped for purposes of analysis. For example, considering the receiver in protocol 3, we could abstract out from all the possible states two important ones: waiting for frame 0 or waiting for frame 1. All other states can be thought of as transient, just steps on the way to one of the main states. Typically, the states are chosen to be those instants that the protocol machine is waiting for the next event to happen [i.e., executing the procedure call 

wait

(

event

) in our examples]. At this point the state of the protocol machine is completely determined by the states of its variables. The number of states is then 2

n

, where 

n

 is the number of bits needed to represent all the variables combined. 

The state of the complete system is the combination of all the states of the two protocol machines and the channel. The state of the channel is determined by its contents. Using protocol 3 again as an example, the channel has four possible states: a 0 frame or a 1 frame moving from sender to receiver, an acknowledgement frame going the other way, or an empty channel. If we model the sender and receiver as each having two states, the complete system has 16 distinct states. 

A word about the channel state is in order. The concept of a frame being ''on the channel'' is an abstraction, of course. What we really mean is that a frame has possibly been received, but not yet processed at the destination. A frame remains ''on the channel'' until the protocol machine executes 

FromPhysicalLayer

 and processes it. 

From each state, there are zero or more possible 

transitions

 to other states. Transitions occur when some event happens. For a protocol machine, a transition might occur when a frame is sent, when a frame arrives, when a timer expires, when an interrupt occurs, etc. For the channel, typical events are insertion of a new frame onto the channel by a protocol machine, delivery of a frame to a protocol machine, or loss of a frame due to noise. Given a complete description of the protocol machines and the channel characteristics, it is possible to draw a directed graph showing all the states as nodes and all the transitions as directed arcs. 

One particular state is designated as the 

initial state

. This state corresponds to the description of the system when it starts running, or at some convenient starting place shortly thereafter. From the initial state, some, perhaps all, of the other states can be reached by a sequence of transitions. Using well-known techniques from graph theory (e.g., computing the transitive closure of a graph), it is possible to determine which states are reachable and which are not. This technique is called 

reachability analysis

 (Lin et al., 1987). This analysis can be helpful in determining whether a protocol is correct. 

Formally, a finite state machine model of a protocol can be regarded as a quadruple (

S

, 

M

, 

I

, 

T

), where: 

S

 is the set of states the processes and channel can be in. 

M

 is the set of frames that can be exchanged over the channel. 

I

 is the set of initial states of the processes. 

T

 is the set of transitions between states. 

171




At the beginning of time, all processes are in their initial states. Then events begin to happen, such as frames becoming available for transmission or timers going off. Each event may cause one of the processes or the channel to take an action and switch to a new state. By carefully enumerating each possible successor to each state, one can build the reachability graph and analyze the protocol. 

Reachability analysis can be used to detect a variety of errors in the protocol specification. For example, if it is possible for a certain frame to occur in a certain state and the finite state machine does not say what action should be taken, the specification is in error (incompleteness). If there exists a set of states from which no exit can be made and from which no progress can be made (i.e., no correct frames can be received any more), we have another error (deadlock). A less serious error is protocol specification that tells how to handle an event in a state in which the event cannot occur (extraneous transition). Other errors can also be detected. 

As an example of a finite state machine model, consider 

Fig. 3-21(a)

. This graph corresponds to protocol 3 as described above: each protocol machine has two states and the channel has four states. A total of 16 states exist, not all of them reachable from the initial one. The unreachable ones are not shown in the figure. Checksum errors are also ignored here for simplicity. 

Figure 3-21. (a) State diagram for protocol 3. (b) Transitions. 

 

Each state is labeled by three characters, 

SRC

, where 

S

 is 0 or 1, corresponding to the frame the sender is trying to send; 

R

 is also 0 or 1, corresponding to the frame the receiver expects, and 

C

 is 0, 1, 

A

, or empty (–), corresponding to the state of the channel. In this example the initial state has been chosen as (000). In other words, the sender has just sent frame 0, the receiver expects frame 0, and frame 0 is currently on the channel. 

Nine kinds of transitions are shown in 

Fig. 3-21

. Transition 0 consists of the channel losing its contents. Transition 1 consists of the channel correctly delivering packet 0 to the receiver, with the receiver then changing its state to expect frame 1 and emitting an acknowledgement. Transition 1 also corresponds to the receiver delivering packet 0 to the network layer. The other transitions are listed in 

Fig. 3-21(b)

. The arrival of a frame with a checksum error has not been shown because it does not change the state (in protocol 3). 

During normal operation, transitions 1, 2, 3, and 4 are repeated in order over and over. In each cycle, two packets are delivered, bringing the sender back to the initial state of trying to send a new frame with sequence number 0. If the channel loses frame 0, it makes a transition from state (000) to state (00–). Eventually, the sender times out (transition 7) and the system moves back to (000). The loss of an acknowledgement is more complicated, requiring two transitions, 7 and 5, or 8 and 6, to repair the damage. 

One of the properties that a protocol with a 1-bit sequence number must have is that no matter what sequence of events happens, the receiver never delivers two odd packets without an intervening even packet, and vice versa. From the graph of 

Fig. 3-21

 we see that this requirement can be stated more formally as ''there must not exist any paths from the initial state on which two occurrences of transition 1 occur without an occurrence of transition 3 between them, or vice versa.'' From the figure it can be seen that the protocol is correct in this respect. 

172




A similar requirement is that there not exist any paths on which the sender changes state twice (e.g., from 0 to 1 and back to 0) while the receiver state remains constant. Were such a path to exist, then in the corresponding sequence of events, two frames would be irretrievably lost without the receiver noticing. The packet sequence delivered would have an undetected gap of two packets in it. 

Yet another important property of a protocol is the absence of deadlocks. A 

deadlock

 is a situation in which the protocol can make no more forward progress (i.e., deliver packets to the network layer) no matter what sequence of events happens. In terms of the graph model, a deadlock is characterized by the existence of a subset of states that is reachable from the initial state and that has two properties: 

1. There is no transition out of the subset. 

2. There are no transitions in the subset that cause forward progress. 

Once in the deadlock situation, the protocol remains there forever. Again, it is easy to see from the graph that protocol 3 does not suffer from deadlocks. 

3.5.2 Petri Net Models 

The finite state machine is not the only technique for formally specifying protocols. In this section we will describe a completely different technique, the 

Petri net

 (Danthine, 1980). A Petri net has four basic elements: places, transitions, arcs, and tokens. A 

place

 represents a state which (part of) the system may be in. 

Figure 3-

22

 shows a Petri net with two places, 

A

 and 

B

, both shown as circles. The system is currently in state 

A

, indicated by the 

token

 (heavy dot) in place 

A

. A 

transition

 is indicated by a horizontal or vertical bar. Each transition has zero or more 

input arcs

 coming from its input places, and zero or more 

output arcs

, going to its output places. 

Figure 3-22. A Petri net with two places and two transitions. 

 

A transition is 

enabled

 if there is at least one input token in each of its input places. Any enabled transition may 

fire

 at will, removing one token from each input place and depositing a token in each output place. If the number of input arcs and output arcs differs, tokens will not be conserved. If two or more transitions are enabled, any one of them may fire. The choice of a transition to fire is indeterminate, which is why Petri nets are useful for modeling protocols. The Petri net of 

Fig. 3-22

 is deterministic and can be used to model any two-phase process (e.g., the behavior of a baby: eat, sleep, eat, sleep, and so on). As with all modeling tools, unnecessary detail is suppressed. 

Figure 3-23

 gives the Petri net model of 

Fig. 3-12

. Unlike the finite state machine model, there are no composite states here; the sender's state, channel state, and receiver's state are represented separately. Transitions 1 and 2 correspond to transmission of frame 0 by the sender, normally, and on a timeout respectively. Transitions 3 and 4 are analogous for frame 1. Transitions 5, 6, and 7 correspond to the loss of frame 0, an acknowledgement, and frame 1, respectively. Transitions 8 and 9 occur when a data frame with the wrong sequence number arrives at the receiver. Transitions 10 and 11 represent the arrival at the receiver of the next frame in sequence and its delivery to the network layer. 

Figure 3-23. A Petri net model for protocol 3. 

173




 

Petri nets can be used to detect protocol failures in a way similar to the use of finite state machines. For example, if some firing sequence included transition 10 twice without transition 11 intervening, the protocol would be incorrect. The concept of a deadlock in a Petri net is similar to its finite state machine counterpart. 

Petri nets can be represented in convenient algebraic form resembling a grammar. Each transition contributes one rule to the grammar. Each rule specifies the input and output places of the transition. Since 

Fig. 3-23

 has 11 transitions, its grammar has 11 rules, numbered 1–11, each one corresponding to the transition with the same number. The grammar for the Petri net of 

Fig. 3-23

 is as follows: 

 1: BD 

 AC  

 2: A 

 A  

 3: AD 

 BE  

 4: B 

 B  

 5: C 

  

 6: D 

  

 7: E 

  

 8: CF 

 DF  

 9: EG 

 DG  

10: CG 

 DF  

11: EF 

 DG  

It is interesting to note how we have managed to reduce a complex protocol to 11 simple grammar rules that can easily be manipulated by a computer program. 

The current state of the Petri net is represented as an unordered collection of places, each place represented in the collection as many times as it has tokens. Any rule, all of whose left-hand side places are present can be fired, removing those places from the current state, and adding its output places to the current state. The marking of 

Fig. 3-23

 is 

ACG

, (i.e., 

A

, 

C

, and 

G

 each have one token). Consequently, rules 2, 5, and 10 are all enabled and any of them can be applied, leading to a new state (possibly with the same marking as the original one). In contrast, rule 3 ( 

AD

 

BE

 ) cannot be applied because 

D

 is not marked. 

174




3.6 Example Data Link Protocols 

In the following sections we will examine several widely-used data link protocols. The first one, HDLC, is a classical bit-oriented protocol whose variants have been in use for decades in many applications. The second one, PPP, is the data link protocol used to connect home computers to the Internet. 

3.6.1 HDLC—High-Level Data Link Control 

In this section we will examine a group of closely related protocols that are a bit old but are still heavily used. They are all derived from the data link protocol first used in the IBM mainframe world: 

SDLC

 (

Synchronous Data Link Control

) protocol. After developing SDLC, IBM submitted it to ANSI and ISO for acceptance as U.S. and international standards, respectively. ANSI modified it to become 

ADCCP

 (

Advanced Data Communication Control Procedure

), and ISO modified it to become 

HDLC

 (

High-level Data Link Control

). CCITT then adopted and modified HDLC for its 

LAP

 (

Link Access Procedure

) as part of the X.25 network interface standard but later modified it again to 

LAPB

, to make it more compatible with a later version of HDLC. The nice thing about standards is that you have so many to choose from. Furthermore, if you do not like any of them, you can just wait for next year's model. 

These protocols are based on the same principles. All are bit oriented, and all use bit stuffing for data transparency. They differ only in minor, but nevertheless irritating, ways. The discussion of bit-oriented protocols that follows is intended as a general introduction. For the specific details of any one protocol, please consult the appropriate definition. 

All the bit-oriented protocols use the frame structure shown in 

Fig. 3-24

. The 

Address

 field is primarily of importance on lines with multiple terminals, where it is used to identify one of the terminals. For point-to-point lines, it is sometimes used to distinguish commands from responses. 

Figure 3-24. Frame format for bit-oriented protocols. 

 

The 

Control

 field is used for sequence numbers, acknowledgements, and other purposes, as discussed below. 

The 

Data

 field may contain any information. It may be arbitrarily long, although the efficiency of the checksum falls off with increasing frame length due to the greater probability of multiple burst errors. 

The 

Checksum

 field is a cyclic redundancy code using the technique we examined in 

Sec. 3-2.2

. 

The frame is delimited with another flag sequence (01111110). On idle point-to-point lines, flag sequences are transmitted continuously. The minimum frame contains three fields and totals 32 bits, excluding the flags on either end. 

There are three kinds of frames: 

Information

, 

Supervisory

, and 

Unnumbered

. The contents of the 

Control

 field for these three kinds are shown in 

Fig. 3-25

. The protocol uses a sliding window, with a 3-bit sequence number. Up to seven unacknowledged frames may be outstanding at any instant. The 

Seq

 field in 

Fig. 3-25(a)

 is the frame sequence number. The 

Next

 field is a piggybacked acknowledgement. However, all the protocols adhere to the convention that instead of piggybacking the number of the last frame received correctly, they use the number of the first frame not yet received (i.e., the next frame expected). The choice of using the last frame received or the next frame expected is arbitrary; it does not matter which convention is used, provided that it is used consistently. 

Figure 3-25. Control field of (a) an information frame, (b) a supervisory frame, (c) an unnumbered frame. 

175




 

The 

P/F

 bit stands for 

Poll/Final

. It is used when a computer (or concentrator) is polling a group of terminals. When used as 

P

, the computer is inviting the terminal to send data. All the frames sent by the terminal, except the final one, have the 

P/F

 bit set to 

P

. The final one is set to 

F

. 

In some of the protocols, the 

P/F

 bit is used to force the other machine to send a Supervisory frame immediately rather than waiting for reverse traffic onto which to piggyback the window information. The bit also has some minor uses in connection with the Unnumbered frames. 

The various kinds of Supervisory frames are distinguished by the 

Type

 field. Type 0 is an acknowledgement frame (officially called RECEIVE READY) used to indicate the next frame expected. This frame is used when there is no reverse traffic to use for piggybacking. 

Type 1 is a negative acknowledgement frame (officially called REJECT). It is used to indicate that a transmission error has been detected. The 

Next

 field indicates the first frame in sequence not received correctly (i.e., the frame to be retransmitted). The sender is required to retransmit all outstanding frames starting at 

Next

. This strategy is similar to our protocol 5 rather than our protocol 6. 

Type 2 is RECEIVE NOT READY. It acknowledges all frames up to but not including 

Next

, just as RECEIVE READY does, but it tells the sender to stop sending. RECEIVE NOT READY is intended to signal certain temporary problems with the receiver, such as a shortage of buffers, and not as an alternative to the sliding window flow control. When the condition has been repaired, the receiver sends a RECEIVE READY, REJECT, or certain control frames. 

Type 3 is the SELECTIVE REJECT. It calls for retransmission of only the frame specified. In this sense it is like our protocol 6 rather than 5 and is therefore most useful when the sender's window size is half the sequence space size, or less. Thus, if a receiver wishes to buffer out-of-sequence frames for potential future use, it can force the retransmission of any specific frame using Selective Reject. HDLC and ADCCP allow this frame type, but SDLC and LAPB do not allow it (i.e., there is no Selective Reject), and type 3 frames are undefined. 

The third class of frame is the Unnumbered frame. It is sometimes used for control purposes but can also carry data when unreliable connectionless service is called for. The various bit-oriented protocols differ considerably here, in contrast with the other two kinds, where they are nearly identical. Five bits are available to indicate the frame type, but not all 32 possibilities are used. 

All the protocols provide a command, DISC (DISConnect), that allows a machine to announce that it is going down (e.g., for preventive maintenance). They also have a command that allows a machine that has just come back on-line to announce its presence and force all the sequence numbers back to zero. This command is called SNRM (Set Normal Response Mode). Unfortunately, ''Normal Response Mode'' is anything but normal. It is an unbalanced (i.e., asymmetric) mode in which one end of the line is the master and the other the slave. SNRM dates from a time when data communication meant a dumb terminal talking to a big host computer, which clearly is asymmetric. To make the protocol more suitable when the two partners are equals, HDLC and LAPB have an additional command, SABM (Set Asynchronous Balanced Mode), which resets the line and declares both parties to be equals. They also have commands SABME and SNRME, which are the same as SABM and SNRM, respectively, except that they enable an extended frame format that uses 7-bit sequence numbers instead of 3-bit sequence numbers. 

A third command provided by all the protocols is FRMR (FRaMe Reject), used to indicate that a frame with a correct checksum but impossible semantics arrived. Examples of impossible semantics are a type 3 Supervisory 

176




frame in LAPB, a frame shorter than 32 bits, an illegal control frame, and an acknowledgement of a frame that was outside the window, etc. FRMR frames contain a 24-bit data field telling what was wrong with the frame. The data include the control field of the bad frame, the window parameters, and a collection of bits used to signal specific errors. 

Control frames can be lost or damaged, just like data frames, so they must be acknowledged too. A special control frame, called UA (Unnumbered Acknowledgement), is provided for this purpose. Since only one control frame may be outstanding, there is never any ambiguity about which control frame is being acknowledged. 

The remaining control frames deal with initialization, polling, and status reporting. There is also a control frame that may contain arbitrary information, UI (Unnumbered Information). These data are not passed to the network layer but are for the receiving data link layer itself. 

Despite its widespread use, HDLC is far from perfect. A discussion of a variety of problems associated with it can be found in (Fiorini et al., 1994). 

3.6.2 The Data Link Layer in the Internet 

The Internet consists of individual machines (hosts and routers) and the communication infrastructure that connects them. Within a single building, LANs are widely used for interconnection, but most of the wide area infrastructure is built up from point-to-point leased lines. In 

Chap. 4

, we will look at LANs; here we will examine the data link protocols used on point-to-point lines in the Internet. 

In practice, point-to-point communication is primarily used in two situations. First, thousands of organizations have one or more LANs, each with some number of hosts (personal computers, user workstations, servers, and so on) along with a router (or a bridge, which is functionally similar). Often, the routers are interconnected by a backbone LAN. Typically, all connections to the outside world go through one or two routers that have point-to-point leased lines to distant routers. It is these routers and their leased lines that make up the communication subnets on which the Internet is built. 

The second situation in which point-to-point lines play a major role in the Internet is the millions of individuals who have home connections to the Internet using modems and dial-up telephone lines. Usually, what happens is that the user's home PC calls up an Internet service provider's router and then acts like a full-blown Internet host. This method of operation is no different from having a leased line between the PC and the router, except that the connection is terminated when the user ends the session. A home PC calling an Internet service provider is illustrated in 

Fig. 3-26

. The modem is shown external to the computer to emphasize its role, but modern computers have internal modems. 

Figure 3-26. A home personal computer acting as an Internet host. 

 

For both the router-router leased line connection and the dial-up host-router connection, some point-to-point data link protocol is required on the line for framing, error control, and the other data link layer functions we have studied in this chapter. The one used in the Internet is called PPP. We will now examine it. 

177




PPP—The Point-to-Point Protocol 

The Internet needs a point-to-point protocol for a variety of purposes, including router-to-router traffic and home user-to-ISP traffic. This protocol is 

PPP

 (

Point-to-Point Protocol

), which is defined in RFC 1661 and further elaborated on in several other RFCs (e.g., RFCs 1662 and 1663). PPP handles error detection, supports multiple protocols, allows IP addresses to be negotiated at connection time, permits authentication, and has many other features. 

PPP provides three features: 

1. A framing method that unambiguously delineates the end of one frame and the start of the next one. The frame format also handles error detection. 

2. A link control protocol for bringing lines up, testing them, negotiating options, and bringing them down again gracefully when they are no longer needed. This protocol is called 

LCP

 (

Link Control Protocol

). It supports synchronous and asynchronous circuits and byte-oriented and bit-oriented encodings. 

3. A way to negotiate network-layer options in a way that is independent of the network layer protocol to be used. The method chosen is to have a different 

NCP

 (

Network Control Protocol

) for each network layer supported. 

To see how these pieces fit together, let us consider the typical scenario of a home user calling up an Internet service provider to make a home PC a temporary Internet host. The PC first calls the provider's router via a modem. After the router's modem has answered the phone and established a physical connection, the PC sends the router a series of LCP packets in the payload field of one or more PPP frames. These packets and their responses select the PPP parameters to be used. 

Once the parameters have been agreed upon, a series of NCP packets are sent to configure the network layer. Typically, the PC wants to run a TCP/IP protocol stack, so it needs an IP address. There are not enough IP addresses to go around, so normally each Internet provider gets a block of them and then dynamically assigns one to each newly attached PC for the duration of its login session. If a provider owns 

n

 IP addresses, it can have up to 

n

 machines logged in simultaneously, but its total customer base may be many times that. The NCP for IP assigns the IP address. 

At this point, the PC is now an Internet host and can send and receive IP packets, just as hardwired hosts can. When the user is finished, NCP tears down the network layer connection and frees up the IP address. Then LCP shuts down the data link layer connection. Finally, the computer tells the modem to hang up the phone, releasing the physical layer connection. 

The PPP frame format was chosen to closely resemble the HDLC frame format, since there was no reason to reinvent the wheel. The major difference between PPP and HDLC is that PPP is character oriented rather than bit oriented. In particular, PPP uses byte stuffing on dial-up modem lines, so all frames are an integral number of bytes. It is not possible to send a frame consisting of 30.25 bytes, as it is with HDLC. Not only can PPP frames be sent over dial-up telephone lines, but they can also be sent over SONET or true bit-oriented HDLC lines (e.g., for router-router connections). The PPP frame format is shown in 

Fig. 3-27

. 

Figure 3-27. The PPP full frame format for unnumbered mode operation. 

 

All PPP frames begin with the standard HDLC flag byte (01111110), which is byte stuffed if it occurs within the payload field. Next comes the 

Address

 field, which is always set to the binary value 11111111 to indicate that all stations are to accept the frame. Using this value avoids the issue of having to assign data link addresses. 

The 

Address

 field is followed by the 

Control

 field, the default value of which is 00000011. This value indicates an unnumbered frame. In other words, PPP does not provide reliable transmission using sequence numbers and 

178




acknowledgements as the default. In noisy environments, such as wireless networks, reliable transmission using numbered mode can be used. The exact details are defined in RFC 1663, but in practice it is rarely used. 

Since the 

Address

 and 

Control

 fields are always constant in the default configuration, LCP provides the necessary mechanism for the two parties to negotiate an option to just omit them altogether and save 2 bytes per frame. 

The fourth PPP field is the 

Protocol

 field. Its job is to tell what kind of packet is in the 

Payload

 field. Codes are defined for LCP, NCP, IP, IPX, AppleTalk, and other protocols. Protocols starting with a 0 bit are network layer protocols such as IP, IPX, OSI CLNP, XNS. Those starting with a 1 bit are used to negotiate other protocols. These include LCP and a different NCP for each network layer protocol supported. The default size of the 

Protocol

 field is 2 bytes, but it can be negotiated down to 1 byte using LCP. 

The 

Payload

 field is variable length, up to some negotiated maximum. If the length is not negotiated using LCP during line setup, a default length of 1500 bytes is used. Padding may follow the payload if need be. 

After the 

Payload

 field comes the 

Checksum

 field, which is normally 2 bytes, but a 4-byte checksum can be negotiated. 

In summary, PPP is a multiprotocol framing mechanism suitable for use over modems, HDLC bit-serial lines, SONET, and other physical layers. It supports error detection, option negotiation, header compression, and, optionally, reliable transmission using an HDLC-type frame format. 

Let us now turn from the PPP frame format to the way lines are brought up and down. The (simplified) diagram of 

Fig. 3-28

 shows the phases that a line goes through when it is brought up, used, and taken down again. This sequence applies both to modem connections and to router-router connections. 

Figure 3-28. A simplified phase diagram for bringing a line up and down. 

 

The protocol starts with the line in the 

DEAD

 state, which means that no physical layer carrier is present and no physical layer connection exists. After physical connection is established, the line moves to 

ESTABLISH

. At that point LCP option negotiation begins, which, if successful, leads to 

AUTHENTICATE

. Now the two parties can check on each other's identities if desired. When the 

NETWORK

 phase is entered, the appropriate NCP protocol is invoked to configure the network layer. If the configuration is successful, 

OPEN

 is reached and data transport can take place. When data transport is finished, the line moves into the 

TERMINATE

 phase, and from there, back to 

DEAD

 when the carrier is dropped. 

LCP negotiates data link protocol options during the 

ESTABLISH

 phase. The LCP protocol is not actually concerned with the options themselves, but with the mechanism for negotiation. It provides a way for the initiating process to make a proposal and for the responding process to accept or reject it, in whole or in part. It also provides a way for the two processes to test the line quality to see if they consider it good enough to set up a connection. Finally, the LCP protocol also allows lines to be taken down when they are no longer needed. 

179




Eleven types of LCP frames are defined in RFC 1661. These are listed in 

Fig. 3-29

. The four 

Configure-

 types allow the initiator (I) to propose option values and the responder (R) to accept or reject them. In the latter case, the responder can make an alternative proposal or announce that it is not willing to negotiate certain options at all. The options being negotiated and their proposed values are part of the LCP frames. 

Figure 3-29. The LCP frame types. 

 

The 

Terminate-

 codes shut a line down when it is no longer needed. The 

Code-reject

 and 

Protocol-reject

 codes indicate that the responder got something that it does not understand. This situation could mean that an undetected transmission error has occurred, but more likely it means that the initiator and responder are running different versions of the LCP protocol. The 

Echo-

 types are used to test the line quality. Finally, 

Discard-request

 help debugging. If either end is having trouble getting bits onto the wire, the programmer can use this type for testing. If it manages to get through, the receiver just throws it away, rather than taking some other action that might confuse the person doing the testing. 

The options that can be negotiated include setting the maximum payload size for data frames, enabling authentication and choosing a protocol to use, enabling line-quality monitoring during normal operation, and selecting various header compression options. 

There is little to say about the NCP protocols in a general way. Each one is specific to some network layer protocol and allows configuration requests to be made that are specific to that protocol. For IP, for example, dynamic address assignment is the most important possibility. 

3.7 Summary 

The task of the data link layer is to convert the raw bit stream offered by the physical layer into a stream of frames for use by the network layer. Various framing methods are used, including character count, byte stuffing, and bit stuffing. Data link protocols can provide error control to retransmit damaged or lost frames. To prevent a fast sender from overrunning a slow receiver, the data link protocol can also provide flow control. The sliding window mechanism is widely used to integrate error control and flow control in a convenient way. 

Sliding window protocols can be categorized by the size of the sender's window and the size of the receiver's window. When both are equal to 1, the protocol is stop-and-wait. When the sender's window is greater than 1, for example, to prevent the sender from blocking on a circuit with a long propagation delay, the receiver can be programmed either to discard all frames other than the next one in sequence or to buffer out-of-order frames until they are needed. 

We examined a series of protocols in this chapter. Protocol 1 is designed for an error-free environment in which the receiver can handle any flow sent to it. Protocol 2 still assumes an error-free environment but introduces flow control. Protocol 3 handles errors by introducing sequence numbers and using the stop-and-wait algorithm. Protocol 4 allows bidirectional communication and introduces the concept of piggybacking. Protocol 5 uses a 

180




sliding window protocol with go back n. Finally, protocol 6 uses selective repeat and negative acknowledgements. 

Protocols can be modeled using various techniques to help demonstrate their correctness (or lack thereof). Finite state machine models and Petri net models are commonly used for this purpose. 

Many networks use one of the bit-oriented protocols—SDLC, HDLC, ADCCP, or LAPB—at the data link level. All of these protocols use flag bytes to delimit frames, and bit stuffing to prevent flag bytes from occurring in the data. All of them also use a sliding window for flow control. The Internet uses PPP as the primary data link protocol over point-to-point lines. 

Problems 

1. An upper-layer packet is split into 10 frames, each of which has an 80 percent chance of arriving undamaged. If no error control is done by the data link protocol, how many times must the message be sent on average to get the entire thing through? 

2. The following character encoding is used in a data link protocol: A: 01000111; B: 11100011; FLAG: 01111110; ESC: 11100000 Show the bit sequence transmitted (in binary) for the four-character frame: A B ESC FLAG when each of the following framing methods are used: 

a. (a) Character count. 

b. (b) Flag bytes with byte stuffing. 

c. (c) Starting and ending flag bytes, with bit stuffing. 

3. The following data fragment occurs in the middle of a data stream for which the byte-stuffing algorithm described in the text is used: A B ESC C ESC FLAG FLAG D. What is the output after stuffing? 

4. One of your classmates, Scrooge, has pointed out that it is wasteful to end each frame with a flag byte and then begin the next one with a second flag byte. One flag byte could do the job as well, and a byte saved is a byte earned. Do you agree? 

5. A bit string, 0111101111101111110, needs to be transmitted at the data link layer. What is the string actually transmitted after bit stuffing? 

6. When bit stuffing is used, is it possible for the loss, insertion, or modification of a single bit to cause an error not detected by the checksum? If not, why not? If so, how? Does the checksum length play a role here? 

7. Can you think of any circumstances under which an open-loop protocol, (e.g., a Hamming code) might be preferable to the feedback-type protocols discussed throughout this chapter? 

8. To provide more reliability than a single parity bit can give, an error-detecting coding scheme uses one parity bit for checking all the odd-numbered bits and a second parity bit for all the even-numbered bits. What is the Hamming distance of this code? 

9. Sixteen-bit messages are transmitted using a Hamming code. How many check bits are needed to ensure that the receiver can detect and correct single bit errors? Show the bit pattern transmitted for the message 1101001100110101. Assume that even parity is used in the Hamming code. 

10. An 8-bit byte with binary value 10101111 is to be encoded using an even-parity Hamming code. What is the binary value after encoding? 

11. A 12-bit Hamming code whose hexadecimal value is 0xE4F arrives at a receiver. What was the original value in hexadecimal? Assume that not more than 1 bit is in error. 

12. One way of detecting errors is to transmit data as a block of 

n

 rows of 

k

 bits per row and adding parity bits to each row and each column. The lower-right corner is a parity bit that checks its row and its column. Will this scheme detect all single errors? Double errors? Triple errors? 

13. A block of bits with 

n

 rows and 

k

 columns uses horizontal and vertical parity bits for error detection. Suppose that exactly 4 bits are inverted due to transmission errors. Derive an expression for the probability that the error will be undetected. 

14. What is the remainder obtained by dividing 

x

7

 + 

x

5

 + 1 by the generator polynomial 

x

3

 + 1? 

15. A bit stream 10011101 is transmitted using the standard CRC method described in the text. The generator polynomial is 

x

3

 + 1. Show the actual bit string transmitted. Suppose the third bit from the left is inverted during transmission. Show that this error is detected at the receiver's end. 

16. Data link protocols almost always put the CRC in a trailer rather than in a header. Why? 

17. A channel has a bit rate of 4 kbps and a propagation delay of 20 msec. For what range of frame sizes does stop-and-wait give an efficiency of at least 50 percent? 

18. A 3000-km-long T1 trunk is used to transmit 64-byte frames using protocol 5. If the propagation speed is 6 µsec/km, how many bits should the sequence numbers be? 

181




19. In protocol 3, is it possible that the sender starts the timer when it is already running? If so, how might this occur? If not, why is it impossible? 

20. Imagine a sliding window protocol using so many bits for sequence numbers that wraparound never occurs. What relations must hold among the four window edges and the window size, which is constant and the same for both the sender and the receiver. 

21. If the procedure 

between

 in protocol 5 checked for the condition 

a

 

b

 

c

 instead of the condition 

a

 

b < c

, would that have any effect on the protocol's correctness or efficiency? Explain your answer. 

22. In protocol 6, when a data frame arrives, a check is made to see if the sequence number differs from the one expected and 

no

_

nak

 is true. If both conditions hold, a NAK is sent. Otherwise, the auxiliary timer is started. Suppose that the 

else

 clause were omitted. Would this change affect the protocol's correctness? 

23. Suppose that the three-statement 

while

 loop near the end of protocol 6 were removed from the code. Would this affect the correctness of the protocol or just the performance? Explain your answer. 

24. Suppose that the case for checksum errors were removed from the 

switch

 statement of protocol 6. How would this change affect the operation of the protocol? 

25. In protocol 6 the code for 

frame

_

arrival

 has a section used for NAKs. This section is invoked if the incoming frame is a NAK and another condition is met. Give a scenario where the presence of this other condition is essential. 

26. Imagine that you are writing the data link layer software for a line used to send data to you, but not from you. The other end uses HDLC, with a 3-bit sequence number and a window size of seven frames. You would like to buffer as many out-of-sequence frames as possible to enhance efficiency, but you are not allowed to modify the software on the sending side. Is it possible to have a receiver window greater than 1, and still guarantee that the protocol will never fail? If so, what is the largest window that can be safely used? 

27. Consider the operation of protocol 6 over a 1-Mbps error-free line. The maximum frame size is 1000 bits. New packets are generated 1 second apart. The timeout interval is 10 msec. If the special acknowledgement timer were eliminated, unnecessary timeouts would occur. How many times would the average message be transmitted? 

28. In protocol 6, 

MAX

_

SEQ

 = 2

n

 - 1. While this condition is obviously desirable to make efficient use of header bits, we have not demonstrated that it is essential. Does the protocol work correctly for 

MAX

_

SEQ

 = 4, for example? 

29. Frames of 1000 bits are sent over a 1-Mbps channel using a geostationary satellite whose propagation time from the earth is 270 msec. Acknowledgements are always piggybacked onto data frames. The headers are very short. Three-bit sequence numbers are used. What is the maximum achievable channel utilization for 

a. (a) Stop-and-wait. 

b. (b) Protocol 5. 

c. (c) Protocol 6. 

30. Compute the fraction of the bandwidth that is wasted on overhead (headers and retransmissions) for protocol 6 on a heavily-loaded 50-kbps satellite channel with data frames consisting of 40 header and 3960 data bits. Assume that the signal propagation time from the earth to the satellite is 270 msec. ACK frames never occur. NAK frames are 40 bits. The error rate for data frames is 1 percent, and the error rate for NAK frames is negligible. The sequence numbers are 8 bits. 

31. Consider an error-free 64-kbps satellite channel used to send 512-byte data frames in one direction, with very short acknowledgements coming back the other way. What is the maximum throughput for window sizes of 1, 7, 15, and 127? The earth-satellite propagation time is 270 msec. 

32. A 100-km-long cable runs at the T1 data rate. The propagation speed in the cable is 2/3 the speed of light in vacuum. How many bits fit in the cable? 

33. Suppose that we model protocol 4 using the finite state machine model. How many states exist for each machine? How many states exist for the communication channel? How many states exist for the complete system (two machines and the channel)? Ignore the checksum errors. 

34. Give the firing sequence for the Petri net of 

Fig. 3-23

 corresponding to the state sequence (000), (01A), (01—), (010), (01A) in 

Fig. 3-21

. Explain in words what the sequence represents. 

35. Given the transition rules 

AC

 

B

, 

B

 

AC

, 

CD

 

E

, and 

E

 

CD

, draw the Petri net described. From the Petri net, draw the finite state graph reachable from the initial state 

ACD.

 What well-known concept do these transition rules model? 

36. PPP is based closely on HDLC, which uses bit stuffing to prevent accidental flag bytes within the payload from causing confusion. Give at least one reason why PPP uses byte stuffing instead. 

37. What is the minimum overhead to send an IP packet using PPP? Count only the overhead introduced by PPP itself, not the IP header overhead. 

182




38. The goal of this lab exercise is to implement an error detection mechanism using the standard CRC algorithm described in the text. Write two programs, generator and verifier. The generator program reads from standard input an 

n

-bit message as a string of 0s and 1s as a line of ASCII text. The second line is the 

k

-bit polynomial, also in ASCII. It outputs to standard output a line of ASCII text with 

n

 + 

k

 0s and 1s representing the message to be transmitted. Then it outputs the polynomial, just as it read it in. The verifier program reads in the output of the generator program and outputs a message indicating whether it is correct or not. Finally, write a program, alter, that inverts one bit on the first line depending on its argument (the bit number counting the leftmost bit as 1) but copies the rest of the two lines correctly. By typing:  

generator <file | verifier  

you should see that the message is correct, but by typing  

generator <file | alter arg | verifier  

you should get the error message. 

39. Write a program to simulate the behavior of a Petri net. The program should read in the transition rules as well as a list of states corresponding to the network link layer issuing a new packet or accepting a new packet. From the initial state, also read in, the program should pick enabled transitions at random and fire them, checking to see if a host ever accepts 2 packets without the other host emitting a new one in between. 

 

183




Chapter 4. The Medium Access Control Sublayer 

As we pointed out in 

Chap. 1

, networks can be divided into two categories: those using point-to-point connections and those using broadcast channels. This chapter deals with broadcast networks and their protocols. 

In any broadcast network, the key issue is how to determine who gets to use the channel when there is competition for it. To make this point clearer, consider a conference call in which six people, on six different telephones, are all connected so that each one can hear and talk to all the others. It is very likely that when one of them stops speaking, two or more will start talking at once, leading to chaos. In a face-to-face meeting, chaos is avoided by external means, for example, at a meeting, people raise their hands to request permission to speak. When only a single channel is available, determining who should go next is much harder. Many protocols for solving the problem are known and form the contents of this chapter. In the literature, broadcast channels are sometimes referred to as 

multiaccess channels

 or 

random access channels

. 

The protocols used to determine who goes next on a multiaccess channel belong to a sublayer of the data link layer called the 

MAC

 (

Medium Access Control

) sublayer. The MAC sublayer is especially important in LANs, many of which use a multiaccess channel as the basis for communication. WANs, in contrast, use point-to-point links, except for satellite networks. Because multiaccess channels and LANs are so closely related, in this chapter we will discuss LANs in general, including a few issues that are not strictly part of the MAC sublayer. 

Technically, the MAC sublayer is the bottom part of the data link layer, so logically we should have studied it before examining all the point-to-point protocols in 

Chap. 3

. Nevertheless, for most people, understanding protocols involving multiple parties is easier after two-party protocols are well understood. For that reason we have deviated slightly from a strict bottom-up order of presentation. 

4.1 The Channel Allocation Problem 

The central theme of this chapter is how to allocate a single broadcast channel among competing users. We will first look at static and dynamic schemes in general. Then we will examine a number of specific algorithms. 

4.1.1 Static Channel Allocation in LANs and MANs 

The traditional way of allocating a single channel, such as a telephone trunk, among multiple competing users is Frequency Division Multiplexing (FDM). If there are 

N

 users, the bandwidth is divided into 

N

 equal-sized portions (see 

Fig. 2-31

), each user being assigned one portion. Since each user has a private frequency band, there is no interference between users. When there is only a small and constant number of users, each of which has a heavy (buffered) load of traffic (e.g., carriers' switching offices), FDM is a simple and efficient allocation mechanism. 

However, when the number of senders is large and continuously varying or the traffic is bursty, FDM presents some problems. If the spectrum is cut up into 

N

 regions and fewer than 

N

 users are currently interested in communicating, a large piece of valuable spectrum will be wasted. If more than 

N

 users want to communicate, some of them will be denied permission for lack of bandwidth, even if some of the users who have been assigned a frequency band hardly ever transmit or receive anything. 

184




However, even assuming that the number of users could somehow be held constant at 

N

, dividing the single available channel into static subchannels is inherently inefficient. The basic problem is that when some users are quiescent, their bandwidth is simply lost. They are not using it, and no one else is allowed to use it either. Furthermore, in most computer systems, data traffic is extremely bursty (peak traffic to mean traffic ratios of 1000:1 are common). Consequently, most of the channels will be idle most of the time. 

The poor performance of static FDM can easily be seen from a simple queueing theory calculation. Let us start with the mean time delay, 

T

, for a channel of capacity 

C

 bps, with an arrival rate of ? frames/sec, each frame having a length drawn from an exponential probability density function with mean 1

/

µ bits/frame. With these parameters the arrival rate is ? frames/sec and the service rate is µ

C

 frames/sec. From queueing theory it can be shown that for Poisson arrival and service times, 

 

 

For example, if 

C

 is 100 Mbps, the mean frame length, 1

/

µ, is 10,000 bits, and the frame arrival rate, ?, is 5000 frames/sec, then 

T

 = 200 µsec. Note that if we ignored the queueing delay and just asked how long it takes to send a 10,000 bit frame on a 100-Mbps network, we would get the (incorrect) answer of 100 µsec. That result only holds when there is no contention for the channel. 

Now let us divide the single channel into 

N

 independent subchannels, each with capacity 

C/N

 bps. The mean input rate on each of the subchannels will now be ?

/N.

 Recomputing 

T

 we get 

Equation 4 

 

 

The mean delay using FDM is 

N

 times worse than if all the frames were somehow magically arranged orderly in a big central queue. 

Precisely the same arguments that apply to FDM also apply to time division multiplexing (TDM). Each user is statically allocated every 

N

th time slot. If a user does not use the allocated slot, it just lies fallow. The same holds if we split up the networks physically. Using our previous example again, if we were to replace the 100-Mbps network with 10 networks of 10 Mbps each and statically allocate each user to one of them, the mean delay would jump from 200 µsec to 2 msec. 

Since none of the traditional static channel allocation methods work well with bursty traffic, we will now explore dynamic methods. 

4.1.2 Dynamic Channel Allocation in LANs and MANs 

Before we get into the first of the many channel allocation methods to be discussed in this chapter, it is worthwhile carefully formulating the allocation problem. Underlying all the work done in this area are five key assumptions, described below. 

185




Station Model. The model consists of 

N

 independent 

stations

 (e.g., computers, telephones, or personal communicators), each with a program or user that generates frames for transmission. Stations are sometimes called 

terminals

. The probability of a frame being generated in an interval of length ?

t

 is ??

t

, where ? is a constant (the arrival rate of new frames). Once a frame has been generated, the station is blocked and does nothing until the frame has been successfully transmitted. 

Single Channel Assumption. A single channel is available for all communication. All stations can transmit on it and all can receive from it. As far as the hardware is concerned, all stations are equivalent, although protocol software may assign priorities to them. 

Collision Assumption. If two frames are transmitted simultaneously, they overlap in time and the resulting signal is garbled. This event is called a 

collision

. All stations can detect collisions. A collided frame must be transmitted again later. There are no errors other than those generated by collisions. 

4a. Continuous Time. Frame transmission can begin at any instant. There is no master clock dividing time into discrete intervals. 

4b. Slotted Time. Time is divided into discrete intervals (slots). Frame transmissions always begin at the start of a slot. A slot may contain 0, 1, or more frames, corresponding to an idle slot, a successful transmission, or a collision, respectively. 

5a. Carrier Sense. Stations can tell if the channel is in use before trying to use it. If the channel is sensed as busy, no station will attempt to use it until it goes idle. 

5b. No Carrier Sense. Stations cannot sense the channel before trying to use it. They just go ahead and transmit. Only later can they determine whether the transmission was successful. 

Some discussion of these assumptions is in order. The first one says that stations are independent and that work is generated at a constant rate. It also implicitly assumes that each station only has one program or user, so while the station is blocked, no new work is generated. More sophisticated models allow multiprogrammed stations that can generate work while a station is blocked, but the analysis of these stations is much more complex. 

The single channel assumption is the heart of the model. There are no external ways to communicate. Stations cannot raise their hands to request that the teacher call on them. 

The collision assumption is also basic, although in some systems (notably spread spectrum), this assumption is relaxed, with surprising results. Also, some LANs, such as token rings, pass a special token from station to station, possession of which allows the current holder to transmit a frame. But in the coming sections we will stick to the single channel with contention and collisions model. 

Two alternative assumptions about time are possible. Either it is continuous (4a) or it is slotted (4b). Some systems use one and some systems use the other, so we will discuss and analyze both. For a given system, only one of them holds. 

Similarly, a network can either have carrier sensing (5a) or not have it (5b). LANs generally have carrier sense. However, wireless networks cannot use it effectively because not every station may be within radio range of every other station. Stations on wired carrier sense networks can terminate their transmission prematurely if they discover that it is colliding with another transmission. Collision detection is rarely done on wireless networks, for engineering reasons. Note that the word ''carrier'' in this sense refers to an electrical signal on the cable and has nothing to do with the common carriers (e.g., telephone companies) that date back to the Pony Express days. 

186




4.2 Multiple Access Protocols 

Many algorithms for allocating a multiple access channel are known. In the following sections we will study a small sample of the more interesting ones and give some examples of their use. 

4.2.1 ALOHA 

In the 1970s, Norman Abramson and his colleagues at the University of Hawaii devised a new and elegant method to solve the channel allocation problem. Their work has been extended by many researchers since then (Abramson, 1985). Although Abramson's work, called the ALOHA system, used ground-based radio broadcasting, the basic idea is applicable to any system in which uncoordinated users are competing for the use of a single shared channel. 

We will discuss two versions of ALOHA here: pure and slotted. They differ with respect to whether time is divided into discrete slots into which all frames must fit. Pure ALOHA does not require global time synchronization; slotted ALOHA does. 

Pure ALOHA 

The basic idea of an ALOHA system is simple: let users transmit whenever they have data to be sent. There will be collisions, of course, and the colliding frames will be damaged. However, due to the feedback property of broadcasting, a sender can always find out whether its frame was destroyed by listening to the channel, the same way other users do. With a LAN, the feedback is immediate; with a satellite, there is a delay of 270 msec before the sender knows if the transmission was successful. If listening while transmitting is not possible for some reason, acknowledgements are needed. If the frame was destroyed, the sender just waits a random amount of time and sends it again. The waiting time must be random or the same frames will collide over and over, in lockstep. Systems in which multiple users share a common channel in a way that can lead to conflicts are widely known as 

contention

 systems. 

A sketch of frame generation in an ALOHA system is given in 

Fig. 4-1

. We have made the frames all the same length because the throughput of ALOHA systems is maximized by having a uniform frame size rather than by allowing variable length frames. 

Figure 4-1. In pure ALOHA, frames are transmitted at completely arbitrary times. 

 

Whenever two frames try to occupy the channel at the same time, there will be a collision and both will be garbled. If the first bit of a new frame overlaps with just the last bit of a frame almost finished, both frames will be totally destroyed and both will have to be retransmitted later. The checksum cannot (and should not) distinguish between a total loss and a near miss. Bad is bad. 

187




An interesting question is: What is the efficiency of an ALOHA channel? In other words, what fraction of all transmitted frames escape collisions under these chaotic circumstances? Let us first consider an infinite collection of interactive users sitting at their computers (stations). A user is always in one of two states: typing or waiting. Initially, all users are in the typing state. When a line is finished, the user stops typing, waiting for a response. The station then transmits a frame containing the line and checks the channel to see if it was successful. If so, the user sees the reply and goes back to typing. If not, the user continues to wait and the frame is retransmitted over and over until it has been successfully sent. 

Let the ''frame time'' denote the amount of time needed to transmit the standard, fixed-length frame (i.e., the frame length divided by the bit rate). At this point we assume that the infinite population of users generates new frames according to a Poisson distribution with mean 

N

 frames per frame time. (The infinite-population assumption is needed to ensure that 

N

 does not decrease as users become blocked.) If 

N >

 1, the user community is generating frames at a higher rate than the channel can handle, and nearly every frame will suffer a collision. For reasonable throughput we would expect 0 

< N <

 1. 

In addition to the new frames, the stations also generate retransmissions of frames that previously suffered collisions. Let us further assume that the probability of 

k

 transmission attempts per frame time, old and new combined, is also Poisson, with mean 

G

 per frame time. Clearly, 

G

 

N.

 At low load (i.e., 

N

 

0), there will be few collisions, hence few retransmissions, so 

G

 

N

. At high load there will be many collisions, so 

G > N.

 Under all loads, the throughput, 

S

, is just the offered load, 

G

, times the probability, 

P

0

, of a transmission succeeding—that is, 

S

 = 

GP

0

, where 

P

0

 is the probability that a frame does not suffer a collision. 

A frame will not suffer a collision if no other frames are sent within one frame time of its start, as shown in 

Fig. 4-2

. Under what conditions will the shaded frame arrive undamaged? Let 

t

 be the time required to send a frame. If any other user has generated a frame between time 

t

0

 and 

t

0

 + 

t

, the end of that frame will collide with the beginning of the shaded one. In fact, the shaded frame's fate was already sealed even before the first bit was sent, but since in pure ALOHA a station does not listen to the channel before transmitting, it has no way of knowing that another frame was already underway. Similarly, any other frame started between 

t

0

 + 

t

 and 

t

0

 + 2

t

 will bump into the end of the shaded frame. 

Figure 4-2. Vulnerable period for the shaded frame. 

 

The probability that 

k

 frames are generated during a given frame time is given by the Poisson distribution: 

Equation 4 

188




 

 

so the probability of zero frames is just 

e

-

G.

 In an interval two frame times long, the mean number of frames generated is 2

G.

 The probability of no other traffic being initiated during the entire vulnerable period is thus given by 

P

0

 = 

e

 

-2

G.

 Using 

S

 = 

GP

0

, we get 

 

 

The relation between the offered traffic and the throughput is shown in 

Fig. 4-3

. The maximum throughput occurs at 

G

 = 0.5, with 

S

 = 1

/

2

e

, which is about 0.184. In other words, the best we can hope for is a channel utilization of 18 percent. This result is not very encouraging, but with everyone transmitting at will, we could hardly have expected a 100 percent success rate. 

Slotted ALOHA 

In 1972, Roberts published a method for doubling the capacity of an ALOHA system (Roberts, 1972). His proposal was to divide time into discrete intervals, each interval corresponding to one frame. This approach requires the users to agree on slot boundaries. One way to achieve synchronization would be to have one special station emit a pip at the start of each interval, like a clock. 

In Roberts' method, which has come to be known as 

slotted ALOHA

, in contrast to Abramson's 

pure ALOHA

, a computer is not permitted to send whenever a carriage return is typed. Instead, it is required to wait for the beginning of the next slot. Thus, the continuous pure ALOHA is turned into a discrete one. Since the vulnerable period is now halved, the probability of no other traffic during the same slot as our test frame is 

e

-

G

 which leads to 

Equation 4 

 

 

As you can see from 

Fig. 4-3

, slotted ALOHA peaks at 

G

 = 1, with a throughput of 

S

 =1

/e

 or about 0.368, twice that of pure ALOHA. If the system is operating at 

G

 = 1, the probability of an empty slot is 0.368 (from 

Eq. 4-2

). The best we can hope for using slotted ALOHA is 37 percent of the slots empty, 37 percent successes, and 26 percent collisions. Operating at higher values of 

G

 reduces the number of empties but increases the number of collisions exponentially. To see how this rapid growth of collisions with 

G

 comes about, consider the transmission of a test frame. The probability that it will avoid a collision is 

e

-

G

, the probability that all the other users are silent in that slot. The probability of a collision is then just 1 - 

e

-

G.

 The probability of a transmission requiring exactly 

k

 attempts, (i.e., 

k

 - 1 collisions followed by one success) is 

Figure 4-3. Throughput versus offered traffic for ALOHA systems. 

189




 

 

 

The expected number of transmissions, 

E

, per carriage return typed is then 

 

 

As a result of the exponential dependence of 

E

 upon 

G

, small increases in the channel load can drastically reduce its performance. 

Slotted Aloha is important for a reason that may not be initially obvious. It was devised in the 1970s, used in a few early experimental systems, then almost forgotten. When Internet access over the cable was invented, all of a sudden there was a problem of how to allocate a shared channel among multiple competing users, and slotted Aloha was pulled out of the garbage can to save the day. It has often happened that protocols that are perfectly valid fall into disuse for political reasons (e.g., some big company wants everyone to do things its way), but years later some clever person realizes that a long-discarded protocol solves his current problem. For this reason, in this chapter we will study a number of elegant protocols that are not currently in widespread use, but might easily be used in future applications, provided that enough network designers are aware of them. Of course, we will also study many protocols that are in current use as well. 

4.2.2 Carrier Sense Multiple Access Protocols 

With slotted ALOHA the best channel utilization that can be achieved is 1

/e.

 This is hardly surprising, since with stations transmitting at will, without paying attention to what the other stations are doing, there are bound to be many collisions. In local area networks, however, it is possible for stations to detect what other stations are doing, and adapt their behavior accordingly. These networks can achieve a much better utilization than 1

/e.

 In this section we will discuss some protocols for improving performance. 

Protocols in which stations listen for a carrier (i.e., a transmission) and act accordingly are called 

carrier sense protocols

. A number of them have been proposed. Kleinrock and Tobagi (1975) have analyzed several such protocols in detail. Below we will mention several versions of the carrier sense protocols. 

Persistent and Nonpersistent CSMA 

190




The first carrier sense protocol that we will study here is called 

1-persistent CSMA

 (Carrier Sense Multiple Access). When a station has data to send, it first listens to the channel to see if anyone else is transmitting at that moment. If the channel is busy, the station waits until it becomes idle. When the station detects an idle channel, it transmits a frame. If a collision occurs, the station waits a random amount of time and starts all over again. The protocol is called 1-persistent because the station transmits with a probability of 1 when it finds the channel idle. 

The propagation delay has an important effect on the performance of the protocol. There is a small chance that just after a station begins sending, another station will become ready to send and sense the channel. If the first station's signal has not yet reached the second one, the latter will sense an idle channel and will also begin sending, resulting in a collision. The longer the propagation delay, the more important this effect becomes, and the worse the performance of the protocol. 

Even if the propagation delay is zero, there will still be collisions. If two stations become ready in the middle of a third station's transmission, both will wait politely until the transmission ends and then both will begin transmitting exactly simultaneously, resulting in a collision. If they were not so impatient, there would be fewer collisions. Even so, this protocol is far better than pure ALOHA because both stations have the decency to desist from interfering with the third station's frame. Intuitively, this approach will lead to a higher performance than pure ALOHA. Exactly the same holds for slotted ALOHA. 

A second carrier sense protocol is 

nonpersistent CSMA

. In this protocol, a conscious attempt is made to be less greedy than in the previous one. Before sending, a station senses the channel. If no one else is sending, the station begins doing so itself. However, if the channel is already in use, the station does not continually sense it for the purpose of seizing it immediately upon detecting the end of the previous transmission. Instead, it waits a random period of time and then repeats the algorithm. Consequently, this algorithm leads to better channel utilization but longer delays than 1-persistent CSMA. 

The last protocol is 

p-persistent CSMA

. It applies to slotted channels and works as follows. When a station becomes ready to send, it senses the channel. If it is idle, it transmits with a probability 

p

. With a probability 

q

 = 1 - 

p

, it defers until the next slot. If that slot is also idle, it either transmits or defers again, with probabilities 

p

 and 

q

. This process is repeated until either the frame has been transmitted or another station has begun transmitting. In the latter case, the unlucky station acts as if there had been a collision (i.e., it waits a random time and starts again). If the station initially senses the channel busy, it waits until the next slot and applies the above algorithm. 

Figure 4-4

 shows the computed throughput versus offered traffic for all three protocols, as well as for pure and slotted ALOHA. 

Figure 4-4. Comparison of the channel utilization versus load for various random access protocols. 

191




 

CSMA with Collision Detection 

Persistent and nonpersistent CSMA protocols are clearly an improvement over ALOHA because they ensure that no station begins to transmit when it senses the channel busy. Another improvement is for stations to abort their transmissions as soon as they detect a collision. In other words, if two stations sense the channel to be idle and begin transmitting simultaneously, they will both detect the collision almost immediately. Rather than finish transmitting their frames, which are irretrievably garbled anyway, they should abruptly stop transmitting as soon as the collision is detected. Quickly terminating damaged frames saves time and bandwidth. This protocol, known as 

CSMA/CD

 (

CSMA with Collision Detection

) is widely used on LANs in the MAC sublayer. In particular, it is the basis of the popular Ethernet LAN, so it is worth devoting some time to looking at it in detail. 

CSMA/CD, as well as many other LAN protocols, uses the conceptual model of 

Fig. 4-5

. At the point marked 

t

0

, a station has finished transmitting its frame. Any other station having a frame to send may now attempt to do so. If two or more stations decide to transmit simultaneously, there will be a collision. Collisions can be detected by looking at the power or pulse width of the received signal and comparing it to the transmitted signal. 

Figure 4-5. CSMA/CD can be in one of three states: contention, transmission, or idle. 

 

After a station detects a collision, it aborts its transmission, waits a random period of time, and then tries again, assuming that no other station has started transmitting in the meantime. Therefore, our model for CSMA/CD will consist of alternating contention and transmission periods, with idle periods occurring when all stations are quiet (e.g., for lack of work). 

Now let us look closely at the details of the contention algorithm. Suppose that two stations both begin transmitting at exactly time 

t

0

.

 How long will it take them to realize that there has been a collision? The answer to this question is vital to determining the length of the contention period and hence what the delay and throughput will be. The minimum time to 

192




detect the collision is then just the time it takes the signal to propagate from one station to the other. 

Based on this reasoning, you might think that a station not hearing a collision for a time equal to the full cable propagation time after starting its transmission could be sure it had seized the cable. By ''seized,'' we mean that all other stations knew it was transmitting and would not interfere. This conclusion is wrong. Consider the following worst-case scenario. Let the time for a signal to propagate between the two farthest stations be t

.

 At 

t

0

, one station begins transmitting. At t - e, an instant before the signal arrives at the most distant station, that station also begins transmitting. Of course, it detects the collision almost instantly and stops, but the little noise burst caused by the collision does not get back to the original station until time 2t - e

.

 In other words, in the worst case a station cannot be sure that it has seized the channel until it has transmitted for 2t without hearing a collision. For this reason we will model the contention interval as a slotted ALOHA system with slot width 2t

.

 On a 1-km long coaxial cable, t 

5 µsec. For simplicity we will assume that each slot contains just 1 bit. Once the channel has been seized, a station can transmit at any rate it wants to, of course, not just at 1 bit per 2t sec. 

It is important to realize that collision detection is an 

analog

 process. The station's hardware must listen to the cable while it is transmitting. If what it reads back is different from what it is putting out, it knows that a collision is occurring. The implication is that the signal encoding must allow collisions to be detected (e.g., a collision of two 0-volt signals may well be impossible to detect). For this reason, special encoding is commonly used. 

It is also worth noting that a sending station must continually monitor the channel, listening for noise bursts that might indicate a collision. For this reason, CSMA/CD with a single channel is inherently a half-duplex system. It is impossible for a station to transmit and receive frames at the same time because the receiving logic is in use, looking for collisions during every transmission. 

To avoid any misunderstanding, it is worth noting that no MAC-sublayer protocol guarantees reliable delivery. Even in the absence of collisions, the receiver may not have copied the frame correctly for various reasons (e.g., lack of buffer space or a missed interrupt). 

4.2.3 Collision-Free Protocols 

Although collisions do not occur with CSMA/CD once a station has unambiguously captured the channel, they can still occur during the contention period. These collisions adversely affect the system performance, especially when the cable is long (i.e., large t) and the frames are short. And CSMA/CD is not universally applicable. In this section, we will examine some protocols that resolve the contention for the channel without any collisions at all, not even during the contention period. Most of these are not currently used in major systems, but in a rapidly changing field, having some protocols with excellent properties available for future systems is often a good thing. 

In the protocols to be described, we assume that there are exactly 

N

 stations, each with a unique address from 0 to 

N

 - 1 ''wired'' into it. It does not matter that some stations may be inactive part of the time. We also assume that propagation delay is negligible. The basic question remains: Which station gets the channel after a successful transmission? We continue using the model of 

Fig. 4-5

 with its discrete contention slots. 

A Bit-Map Protocol 

In our first collision-free protocol, the 

basic bit-map method

, each contention period consists of exactly 

N

 slots. If station 0 has a frame to send, it transmits a 1 bit during the zeroth slot. No other station is allowed to transmit during this slot. Regardless of what station 0 does, 

193




station 1 gets the opportunity to transmit a 1 during slot 1, but only if it has a frame queued. In general, station 

j

 may announce that it has a frame to send by inserting a 1 bit into slot 

j.

 After all 

N

 slots have passed by, each station has complete knowledge of which stations wish to transmit. At that point, they begin transmitting in numerical order (see 

Fig. 4-6

). 

Figure 4-6. The basic bit-map protocol. 

 

Since everyone agrees on who goes next, there will never be any collisions. After the last ready station has transmitted its frame, an event all stations can easily monitor, another 

N

 bit contention period is begun. If a station becomes ready just after its bit slot has passed by, it is out of luck and must remain silent until every station has had a chance and the bit map has come around again. Protocols like this in which the desire to transmit is broadcast before the actual transmission are called 

reservation protocols

. 

Let us briefly analyze the performance of this protocol. For convenience, we will measure time in units of the contention bit slot, with data frames consisting of 

d

 time units. Under conditions of low load, the bit map will simply be repeated over and over, for lack of data frames. 

Consider the situation from the point of view of a low-numbered station, such as 0 or 1. Typically, when it becomes ready to send, the ''current'' slot will be somewhere in the middle of the bit map. On average, the station will have to wait 

N/

2 slots for the current scan to finish and another full 

N

 slots for the following scan to run to completion before it may begin transmitting. 

The prospects for high-numbered stations are brighter. Generally, these will only have to wait half a scan (

N/

2 bit slots) before starting to transmit. High-numbered stations rarely have to wait for the next scan. Since low-numbered stations must wait on average 1.5

N

 slots and high-numbered stations must wait on average 0.5

N

 slots, the mean for all stations is 

N

 slots. The channel efficiency at low load is easy to compute. The overhead per frame is 

N

 bits, and the amount of data is 

d

 bits, for an efficiency of 

d/

(

N

 + 

d

)

.

At high load, when all the stations have something to send all the time, the 

N

 bit contention period is prorated over 

N

 frames, yielding an overhead of only 1 bit per frame, or an efficiency of 

d/

(

d

 + 1)

.

 The mean delay for a frame is equal to the sum of the time it queues inside its station, plus an additional 

N

(

d

 + 1)

/

2 once it gets to the head of its internal queue. 

Binary Countdown 

A problem with the basic bit-map protocol is that the overhead is 1 bit per station, so it does not scale well to networks with thousands of stations. We can do better than that by using binary station addresses. A station wanting to use the channel now broadcasts its address as a binary bit string, starting with the high-order bit. All addresses are assumed to be the same length. The bits in each address position from different stations are BOOLEAN ORed together. We will call this protocol 

binary countdown

. It was used in Datakit (Fraser, 1987). It implicitly assumes that the transmission delays are negligible so that all stations see asserted bits essentially instantaneously. 

To avoid conflicts, an arbitration rule must be applied: as soon as a station sees that a high-order bit position that is 0 in its address has been overwritten with a 1, it gives up. For example, if stations 0010, 0100, 1001, and 1010 are all trying to get the channel, in the first 

194




bit time the stations transmit 0, 0, 1, and 1, respectively. These are ORed together to form a 1. Stations 0010 and 0100 see the 1 and know that a higher-numbered station is competing for the channel, so they give up for the current round. Stations 1001 and 1010 continue. 

The next bit is 0, and both stations continue. The next bit is 1, so station 1001 gives up. The winner is station 1010 because it has the highest address. After winning the bidding, it may now transmit a frame, after which another bidding cycle starts. The protocol is illustrated in 

Fig. 4-7

. It has the property that higher-numbered stations have a higher priority than lower-numbered stations, which may be either good or bad, depending on the context. 

Figure 4-7. The binary countdown protocol. A dash indicates silence. 

 

The channel efficiency of this method is 

d/

(

d

 + log

2

 

N

). If, however, the frame format has been cleverly chosen so that the sender's address is the first field in the frame, even these log

2

 

N

 bits are not wasted, and the efficiency is 100 percent. 

Mok and Ward (1979) have described a variation of binary countdown using a parallel rather than a serial interface. They also suggest using virtual station numbers, with the virtual station numbers from 0 up to and including the successful station being circularly permuted after each transmission, in order to give higher priority to stations that have been silent unusually long. For example, if stations 

C

, 

H

, 

D

, 

A

, 

G

, 

B

, 

E

, 

F

 have priorities 7, 6, 5, 4, 3, 2, 1, and 0, respectively, then a successful transmission by 

D

 puts it at the end of the list, giving a priority order of 

C

, 

H

, 

A

, 

G

, 

B

, 

E

, 

F

, 

D

. Thus, 

C

 remains virtual station 7, but 

A

 moves up from 4 to 5 and 

D

 drops from 5 to 0. Station 

D

 will now only be able to acquire the channel if no other station wants it. 

Binary countdown is an example of a simple, elegant, and efficient protocol that is waiting to be rediscovered. Hopefully, it will find a new home some day. 

4.2.4 Limited-Contention Protocols 

We have now considered two basic strategies for channel acquisition in a cable network: contention, as in CSMA, and collision-free methods. Each strategy can be rated as to how well it does with respect to the two important performance measures, delay at low load and channel efficiency at high load. Under conditions of light load, contention (i.e., pure or slotted ALOHA) is preferable due to its low delay. As the load increases, contention becomes increasingly less attractive, because the overhead associated with channel arbitration becomes greater. Just the reverse is true for the collision-free protocols. At low load, they have high delay, but as the load increases, the channel efficiency improves rather than gets worse as it does for contention protocols. 

195




Obviously, it would be nice if we could combine the best properties of the contention and collision-free protocols, arriving at a new protocol that used contention at low load to provide low delay, but used a collision-free technique at high load to provide good channel efficiency. Such protocols, which we will call 

limited-contention protocols

, do, in fact, exist, and will conclude our study of carrier sense networks. 

Up to now the only contention protocols we have studied have been symmetric, that is, each station attempts to acquire the channel with some probability, 

p

, with all stations using the same 

p.

 Interestingly enough, the overall system performance can sometimes be improved by using a protocol that assigns different probabilities to different stations. 

Before looking at the asymmetric protocols, let us quickly review the performance of the symmetric case. Suppose that 

k

 stations are contending for channel access. Each has a probability 

p

 of transmitting during each slot. The probability that some station successfully acquires the channel during a given slot is then 

kp

(1 - 

p

)

k

 

- 1

.

 To find the optimal value of 

p

, we differentiate with respect to 

p

, set the result to zero, and solve for 

p.

 Doing so, we find that the best value of 

p

 is 1

/k.

 Substituting 

p

 = 1

/k

, we get 

Equation 4 

 

 

This probability is plotted in 

Fig. 4-8

. For small numbers of stations, the chances of success are good, but as soon as the number of stations reaches even five, the probability has dropped close to its asymptotic value of 1

/e.

Figure 4-8. Acquisition probability for a symmetric contention channel. 

 

From 

Fig. 4-8

, it is fairly obvious that the probability of some station acquiring the channel can be increased only by decreasing the amount of competition. The limited-contention protocols do precisely that. They first divide the stations into (not necessarily disjoint) groups. Only the members of group 0 are permitted to compete for slot 0. If one of them succeeds, it acquires the channel and transmits its frame. If the slot lies fallow or if there is a collision, the members of group 1 contend for slot 1, etc. By making an appropriate division of stations into groups, the amount of contention for each slot can be reduced, thus operating each slot near the left end of 

Fig. 4-8

. 

196




The trick is how to assign stations to slots. Before looking at the general case, let us consider some special cases. At one extreme, each group has but one member. Such an assignment guarantees that there will never be collisions because at most one station is contending for any given slot. We have seen such protocols before (e.g., binary countdown). The next special case is to assign two stations per group. The probability that both will try to transmit during a slot is 

p

2

, which for small 

p

 is negligible. As more and more stations are assigned to the same slot, the probability of a collision grows, but the length of the bit-map scan needed to give everyone a chance shrinks. The limiting case is a single group containing all stations (i.e., slotted ALOHA). What we need is a way to assign stations to slots dynamically, with many stations per slot when the load is low and few (or even just one) station per slot when the load is high. 

The Adaptive Tree Walk Protocol 

One particularly simple way of performing the necessary assignment is to use the algorithm devised by the U.S. Army for testing soldiers for syphilis during World War II (Dorfman, 1943). In short, the Army took a blood sample from 

N

 soldiers. A portion of each sample was poured into a single test tube. This mixed sample was then tested for antibodies. If none were found, all the soldiers in the group were declared healthy. If antibodies were present, two new mixed samples were prepared, one from soldiers 1 through 

N/

2 and one from the rest. The process was repeated recursively until the infected soldiers were determined. 

For the computerized version of this algorithm (Capetanakis, 1979), it is convenient to think of the stations as the leaves of a binary tree, as illustrated in 

Fig. 4-9

. In the first contention slot following a successful frame transmission, slot 0, all stations are permitted to try to acquire the channel. If one of them does so, fine. If there is a collision, then during slot 1 only those stations falling under node 2 in the tree may compete. If one of them acquires the channel, the slot following the frame is reserved for those stations under node 3. If, on the other hand, two or more stations under node 2 want to transmit, there will be a collision during slot 1, in which case it is node 4's turn during slot 2. 

Figure 4-9. The tree for eight stations. 

 

In essence, if a collision occurs during slot 0, the entire tree is searched, depth first, to locate all ready stations. Each bit slot is associated with some particular node in the tree. If a collision occurs, the search continues recursively with the node's left and right children. If a bit slot is idle or if only one station transmits in it, the searching of its node can stop because all ready stations have been located. (Were there more than one, there would have been a collision.) 

When the load on the system is heavy, it is hardly worth the effort to dedicate slot 0 to node 1, because that makes sense only in the unlikely event that precisely one station has a frame to send. Similarly, one could argue that nodes 2 and 3 should be skipped as well for the same reason. Put in more general terms, at what level in the tree should the search begin? Clearly, the heavier the load, the farther down the tree the search should begin. We will assume that 

197




each station has a good estimate of the number of ready stations, 

q

, for example, from monitoring recent traffic. 

To proceed, let us number the levels of the tree from the top, with node 1 in 

Fig. 4-9

 at level 0, nodes 2 and 3 at level 1, etc. Notice that each node at level 

i

 has a fraction 2

-

i

 of the stations below it. If the 

q

 ready stations are uniformly distributed, the expected number of them below a specific node at level 

i

 is just 2

-

iq.

 Intuitively, we would expect the optimal level to begin searching the tree as the one at which the mean number of contending stations per slot is 1, that is, the level at which 2

-

iq

 = 1

.

 Solving this equation, we find that 

i

 = log

2

 

q.

Numerous improvements to the basic algorithm have been discovered and are discussed in some detail by Bertsekas and Gallager (1992). For example, consider the case of stations 

G

 and 

H

 being the only ones wanting to transmit. At node 1 a collision will occur, so 2 will be tried and discovered idle. It is pointless to probe node 3 since it is guaranteed to have a collision (we know that two or more stations under 1 are ready and none of them are under 2, so they must all be under 3). The probe of 3 can be skipped and 6 tried next. When this probe also turns up nothing, 7 can be skipped and node 

G

 tried next. 

4.2.5 Wavelength Division Multiple Access Protocols 

A different approach to channel allocation is to divide the channel into subchannels using FDM, TDM, or both, and dynamically allocate them as needed. Schemes like this are commonly used on fiber optic LANs to permit different conversations to use different wavelengths (i.e., frequencies) at the same time. In this section we will examine one such protocol (Humblet et al., 1992). 

A simple way to build an all-optical LAN is to use a passive star coupler (see 

Fig. 2-10

). In effect, two fibers from each station are fused to a glass cylinder. One fiber is for output to the cylinder and one is for input from the cylinder. Light output by any station illuminates the cylinder and can be detected by all the other stations. Passive stars can handle hundreds of stations. 

To allow multiple transmissions at the same time, the spectrum is divided into channels (wavelength bands), as shown in 

Fig. 2-31

. In this protocol, 

WDMA

 (

Wavelength Division Multiple Access

), each station is assigned two channels. A narrow channel is provided as a control channel to signal the station, and a wide channel is provided so the station can output data frames. 

Each channel is divided into groups of time slots, as shown in 

Fig. 4-10

. Let us call the number of slots in the control channel 

m

 and the number of slots in the data channel 

n

 + 1, where 

n

 of these are for data and the last one is used by the station to report on its status (mainly, which slots on both channels are free). On both channels, the sequence of slots repeats endlessly, with slot 0 being marked in a special way so latecomers can detect it. All channels are synchronized by a single global clock. 

Figure 4-10. Wavelength division multiple access. 

198




 

The protocol supports three traffic classes : (1) constant data rate connection-oriented traffic, such as uncompressed video, (2) variable data rate connection-oriented traffic, such as file transfer, and (3) datagram traffic, such as UDP packets. For the two connection-oriented protocols, the idea is that for 

A

 to communicate with 

B

, it must first insert a CONNECTION REQUEST frame in a free slot on 

B

's control channel. If 

B

 accepts, communication can take place on 

A

's data channel. 

Each station has two transmitters and two receivers, as follows: 

1. A fixed-wavelength receiver for listening to its own control channel. 

2. A tunable transmitter for sending on other stations' control channels. 

3. A fixed-wavelength transmitter for outputting data frames. 

4. A tunable receiver for selecting a data transmitter to listen to. 

In other words, every station listens to its own control channel for incoming requests but has to tune to the transmitter's wavelength to get the data. Wavelength tuning is done by a Fabry-Perot or Mach-Zehnder interferometer that filters out all wavelengths except the desired wavelength band. 

Let us now consider how station 

A

 sets up a class 2 communication channel with station 

B

 for, say, file transfer. First, 

A

 tunes its data receiver to 

B

's data channel and waits for the status slot. This slot tells which control slots are currently assigned and which are free. In 

Fig. 4-10

, for example, we see that of 

B

's eight control slots, 0, 4, and 5 are free. The rest are occupied (indicated by crosses). 

A

 picks one of the free control slots, say, 4, and inserts its CONNECTION REQUEST message there. Since 

B

 constantly monitors its control channel, it sees the request and grants it by assigning slot 4 to 

A

. This assignment is announced in the status slot of 

B

's data channel. When 

A

 sees the announcement, it knows it has a unidirectional connection. If 

A

 asked for a two-way connection, 

B

 now repeats the same algorithm with 

A

. 

It is possible that at the same time 

A

 tried to grab 

B

's control slot 4, 

C

 did the same thing. Neither will get it, and both will notice the failure by monitoring the status slot in 

B

's control channel. They now each wait a random amount of time and try again later. 

At this point, each party has a conflict-free way to send short control messages to the other one. To perform the file transfer, 

A

 now sends 

B

 a control message saying, for example, ''Please watch my next data output slot 3. There is a data frame for you in it.'' When 

B

 gets 

199




the control message, it tunes its receiver to 

A

's output channel to read the data frame. Depending on the higher-layer protocol, 

B

 can use the same mechanism to send back an acknowledgement if it wishes. 

Note that a problem arises if both 

A

 and 

C

 have connections to 

B

 and each of them suddenly tells 

B

 to look at slot 3. 

B

 will pick one of these requests at random, and the other transmission will be lost. 

For constant rate traffic, a variation of this protocol is used. When 

A

 asks for a connection, it simultaneously says something like: Is it all right if I send you a frame in every occurrence of slot 3? If 

B

 is able to accept (i.e., has no previous commitment for slot 3), a guaranteed bandwidth connection is established. If not, 

A

 can try again with a different proposal, depending on which output slots it has free. 

Class 3 (datagram) traffic uses still another variation. Instead of writing a CONNECTION REQUEST message into the control slot it just found (4), it writes a DATA FOR YOU IN SLOT 3 message. If 

B

 is free during the next data slot 3, the transmission will succeed. Otherwise, the data frame is lost. In this manner, no connections are ever needed. 

Several variants of the protocol are possible. For example, instead of each station having its own control channel, a single control channel can be shared by all stations. Each station is assigned a block of slots in each group, effectively multiplexing multiple virtual channels onto one physical one. 

It is also possible to make do with a single tunable transmitter and a single tunable receiver per station by having each station's channel be divided into 

m

 control slots followed by 

n

 + 1 data slots. The disadvantage here is that senders have to wait longer to capture a control slot and consecutive data frames are farther apart because some control information is in the way. 

Numerous other WDMA protocols have been proposed and implemented, differing in various details. Some have only one control channel; others have multiple control channels. Some take propagation delay into account; others do not. Some make tuning time an explicit part of the model; others ignore it. The protocols also differ in terms of processing complexity, throughput, and scalability. When a large number of frequencies are being used, the system is sometimes called 

DWDM

 (

Dense Wavelength Division Multiplexing

). For more information see (Bogineni et al., 1993; Chen, 1994; Goralski, 2001; Kartalopoulos, 1999; and Levine and Akyildiz, 1995). 

4.2.6 Wireless LAN Protocols 

As the number of mobile computing and communication devices grows, so does the demand to connect them to the outside world. Even the very first mobile telephones had the ability to connect to other telephones. The first portable computers did not have this capability, but soon afterward, modems became commonplace on notebook computers. To go on-line, these computers had to be plugged into a telephone wall socket. Requiring a wired connection to the fixed network meant that the computers were portable, but not mobile. 

To achieve true mobility, notebook computers need to use radio (or infrared) signals for communication. In this manner, dedicated users can read and send e-mail while hiking or boating. A system of notebook computers that communicate by radio can be regarded as a wireless LAN, as we discussed in 

Sec. 1.5.4

. These LANs have somewhat different properties than conventional LANs and require special MAC sublayer protocols. In this section we will examine some of these protocols. More information about wireless LANs can be found in (Geier, 2002; and O'Hara and Petrick, 1999). 

200




A common configuration for a wireless LAN is an office building with base stations (also called access points) strategically placed around the building. All the base stations are wired together using copper or fiber. If the transmission power of the base stations and notebooks is adjusted to have a range of 3 or 4 meters, then each room becomes a single cell and the entire building becomes a large cellular system, as in the traditional cellular telephony systems we studied in 

Chap. 2

. Unlike cellular telephone systems, each cell has only one channel, covering the entire available bandwidth and covering all the stations in its cell. Typically, its bandwidth is 11 to 54 Mbps. 

In our discussions below, we will make the simplifying assumption that all radio transmitters have some fixed range. When a receiver is within range of two active transmitters, the resulting signal will generally be garbled and useless, in other words, we will not consider CDMA-type systems further in this discussion. It is important to realize that in some wireless LANs, not all stations are within range of one another, which leads to a variety of complications. Furthermore, for indoor wireless LANs, the presence of walls between stations can have a major impact on the effective range of each station. 

A naive approach to using a wireless LAN might be to try CSMA: just listen for other transmissions and only transmit if no one else is doing so. The trouble is, this protocol is not really appropriate because what matters is interference at the receiver, not at the sender. To see the nature of the problem, consider 

Fig. 4-11

, where four wireless stations are illustrated. For our purposes, it does not matter which are base stations and which are notebooks. The radio range is such that 

A

 and 

B

 are within each other's range and can potentially interfere with one another. 

C

 can also potentially interfere with both 

B

 and 

D

, but not with 

A

. 

Figure 4-11. A wireless LAN. (a) 

A

 transmitting. (b) 

B

 transmitting. 

 

First consider what happens when 

A

 is transmitting to 

B

, as depicted in 

Fig. 4-11(a)

. If 

C

 senses the medium, it will not hear 

A

 because 

A

 is out of range, and thus falsely conclude that it can transmit to 

B

. If 

C

 does start transmitting, it will interfere at 

B

, wiping out the frame from 

A

. The problem of a station not being able to detect a potential competitor for the medium because the competitor is too far away is called the 

hidden station problem

. 

Now let us consider the reverse situation: 

B

 transmitting to 

A

, as shown in 

Fig. 4-11(b)

. If 

C

 senses the medium, it will hear an ongoing transmission and falsely conclude that it may not send to 

D

, when in fact such a transmission would cause bad reception only in the zone between 

B

 and 

C

, where neither of the intended receivers is located. This is called the 

exposed station problem

. 

The problem is that before starting a transmission, a station really wants to know whether there is activity around the receiver. CSMA merely tells it whether there is activity around the station sensing the carrier. With a wire, all signals propagate to all stations so only one transmission can take place at once anywhere in the system. In a system based on short-range radio waves, multiple transmissions can occur simultaneously if they all have different destinations and these destinations are out of range of one another. 

Another way to think about this problem is to imagine an office building in which every employee has a wireless notebook computer. Suppose that Linda wants to send a message to Milton. Linda's computer senses the local environment and, detecting no activity, starts sending. However, there may still be a collision in Milton's office because a third party may 

201




currently be sending to him from a location so far from Linda that her computer could not detect it. 

MACA and MACAW 

An early protocol designed for wireless LANs is 

MACA

 (

Multiple Access with Collision Avoidance

) (Karn, 1990). The basic idea behind it is for the sender to stimulate the receiver into outputting a short frame, so stations nearby can detect this transmission and avoid transmitting for the duration of the upcoming (large) data frame. MACA is illustrated in 

Fig. 4-

12

. 

Figure 4-12. The MACA protocol. (a) 

A

 sending an RTS to 

B

. (b) 

B

 responding with a CTS to 

A

. 

 

Let us now consider how 

A

 sends a frame to 

B

. 

A

 starts by sending an 

RTS

 (

Request To Send

) frame to 

B

, as shown in 

Fig. 4-12(a)

. This short frame (30 bytes) contains the length of the data frame that will eventually follow. Then 

B

 replies with a 

CTS

 (

Clear to Send

) frame, as shown in 

Fig. 4-12(b)

. The CTS frame contains the data length (copied from the RTS frame). Upon receipt of the CTS frame, 

A

 begins transmission. 

Now let us see how stations overhearing either of these frames react. Any station hearing the RTS is clearly close to 

A

 and must remain silent long enough for the CTS to be transmitted back to 

A

 without conflict. Any station hearing the CTS is clearly close to 

B

 and must remain silent during the upcoming data transmission, whose length it can tell by examining the CTS frame. 

In 

Fig. 4-12

, 

C

 is within range of 

A

 but not within range of 

B

. Therefore, it hears the RTS from 

A

 but not the CTS from 

B

. As long as it does not interfere with the CTS, it is free to transmit while the data frame is being sent. In contrast, 

D

 is within range of 

B

 but not 

A

. It does not hear the RTS but does hear the CTS. Hearing the CTS tips it off that it is close to a station that is about to receive a frame, so it defers sending anything until that frame is expected to be finished. Station 

E

 hears both control messages and, like 

D

, must be silent until the data frame is complete. 

Despite these precautions, collisions can still occur. For example, 

B

 and 

C

 could both send RTS frames to 

A

 at the same time. These will collide and be lost. In the event of a collision, an unsuccessful transmitter (i.e., one that does not hear a CTS within the expected time interval) waits a random amount of time and tries again later. The algorithm used is binary exponential backoff, which we will study when we come to Ethernet. 

Based on simulation studies of MACA, Bharghavan et al. (1994) fine tuned MACA to improve its performance and renamed their new protocol 

MACAW

 (

MACA for Wireless

). To start with, 

202




they noticed that without data link layer acknowledgements, lost frames were not retransmitted until the transport layer noticed their absence, much later. They solved this problem by introducing an ACK frame after each successful data frame. They also observed that CSMA has some use, namely, to keep a station from transmitting an RTS at the same time another nearby station is also doing so to the same destination, so carrier sensing was added. In addition, they decided to run the backoff algorithm separately for each data stream (source-destination pair), rather than for each station. This change improves the fairness of the protocol. Finally, they added a mechanism for stations to exchange information about congestion and a way to make the backoff algorithm react less violently to temporary problems, to improve system performance. 

4.3 Ethernet 

We have now finished our general discussion of channel allocation protocols in the abstract, so it is time to see how these principles apply to real systems, in particular, LANs. As discussed in 

Sec. 1.5.3

, the IEEE has standardized a number of local area networks and metropolitan area networks under the name of IEEE 802. A few have survived but many have not, as we saw in 

Fig. 1-38

. Some people who believe in reincarnation think that Charles Darwin came back as a member of the IEEE Standards Association to weed out the unfit. The most important of the survivors are 802.3 (Ethernet) and 802.11 (wireless LAN). With 802.15 (Bluetooth) and 802.16 (wireless MAN), it is too early to tell. Please consult the 5th edition of this book to find out. Both 802.3 and 802.11 have different physical layers and different MAC sublayers but converge on the same logical link control sublayer (defined in 802.2), so they have the same interface to the network layer. 

We introduced Ethernet in 

Sec. 1.5.3

 and will not repeat that material here. Instead we will focus on the technical details of Ethernet, the protocols, and recent developments in high-speed (gigabit) Ethernet. Since Ethernet and IEEE 802.3 are identical except for two minor differences that we will discuss shortly, many people use the terms ''Ethernet'' and ''IEEE 802.3'' interchangeably, and we will do so, too. For more information about Ethernet, see (Breyer and Riley, 1999 ; Seifert, 1998; and Spurgeon, 2000). 

4.3.1 Ethernet Cabling 

Since the name ''Ethernet'' refers to the cable (the ether), let us start our discussion there. Four types of cabling are commonly used, as shown in 

Fig. 4-13

. 

Figure 4-13. The most common kinds of Ethernet cabling. 

 

Historically, 

10Base5

 cabling, popularly called 

thick Ethernet,

 came first. It resembles a yellow garden hose, with markings every 2.5 meters to show where the taps go. (The 802.3 standard does not actually 

require

 the cable to be yellow, but it does 

suggest

 it.) Connections to it are generally made using 

vampire taps

, in which a pin is 

very

 carefully forced halfway into the coaxial cable's core. The notation 10Base5 means that it operates at 10 Mbps, uses baseband signaling, and can support segments of up to 500 meters. The first number is the speed in Mbps. Then comes the word ''Base'' (or sometimes ''BASE'') to indicate baseband transmission. There used to be a broadband variant, 10Broad36, but it never caught on in the marketplace and has since vanished. Finally, if the medium is coax, its length is given rounded to units of 100 m after ''Base.'' 

203




Historically, the second cable type was 

10Base2

, or 

thin Ethernet,

 which, in contrast to the garden-hose-like thick Ethernet, bends easily. Connections to it are made using industry-standard BNC connectors to form T junctions, rather than using vampire taps. BNC connectors are easier to use and more reliable. Thin Ethernet is much cheaper and easier to install, but it can run for only 185 meters per segment, each of which can handle only 30 machines. 

Detecting cable breaks, excessive length, bad taps, or loose connectors can be a major problem with both media. For this reason, techniques have been developed to track them down. Basically, a pulse of known shape is injected into the cable. If the pulse hits an obstacle or the end of the cable, an echo will be generated and sent back. By carefully timing the interval between sending the pulse and receiving the echo, it is possible to localize the origin of the echo. This technique is called 

time domain reflectometry

. 

The problems associated with finding cable breaks drove systems toward a different kind of wiring pattern, in which all stations have a cable running to a central 

hub

 in which they are all connected electrically (as if they were soldered together). Usually, these wires are telephone company twisted pairs, since most office buildings are already wired this way, and normally plenty of spare pairs are available. This scheme is called 

10Base-T

. Hubs do not buffer incoming traffic. We will discuss an improved version of this idea (switches), which do buffer incoming traffic later in this chapter. 

These three wiring schemes are illustrated in 

Fig. 4-14

. For 10Base5, a 

transceiver

 is clamped securely around the cable so that its tap makes contact with the inner core. The transceiver contains the electronics that handle carrier detection and collision detection. When a collision is detected, the transceiver also puts a special invalid signal on the cable to ensure that all other transceivers also realize that a collision has occurred. 

Figure 4-14. Three kinds of Ethernet cabling. (a) 10Base5. (b) 10Base2. (c) 10Base-T. 

 

With 10Base5, a 

transceiver cable

 or 

drop cable

 connects the transceiver to an interface board in the computer. The transceiver cable may be up to 50 meters long and contains five individually shielded twisted pairs. Two of the pairs are for data in and data out, respectively. Two more are for control signals in and out. The fifth pair, which is not always used, allows the computer to power the transceiver electronics. Some transceivers allow up to eight nearby computers to be attached to them, to reduce the number of transceivers needed. 

The transceiver cable terminates on an interface board inside the computer. The interface board contains a controller chip that transmits frames to, and receives frames from, the transceiver. The controller is responsible for assembling the data into the proper frame format, as well as computing checksums on outgoing frames and verifying them on incoming frames. 

204




Some controller chips also manage a pool of buffers for incoming frames, a queue of buffers to be transmitted, direct memory transfers with the host computers, and other aspects of network management. 

With 10Base2, the connection to the cable is just a passive BNC T-junction connector. The transceiver electronics are on the controller board, and each station always has its own transceiver. 

With 10Base-T, there is no shared cable at all, just the hub (a box full of electronics) to which each station is connected by a dedicated (i.e., not shared) cable. Adding or removing a station is simpler in this configuration, and cable breaks can be detected easily. The disadvantage of 10Base-T is that the maximum cable run from the hub is only 100 meters, maybe 200 meters if very high quality category 5 twisted pairs are used. Nevertheless, 10Base-T quickly became dominant due to its use of existing wiring and the ease of maintenance that it offers. A faster version of 10Base-T (100Base-T) will be discussed later in this chapter. 

A fourth cabling option for Ethernet is 

10Base-F

, which uses fiber optics. This alternative is expensive due to the cost of the connectors and terminators, but it has excellent noise immunity and is the method of choice when running between buildings or widely-separated hubs. Runs of up to km are allowed. It also offers good security since wiretapping fiber is much more difficult than wiretapping copper wire. 

Figure 4-15

 shows different ways of wiring a building. In 

Fig. 4-15(a)

, a single cable is snaked from room to room, with each station tapping into it at the nearest point. In 

Fig. 4-15(b)

, a vertical spine runs from the basement to the roof, with horizontal cables on each floor connected to the spine by special amplifiers (repeaters). In some buildings, the horizontal cables are thin and the backbone is thick. The most general topology is the tree, as in 

Fig. 4-

15(c)

, because a network with two paths between some pairs of stations would suffer from interference between the two signals. 

Figure 4-15. Cable topologies. (a) Linear. (b) Spine. (c) Tree. (d) Segmented. 

 

Each version of Ethernet has a maximum cable length per segment. To allow larger networks, multiple cables can be connected by 

repeaters

, as shown in 

Fig. 4-15(d)

. A repeater is a physical layer device. It receives, amplifies (regenerates), and retransmits signals in both directions. As far as the software is concerned, a series of cable segments connected by repeaters is no different from a single cable (except for some delay introduced by the repeaters). A system may contain multiple cable segments and multiple repeaters, but no two transceivers may be more than 2.5 km apart and no path between any two transceivers may traverse more than four repeaters. 

205




4.3.2 Manchester Encoding 

None of the versions of Ethernet uses straight binary encoding with 0 volts for a 0 bit and 5 volts for a 1 bit because it leads to ambiguities. If one station sends the bit string 0001000, others might falsely interpret it as 10000000 or 01000000 because they cannot tell the difference between an idle sender (0 volts) and a 0 bit (0 volts). This problem can be solved by using +1 volts for a 1 and -1 volts for a 0, but there is still the problem of a receiver sampling the signal at a slightly different frequency than the sender used to generate it. Different clock speeds can cause the receiver and sender to get out of synchronization about where the bit boundaries are, especially after a long run of consecutive 0s or a long run of consecutive 1s. 

What is needed is a way for receivers to unambiguously determine the start, end, or middle of each bit without reference to an external clock. Two such approaches are called 

Manchester encoding

 and 

differential Manchester encoding

. With Manchester encoding, each bit period is divided into two equal intervals. A binary 1 bit is sent by having the voltage set high during the first interval and low in the second one. A binary 0 is just the reverse: first low and then high. This scheme ensures that every bit period has a transition in the middle, making it easy for the receiver to synchronize with the sender. A disadvantage of Manchester encoding is that it requires twice as much bandwidth as straight binary encoding because the pulses are half the width. For example, to send data at 10 Mbps, the signal has to change 20 million times/sec. Manchester encoding is shown in 

Fig. 4-16(b)

. 

Figure 4-16. (a) Binary encoding. (b) Manchester encoding. (c) Differential Manchester encoding. 

 

Differential Manchester encoding, shown in 

Fig. 4-16(c)

, is a variation of basic Manchester encoding. In it, a 1 bit is indicated by the absence of a transition at the start of the interval. A 0 bit is indicated by the presence of a transition at the start of the interval. In both cases, there is a transition in the middle as well. The differential scheme requires more complex equipment but offers better noise immunity. All Ethernet systems use Manchester encoding due to its simplicity. The high signal is + 0.85 volts and the low signal is - 0.85 volts, giving a DC value of 0 volts. Ethernet does not use differential Manchester encoding, but other LANs (e.g., the 802.5 token ring) do use it. 

4.3.3 The Ethernet MAC Sublayer Protocol 

The original DIX (DEC, Intel, Xerox) frame structure is shown in 

Fig. 4-17(a)

. Each frame starts with a 

Preamble

 of 8 bytes, each containing the bit pattern 10101010. The Manchester encoding of this pattern produces a 10-MHz square wave for 6.4 µsec to allow the receiver's clock to synchronize with the sender's. They are required to stay synchronized for the rest of the frame, using the Manchester encoding to keep track of the bit boundaries. 

Figure 4-17. Frame formats. (a) DIX Ethernet. (b) IEEE 802.3. 

206




 

The frame contains two addresses, one for the destination and one for the source. The standard allows 2-byte and 6-byte addresses, but the parameters defined for the 10-Mbps baseband standard use only the 6-byte addresses. The high-order bit of the destination address is a 0 for ordinary addresses and 1 for group addresses. Group addresses allow multiple stations to listen to a single address. When a frame is sent to a group address, all the stations in the group receive it. Sending to a group of stations is called 

multicast

. The address consisting of all 1 bits is reserved for 

broadcast

. A frame containing all 1s in the destination field is accepted by all stations on the network. The difference between multicast and broadcast is important enough to warrant repeating. A multicast frame is sent to a selected group of stations on the Ethernet; a broadcast frame is sent to all stations on the Ethernet. Multicast is more selective, but involves group management. Broadcasting is coarser but does not require any group management. 

Another interesting feature of the addressing is the use of bit 46 (adjacent to the high-order bit) to distinguish local from global addresses. Local addresses are assigned by each network administrator and have no significance outside the local network. Global addresses, in contrast, are assigned centrally by IEEE to ensure that no two stations anywhere in the world have the same global address. With 48 - 2 = 46 bits available, there are about 7 x 10

13

 global addresses. The idea is that any station can uniquely address any other station by just giving the right 48-bit number. It is up to the network layer to figure out how to locate the destination. 

Next comes the 

Type

 field, which tells the receiver what to do with the frame. Multiple network-layer protocols may be in use at the same time on the same machine, so when an Ethernet frame arrives, the kernel has to know which one to hand the frame to. The 

Type

 field specifies which process to give the frame to. 

Next come the data, up to 1500 bytes. This limit was chosen somewhat arbitrarily at the time the DIX standard was cast in stone, mostly based on the fact that a transceiver needs enough RAM to hold an entire frame and RAM was expensive in 1978. A larger upper limit would have meant more RAM, hence a more expensive transceiver. 

In addition to there being a maximum frame length, there is also a minimum frame length. While a data field of 0 bytes is sometimes useful, it causes a problem. When a transceiver detects a collision, it truncates the current frame, which means that stray bits and pieces of frames appear on the cable all the time. To make it easier to distinguish valid frames from garbage, Ethernet requires that valid frames must be at least 64 bytes long, from destination address to checksum, including both. If the data portion of a frame is less than 46 bytes, the 

Pad

 field is used to fill out the frame to the minimum size. 

Another (and more important) reason for having a minimum length frame is to prevent a station from completing the transmission of a short frame before the first bit has even reached the far end of the cable, where it may collide with another frame. This problem is illustrated in 

Fig. 4-18

. At time 0, station 

A

, at one end of the network, sends off a frame. Let us call the propagation time for this frame to reach the other end t. Just before the frame gets to the other end (i.e., at time t-e), the most distant station, 

B

, starts transmitting. When 

B

 detects that it is receiving more power than it is putting out, it knows that a collision has occurred, so it aborts its transmission and generates a 48-bit noise burst to warn all other stations. In other words, it jams the ether to make sure the sender does not miss the collision. At about time 2t, 

207




the sender sees the noise burst and aborts its transmission, too. It then waits a random time before trying again. 

Figure 4-18. Collision detection can take as long as 2t. 

 

If a station tries to transmit a very short frame, it is conceivable that a collision occurs, but the transmission completes before the noise burst gets back at 2t. The sender will then incorrectly conclude that the frame was successfully sent. To prevent this situation from occurring, all frames must take more than 2t to send so that the transmission is still taking place when the noise burst gets back to the sender. For a 10-Mbps LAN with a maximum length of 2500 meters and four repeaters (from the 802.3 specification), the round-trip time (including time to propagate through the four repeaters) has been determined to be nearly 50 µsec in the worst case, including the time to pass through the repeaters, which is most certainly not zero. Therefore, the minimum frame must take at least this long to transmit. At 10 Mbps, a bit takes 100 nsec, so 500 bits is the smallest frame that is guaranteed to work. To add some margin of safety, this number was rounded up to 512 bits or 64 bytes. Frames with fewer than 64 bytes are padded out to 64 bytes with the 

Pad

 field. 

As the network speed goes up, the minimum frame length must go up or the maximum cable length must come down, proportionally. For a 2500-meter LAN operating at 1 Gbps, the minimum frame size would have to be 6400 bytes. Alternatively, the minimum frame size could be 640 bytes and the maximum distance between any two stations 250 meters. These restrictions are becoming increasingly painful as we move toward multigigabit networks. 

The final Ethernet field is the 

Checksum

. It is effectively a 32-bit hash code of the data. If some data bits are erroneously received (due to noise on the cable), the checksum will almost certainly be wrong and the error will be detected. The checksum algorithm is a cyclic redundancy check (CRC) of the kind discussed in 

Chap. 3

. It just does error detection, not forward error correction. 

When IEEE standardized Ethernet, the committee made two changes to the DIX format, as shown in 

Fig. 4-17(b)

. The first one was to reduce the preamble to 7 bytes and use the last byte for a 

Start of Frame

 delimiter, for compatibility with 802.4 and 802.5. The second one was to change the 

Type

 field into a 

Length

 field. Of course, now there was no way for the receiver to figure out what to do with an incoming frame, but that problem was handled by the addition of a small header to the data portion itself to provide this information. We will discuss the format of the data portion when we come to logical link control later in this chapter. 

Unfortunately, by the time 802.3 was published, so much hardware and software for DIX Ethernet was already in use that few manufacturers and users were enthusiastic about converting the 

Type

 field into a 

Length

 field. In 1997 IEEE threw in the towel and said that both ways were fine with it. Fortunately, all the 

Type

 fields in use before 1997 were greater than 1500. Consequently, any number there less than or equal to 1500 can be interpreted as 

Length

, and any number greater than 1500 can be interpreted as 

Type

. Now IEEE can 

208




maintain that everyone is using its standard and everybody else can keep on doing what they were already doing without feeling guilty about it. 

4.3.4 The Binary Exponential Backoff Algorithm 

Let us now see how randomization is done when a collision occurs. The model is that of 

Fig. 4-

5

. After a collision, time is divided into discrete slots whose length is equal to the worst-case round-trip propagation time on the ether (2t). To accommodate the longest path allowed by Ethernet, the slot time has been set to 512 bit times, or 51.2 µsec as mentioned above. 

After the first collision, each station waits either 0 or 1 slot times before trying again. If two stations collide and each one picks the same random number, they will collide again. After the second collision, each one picks either 0, 1, 2, or 3 at random and waits that number of slot times. If a third collision occurs (the probability of this happening is 0.25), then the next time the number of slots to wait is chosen at random from the interval 0 to 2

3

 - 1. 

In general, after 

i

 collisions, a random number between 0 and 2

i

 - 1 is chosen, and that number of slots is skipped. However, after ten collisions have been reached, the randomization interval is frozen at a maximum of 1023 slots. After 16 collisions, the controller throws in the towel and reports failure back to the computer. Further recovery is up to higher layers. 

This algorithm, called 

binary exponential backoff

, was chosen to dynamically adapt to the number of stations trying to send. If the randomization interval for all collisions was 1023, the chance of two stations colliding for a second time would be negligible, but the average wait after a collision would be hundreds of slot times, introducing significant delay. On the other hand, if each station always delayed for either zero or one slots, then if 100 stations ever tried to send at once, they would collide over and over until 99 of them picked 1 and the remaining station picked 0. This might take years. By having the randomization interval grow exponentially as more and more consecutive collisions occur, the algorithm ensures a low delay when only a few stations collide but also ensures that the collision is resolved in a reasonable interval when many stations collide. Truncating the backoff at 1023 keeps the bound from growing too large. 

As described so far, CSMA/CD provides no acknowledgements. Since the mere absence of collisions does not guarantee that bits were not garbled by noise spikes on the cable, for reliable communication the destination must verify the checksum, and if correct, send back an acknowledgement frame to the source. Normally, this acknowledgement would be just another frame as far as the protocol is concerned and would have to fight for channel time just like a data frame. However, a simple modification to the contention algorithm would allow speedy confirmation of frame receipt (Tokoro and Tamaru, 1977). All that would be needed is to reserve the first contention slot following successful transmission for the destination station. Unfortunately, the standard does not provide for this possibility. 

4.3.5 Ethernet Performance 

Now let us briefly examine the performance of Ethernet under conditions of heavy and constant load, that is, 

k

 stations always ready to transmit. A rigorous analysis of the binary exponential backoff algorithm is complicated. Instead, we will follow Metcalfe and Boggs (1976) and assume a constant retransmission probability in each slot. If each station transmits during a contention slot with probability 

p

, the probability 

A

 that some station acquires the channel in that slot is 

Equation 4 

 

209




 

A

 is maximized when 

p

 = 1

/k

, with 

A

 

1

/e

 as 

k

 

.

 The probability that the contention interval has exactly 

j

 slots in it is 

A

(1 - 

A

)

j

 

- 1

, so the mean number of slots per contention is given by 

 

 

Since each slot has a duration 2t, the mean contention interval, 

w

, is 2t

/A.

 Assuming optimal 

p

, the mean number of contention slots is never more than 

e

, so 

w

 is at most 2t

e

 

5.4t. 

If the mean frame takes 

P

 sec to transmit, when many stations have frames to send, 

Equation 4 

 

 

Here we see where the maximum cable distance between any two stations enters into the performance figures, giving rise to topologies other than that of 

Fig. 4-15(a)

. The longer the cable, the longer the contention interval. This observation is why the Ethernet standard specifies a maximum cable length. 

It is instructive to formulate 

Eq. (4-6)

 in terms of the frame length, 

F

, the network bandwidth, 

B

, the cable length, 

L

, and the speed of signal propagation, 

c

, for the optimal case of 

e

 contention slots per frame. With 

P

 = 

F/B

, 

Eq. (4-6)

 becomes 

Equation 4 

 

 

When the second term in the denominator is large, network efficiency will be low. More specifically, increasing network bandwidth or distance (the 

BL

 product) reduces efficiency for a given frame size. Unfortunately, much research on network hardware is aimed precisely at increasing this product. People want high bandwidth over long distances (fiber optic MANs, for example), which suggests that Ethernet implemented in this manner may not be the best system for these applications. We will see other ways of implementing Ethernet when we come to switched Ethernet later in this chapter. 

In 

Fig. 4-19

, the channel efficiency is plotted versus number of ready stations for 2t=51.2 µsec and a data rate of 10 Mbps, using 

Eq. (4-7)

. With a 64-byte slot time, it is not surprising that 64-byte frames are not efficient. On the other hand, with 1024-byte frames and an asymptotic value of 

e

 64-byte slots per contention interval, the contention period is 174 bytes long and the efficiency is 0.85. 

210




Figure 4-19. Efficiency of Ethernet at 10 Mbps with 512-bit slot times. 

 

To determine the mean number of stations ready to transmit under conditions of high load, we can use the following (crude) observation. Each frame ties up the channel for one contention period and one frame transmission time, for a total of 

P

 + 

w

 sec. The number of frames per second is therefore 1

/

(

P

 + 

w

)

.

 If each station generates frames at a mean rate of ? frames/sec, then when the system is in state 

k

, the total input rate of all unblocked stations combined is 

k

? frames/sec. Since in equilibrium the input and output rates must be identical, we can equate these two expressions and solve for 

k.

 (Notice that 

w

 is a function of 

k.

) A more sophisticated analysis is given in (Bertsekas and Gallager, 1992). 

It is probably worth mentioning that there has been a large amount of theoretical performance analysis of Ethernet (and other networks). Virtually all of this work has assumed that traffic is Poisson. As researchers have begun looking at real data, it now appears that network traffic is rarely Poisson, but self-similar (Paxson and Floyd, 1994; and Willinger et al., 1995). What this means is that averaging over long periods of time does not smooth out the traffic. The average number of frames in each minute of an hour has as much variance as the average number of frames in each second of a minute. The consequence of this discovery is that most models of network traffic do not apply to the real world and should be taken with a grain (or better yet, a metric ton) of salt. 

4.3.6 Switched Ethernet 

As more and more stations are added to an Ethernet, the traffic will go up. Eventually, the LAN will saturate. One way out is to go to a higher speed, say, from 10 Mbps to 100 Mbps. But with the growth of multimedia, even a 100-Mbps or 1-Gbps Ethernet can become saturated. 

Fortunately, there is an additional way to deal with increased load: switched Ethernet, as shown in 

Fig. 4-20

. The heart of this system is a 

switch

 containing a high-speed backplane and room for typically 4 to 32 plug-in line cards, each containing one to eight connectors. Most often, each connector has a 10Base-T twisted pair connection to a single host computer. 

Figure 4-20. A simple example of switched Ethernet. 

211




 

When a station wants to transmit an Ethernet frame, it outputs a standard frame to the switch. The plug-in card getting the frame may check to see if it is destined for one of the other stations connected to the same card. If so, the frame is copied there. If not, the frame is sent over the high-speed backplane to the destination station's card. The backplane typically runs at many Gbps, using a proprietary protocol. 

What happens if two machines attached to the same plug-in card transmit frames at the same time? It depends on how the card has been constructed. One possibility is for all the ports on the card to be wired together to form a local on-card LAN. Collisions on this on-card LAN will be detected and handled the same as any other collisions on a CSMA/CD network—with retransmissions using the binary exponential backoff algorithm. With this kind of plug-in card, only one transmission per card is possible at any instant, but all the cards can be transmitting in parallel. With this design, each card forms its own 

collision domain

, independent of the others. With only one station per collision domain, collisions are impossible and performance is improved. 

With the other kind of plug-in card, each input port is buffered, so incoming frames are stored in the card's on-board RAM as they arrive. This design allows all input ports to receive (and transmit) frames at the same time, for parallel, full-duplex operation, something not possible with CSMA/CD on a single channel. Once a frame has been completely received, the card can then check to see if the frame is destined for another port on the same card or for a distant port. In the former case, it can be transmitted directly to the destination. In the latter case, it must be transmitted over the backplane to the proper card. With this design, each port is a separate collision domain, so collisions do not occur. The total system throughput can often be increased by an order of magnitude over 10Base5, which has a single collision domain for the entire system. 

Since the switch just expects standard Ethernet frames on each input port, it is possible to use some of the ports as concentrators. In 

Fig. 4-20

, the port in the upper-right corner is connected not to a single station, but to a 12-port hub. As frames arrive at the hub, they contend for the ether in the usual way, including collisions and binary backoff. Successful frames make it to the switch and are treated there like any other incoming frames: they are switched to the correct output line over the high-speed backplane. Hubs are cheaper than switches, but due to falling switch prices, they are rapidly becoming obsolete. Nevertheless, legacy hubs still exist. 

4.3.7 Fast Ethernet 

At first, 10 Mbps seemed like heaven, just as 1200-bps modems seemed like heaven to the early users of 300-bps acoustic modems. But the novelty wore off quickly. As a kind of corollary to Parkinson's Law (''Work expands to fill the time available for its completion''), it seemed that data expanded to fill the bandwidth available for their transmission. To pump up the speed, various industry groups proposed two new ring-based optical LANs. One was called 

212




FDDI

 (

Fiber Distributed Data Interface

) and the other was called 

Fibre Channel

 

[

]

. To make a long story short, while both were used as backbone networks, neither one made the breakthrough to the desktop. In both cases, the station management was too complicated, which led to complex chips and high prices. The lesson that should have been learned here was KISS (Keep It Simple, Stupid). 

[

]

 It is called ''fibre channel'' and not ''fiber channel'' because the document editor was British. 

In any event, the failure of the optical LANs to catch fire left a gap for garden-variety Ethernet at speeds above 10 Mbps. Many installations needed more bandwidth and thus had numerous 10-Mbps LANs connected by a maze of repeaters, bridges, routers, and gateways, although to the network managers it sometimes felt that they were being held together by bubble gum and chicken wire. 

It was in this environment that IEEE reconvened the 802.3 committee in 1992 with instructions to come up with a faster LAN. One proposal was to keep 802.3 exactly as it was, but just make it go faster. Another proposal was to redo it totally to give it lots of new features, such as real-time traffic and digitized voice, but just keep the old name (for marketing reasons). After some wrangling, the committee decided to keep 802.3 the way it was, but just make it go faster. The people behind the losing proposal did what any computer-industry people would have done under these circumstances—they stomped off and formed their own committee and standardized their LAN anyway (eventually as 802.12). It flopped miserably. 

The 802.3 committee decided to go with a souped-up Ethernet for three primary reasons: 

1. The need to be backward compatible with existing Ethernet LANs. 

2. The fear that a new protocol might have unforeseen problems. 

3. The desire to get the job done before the technology changed. 

The work was done quickly (by standards committees' norms), and the result, 

802.3u

, was officially approved by IEEE in June 1995. Technically, 802.3u is not a new standard, but an addendum to the existing 802.3 standard (to emphasize its backward compatibility). Since practically everyone calls it 

fast Ethernet

, rather than 802.3u, we will do that, too. 

The basic idea behind fast Ethernet was simple: keep all the old frame formats, interfaces, and procedural rules, but just reduce the bit time from 100 nsec to 10 nsec. Technically, it would have been possible to copy either 10Base-5 or 10Base-2 and still detect collisions on time by just reducing the maximum cable length by a factor of ten. However, the advantages of 10Base-T wiring were so overwhelming that fast Ethernet is based entirely on this design. Thus, all fast Ethernet systems use hubs and switches; multidrop cables with vampire taps or BNC connectors are not permitted. 

Nevertheless, some choices still had to be made, the most important being which wire types to support. One contender was category 3 twisted pair. The argument for it was that practically every office in the Western world has at least four category 3 (or better) twisted pairs running from it to a telephone wiring closet within 100 meters. Sometimes two such cables exist. Thus, using category 3 twisted pair would make it possible to wire up desktop computers using fast Ethernet without having to rewire the building, an enormous advantage for many organizations. 

The main disadvantage of category 3 twisted pair is its inability to carry 200 megabaud signals (100 Mbps with Manchester encoding) 100 meters, the maximum computer-to-hub distance specified for 10Base-T (see 

Fig. 4-13

). In contrast, category 5 twisted pair wiring can handle 100 meters easily, and fiber can go much farther. The compromise chosen was to allow all three possibilities, as shown in 

Fig. 4-21

, but to pep up the category 3 solution to give it the additional carrying capacity needed. 

213




Figure 4-21. The original fast Ethernet cabling. 

 

The category 3 UTP scheme, called 

100Base-T4

, uses a signaling speed of 25 MHz, only 25 percent faster than standard Ethernet's 20 MHz (remember that Manchester encoding, as shown in 

Fig. 4-16

, requires two clock periods for each of the 10 million bits each second). However, to achieve the necessary bandwidth, 100Base-T4 requires four twisted pairs. Since standard telephone wiring for decades has had four twisted pairs per cable, most offices are able to handle this. Of course, it means giving up your office telephone, but that is surely a small price to pay for faster e-mail. 

Of the four twisted pairs, one is always to the hub, one is always from the hub, and the other two are switchable to the current transmission direction. To get the necessary bandwidth, Manchester encoding is not used, but with modern clocks and such short distances, it is no longer needed. In addition, ternary signals are sent, so that during a single clock period the wire can contain a 0, a 1, or a 2. With three twisted pairs going in the forward direction and ternary signaling, any one of 27 possible symbols can be transmitted, making it possible to send 4 bits with some redundancy. Transmitting 4 bits in each of the 25 million clock cycles per second gives the necessary 100 Mbps. In addition, there is always a 33.3-Mbps reverse channel using the remaining twisted pair. This scheme, known as 

8B/6T

 (8 bits map to 6 trits), is not likely to win any prizes for elegance, but it works with the existing wiring plant. 

For category 5 wiring, the design, 

100Base-TX

, is simpler because the wires can handle clock rates of 125 MHz. Only two twisted pairs per station are used, one to the hub and one from it. Straight binary coding is not used; instead a scheme called used

4B/5B

is It is taken from FDDI and compatible with it. Every group of five clock periods, each containing one of two signal values, yields 32 combinations. Sixteen of these combinations are used to transmit the four bit groups 0000, 0001, 0010, ..., 1111. Some of the remaining 16 are used for control purposes such as marking frames boundaries. The combinations used have been carefully chosen to provide enough transitions to maintain clock synchronization. The 100Base-TX system is full duplex; stations can transmit at 100 Mbps and receive at 100 Mbps at the same time. Often 100Base-TX and 100Base-T4 are collectively referred to as 

100Base-T

. 

The last option, 

100Base-FX

, uses two strands of multimode fiber, one for each direction, so it, too, is full duplex with 100 Mbps in each direction. In addition, the distance between a station and the hub can be up to 2 km. 

In response to popular demand, in 1997 the 802 committee added a new cabling type, 100Base-T2, allowing fast Ethernet to run over two pairs of existing category 3 wiring. However, a sophisticated digital signal processor is needed to handle the encoding scheme required, making this option fairly expensive. So far, it is rarely used due to its complexity, cost, and the fact that many office buildings have already been rewired with category 5 UTP. 

Two kinds of interconnection devices are possible with 100Base-T: hubs and switches, as shown in 

Fig. 4-20

. In a hub, all the incoming lines (or at least all the lines arriving at one plug-in card) are logically connected, forming a single collision domain. All the standard rules, including the binary exponential backoff algorithm, apply, so the system works just like old-fashioned Ethernet. In particular, only one station at a time can be transmitting. In other words, hubs require half-duplex communication. 

In a switch, each incoming frame is buffered on a plug-in line card and passed over a high-speed backplane from the source card to the destination card if need be. The backplane has 

214




not been standardized, nor does it need to be, since it is entirely hidden deep inside the switch. If past experience is any guide, switch vendors will compete vigorously to produce ever faster backplanes in order to improve system throughput. Because 100Base-FX cables are too long for the normal Ethernet collision algorithm, they must be connected to switches, so each one is a collision domain unto itself. Hubs are not permitted with 100Base-FX. 

As a final note, virtually all switches can handle a mix of 10-Mbps and 100-Mbps stations, to make upgrading easier. As a site acquires more and more 100-Mbps workstations, all it has to do is buy the necessary number of new line cards and insert them into the switch. In fact, the standard itself provides a way for two stations to automatically negotiate the optimum speed (10 or 100 Mbps) and duplexity (half or full). Most fast Ethernet products use this feature to autoconfigure themselves. 

4.3.8 Gigabit Ethernet 

The ink was barely dry on the fast Ethernet standard when the 802 committee began working on a yet faster Ethernet (1995). It was quickly dubbed 

gigabit Ethernet

 and was ratified by IEEE in 1998 under the name 802.3z. This identifier suggests that gigabit Ethernet is going to be the end of the line unless somebody quickly invents a new letter after z. Below we will discuss some of the key features of gigabit Ethernet. More information can be found in (Seifert, 1998). 

The 802.3z committee's goals were essentially the same as the 802.3u committee's goals: make Ethernet go 10 times faster yet remain backward compatible with all existing Ethernet standards. In particular, gigabit Ethernet had to offer unacknowledged datagram service with both unicast and multicast, use the same 48-bit addressing scheme already in use, and maintain the same frame format, including the minimum and maximum frame sizes. The final standard met all these goals. 

All configurations of gigabit Ethernet are point-to-point rather than multidrop as in the original 10 Mbps standard, now honored as 

classic Ethernet

. In the simplest gigabit Ethernet configuration, illustrated in 

Fig. 4-22(a)

, two computers are directly connected to each other. The more common case, however, is having a switch or a hub connected to multiple computers and possibly additional switches or hubs, as shown in 

Fig. 4-22(b)

. In both configurations each individual Ethernet cable has exactly two devices on it, no more and no fewer. 

Figure 4-22. (a) A two-station Ethernet. (b) A multistation Ethernet. 

 

Gigabit Ethernet supports two different modes of operation: full-duplex mode and half-duplex mode. The ''normal'' mode is full-duplex mode, which allows traffic in both directions at the same time. This mode is used when there is a central switch connected to computers (or other switches) on the periphery. In this configuration, all lines are buffered so each computer and switch is free to send frames whenever it wants to. The sender does not have to sense the channel to see if anybody else is using it because contention is impossible. On the line between 

215




a computer and a switch, the computer is the only possible sender on that line to the switch and the transmission succeeds even if the switch is currently sending a frame to the computer (because the line is full duplex). Since no contention is possible, the CSMA/CD protocol is not used, so the maximum length of the cable is determined by signal strength issues rather than by how long it takes for a noise burst to propagate back to the sender in the worst case. Switches are free to mix and match speeds. Autoconfiguration is supported just as in fast Ethernet. 

The other mode of operation, half-duplex, is used when the computers are connected to a hub rather than a switch. A hub does not buffer incoming frames. Instead, it electrically connects all the lines internally, simulating the multidrop cable used in classic Ethernet. In this mode, collisions are possible, so the standard CSMA/CD protocol is required. Because a minimum (i.e., 64-byte) frame can now be transmitted 100 times faster than in classic Ethernet, the maximum distance is 100 times less, or 25 meters, to maintain the essential property that the sender is still transmitting when the noise burst gets back to it, even in the worst case. With a 2500-meter-long cable, the sender of a 64-byte frame at 1 Gbps would be long done before the frame got even a tenth of the way to the other end, let alone to the end and back. 

The 802.3z committee considered a radius of 25 meters to be unacceptable and added two features to the standard to increase the radius. The first feature, called 

carrier extension

, essentially tells the hardware to add its own padding after the normal frame to extend the frame to 512 bytes. Since this padding is added by the sending hardware and removed by the receiving hardware, the software is unaware of it, meaning that no changes are needed to existing software. Of course, using 512 bytes worth of bandwidth to transmit 46 bytes of user data (the payload of a 64-byte frame) has a line efficiency of 9%. 

The second feature, called 

frame bursting

, allows a sender to transmit a concatenated sequence of multiple frames in a single transmission. If the total burst is less than 512 bytes, the hardware pads it again. If enough frames are waiting for transmission, this scheme is highly efficient and preferred over carrier extension. These new features extend the radius of the network to 200 meters, which is probably enough for most offices. 

In all fairness, it is hard to imagine an organization going to the trouble of buying and installing gigabit Ethernet cards to get high performance and then connecting the computers with a hub to simulate classic Ethernet with all its collisions. While hubs are somewhat cheaper than switches, gigabit Ethernet interface cards are still relatively expensive. To then economize by buying a cheap hub and slash the performance of the new system is foolish. Still, backward compatibility is sacred in the computer industry, so the 802.3z committee was required to put it in. 

Gigabit Ethernet supports both copper and fiber cabling, as listed in 

Fig. 4-23

. Signaling at or near 1 Gbps over fiber means that the light source has to be turned on and off in under 1 nsec. LEDs simply cannot operate this fast, so lasers are required. Two wavelengths are permitted: 0.85 microns (Short) and 1.3 microns (Long). Lasers at 0.85 microns are cheaper but do not work on single-mode fiber. 

Figure 4-23. Gigabit Ethernet cabling. 

 

Three fiber diameters are permitted: 10, 50, and 62.5 microns. The first is for single mode and the last two are for multimode. Not all six combinations are allowed, however, and the 

216




maximum distance depends on the combination used. The numbers given in 

Fig. 4-23

 are for the best case. In particular, 5000 meters is only achievable with 1.3 micron lasers operating over 10 micron fiber in single mode, but this is the best choice for campus backbones and is expected to be popular, despite its being the most expensive choice. 

The 1000Base-CX option uses short shielded copper cables. Its problem is that it is competing with high-performance fiber from above and cheap UTP from below. It is unlikely to be used much, if at all. 

The last option is bundles of four category 5 UTP wires working together. Because so much of this wiring is already installed, it is likely to be the poor man's gigabit Ethernet. 

Gigabit Ethernet uses new encoding rules on the fibers. Manchester encoding at 1 Gbps would require a 2 Gbaud signal, which was considered too difficult and also too wasteful of bandwidth. Instead a new scheme, called 

8B/10B

, was chosen, based on fibre channel. Each 8-bit byte is encoded on the fiber as 10 bits, hence the name 8B/10B. Since there are 1024 possible output codewords for each input byte, some leeway was available in choosing which codewords to allow. The following two rules were used in making the choices: 

1. No codeword may have more than four identical bits in a row. 

2. No codeword may have more than six 0s or six 1s. 

These choices were made to keep enough transitions in the stream to make sure the receiver stays in sync with the sender and also to keep the number of 0s and 1s on the fiber as close to equal as possible. In addition, many input bytes have two possible codewords assigned to them. When the encoder has a choice of codewords, it always chooses the codeword that moves in the direction of equalizing the number of 0s and 1s transmitted so far. This emphasis of balancing 0s and 1s is needed to keep the DC component of the signal as low as possible to allow it to pass through transformers unmodified. While computer scientists are not fond of having the properties of transformers dictate their coding schemes, life is like that sometimes. 

Gigabit Ethernets using 1000Base-T use a different encoding scheme since clocking data onto copper wire in 1 nsec is too difficult. This solution uses four category 5 twisted pairs to allow four symbols to be transmitted in parallel. Each symbol is encoded using one of five voltage levels. This scheme allows a single symbol to encode 00, 01, 10, 11, or a special value for control purposes. Thus, there are 2 data bits per twisted pair or 8 data bits per clock cycle. The clock runs at 125 MHz, allowing 1-Gbps operation. The reason for allowing five voltage levels instead of four is to have combinations left over for framing and control purposes. 

A speed of 1 Gbps is quite fast. For example, if a receiver is busy with some other task for even 1 msec and does not empty the input buffer on some line, up to 1953 frames may have accumulated there in that 1 ms gap. Also, when a computer on a gigabit Ethernet is shipping data down the line to a computer on a classic Ethernet, buffer overruns are very likely. As a consequence of these two observations, gigabit Ethernet supports flow control (as does fast Ethernet, although the two are different). 

The flow control consists of one end sending a special control frame to the other end telling it to pause for some period of time. Control frames are normal Ethernet frames containing a type of 0x8808. The first two bytes of the data field give the command; succeeding bytes provide the parameters, if any. For flow control, PAUSE frames are used, with the parameter telling how long to pause, in units of the minimum frame time. For gigabit Ethernet, the time unit is 512 nsec, allowing for pauses as long as 33.6 msec. 

As soon as gigabit Ethernet was standardized, the 802 committee got bored and wanted to get back to work. IEEE told them to start on 10-gigabit Ethernet. After searching hard for a letter to follow z, they abandoned that approach and went over to two-letter suffixes. They got to 

217




work and that standard was approved by IEEE in 2002 as 802.3ae. Can 100-gigabit Ethernet be far behind? 

4.3.9 IEEE 802.2: Logical Link Control 

It is now perhaps time to step back and compare what we have learned in this chapter with what we studied in the previous one. In 

Chap. 3

, we saw how two machines could communicate reliably over an unreliable line by using various data link protocols. These protocols provided error control (using acknowledgements) and flow control (using a sliding window). 

In contrast, in this chapter, we have not said a word about reliable communication. All that Ethernet and the other 802 protocols offer is a best-efforts datagram service. Sometimes, this service is adequate. For example, for transporting IP packets, no guarantees are required or even expected. An IP packet can just be inserted into an 802 payload field and sent on its way. If it gets lost, so be it. 

Nevertheless, there are also systems in which an error-controlled, flow-controlled data link protocol is desired. IEEE has defined one that can run on top of Ethernet and the other 802 protocols. In addition, this protocol, called 

LLC

 (

Logical Link Control

), hides the differences between the various kinds of 802 networks by providing a single format and interface to the network layer. This format, interface, and protocol are all closely based on the HDLC protocol we studied in 

Chap. 3

. LLC forms the upper half of the data link layer, with the MAC sublayer below it, as shown in 

Fig. 4-24

. 

Figure 4-24. (a) Position of LLC. (b) Protocol formats. 

 

Typical usage of LLC is as follows. The network layer on the sending machine passes a packet to LLC, using the LLC access primitives. The LLC sublayer then adds an LLC header, containing sequence and acknowledgement numbers. The resulting structure is then inserted into the payload field of an 802 frame and transmitted. At the receiver, the reverse process takes place. 

LLC provides three service options: unreliable datagram service, acknowledged datagram service, and reliable connection-oriented service. The LLC header contains three fields: a destination access point, a source access point, and a control field. The access points tell which process the frame came from and where it is to be delivered, replacing the DIX 

Type

 field. The control field contains sequence and acknowledgement numbers, very much in the style of HDLC (see 

Fig. 3-24

), but not identical to it. These fields are primarily used when a reliable connection is needed at the data link level, in which case protocols similar to the ones discussed in 

Chap. 3

 would be used. For the Internet, best-efforts attempts to deliver IP packets is sufficient, so no acknowledgements at the LLC level are required. 

218




4.3.10 Retrospective on Ethernet 

Ethernet has been around for over 20 years and has no serious competitors in sight, so it is likely to be around for many years to come. Few CPU architectures, operating systems, or programming languages have been king of the mountain for two decades going on three. Clearly, Ethernet did something right. What? 

Probably the main reason for its longevity is that Ethernet is simple and flexible. In practice, simple translates into reliable, cheap, and easy to maintain. Once the vampire taps were replaced by BNC connectors, failures became extremely rare. People hesitate to replace something that works perfectly all the time, especially when they know that an awful lot of things in the computer industry work very poorly, so that many so-called ''upgrades'' are appreciably worse than what they replaced. 

Simple also translates into cheap. Thin Ethernet and twisted pair wiring is relatively inexpensive. The interface cards are also low cost. Only when hubs and switches were introduced were substantial investments required, but by the time they were in the picture, Ethernet was already well established. 

Ethernet is easy to maintain. There is no software to install (other than the drivers) and there are no configuration tables to manage (and get wrong). Also, adding new hosts is as simple as just plugging them in. 

Another point is that Ethernet interworks easily with TCP/IP, which has become dominant. IP is a connectionless protocol, so it fits perfectly with Ethernet, which is also connectionless. IP fits much less well with ATM, which is connection oriented. This mismatch definitely hurt ATM's chances. 

Lastly, Ethernet has been able to evolve in certain crucial ways. Speeds have gone up by several orders of magnitude and hubs and switches have been introduced, but these changes have not required changing the software. When a network salesman shows up at a large installation and says: ''I have this fantastic new network for you. All you have to do is throw out all your hardware and rewrite all your software,'' he has a problem. FDDI, Fibre Channel, and ATM were all faster than Ethernet when introduced, but they were incompatible with Ethernet, far more complex, and harder to manage. Eventually, Ethernet caught up with them in terms of speed, so they had no advantages left and quietly died off except for ATM's use deep within the core of the telephone system. 

4.4 Wireless LANs 

Although Ethernet is widely used, it is about to get some competition. Wireless LANs are increasingly popular, and more and more office buildings, airports, and other public places are being outfitted with them. Wireless LANs can operate in one of two configurations, as we saw in 

Fig. 1-35

: with a base station and without a base station. Consequently, the 802.11 LAN standard takes this into account and makes provision for both arrangements, as we will see shortly. 

We gave some background information on 802.11 in 

Sec. 1.5.4

. Now is the time to take a closer look at the technology. In the following sections we will look at the protocol stack, physical layer radio transmission techniques, MAC sublayer protocol, frame structure, and services. For more information about 802.11, see (Crow et al., 1997; Geier, 2002; Heegard et al., 2001; Kapp, 2002; O'Hara and Petrick, 1999; and Severance, 1999). To hear the truth from the mouth of the horse, consult the published 802.11 standard itself. 

219




4.4.1 The 802.11 Protocol Stack 

The protocols used by all the 802 variants, including Ethernet, have a certain commonality of structure. A partial view of the 802.11 protocol stack is given in 

Fig. 4-25

. The physical layer corresponds to the OSI physical layer fairly well, but the data link layer in all the 802 protocols is split into two or more sublayers. In 802.11, the MAC (Medium Access Control) sublayer determines how the channel is allocated, that is, who gets to transmit next. Above it is the LLC (Logical Link Control) sublayer, whose job it is to hide the differences between the different 802 variants and make them indistinguishable as far as the network layer is concerned. We studied the LLC when examining Ethernet earlier in this chapter and will not repeat that material here. 

Figure 4-25. Part of the 802.11 protocol stack. 

 

The 1997 802.11 standard specifies three transmission techniques allowed in the physical layer. The infrared method uses much the same technology as television remote controls do. The other two use short-range radio, using techniques called FHSS and DSSS. Both of these use a part of the spectrum that does not require licensing (the 2.4-GHz ISM band). Radio-controlled garage door openers also use this piece of the spectrum, so your notebook computer may find itself in competition with your garage door. Cordless telephones and microwave ovens also use this band. All of these techniques operate at 1 or 2 Mbps and at low enough power that they do not conflict too much. In 1999, two new techniques were introduced to achieve higher bandwidth. These are called OFDM and HR-DSSS. They operate at up to 54 Mbps and 11 Mbps, respectively. In 2001, a second OFDM modulation was introduced, but in a different frequency band from the first one. Now we will examine each of them briefly. Technically, these belong to the physical layer and should have been examined in 

Chapter 2

, but since they are so closely tied to LANs in general and the 802.11 MAC sublayer, we treat them here instead. 

4.4.2 The 802.11 Physical Layer 

Each of the five permitted transmission techniques makes it possible to send a MAC frame from one station to another. They differ, however, in the technology used and speeds achievable. A detailed discussion of these technologies is far beyond the scope of this book, but a few words on each one, along with some of the key words, may provide interested readers with terms to search for on the Internet or elsewhere for more information. 

The infrared option uses diffused (i.e., not line of sight) transmission at 0.85 or 0.95 microns. Two speeds are permitted: 1 Mbps and 2 Mbps. At 1 Mbps, an encoding scheme is used in which a group of 4 bits is encoded as a 16-bit codeword containing fifteen 0s and a single 1, 

220




using what is called 

Gray code

. This code has the property that a small error in time synchronization leads to only a single bit error in the output. At 2 Mbps, the encoding takes 2 bits and produces a 4-bit codeword, also with only a single 1, that is one of 0001, 0010, 0100, or 1000. Infrared signals cannot penetrate walls, so cells in different rooms are well isolated from each other. Nevertheless, due to the low bandwidth (and the fact that sunlight swamps infrared signals), this is not a popular option. 

FHSS

 (

Frequency Hopping Spread Spectrum

) uses 79 channels, each 1-MHz wide, starting at the low end of the 2.4-GHz ISM band. A pseudorandom number generator is used to produce the sequence of frequencies hopped to. As long as all stations use the same seed to the pseudorandom number generator and stay synchronized in time, they will hop to the same frequencies simultaneously. The amount of time spent at each frequency, the 

dwell time

, is an adjustable parameter, but must be less than 400 msec. FHSS' randomization provides a fair way to allocate spectrum in the unregulated ISM band. It also provides a modicum of security since an intruder who does not know the hopping sequence or dwell time cannot eavesdrop on transmissions. Over longer distances, multipath fading can be an issue, and FHSS offers good resistance to it. It is also relatively insensitive to radio interference, which makes it popular for building-to-building links. Its main disadvantage is its low bandwidth. 

The third modulation method, 

DSSS

 (

Direct Sequence Spread Spectrum

), is also restricted to 1 or 2 Mbps. The scheme used has some similarities to the CDMA system we examined in 

Sec. 2.6.2

, but differs in other ways. Each bit is transmitted as 11 chips, using what is called a 

Barker sequence

. It uses phase shift modulation at 1 Mbaud, transmitting 1 bit per baud when operating at 1 Mbps and 2 bits per baud when operating at 2 Mbps. For years, the FCC required all wireless communications equipment operating in the ISM bands in the U.S. to use spread spectrum, but in May 2002, that rule was dropped as new technologies emerged. 

The first of the high-speed wireless LANs, 

802.11a

, uses 

OFDM

 (

Orthogonal Frequency Division Multiplexing

) to deliver up to 54 Mbps in the wider 5-GHz ISM band. As the term FDM suggests, different frequencies are used—52 of them, 48 for data and 4 for synchronization—not unlike ADSL. Since transmissions are present on multiple frequencies at the same time, this technique is considered a form of spread spectrum, but different from both CDMA and FHSS. Splitting the signal into many narrow bands has some key advantages over using a single wide band, including better immunity to narrowband interference and the possibility of using noncontiguous bands. A complex encoding system is used, based on phase-shift modulation for speeds up to 18 Mbps and on QAM above that. At 54 Mbps, 216 data bits are encoded into 288-bit symbols. Part of the motivation for OFDM is compatibility with the European HiperLAN/2 system (Doufexi et al., 2002). The technique has a good spectrum efficiency in terms of bits/Hz and good immunity to multipath fading. 

Next, we come to 

HR-DSSS

 (

High Rate Direct Sequence Spread Spectrum

), another spread spectrum technique, which uses 11 million chips/sec to achieve 11 Mbps in the 2.4-GHz band. It is called 

802.11b

 but is not a follow-up to 802.11a. In fact, its standard was approved first and it got to market first. Data rates supported by 802.11b are 1, 2, 5.5, and 11 Mbps. The two slow rates run at 1 Mbaud, with 1 and 2 bits per baud, respectively, using phase shift modulation (for compatibility with DSSS). The two faster rates run at 1.375 Mbaud, with 4 and 8 bits per baud, respectively, using 

Walsh/Hadamard

 codes. The data rate may be dynamically adapted during operation to achieve the optimum speed possible under current conditions of load and noise. In practice, the operating speed of 802.11b is nearly always 11 Mbps. Although 802.11b is slower than 802.11a, its range is about 7 times greater, which is more important in many situations. 

An enhanced version of 802.11b, 

802.11g

, was approved by IEEE in November 2001 after much politicking about whose patented technology it would use. It uses the OFDM modulation method of 802.11a but operates in the narrow 2.4-GHz ISM band along with 802.11b. In theory it can operate at up to 54 MBps. It is not yet clear whether this speed will be realized in practice. What it does mean is that the 802.11 committee has produced three different high-speed wireless LANs: 802.11a, 802.11b, and 802.11g (not to mention three low-speed 

221




wireless LANs). One can legitimately ask if this is a good thing for a standards committee to do. Maybe three was their lucky number. 

4.4.3 The 802.11 MAC Sublayer Protocol 

Let us now return from the land of electrical engineering to the land of computer science. The 802.11 MAC sublayer protocol is quite different from that of Ethernet due to the inherent complexity of the wireless environment compared to that of a wired system. With Ethernet, a station just waits until the ether goes silent and starts transmitting. If it does not receive a noise burst back within the first 64 bytes, the frame has almost assuredly been delivered correctly. With wireless, this situation does not hold. 

To start with, there is the hidden station problem mentioned earlier and illustrated again in 

Fig. 

4-26(a)

. Since not all stations are within radio range of each other, transmissions going on in one part of a cell may not be received elsewhere in the same cell. In this example, station 

C

 is transmitting to station 

B

. If 

A

 senses the channel, it will not hear anything and falsely conclude that it may now start transmitting to 

B

. 

Figure 4-26. (a) The hidden station problem. (b) The exposed station problem. 

 

In addition, there is the inverse problem, the exposed station problem, illustrated in 

Fig. 4-

26(b)

. Here 

B

 wants to send to 

C

 so it listens to the channel. When it hears a transmission, it falsely concludes that it may not send to 

C

, even though 

A

 may be transmitting to 

D

 (not shown). In addition, most radios are half duplex, meaning that they cannot transmit and listen for noise bursts at the same time on a single frequency. As a result of these problems, 802.11 does not use CSMA/CD, as Ethernet does. 

To deal with this problem, 802.11 supports two modes of operation. The first, called 

DCF

 (

Distributed Coordination Function

), does not use any kind of central control (in that respect, similar to Ethernet). The other, called 

PCF

 (

Point Coordination Function

), uses the base station to control all activity in its cell. All implementations must support DCF but PCF is optional. We will now discuss these two modes in turn. 

When DCF is employed, 802.11 uses a protocol called 

CSMA/CA

 (

CSMA with Collision Avoidance

). In this protocol, both physical channel sensing and virtual channel sensing are used. Two methods of operation are supported by CSMA/CA. In the first method, when a station wants to transmit, it senses the channel. If it is idle, it just starts transmitting. It does not sense the channel while transmitting but emits its entire frame, which may well be destroyed at the receiver due to interference there. If the channel is busy, the sender defers until it goes idle and then starts transmitting. If a collision occurs, the colliding stations wait a 

222




random time, using the Ethernet binary exponential backoff algorithm, and then try again later. 

The other mode of CSMA/CA operation is based on MACAW and uses virtual channel sensing, as illustrated in 

Fig. 4-27

. In this example, 

A

 wants to send to 

B

. C is a station within range of 

A

 (and possibly within range of 

B

, but that does not matter). 

D

 is a station within range of 

B

 but not within range of 

A

. 

Figure 4-27. The use of virtual channel sensing using CSMA/CA. 

 

The protocol starts when 

A

 decides it wants to send data to 

B

. It begins by sending an RTS frame to 

B

 to request permission to send it a frame. When 

B

 receives this request, it may decide to grant permission, in which case it sends a CTS frame back. Upon receipt of the CTS, 

A

 now sends its frame and starts an ACK timer. Upon correct receipt of the data frame, 

B

 responds with an ACK frame, terminating the exchange. If 

A

's ACK timer expires before the ACK gets back to it, the whole protocol is run again. 

Now let us consider this exchange from the viewpoints of 

C

 and 

D

. 

C

 is within range of 

A

, so it may receive the RTS frame. If it does, it realizes that someone is going to send data soon, so for the good of all it desists from transmitting anything until the exchange is completed. From the information provided in the RTS request, it can estimate how long the sequence will take, including the final ACK, so it asserts a kind of virtual channel busy for itself, indicated by 

NAV

 (

Network Allocation Vector

) in 

Fig. 4-27

. 

D

 does not hear the RTS, but it does hear the CTS, so it also asserts the 

NAV

 signal for itself. Note that the 

NAV

 signals are not transmitted; they are just internal reminders to keep quiet for a certain period of time. 

In contrast to wired networks, wireless networks are noisy and unreliable, in no small part due to microwave ovens, which also use the unlicensed ISM bands. As a consequence, the probability of a frame making it through successfully decreases with frame length. If the probability of any bit being in error is 

p

, then the probability of an 

n

-bit frame being received entirely correctly is (1 - 

p

)

n

. For example, for 

p

 = 10

-4

, the probability of receiving a full Ethernet frame (12,144 bits) correctly is less than 30%. If 

p

 = 10

-5

, about one frame in 9 will be damaged. Even if 

p

 = 10

-6

, over 1% of the frames will be damaged, which amounts to almost a dozen per second, and more if frames shorter than the maximum are used. In summary, if a frame is too long, it has very little chance of getting through undamaged and will probably have to be retransmitted. 

To deal with the problem of noisy channels, 802.11 allows frames to be fragmented into smaller pieces, each with its own checksum. The fragments are individually numbered and acknowledged using a stop-and-wait protocol (i.e., the sender may not transmit fragment 

k

 + 1 until it has received the acknowledgment for fragment 

k

). Once the channel has been acquired using RTS and CTS, multiple fragments can be sent in a row, as shown in 

Fig. 4-28

. sequence of fragments is called a 

fragment burst

. 

Figure 4-28. A fragment burst. 

223




 

Fragmentation increases the throughput by restricting retransmissions to the bad fragments rather than the entire frame. The fragment size is not fixed by the standard but is a parameter of each cell and can be adjusted by the base station. The NAV mechanism keeps other stations quiet only until the next acknowledgement, but another mechanism (described below) is used to allow a whole fragment burst to be sent without interference. 

All of the above discussion applies to the 802.11 DCF mode. In this mode, there is no central control, and stations compete for air time, just as they do with Ethernet. The other allowed mode is PCF, in which the base station polls the other stations, asking them if they have any frames to send. Since transmission order is completely controlled by the base station in PCF mode, no collisions ever occur. The standard prescribes the mechanism for polling, but not the polling frequency, polling order, or even whether all stations need to get equal service. 

The basic mechanism is for the base station to broadcast a 

beacon frame

 periodically (10 to 100 times per second). The beacon frame contains system parameters, such as hopping sequences and dwell times (for FHSS), clock synchronization, etc. It also invites new stations to sign up for polling service. Once a station has signed up for polling service at a certain rate, it is effectively guaranteed a certain fraction of the bandwidth, thus making it possible to give quality-of-service guarantees. 

Battery life is always an issue with mobile wireless devices, so 802.11 pays attention to the issue of power management. In particular, the base station can direct a mobile station to go into sleep state until explicitly awakened by the base station or the user. Having told a station to go to sleep, however, means that the base station has the responsibility for buffering any frames directed at it while the mobile station is asleep. These can be collected later. 

PCF and DCF can coexist within one cell. At first it might seem impossible to have central control and distributed control operating at the same time, but 802.11 provides a way to achieve this goal. It works by carefully defining the interframe time interval. After a frame has been sent, a certain amount of dead time is required before any station may send a frame. Four different intervals are defined, each for a specific purpose. The four intervals are depicted in 

Fig. 4-29

. 

Figure 4-29. Interframe spacing in 802.11 

224




 

The shortest interval is 

SIFS

 (

Short InterFrame Spacing

). It is used to allow the parties in a single dialog the chance to go first. This includes letting the receiver send a CTS to respond to an RTS, letting the receiver send an ACK for a fragment or full data frame, and letting the sender of a fragment burst transmit the next fragment without having to send an RTS again. 

There is always exactly one station that is entitled to respond after a SIFS interval. If it fails to make use of its chance and a time 

PIFS

 (

PCF InterFrame Spacing

) elapses, the base station may send a beacon frame or poll frame. This mechanism allows a station sending a data frame or fragment sequence to finish its frame without anyone else getting in the way, but gives the base station a chance to grab the channel when the previous sender is done without having to compete with eager users. 

If the base station has nothing to say and a time 

DIFS

 (

DCF InterFrame Spacing

) elapses, any station may attempt to acquire the channel to send a new frame. The usual contention rules apply, and binary exponential backoff may be needed if a collision occurs. 

The last time interval, 

EIFS

 (

Extended InterFrame Spacing

), is used only by a station that has just received a bad or unknown frame to report the bad frame. The idea of giving this event the lowest priority is that since the receiver may have no idea of what is going on, it should wait a substantial time to avoid interfering with an ongoing dialog between two stations. 

4.4.4 The 802.11 Frame Structure 

The 802.11 standard defines three different classes of frames on the wire: data, control, and management. Each of these has a header with a variety of fields used within the MAC sublayer. In addition, there are some headers used by the physical layer but these mostly deal with the modulation techniques used, so we will not discuss them here. 

The format of the data frame is shown in 

Fig. 4-30

. First comes the 

Frame Control

 field. It itself has 11 subfields. The first of these is the 

Protocol version

, which allows two versions of the protocol to operate at the same time in the same cell. Then come the 

Type

 (data, control, or management) and 

Subtype

 fields (e.g., RTS or CTS). The 

To DS

 and 

From DS

 bits indicate the frame is going to or coming from the intercell distribution system (e.g., Ethernet). The 

MF

 bit means that more fragments will follow. The 

Retry

 bit marks a retransmission of a frame sent earlier. The 

Power management

 bit is used by the base station to put the receiver into sleep state or take it out of sleep state. The 

More

 bit indicates that the sender has additional frames for the receiver. The 

W

 bit specifies that the frame body has been encrypted using the 

WEP

 (

Wired Equivalent Privacy

) algorithm. Finally, the 

O

 bit tells the receiver that a sequence of frames with this bit on must be processed strictly in order. 

Figure 4-30. The 802.11 data frame. 

225




 

The second field of the data frame, the 

Duration

 field, tells how long the frame and its acknowledgement will occupy the channel. This field is also present in the control frames and is how other stations manage the NAV mechanism. The frame header contains four addresses, all in standard IEEE 802 format. The source and destination are obviously needed, but what are the other two for? Remember that frames may enter or leave a cell via a base station. The other two addresses are used for the source and destination base stations for intercell traffic. 

The 

Sequence

 field allows fragments to be numbered. Of the 16 bits available, 12 identify the frame and 4 identify the fragment. The 

Data

 field contains the payload, up to 2312 bytes, followed by the usual 

Checksum

. 

Management frames have a format similar to that of data frames, except without one of the base station addresses, because management frames are restricted to a single cell. Control frames are shorter still, having only one or two addresses, no 

Data

 field, and no 

Sequence

 field. The key information here is in the 

Subtype

 field, usually RTS, CTS, or ACK. 

4.4.5 Services 

The 802.11 standard states that each conformant wireless LAN must provide nine services. These services are divided into two categories: five distribution services and four station services. The distribution services relate to managing cell membership and interacting with stations outside the cell. In contrast, the station services relate to activity within a single cell. 

The five distribution services are provided by the base stations and deal with station mobility as they enter and leave cells, attaching themselves to and detaching themselves from base stations. They are as follows. 

1. Association. This service is used by mobile stations to connect themselves to base stations. Typically, it is used just after a station moves within the radio range of the base station. Upon arrival, it announces its identity and capabilities. The capabilities include the data rates supported, need for PCF services (i.e., polling), and power management requirements. The base station may accept or reject the mobile station. If the mobile station is accepted, it must then authenticate itself. 

2. Disassociation. Either the station or the base station may disassociate, thus breaking the relationship. A station should use this service before shutting down or leaving, but the base station may also use it before going down for maintenance. 

3. Reassociation. A station may change its preferred base station using this service. This facility is useful for mobile stations moving from one cell to another. If it is used correctly, no data will be lost as a consequence of the handover. (But 802.11, like Ethernet, is just a best-efforts service.) 

4. Distribution. This service determines how to route frames sent to the base station. If the destination is local to the base station, the frames can be sent out directly over the air. Otherwise, they will have to be forwarded over the wired network. 

5. Integration. If a frame needs to be sent through a non-802.11 network with a different addressing scheme or frame format, this service handles the translation from the 802.11 format to the format required by the destination network. 

226




The remaining four services are intracell (i.e., relate to actions within a single cell). They are used after association has taken place and are as follows. 

1. Authentication. Because wireless communication can easily be sent or received by unauthorized stations, a station must authenticate itself before it is permitted to send data. After a mobile station has been associated by the base station (i.e., accepted into its cell), the base station sends a special challenge frame to it to see if the mobile station knows the secret key (password) that has been assigned to it. It proves its knowledge of the secret key by encrypting the challenge frame and sending it back to the base station. If the result is correct, the mobile is fully enrolled in the cell. In the initial standard, the base station does not have to prove its identity to the mobile station, but work to repair this defect in the standard is underway. 

2. Deauthentication. When a previously authenticated station wants to leave the network, it is deauthenticated. After deauthentication, it may no longer use the network. 

3. Privacy. For information sent over a wireless LAN to be kept confidential, it must be encrypted. This service manages the encryption and decryption. The encryption algorithm specified is RC4, invented by Ronald Rivest of M.I.T. 

4. Data delivery. Finally, data transmission is what it is all about, so 802.11 naturally provides a way to transmit and receive data. Since 802.11 is modeled on Ethernet and transmission over Ethernet is not guaranteed to be 100% reliable, transmission over 802.11 is not guaranteed to be reliable either. Higher layers must deal with detecting and correcting errors. 

An 802.11 cell has some parameters that can be inspected and, in some cases, adjusted. They relate to encryption, timeout intervals, data rates, beacon frequency, and so on. 

Wireless LANs based on 802.11 are starting to be deployed in office buildings, airports, hotels, restaurants, and campuses around the world. Rapid growth is expected. For some experience about the widespread deployment of 802.11 at CMU, see (Hills, 2001). 

 

4.5 Broadband Wireless 

We have been indoors too long. Let us now go outside and see if any interesting networking is going on there. It turns out that quite a bit is going on there, and some of it has to do with the so-called last mile. With the deregulation of the telephone system in many countries, competitors to the entrenched telephone company are now often allowed to offer local voice and high-speed Internet service. There is certainly plenty of demand. The problem is that running fiber, coax, or even category 5 twisted pair to millions of homes and businesses is prohibitively expensive. What is a competitor to do? 

The answer is broadband wireless. Erecting a big antenna on a hill just outside of town and installing antennas directed at it on customers' roofs is much easier and cheaper than digging trenches and stringing cables. Thus, competing telecommunication companies have a great interest in providing a multimegabit wireless communication service for voice, Internet, movies on demand, etc. As we saw in 

Fig. 2-30

, LMDS was invented for this purpose. However, until recently, every carrier devised its own system. This lack of standards meant that hardware and software could not be mass produced, which kept prices high and acceptance low. 

Many people in the industry realized that having a broadband wireless standard was the key element missing, so IEEE was asked to form a committee composed of people from key companies and academia to draw up the standard. The next number available in the 802 numbering space was 

802.16

, so the standard got this number. Work was started in July 1999, and the final standard was approved in April 2002. Officially the standard is called ''Air Interface for Fixed Broadband Wireless Access Systems.'' However, some people prefer to call 

227




it a 

wireless MAN (Metropolitan Area Network)

 or a 

wireless local loop

. We regard all these terms as interchangeable. 

Like some of the other 802 standards, 802.16 was heavily influenced by the OSI model, including the (sub)layers, terminology, service primitives, and more. Unfortunately, also like OSI, it is fairly complicated. In the following sections we will give a brief description of some of the highlights of 802.16, but this treatment is far from complete and leaves out many details. For additional information about broadband wireless in general, see (Bolcskei et al., 2001; and Webb, 2001). For information about 802.16 in particular, see (Eklund et al., 2002). 

4.5.1 Comparison of 802.11 with 802.16 

At this point you may be thinking: Why devise a new standard? Why not just use 802.11? There are some very good reasons for not using 802.11, primarily because 802.11 and 802.16 solve different problems. Before getting into the technology of 802.16, it is probably worthwhile saying a few words about why a new standard is needed at all. 

The environments in which 802.11 and 802.16 operate are similar in some ways, primarily in that they were designed to provide high-bandwidth wireless communications. But they also differ in some major ways. To start with, 802.16 provides service to buildings, and buildings are not mobile. They do not migrate from cell to cell often. Much of 802.11 deals with mobility, and none of that is relevant here. Next, buildings can have more than one computer in them, a complication that does not occur when the end station is a single notebook computer. Because building owners are generally willing to spend much more money for communication gear than are notebook owners, better radios are available. This difference means that 802.16 can use full-duplex communication, something 802.11 avoids to keep the cost of the radios low. 

Because 802.16 runs over part of a city, the distances involved can be several kilometers, which means that the perceived power at the base station can vary widely from station to station. This variation affects the signal-to-noise ratio, which, in, turn, dictates multiple modulation schemes. Also, open communication over a city means that security and privacy are essential and mandatory. 

Furthermore, each cell is likely to have many more users than will a typical 802.11 cell, and these users are expected to use more bandwidth than will a typical 802.11 user. After all it is rare for a company to invite 50 employees to show up in a room with their laptops to see if they can saturate the 802.11 wireless network by watching 50 separate movies at once. For this reason, more spectrum is needed than the ISM bands can provide, forcing 802.16 to operate in the much higher 10-to-66 GHz frequency range, the only place unused spectrum is still available. 

But these millimeter waves have different physical properties than the longer waves in the ISM bands, which in turn requires a completely different physical layer. One property that millimeter waves have is that they are strongly absorbed by water (especially rain, but to some extent also by snow, hail, and with a bit of bad luck, heavy fog). Consequently, error handling is more important than in an indoor environment. Millimeter waves can be focused into directional beams (802.11 is omnidirectional), so choices made in 802.11 relating to multipath propagation are moot here. 

Another issue is quality of service. While 802.11 provides some support for real-time traffic (using PCF mode), it was not really designed for telephony and heavy-duty multimedia usage. In contrast, 802.16 is expected to support these applications completely because it is intended for residential as well as business use. 

In short, 802.11 was designed to be mobile Ethernet, whereas 802.16 was designed to be wireless, but stationary, cable television. These differences are so big that the resulting standards are very different as they try to optimize different things. 

228




A very brief comparison with the cellular phone system is also worthwhile. With mobile phones, we are talking about narrow-band, voice-oriented, low-powered, mobile stations that communicate using medium-length microwaves. Nobody watches high-resolution, two-hour movies on GSM mobile phones (yet). Even UMTS has little hope of changing this situation. In short, the wireless MAN world is far more demanding than is the mobile phone world, so a completely different system is needed. Whether 802.16 could be used for mobile devices in the future is an interesting question. It was not optimized for them, but the possibility is there. For the moment it is focused on fixed wireless. 

4.5.2 The 802.16 Protocol Stack 

The 802.16 protocol stack is illustrated in 

Fig. 4-31

. The general structure is similar to that of the other 802 networks, but with more sublayers. The bottom sublayer deals with transmission. Traditional narrow-band radio is used with conventional modulation schemes. Above the physical transmission layer comes a convergence sublayer to hide the different technologies from the data link layer. Actually, 802.11 has something like this too, only the committee chose not to formalize it with an OSI-type name. 

Figure 4-31. The 802.16 protocol stack. 

 

Although we have not shown them in the figure, work is already underway to add two new physical layer protocols. The 802.16a standard will support OFDM in the 2-to-11 GHz frequency range. The 802.16b standard will operate in the 5-GHz ISM band. Both of these are attempts to move closer to 802.11. 

The data link layer consists of three sublayers. The bottom one deals with privacy and security, which is far more crucial for public outdoor networks than for private indoor networks. It manages encryption, decryption, and key management. 

Next comes the MAC sublayer common part. This is where the main protocols, such as channel management, are located. The model is that the base station controls the system. It can schedule the downstream (i.e., base to subscriber) channels very efficiently and plays a major role in managing the upstream (i.e., subscriber to base) channels as well. An unusual feature of the MAC sublayer is that, unlike those of the other 802 networks, it is completely connection oriented, in order to provide quality-of-service guarantees for telephony and multimedia communication. 

The service-specific convergence sublayer takes the place of the logical link sublayer in the other 802 protocols. Its function is to interface to the network layer. A complication here is that 802.16 was designed to integrate seamlessly with both datagram protocols (e.g., PPP, IP, and Ethernet) and ATM. The problem is that packet protocols are connectionless and ATM is connection oriented. This means that every ATM connection has to map onto an 802.16 connection, in principle a straightforward matter. But onto which 802.16 connection should an incoming IP packet be mapped? That problem is dealt with in this sublayer. 

229




4.5.3 The 802.16 Physical Layer 

As mentioned above, broadband wireless needs a lot of spectrum, and the only place to find it is in the 10-to-66 GHz range. These millimeter waves have an interesting property that longer microwaves do not: they travel in straight lines, unlike sound but similar to light. As a consequence, the base station can have multiple antennas, each pointing at a different sector of the surrounding terrain, as shown in 

Fig. 4-32

. Each sector has its own users and is fairly independent of the adjoining ones, something not true of cellular radio, which is omnidirectional. 

Figure 4-32. The 802.16 transmission environment. 

 

Because signal strength in the millimeter band falls off sharply with distance from the base station, the signal-to-noise ratio also drops with distance from the base station. For this reason, 802.16 employs three different modulation schemes, depending on how far the subscriber station is from the base station. For close-in subscribers, QAM-64 is used, with 6 bits/baud. For medium-distance subscribers, QAM-16 is used, with 4 bits/baud. For distant subscribers, QPSK is used, with 2 bits/baud. For example, for a typical value of 25 MHz worth of spectrum, QAM-64 gives 150 Mbps, QAM-16 gives 100 Mbps, and QPSK gives 50 Mbps. In other words, the farther the subscriber is from the base station, the lower the data rate (similar to what we saw with ADSL in 

Fig. 2-27

). The constellation diagrams for these three modulation techniques were shown in 

Fig. 2-25

. 

Given the goal of producing a broadband system, and subject to the above physical constraints, the 802.16 designers worked hard to use the available spectrum efficiently. One thing they did not like was the way GSM and DAMPS work. Both of those use different but equal frequency bands for upstream and downstream traffic. For voice, traffic is probably symmetric for the most part, but for Internet access, there is often more downstream traffic than upstream traffic. Consequently, 802.16 provides a more flexible way to allocate the bandwidth. Two schemes are used, 

FDD

 (

Frequency Division Duplexing

) and 

TDD

 (

Time Division Duplexing

). The latter is illustrated in 

Fig. 4-33

. Here the base station periodically sends out frames. Each frame contains time slots. The first ones are for downstream traffic. Then comes a guard time used by the stations to switch direction. Finally, we have slots for upstream traffic. The number of time slots devoted to each direction can be changed dynamically to match the bandwidth in each direction to the traffic. 

Figure 4-33. Frames and time slots for time division duplexing. 

230




 

Downstream traffic is mapped onto time slots by the base station. The base station is completely in control for this direction. Upstream traffic is more complex and depends on the quality of service required. We will come to slot allocation when we discuss the MAC sublayer below. 

Another interesting feature of the physical layer is its ability to pack multiple MAC frames back-to back in a single physical transmission. The feature enhances spectral efficiency by reducing the number of preambles and physical layer headers needed. 

Also noteworthy is the use of Hamming codes to do forward error correction in the physical layer. Nearly all other networks simply rely on checksums to detect errors and request retransmission when frames are received in error. But in the wide area broadband environment, so many transmission errors are expected that error correction is employed in the physical layer, in addition to checksums in the higher layers. The net effect of the error correction is to make the channel look better than it really is (in the same way that CD-ROMs appear to be very reliable, but only because more than half the total bits are devoted to error correction in the physical layer). 

4.5.4 The 802.16 MAC Sublayer Protocol 

The data link layer is divided into three sublayers, as we saw in 

Fig. 4-31

. Since we will not study cryptography until 

Chap. 8

, it is difficult to explain now how the security sublayer works. Suffice it to say that encryption is used to keep secret all data transmitted. Only the frame payloads are encrypted; the headers are not. This property means that a snooper can see who is talking to whom but cannot tell what they are saying to each other. 

If you already know something about cryptography, here comes a one-paragraph explanation of the security sublayer. If you know nothing about cryptography, you are not likely to find the next paragraph terribly enlightening (but you might consider rereading it after finishing 

Chap. 

8

). 

At the time a subscriber connects to a base station, they perform mutual authentication with RSA public-key cryptography using X.509 certificates. The payloads themselves are encrypted using a symmetric-key system, either DES with cipher block chaining or triple DES with two keys. AES (Rijndael) is likely to be added soon. Integrity checking uses SHA-1. Now that was not so bad, was it? 

Let us now look at the MAC sublayer common part. MAC frames occupy an integral number of physical layer time slots. Each frame is composed of sub-frames, the first two of which are the downstream and upstream maps. These maps tell what is in which time slot and which time slots are free. The downstream map also contains various system parameters to inform new stations as they come on-line. 

The downstream channel is fairly straightforward. The base station simply decides what to put in which subframe. The upstream channel is more complicated since there are competing uncoordinated subscribers that need access to it. Its allocation is tied closely to the quality-of-service issue. Four classes of service are defined as follows: 

1. Constant bit rate service. 

2. Real-time variable bit rate service. 

231




3. Non-real-time variable bit rate service. 

4. Best-efforts service. 

All service in 802.16 is connection-oriented, and each connection gets one of the above classes of service, determined when the connection is set up. This design is very different from that of 802.11 or Ethernet, which have no connections in the MAC sublayer. 

Constant bit rate service is intended for transmitting uncompressed voice such as on a T1 channel. This service needs to send a predetermined amount of data at predetermined time intervals. It is accommodated by dedicating certain time slots to each connection of this type. Once the bandwidth has been allocated, the time slots are available automatically, without the need to ask for each one. 

Real-time variable bit rate service is for compressed multimedia and other soft real-time applications in which the amount of bandwidth needed each instant may vary. It is accommodated by the base station polling the subscriber at a fixed interval to ask how much bandwidth is needed this time. 

Non-real-time variable bit rate service is for heavy transmissions that are not real time, such as large file transfers. For this service the base station polls the subscriber often, but not at rigidly-prescribed time intervals. A constant bit rate customer can set a bit in one of its frames requesting a poll in order to send additional (variable bit rate) traffic. 

If a station does not respond to a poll 

k

 times in a row, the base station puts it into a multicast group and takes away its personal poll. Instead, when the multicast group is polled, any of the stations in it can respond, contending for service. In this way, stations with little traffic do not waste valuable polls. 

Finally, best-efforts service is for everything else. No polling is done and the subscriber must contend for bandwidth with other best-efforts subscribers. Requests for bandwidth are done in time slots marked in the upstream map as available for contention. If a request is successful, its success will be noted in the next downstream map. If it is not successful, unsuccessful subscribers have to try again later. To minimize collisions, the Ethernet binary exponential backoff algorithm is used. 

The standard defines two forms of bandwidth allocation: per station and per connection. In the former case, the subscriber station aggregates the needs of all the users in the building and makes collective requests for them. When it is granted bandwidth, it doles out that bandwidth to its users as it sees fit. In the latter case, the base station manages each connection directly. 

4.5.5 The 802.16 Frame Structure 

All MAC frames begin with a generic header. The header is followed by an optional payload and an optional checksum (CRC), as illustrated in 

Fig. 4-34

. The payload is not needed in control frames, for example, those requesting channel slots. The checksum is (surprisingly) also optional due to the error correction in the physical layer and the fact that no attempt is ever made to retransmit real-time frames. If no retransmissions will be attempted, why even bother with a checksum? 

Figure 4-34. (a) A generic frame. (b) A bandwidth request frame. 

232




 

A quick rundown of the header fields of 

Fig. 4-34(a)

 is as follows. The 

EC

 bit tells whether the payload is encrypted. The 

Type

 field identifies the frame type, mostly telling whether packing and fragmentation are present. The 

CI

 field indicates the presence or absence of the final checksum. The 

EK

 field tells which of the encryption keys is being used (if any). The 

Length

 field gives the complete length of the frame, including the header. The 

Connection identifier

 tells which connection this frame belongs to. Finally, the 

HeaderCRC

 field is a checksum over the header only, using the polynomial 

x

8

 + 

x

2

 + 

x

 + 1. 

A second header type, for frames that request bandwidth, is shown in 

Fig. 4-34(b)

. It starts with a 1 bit instead of a 0 bit and is similar to the generic header except that the second and third bytes form a 16-bit number telling how much bandwidth is needed to carry the specified number of bytes. Bandwidth request frames do not carry a payload or full-frame CRC. 

A great deal more could be said about 802.16, but this is not the place to say it. For more information, please consult the standard itself. 

4.6 Bluetooth 

In 1994, the L. M. Ericsson company became interested in connecting its mobile phones to other devices (e.g., PDAs) without cables. Together with four other companies (IBM, Intel, Nokia, and Toshiba), it formed a SIG (Special Interest Group, i.e., consortium) to develop a wireless standard for interconnecting computing and communication devices and accessories using short-range, low-power, inexpensive wireless radios. The project was named 

Bluetooth

, after Harald Blaatand (Bluetooth) II (940-981), a Viking king who unified (i.e., conquered) Denmark and Norway, also without cables. 

Although the original idea was just to get rid of the cables between devices, it soon began to expand in scope and encroach on the area of wireless LANs. While this move makes the standard more useful, it also creates some competition for mindshare with 802.11. To make matters worse, the two systems also interfere with each other electrically. It is also worth noting that Hewlett-Packard introduced an infrared network for connecting computer peripherals without wires some years ago, but it never really caught on in a big way. 

Undaunted by all this, in July 1999 the Bluetooth SIG issued a 1500-page specification of V1.0. Shortly thereafter, the IEEE standards group looking at wireless personal area networks, 802.15, adopted the Bluetooth document as a basis and began hacking on it. While it might seem strange to standardize something that already had a very detailed specification and no incompatible implementations that needed to be harmonized, history shows that having an open standard managed by a neutral body such as the IEEE often promotes the use of a technology. To be a bit more precise, it should be noted that the Bluetooth specification is for a complete system, from the physical layer to the application layer. The IEEE 802.15 committee is standardizing only the physical and data link layers; the rest of the protocol stack falls outside its charter. 

Even though IEEE approved the first PAN standard, 802.15.1, in 2002, the Bluetooth SIG is still active busy with improvements. Although the Bluetooth SIG and IEEE versions are not identical, it is hoped that they will soon converge to a single standard. 

233




4.6.1 Bluetooth Architecture 

Let us start our study of the Bluetooth system with a quick overview of what it contains and what it is intended to do. The basic unit of a Bluetooth system is a 

piconet

, which consists of a master node and up to seven active slave nodes within a distance of 10 meters. Multiple piconets can exist in the same (large) room and can even be connected via a bridge node, as shown in 

Fig. 4-35

. An interconnected collection of piconets is called a 

scatternet

. 

Figure 4-35. Two piconets can be connected to form a scatternet. 

 

In addition to the seven active slave nodes in a piconet, there can be up to 255 parked nodes in the net. These are devices that the master has switched to a low-power state to reduce the drain on their batteries. In parked state, a device cannot do anything except respond to an activation or beacon signal from the master. There are also two intermediate power states, hold and sniff, but these will not concern us here. 

The reason for the master/slave design is that the designers intended to facilitate the implementation of complete Bluetooth chips for under $5. The consequence of this decision is that the slaves are fairly dumb, basically just doing whatever the master tells them to do. At its heart, a piconet is a centralized TDM system, with the master controlling the clock and determining which device gets to communicate in which time slot. All communication is between the master and a slave; direct slave-slave communication is not possible. 

4.6.2 Bluetooth Applications 

Most network protocols just provide channels between communicating entities and let applications designers figure out what they want to use them for. For example, 802.11 does not specify whether users should use their notebook computers for reading e-mail, surfing the Web, or something else. In contrast, the Bluetooth V1.1 specification names 13 specific applications to be supported and provides different protocol stacks for each one. Unfortunately, this approach leads to a very large amount of complexity, which we will omit here. The 13 applications, which are called 

profiles

, are listed in 

Fig. 4-36

. By looking at them briefly now, we may see more clearly what the Bluetooth SIG is trying to accomplish. 

Figure 4-36. The Bluetooth profiles. 

234




 

The generic access profile is not really an application, but rather the basis upon which the real applications are built. Its main job is to provide a way to establish and maintain secure links (channels) between the master and the slaves. Also relatively generic is the service discovery profile, which is used by devices to discover what services other devices have to offer. All Bluetooth devices are expected to implement these two profiles. The remaining ones are optional. 

The serial port profile is a transport protocol that most of the remaining profiles use. It emulates a serial line and is especially useful for legacy applications that expect a serial line. 

The generic object exchange profile defines a client-server relationship for moving data around. Clients initiate operations, but a slave can be either a client or a server. Like the serial port profile, it is a building block for other profiles. 

The next group of three profiles is for networking. The LAN access profile allows a Bluetooth device to connect to a fixed network. This profile is a direct competitor to 802.11. The dial-up networking profile was the original motivation for the whole project. It allows a notebook computer to connect to a mobile phone containing a built-in modem without wires. The fax profile is similar to dial-up networking, except that it allows wireless fax machines to send and receive faxes using mobile phones without a wire between the two. 

The next three profiles are for telephony. The cordless telephony profile provides a way to connect the handset of a cordless telephone to the base station. Currently, most cordless telephones cannot also be used as mobile phones, but in the future, cordless and mobile phones may merge. The intercom profile allows two telephones to connect as walkie-talkies. Finally, the headset profile provides hands-free voice communication between the headset and its base station, for example, for hands-free telephony while driving a car. 

The remaining three profiles are for actually exchanging objects between two wireless devices. These could be business cards, pictures, or data files. The synchronization profile, in particular, is intended for loading data into a PDA or notebook computer when it leaves home and collecting data from it when it returns. 

Was it really necessary to spell out all these applications in detail and provide different protocol stacks for each one? Probably not, but there were a number of different working groups that devised different parts of the standard, and each one just focused on its specific problem and generated its own profile. Think of this as Conway's law in action. (In the April 1968 issue of 

Datamation

 magazine, Melvin Conway observed that if you assign 

n

 people to write a compiler, you will get an 

n

-pass compiler, or more generally, the software structure mirrors the structure 

235




of the group that produced it.) It would probably have been possible to get away with two protocol stacks instead of 13, one for file transfer and one for streaming real-time communication. 

4.6.3 The Bluetooth Protocol Stack 

The Bluetooth standard has many protocols grouped loosely into layers. The layer structure does not follow the OSI model, the TCP/IP model, the 802 model, or any other known model. However, IEEE is working on modifying Bluetooth to shoehorn it into the 802 model better. The basic Bluetooth protocol architecture as modified by the 802 committee is shown in 

Fig. 4-

37

. 

Figure 4-37. The 802.15 version of the Bluetooth protocol architecture. 

 

The bottom layer is the physical radio layer, which corresponds fairly well to the physical layer in the OSI and 802 models. It deals with radio transmission and modulation. Many of the concerns here have to do with the goal of making the system inexpensive so that it can become a mass market item. 

The baseband layer is somewhat analogous to the MAC sublayer but also includes elements of the physical layer. It deals with how the master controls time slots and how these slots are grouped into frames. 

Next comes a layer with a group of somewhat related protocols. The link manager handles the establishment of logical channels between devices, including power management, authentication, and quality of service. The logical link control adaptation protocol (often called L2CAP) shields the upper layers from the details of transmission. It is analogous to the standard 802 LLC sublayer, but technically different from it. As the names suggest, the audio and control protocols deal with audio and control, respectively. The applications can get at them directly, without having to go through the L2CAP protocol. 

The next layer up is the middleware layer, which contains a mix of different protocols. The 802 LLC was inserted here by IEEE for compatibility with its other 802 networks. The RFcomm, telephony, and service discovery protocols are native. RFcomm (Radio Frequency communication) is the protocol that emulates the standard serial port found on PCs for connecting the keyboard, mouse, and modem, among other devices. It has been designed to allow legacy devices to use it easily. The telephony protocol is a real-time protocol used for the three speech-oriented profiles. It also manages call setup and termination. Finally, the service discovery protocol is used to locate services within the network. 

The top layer is where the applications and profiles are located. They make use of the protocols in lower layers to get their work done. Each application has its own dedicated subset of the protocols. Specific devices, such as a headset, usually contain only those protocols needed by that application and no others. 

236




In the following sections we will examine the three lowest layers of the Bluetooth protocol stack since these roughly correspond to the physical and MAC sublayers. 

4.6.4 The Bluetooth Radio Layer 

The radio layer moves the bits from master to slave, or vice versa. It is a low-power system with a range of 10 meters operating in the 2.4-GHz ISM band. The band is divided into 79 channels of 1 MHz each. Modulation is frequency shift keying, with 1 bit per Hz giving a gross data rate of 1 Mbps, but much of this spectrum is consumed by overhead. To allocate the channels fairly, frequency hopping spread spectrum is used with 1600 hops/sec and a dwell time of 625 µsec. All the nodes in a piconet hop simultaneously, with the master dictating the hop sequence. 

Because both 802.11 and Bluetooth operate in the 2.4-GHz ISM band on the same 79 channels, they interfere with each other. Since Bluetooth hops far faster than 802.11, it is far more likely that a Bluetooth device will ruin 802.11 transmissions than the other way around. Since 802.11 and 802.15 are both IEEE standards, IEEE is looking for a solution to this problem, but it is not so easy to find since both systems use the ISM band for the same reason: no license is required there. The 802.11a standard uses the other (5 GHz) ISM band, but it has a much shorter range than 802.11b (due to the physics of radio waves), so using 802.11a is not a perfect solution for all cases. Some companies have solved the problem by banning Bluetooth altogether. A market-based solution is for the network with more power (politically and economically, not electrically) to demand that the weaker party modify its standard to stop interfering with it. Some thoughts on this matter are given in (Lansford et al., 2001). 

4.6.5 The Bluetooth Baseband Layer 

The baseband layer is the closest thing Bluetooth has to a MAC sublayer. It turns the raw bit stream into frames and defines some key formats. In the simplest form, the master in each piconet defines a series of 625 µsec time slots, with the master's transmissions starting in the even slots and the slaves' transmissions starting in the odd ones. This is traditional time division multiplexing, with the master getting half the slots and the slaves sharing the other half. Frames can be 1, 3, or 5 slots long. 

The frequency hopping timing allows a settling time of 250–260 µsec per hop to allow the radio circuits to become stable. Faster settling is possible, but only at higher cost. For a single-slot frame, after settling, 366 of the 625 bits are left over. Of these, 126 are for an access code and the header, leaving 240 bits for data. When five slots are strung together, only one settling period is needed and a slightly shorter settling period is used, so of the 5 x 625 = 3125 bits in five time slots, 2781 are available to the baseband layer. Thus, longer frames are much more efficient than single-slot frames. 

Each frame is transmitted over a logical channel, called a 

link

, between the master and a slave. Two kinds of links exist. The first is the 

ACL

 (

Asynchronous Connection-Less

) link, which is used for packet-switched data available at irregular intervals. These data come from the L2CAP layer on the sending side and are delivered to the L2CAP layer on the receiving side. ACL traffic is delivered on a best-efforts basis. No guarantees are given. Frames can be lost and may have to be retransmitted. A slave may have only one ACL link to its master. 

The other is the 

SCO

 (

Synchronous Connection Oriented

) link, for real-time data, such as telephone connections. This type of channel is allocated a fixed slot in each direction. Due to the time-critical nature of SCO links, frames sent over them are never retransmitted. Instead, forward error correction can be used to provide high reliability. A slave may have up to three SCO links with its master. Each SCO link can transmit one 64,000 bps PCM audio channel. 

237




4.6.6 The Bluetooth L2CAP Layer 

The L2CAP layer has three major functions. First, it accepts packets of up to 64 KB from the upper layers and breaks them into frames for transmission. At the far end, the frames are reassembled into packets again. 

Second, it handles the multiplexing and demultiplexing of multiple packet sources. When a packet has been reassembled, the L2CAP layer determines which upper-layer protocol to hand it to, for example, RFcomm or telephony. 

Third, L2CAP handles the quality of service requirements, both when links are established and during normal operation. Also negotiated at setup time is the maximum payload size allowed, to prevent a large-packet device from drowning a small-packet device. This feature is needed because not all devices can handle the 64-KB maximum packet. 

4.6.7 The Bluetooth Frame Structure 

There are several frame formats, the most important of which is shown in 

Fig. 4-38

. It begins with an access code that usually identifies the master so that slaves within radio range of two masters can tell which traffic is for them. Next comes a 54-bit header containing typical MAC sublayer fields. Then comes the data field, of up to 2744 bits (for a five-slot transmission). For a single time slot, the format is the same except that the data field is 240 bits. 

Figure 4-38. A typical Bluetooth data frame. 

 

Let us take a quick look at the header. The 

Address

 field identifies which of the eight active devices the frame is intended for. The 

Type

 field identifies the frame type (ACL, SCO, poll, or null), the type of error correction used in the data field, and how many slots long the frame is. The 

Flow

 bit is asserted by a slave when its buffer is full and cannot receive any more data. This is a primitive form of flow control. The 

Acknowledgement

 bit is used to piggyback an ACK onto a frame. The 

Sequence

 bit is used to number the frames to detect retransmissions. The protocol is stop-and-wait, so 1 bit is enough. Then comes the 8-bit header 

Checksum

. The entire 18-bit header is repeated three times to form the 54-bit header shown in 

Fig. 4-38

. On the receiving side, a simple circuit examines all three copies of each bit. If all three are the same, the bit is accepted. If not, the majority opinion wins. Thus, 54 bits of transmission capacity are used to send 10 bits of header. The reason is that to reliably send data in a noisy environment using cheap, low-powered (2.5 mW) devices with little computing capacity, a great deal of redundancy is needed. 

Various formats are used for the data field for ACL frames. The SCO frames are simpler though: the data field is always 240 bits. Three variants are defined, permitting 80, 160, or 240 bits of actual payload, with the rest being used for error correction. In the most reliable version (80-bit payload), the contents are just repeated three times, the same as the header. 

Since the slave may use only the odd slots, it gets 800 slots/sec, just as the master does. With an 80-bit payload, the channel capacity from the slave is 64,000 bps and the channel capacity 

238




from the master is also 64,000 bps, exactly enough for a single full-duplex PCM voice channel (which is why a hop rate of 1600 hops/sec was chosen). These numbers mean that a full-duplex voice channel with 64,000 bps in each direction using the most reliable format completely saturates the piconet despite a raw bandwidth of 1 Mbps. For the least reliable variant (240 bits/slot with no redundancy at this level), three full-duplex voice channels can be supported at once, which is why a maximum of three SCO links is permitted per slave. 

There is much more to be said about Bluetooth, but no more space to say it here. For more information, see (Bhagwat, 2001; Bisdikian, 2001; Bray and Sturman, 2002; Haartsen, 2000; Johansson et al., 2001; Miller and Bisdikian, 2001; and Sairam et al., 2002). 

4.7 Data Link Layer Switching 

Many organizations have multiple LANs and wish to connect them. LANs can be connected by devices called 

bridges

, which operate in the data link layer. Bridges examine the data layer link addresses to do routing. Since they are not supposed to examine the payload field of the frames they route, they can transport IPv4 (used in the Internet now), IPv6 (will be used in the Internet in the future), AppleTalk, ATM, OSI, or any other kinds of packets. In contrast, 

routers

 examine the addresses in packets and route based on them. Although this seems like a clear division between bridges and routers, some modern developments, such as the advent of switched Ethernet, have muddied the waters, as we will see later. In the following sections we will look at bridges and switches, especially for connecting different 802 LANs. For a comprehensive treatment of bridges, switches, and related topics, see (Perlman, 2000). 

Before getting into the technology of bridges, it is worthwhile taking a look at some common situations in which bridges are used. We will mention six reasons why a single organization may end up with multiple LANs. 

First, many university and corporate departments have their own LANs, primarily to connect their own personal computers, workstations, and servers. Since the goals of the various departments differ, different departments choose different LANs, without regard to what other departments are doing. Sooner or later, there is a need for interaction, so bridges are needed. In this example, multiple LANs came into existence due to the autonomy of their owners. 

Second, the organization may be geographically spread over several buildings separated by considerable distances. It may be cheaper to have separate LANs in each building and connect them with bridges and laser links than to run a single cable over the entire site. 

Third, it may be necessary to split what is logically a single LAN into separate LANs to accommodate the load. At many universities, for example, thousands of workstations are available for student and faculty computing. Files are normally kept on file server machines and are downloaded to users' machines upon request. The enormous scale of this system precludes putting all the workstations on a single LAN—the total bandwidth needed is far too high. Instead, multiple LANs connected by bridges are used, as shown in 

Fig. 4-39

. Each LAN contains a cluster of workstations with its own file server so that most traffic is restricted to a single LAN and does not add load to the backbone. 

Figure 4-39. Multiple LANs connected by a backbone to handle a total load higher than the capacity of a single LAN. 

239




 

It is worth noting that although we usually draw LANs as multidrop cables as in 

Fig. 4-39

 (the classic look), they are more often implemented with hubs or especially switches nowadays. However, a long multidrop cable with multiple machines plugged into it and a hub with the machines connected inside the hub are functionally identical. In both cases, all the machines belong to the same collision domain, and all use the CSMA/CD protocol to send frames. Switched LANs are different, however, as we saw before and will see again shortly. 

Fourth, in some situations, a single LAN would be adequate in terms of the load, but the physical distance between the most distant machines is too great (e.g., more than 2.5 km for Ethernet). Even if laying the cable is easy to do, the network would not work due to the excessively long round-trip delay. The only solution is to partition the LAN and install bridges between the segments. Using bridges, the total physical distance covered can be increased. 

Fifth, there is the matter of reliability. On a single LAN, a defective node that keeps outputting a continuous stream of garbage can cripple the LAN. Bridges can be inserted at critical places, like fire doors in a building, to prevent a single node that has gone berserk from bringing down the entire system. Unlike a repeater, which just copies whatever it sees, a bridge can be programmed to exercise some discretion about what it forwards and what it does not forward. 

Sixth, and last, bridges can contribute to the organization's security. Most LAN interfaces have a 

promiscuous mode

, in which 

all

 frames are given to the computer, not just those addressed to it. Spies and busybodies love this feature. By inserting bridges at various places and being careful not to forward sensitive traffic, a system administrator can isolate parts of the network so that its traffic cannot escape and fall into the wrong hands. 

Ideally, bridges should be fully transparent, meaning it should be possible to move a machine from one cable segment to another without changing any hardware, software, or configuration tables. Also, it should be possible for machines on any segment to communicate with machines on any other segment without regard to the types of LANs being used on the two segments or on segments in between them. This goal is sometimes achieved, but not always. 

4.7.1 Bridges from 802.x to 802.y 

Having seen why bridges are needed, let us now turn to the question of how they work. 

Figure 

4-40

 illustrates the operation of a simple two-port bridge. Host 

A

 on a wireless (802.11) LAN has a packet to send to a fixed host, 

B

, on an (802.3) Ethernet to which the wireless LAN is connected. The packet descends into the LLC sublayer and acquires an LLC header (shown in black in the figure). Then it passes into the MAC sublayer and an 802.11 header is prepended to it (also a trailer, not shown in the figure). This unit goes out over the air and is picked up by the base station, which sees that it needs to go to the fixed Ethernet. When it hits the bridge connecting the 802.11 network to the 802.3 network, it starts in the physical layer and works 

240




its way upward. In the MAC sublayer in the bridge, the 802.11 header is stripped off. The bare packet (with LLC header) is then handed off to the LLC sublayer in the bridge. In this example, the packet is destined for an 802.3 LAN, so it works its way down the 802.3 side of the bridge and off it goes on the Ethernet. Note that a bridge connecting 

k

 different LANs will have 

k

 different MAC sublayers and 

k

 different physical layers, one for each type. 

Figure 4-40. Operation of a LAN bridge from 802.11 to 802.3. 

 

So far it looks like moving a frame from one LAN to another is easy. Such is not the case. In this section we will point out some of the difficulties that one encounters when trying to build a bridge between the various 802 LANs (and MANs). We will focus on 802.3, 802.11, and 802.16, but there are others as well, each with its unique problems. 

To start with, each of the LANs uses a different frame format (see 

Fig. 4-41

). Unlike the differences between Ethernet, token bus, and token ring, which were due to history and big corporate egos, here the differences are to some extent legitimate. For example, the 

Duration

 field in 802.11 is there due to the MACAW protocol and makes no sense in Ethernet. As a result, any copying between different LANs requires reformatting, which takes CPU time, requires a new checksum calculation, and introduces the possibility of undetected errors due to bad bits in the bridge's memory. 

Figure 4-41. The IEEE 802 frame formats. The drawing is not to scale. 

 

A second problem is that interconnected LANs do not necessarily run at the same data rate. When forwarding a long run of back-to-back frames from a fast LAN to a slower one, the bridge will not be able to get rid of the frames as fast as they come in. For example, if a gigabit Ethernet is pouring bits into an 11-Mbps 802.11b LAN at top speed, the bridge will have to buffer them, hoping not to run out of memory. Bridges that connect three or more 

241




LANs have a similar problem when several LANs are trying to feed the same output LAN at the same time even if all the LANs run at the same speed. 

A third problem, and potentially the most serious of all, is that different 802 LANs have different maximum frame lengths. An obvious problem arises when a long frame must be forwarded onto a LAN that cannot accept it. Splitting the frame into pieces is out of the question in this layer. All the protocols assume that frames either arrive or they do not. There is no provision for reassembling frames out of smaller units. This is not to say that such protocols could not be devised. They could be and have been. It is just that no data link protocols provide this feature, so bridges must keep their hands off the frame payload. Basically, there is no solution. Frames that are too large to be forwarded must be discarded. So much for transparency. 

Another point is security. Both 802.11 and 802.16 support encryption in the data link layer. Ethernet does not. This means that the various encryption services available to the wireless networks are lost when traffic passes over an Ethernet. Worse yet, if a wireless station uses data link layer encryption, there will be no way to decrypt it when it arrives over an Ethernet. If the wireless station does not use encryption, its traffic will be exposed over the air link. Either way there is a problem. 

One solution to the security problem is to do encryption in a higher layer, but then the 802.11 station has to know whether it is talking to another station on an 802.11 network (meaning use data link layer encryption) or not (meaning do not use it). Forcing the station to make a choice destroys transparency. 

A final point is quality of service. Both 802.11 and 802.16 provide it in various forms, the former using PCF mode and the latter using constant bit rate connections. Ethernet has no concept of quality of service, so traffic from either of the others will lose its quality of service when passing over an Ethernet. 

4.7.2 Local Internetworking 

The previous section dealt with the problems encountered in connecting two different IEEE 802 LANs via a single bridge. However, in large organizations with many LANs, just interconnecting them all raises a variety of issues, even if they are all just Ethernet. Ideally, it should be possible to go out and buy bridges designed to the IEEE standard, plug the connectors into the bridges, and everything should work perfectly, instantly. There should be no hardware changes required, no software changes required, no setting of address switches, no downloading of routing tables or parameters, nothing. Just plug in the cables and walk away. Furthermore, the operation of the existing LANs should not be affected by the bridges at all. In other words, the bridges should be completely transparent (invisible to all the hardware and software). Surprisingly enough, this is actually possible. Let us now take a look at how this magic is accomplished. 

In its simplest form, a transparent bridge operates in promiscuous mode, accepting every frame transmitted on all the LANs to which it is attached. As an example, consider the configuration of 

Fig. 4-42

. Bridge B1 is connected to LANs 1 and 2, and bridge B2 is connected to LANs 2, 3, and 4. A frame arriving at bridge B1 on LAN 1 destined for 

A

 can be discarded immediately, because it is already on the correct LAN, but a frame arriving on LAN 1 for 

C

 or 

F

 must be forwarded. 

Figure 4-42. A configuration with four LANs and two bridges. 

242




 

When a frame arrives, a bridge must decide whether to discard or forward it, and if the latter, on which LAN to put the frame. This decision is made by looking up the destination address in a big (hash) table inside the bridge. The table can list each possible destination and tell which output line (LAN) it belongs on. For example, B2's table would list 

A

 as belonging to LAN 2, since all B2 has to know is which LAN to put frames for 

A

 on. That, in fact, more forwarding happens later is not of interest to it. 

When the bridges are first plugged in, all the hash tables are empty. None of the bridges know where any of the destinations are, so they use a flooding algorithm: every incoming frame for an unknown destination is output on all the LANs to which the bridge is connected except the one it arrived on. As time goes on, the bridges learn where destinations are, as described below. Once a destination is known, frames destined for it are put on only the proper LAN and are not flooded. 

The algorithm used by the transparent bridges is 

backward learning

.As mentioned above, the bridges operate in promiscuous mode, so they see every frame sent on any of their LANs. By looking at the source address, they can tell which machine is accessible on which LAN. For example, if bridge B1 in 

Fig. 4-42

 sees a frame on LAN 2 coming from 

C

, it knows that 

C

 must be reachable via LAN 2, so it makes an entry in its hash table noting that frames going to 

C

 should use LAN 2. Any subsequent frame addressed to 

C

 coming in on LAN 1 will be forwarded, but a frame for 

C

 coming in on LAN 2 will be discarded. 

The topology can change as machines and bridges are powered up and down and moved around. To handle dynamic topologies, whenever a hash table entry is made, the arrival time of the frame is noted in the entry. Whenever a frame whose source is already in the table arrives, its entry is updated with the current time. Thus, the time associated with every entry tells the last time a frame from that machine was seen. 

Periodically, a process in the bridge scans the hash table and purges all entries more than a few minutes old. In this way, if a computer is unplugged from its LAN, moved around the building, and plugged in again somewhere else, within a few minutes it will be back in normal operation, without any manual intervention. This algorithm also means that if a machine is quiet for a few minutes, any traffic sent to it will have to be flooded until it next sends a frame itself. 

The routing procedure for an incoming frame depends on the LAN it arrives on (the source LAN) and the LAN its destination is on (the destination LAN), as follows: 

1. If destination and source LANs are the same, discard the frame. 

2. If the destination and source LANs are different, forward the frame. 

3. If the destination LAN is unknown, use flooding. 

As each frame arrives, this algorithm must be applied. Special-purpose VLSI chips do the lookup and update the table entry, all in a few microseconds. 

243




4.7.3 Spanning Tree Bridges 

To increase reliability, some sites use two or more bridges in parallel between pairs of LANs, as shown in 

Fig. 4-43

. This arrangement, however, also introduces some additional problems because it creates loops in the topology. 

Figure 4-43. Two parallel transparent bridges. 

 

A simple example of these problems can be seen by observing how a frame, 

F

, with unknown destination is handled in 

Fig. 4-43

. Each bridge, following the normal rules for handling unknown destinations, uses flooding, which in this example just means copying it to LAN 2. Shortly thereafter, bridge 1 sees 

F

2

, a frame with an unknown destination, which it copies to LAN 1, generating 

F

3

 (not shown). Similarly, bridge 2 copies 

F

1

 to LAN 1 generating 

F

4

 (also not shown). Bridge 1 now forwards 

F

4

 and bridge 2 copies 

F

3

. This cycle goes on forever. 

The solution to this difficulty is for the bridges to communicate with each other and overlay the actual topology with a spanning tree that reaches every LAN. In effect, some potential connections between LANs are ignored in the interest of constructing a fictitious loop-free topology. For example, in 

Fig. 4-44(a)

 we see nine LANs interconnected by ten bridges. This configuration can be abstracted into a graph with the LANs as the nodes. An arc connects any two LANs that are connected by a bridge. The graph can be reduced to a spanning tree by dropping the arcs shown as dotted lines in 

Fig. 4-44(b)

. Using this spanning tree, there is exactly one path from every LAN to every other LAN. Once the bridges have agreed on the spanning tree, all forwarding between LANs follows the spanning tree. Since there is a unique path from each source to each destination, loops are impossible. 

Figure 4-44. (a) Interconnected LANs. (b) A spanning tree covering the LANs. The dotted lines are not part of the spanning tree. 

 

244




To build the spanning tree, first the bridges have to choose one bridge to be the root of the tree. They make this choice by having each one broadcast its serial number, installed by the manufacturer and guaranteed to be unique worldwide. The bridge with the lowest serial number becomes the root. Next, a tree of shortest paths from the root to every bridge and LAN is constructed. This tree is the spanning tree. If a bridge or LAN fails, a new one is computed. 

The result of this algorithm is that a unique path is established from every LAN to the root and thus to every other LAN. Although the tree spans all the LANs, not all the bridges are necessarily present in the tree (to prevent loops). Even after the spanning tree has been established, the algorithm continues to run during normal operation in order to automatically detect topology changes and update the tree. The distributed algorithm used for constructing the spanning tree was invented by Radia Perlman and is described in detail in (Perlman, 2000). It is standardized in IEEE 802.1D. 

4.7.4 Remote Bridges 

A common use of bridges is to connect two (or more) distant LANs. For example, a company might have plants in several cities, each with its own LAN. Ideally, all the LANs should be interconnected, so the complete system acts like one large LAN. 

This goal can be achieved by putting a bridge on each LAN and connecting the bridges pairwise with point-to-point lines (e.g., lines leased from a telephone company). A simple system, with three LANs, is illustrated in 

Fig. 4-45

. The usual routing algorithms apply here. The simplest way to see this is to regard the three point-to-point lines as hostless LANs. Then we have a normal system of six LANS interconnected by four bridges. Nothing in what we have studied so far says that a LAN must have hosts on it. 

Figure 4-45. Remote bridges can be used to interconnect distant LANs. 

 

Various protocols can be used on the point-to-point lines. One possibility is to choose some standard point-to-point data link protocol such as PPP, putting complete MAC frames in the payload field. This strategy works best if all the LANs are identical, and the only problem is getting frames to the correct LAN. Another option is to strip off the MAC header and trailer at the source bridge and put what is left in the payload field of the point-to-point protocol. A new MAC header and trailer can then be generated at the destination bridge. A disadvantage of this approach is that the checksum that arrives at the destination host is not the one computed by the source host, so errors caused by bad bits in a bridge's memory may not be detected. 

4.7.5 Repeaters, Hubs, Bridges, Switches, Routers, and Gateways 

So far in this book we have looked at a variety of ways to get frames and packets from one cable segment to another. We have mentioned repeaters, bridges, switches, hubs, routers, and gateways. All of these devices are in common use, but they all differ in subtle and not-so-subtle ways. Since there are so many of them, it is probably worth taking a look at them together to see what the similarities and differences are. 

245




To start with, these devices operate in different layers, as illustrated in 

Fig. 4-46(a)

. The layer matters because different devices use different pieces of information to decide how to switch. In a typical scenario, the user generates some data to be sent to a remote machine. Those data are passed to the transport layer, which then adds a header, for example, a TCP header, and passes the resulting unit down to the network layer. The network layer adds its own header to form a network layer packet, for example, an IP packet. In 

Fig. 4-46(b)

 we see the IP packet shaded in gray. Then the packet goes to the data link layer, which adds its own header and checksum (CRC) and gives the resulting frame to the physical layer for transmission, for example, over a LAN. 

Figure 4-46. (a) Which device is in which layer. (b) Frames, packets, and headers. 

 

Now let us look at the switching devices and see how they relate to the packets and frames. At the bottom, in the physical layer, we find the repeaters. These are analog devices that are connected to two cable segments. A signal appearing on one of them is amplified and put out on the other. Repeaters do not understand frames, packets, or headers. They understand volts. Classic Ethernet, for example, was designed to allow four repeaters, in order to extend the maximum cable length from 500 meters to 2500 meters. 

Next we come to the hubs. A hub has a number of input lines that it joins electrically. Frames arriving on any of the lines are sent out on all the others. If two frames arrive at the same time, they will collide, just as on a coaxial cable. In other words, the entire hub forms a single collision domain. All the lines coming into a hub must operate at the same speed. Hubs differ from repeaters in that they do not (usually) amplify the incoming signals and are designed to hold multiple line cards each with multiple inputs, but the differences are slight. Like repeaters, hubs do not examine the 802 addresses or use them in any way. A hub is shown in 

Fig. 4-

47(a)

. 

Figure 4-47. (a) A hub. (b) A bridge. (c) A switch. 

 

Now let us move up to the data link layer where we find bridges and switches. We just studied bridges at some length. A bridge connects two or more LANs, as shown in 

Fig. 4-47(b)

. When a frame arrives, software in the bridge extracts the destination address from the frame header 

246




and looks it up in a table to see where to send the frame. For Ethernet, this address is the 48-bit destination address shown in 

Fig. 4-17

. Like a hub, a modern bridge has line cards, usually for four or eight input lines of a certain type. A line card for Ethernet cannot handle, say, token ring frames, because it does not know where to find the destination address in the frame header. However, a bridge may have line cards for different network types and different speeds. With a bridge, each line is its own collision domain, in contrast to a hub. 

Switches are similar to bridges in that both route on frame addresses. In fact, many people uses the terms interchangeably. The main difference is that a switch is most often used to connect individual computers, as shown in 

Fig. 4-47(c)

. As a consequence, when host 

A

 in 

Fig. 

4-47(b)

 wants to send a frame to host 

B

, the bridge gets the frame but just discards it. In contrast, in 

Fig. 4-47(c)

, the switch must actively forward the frame from 

A

 to 

B

 because there is no other way for the frame to get there. Since each switch port usually goes to a single computer, switches must have space for many more line cards than do bridges intended to connect only LANs. Each line card provides buffer space for frames arriving on its ports. Since each port is its own collision domain, switches never lose frames to collisions. However, if frames come in faster than they can be retransmitted, the switch may run out of buffer space and have to start discarding frames. 

To alleviate this problem slightly, modern switches start forwarding frames as soon as the destination header field has come in, but before the rest of the frame has arrived (provided the output line is available, of course). These switches do not use store-and-forward switching. Sometimes they are referred to as 

cut-through switches

. Usually, cut-through is handled entirely in hardware, whereas bridges traditionally contained an actual CPU that did store-and-forward switching in software. But since all modern bridges and switches contain special integrated circuits for switching, the difference between a switch and bridge is more a marketing issue than a technical one. 

So far we have seen repeaters and hubs, which are quite similar, as well as bridges and switches, which are also very similar to each other. Now we move up to routers, which are different from all of the above. When a packet comes into a router, the frame header and trailer are stripped off and the packet located in the frame's payload field (shaded in 

Fig. 4-46

) is passed to the routing software. This software uses the packet header to choose an output line. For an IP packet, the packet header will contain a 32-bit (IPv4) or 128-bit (IPv6) address, but not a 48-bit 802 address. The routing software does not see the frame addresses and does not even know whether the packet came in on a LAN or a point-to-point line. We will study routers and routing in 

Chap. 5

. 

Up another layer we find transport gateways. These connect two computers that use different connection-oriented transport protocols. For example, suppose a computer using the connection-oriented TCP/IP protocol needs to talk to a computer using the connection-oriented ATM transport protocol. The transport gateway can copy the packets from one connection to the other, reformatting them as need be. 

Finally, application gateways understand the format and contents of the data and translate messages from one format to another. An e-mail gateway could translate Internet messages into SMS messages for mobile phones, for example. 

4.7.6 Virtual LANs 

In the early days of local area networking, thick yellow cables snaked through the cable ducts of many office buildings. Every computer they passed was plugged in. Often there were many cables, which were connected to a central backbone (as in 

Fig. 4-39

) or to a central hub. No thought was given to which computer belonged on which LAN. All the people in adjacent offices were put on the same LAN whether they belonged together or not. Geography trumped logic. 

247




With the advent of 10Base-T and hubs in the 1990s, all that changed. Buildings were rewired (at considerable expense) to rip out all the yellow garden hoses and install twisted pairs from every office to central wiring closets at the end of each corridor or in a central machine room, as illustrated in 

Fig. 4-48

. If the Vice President in Charge of Wiring was a visionary, category 5 twisted pairs were installed; if he was a bean counter, the existing (category 3) telephone wiring was used (only to be replaced a few years later when fast Ethernet emerged). 

Figure 4-48. A building with centralized wiring using hubs and a switch. 

 

With hubbed (and later, switched) Ethernet, it was often possible to configure LANs logically rather than physically. If a company wants 

k

 LANs, it buys 

k

 hubs. By carefully choosing which connectors to plug into which hubs, the occupants of a LAN can be chosen in a way that makes organizational sense, without too much regard to geography. Of course, if two people in the same department work in different buildings, they are probably going to be on different hubs and thus different LANs. Nevertheless, the situation is a lot better than having LAN membership entirely based on geography. 

Does it matter who is on which LAN? After all, in virtually all organizations, all the LANs are interconnected. In short, yes, it often matters. Network administrators like to group users on LANs to reflect the organizational structure rather than the physical layout of the building for a variety of reasons. One issue is security. Any network interface can be put in promiscuous mode, copying all the traffic that comes down the pipe. Many departments, such as research, patents, and accounting, have information that they do not want passed outside their department. In such a situation, putting all the people in a department on a single LAN and not letting any of that traffic off the LAN makes sense. Management does not like hearing that such an arrangement is impossible unless all the people in each department are located in adjacent offices with no interlopers. 

A second issue is load. Some LANs are more heavily used than others and it may be desirable to separate them at times. For example, if the folks in research are running all kinds of nifty experiments that sometimes get out of hand and saturate their LAN, the folks in accounting may not be enthusiastic about donating some of their capacity to help out. 

A third issue is broadcasting. Most LANs support broadcasting, and many upper-layer protocols use this feature extensively. For example, when a user wants to send a packet to an IP address 

x

, how does it know which MAC address to put in the frame? We will study this question in 

Chap. 5

, but briefly summarized, the answer is that it broadcasts a frame containing the question: Who owns IP address 

x

? Then it waits for an answer. And there are 

248




many more examples of where broadcasting is used. As more and more LANs get interconnected, the number of broadcasts passing each machine tends to increase linearly with the number of machines. 

Related to broadcasts is the problem that once in a while a network interface will break down and begin generating an endless stream of broadcast frames. The result of this 

broadcast storm

 is that (1) the entire LAN capacity is occupied by these frames, and (2) all the machines on all the interconnected LANs are crippled just processing and discarding all the frames being broadcast. 

At first it might appear that broadcast storms could be limited in scope by separating the LANs with bridges or switches, but if the goal is to achieve transparency (i.e., a machine can be moved to a different LAN across the bridge without anyone noticing it), then bridges have to forward broadcast frames. 

Having seen why companies might want multiple LANs with restricted scope, let us get back to the problem of decoupling the logical topology from the physical topology. Suppose that a user gets shifted within the company from one department to another without changing offices or changes offices without changing departments. With hubbed wiring, moving the user to the correct LAN means having the network administrator walk down to the wiring closet and pull the connector for the user's machine from one hub and put it into a new hub. 

In many companies, organizational changes occur all the time, meaning that system administrators spend a lot of time pulling out plugs and pushing them back in somewhere else. Also, in some cases, the change cannot be made at all because the twisted pair from the user's machine is too far from the correct hub (e.g., in the wrong building). 

In response to user requests for more flexibility, network vendors began working on a way to rewire buildings entirely in software. The resulting concept is called a 

VLAN

 (

Virtual LAN

) and has even been standardized by the 802 committee. It is now being deployed in many organizations. Let us now take a look at it. For additional information about VLANs, see (Breyer and Riley, 1999; and Seifert, 2000). 

VLANs are based on specially-designed VLAN-aware switches, although they may also have some hubs on the periphery, as in 

Fig. 4-48

. To set up a VLAN-based network, the network administrator decides how many VLANs there will be, which computers will be on which VLAN, and what the VLANs will be called. Often the VLANs are (informally) named by colors, since it is then possible to print color diagrams showing the physical layout of the machines, with the members of the red LAN in red, members of the green LAN in green, and so on. In this way, both the physical and logical layouts are visible in a single view. 

As an example, consider the four LANs of 

Fig. 4-49(a)

, in which eight of the machines belong to the G (gray) VLAN and seven of them belong to the W (white) VLAN. The four physical LANs are connected by two bridges, 

B1

 and 

B2

. If centralized twisted pair wiring is used, there might also be four hubs (not shown), but logically a multidrop cable and a hub are the same thing. Drawing it this way just makes the figure a little less cluttered. Also, the term ''bridge'' tends to be used nowadays mostly when there are multiple machines on each port, as in this figure, but otherwise, ''bridge'' and ''switch'' are essentially interchangeable. 

Fig. 4-49(b)

 shows the same machines and same VLANs using switches with a single computer on each port. 

Figure 4-49. (a) Four physical LANs organized into two VLANs, gray and white, by two bridges. (b) The same 15 machines organized into two VLANs by switches. 

249




 

To make the VLANs function correctly, configuration tables have to be set up in the bridges or switches. These tables tell which VLANs are accessible via which ports (lines). When a frame comes in from, say, the gray VLAN, it must be forwarded on all the ports marked G. This holds for ordinary (i.e., unicast) traffic as well as for multicast and broadcast traffic. 

Note that a port may be labeled with multiple VLAN colors. We see this most clearly in 

Fig. 4-

49(a)

. Suppose that machine 

A

 broadcasts a frame. Bridge 

B1

 receives the frame and sees that it came from a machine on the gray VLAN, so it forwards it on all ports labeled G (except the incoming port). Since 

B1

 has only two other ports and both of them are labeled G, the frame is sent to both of them. 

At 

B2

 the story is different. Here the bridge knows that there are no gray machines on LAN 4, so the frame is not forwarded there. It goes only to LAN 2. If one of the users on LAN 4 should change departments and be moved to the gray VLAN, then the tables inside 

B2

 have to be updated to relabel that port as GW instead of W. If machine 

F

 goes gray, then the port to LAN 2 has to be changed to G instead of GW. 

Now let us imagine that all the machines on both LAN 2 and LAN 4 become gray. Then not only do 

B2

's ports to LAN 2 and LAN 4 get marked G, but 

B1

's port to 

B2

 also has to change from GW to G since white frames arriving at 

B1

 from LANs 1 and 3 no longer have to be forwarded to 

B2

. In 

Fig. 4-49(b)

 the same situation holds, only here all the ports that go to a single machine are labeled with a single color because only one VLAN is out there. 

So far we have assumed that bridges and switches somehow know what color an incoming frame is. How do they know this? Three methods are in use, as follows: 

1. Every port is assigned a VLAN color. 

2. Every MAC address is assigned a VLAN color. 

3. Every layer 3 protocol or IP address is assigned a VLAN color. 

In the first method, each port is labeled with VLAN color. However, this method only works if all machines on a port belong to the same VLAN. In 

Fig. 4-49(a)

, this property holds for 

B1

 for the port to LAN 3 but not for the port to LAN 1. 

In the second method, the bridge or switch has a table listing the 48-bit MAC address of each machine connected to it along with the VLAN that machine is on. Under these conditions, it is possible to mix VLANs on a physical LAN, as in LAN 1 in 

Fig. 4-49(a)

. When a frame arrives, all the bridge or switch has to do is to extract the MAC address and look it up in a table to see which VLAN the frame came from. 

The third method is for the bridge or switch to examine the payload field of the frame, for example, to classify all IP machines as belonging to one VLAN and all AppleTalk machines as belonging to another. For the former, the IP address can also be used to identify the machine. 

250




This strategy is most useful when many machines are notebook computers that can be docked in any one of several places. Since each docking station has its own MAC address, just knowing which docking station was used does not say anything about which VLAN the notebook is on. 

The only problem with this approach is that it violates the most fundamental rule of networking: independence of the layers. It is none of the data link layer's business what is in the payload field. It should not be examining the payload and certainly not be making decisions based on the contents. A consequence of using this approach is that a change to the layer 3 protocol (for example, an upgrade from IPv4 to IPv6) suddenly causes the switches to fail. Unfortunately, switches that work this way are on the market. 

Of course, there is nothing wrong with routing based on IP addresses—nearly all of 

Chap. 5

 is devoted to IP routing—but mixing the layers is looking for trouble. A switch vendor might pooh-pooh this argument saying that its switches understand both IPv4 and IPv6, so everything is fine. But what happens when IPv7 happens? The vendor would probably say: Buy new switches, is that so bad? 

The IEEE 802.1Q Standard 

Some more thought on this subject reveals that what actually matters is the VLAN of the frame itself, not the VLAN of the sending machine. If there were some way to identify the VLAN in the frame header, then the need to inspect the payload would vanish. For a new LAN, such as 802.11 or 802.16, it would have been easy enough to just add a VLAN field in the header. In fact, the 

Connection Identifier

 field in 802.16 is somewhat similar in spirit to a VLAN identifier. But what to do about Ethernet, which is the dominant LAN, and does not have any spare fields lying around for the VLAN identifier? 

The IEEE 802 committee had this problem thrown into its lap in 1995. After much discussion, it did the unthinkable and changed the Ethernet header. The new format was published in IEEE standard 

802.1Q

, issued in 1998. The new format contains a VLAN tag; we will examine it shortly. Not surprisingly, changing something as well established as the Ethernet header is not entirely trivial. A few questions that come to mind are: 

1. Need we throw out several hundred million existing Ethernet cards? 

2. If not, who generates the new fields? 

3. What happens to frames that are already the maximum size? 

Of course, the 802 committee was (only too painfully) aware of these problems and had to come up with solutions, which it did. 

The key to the solution is to realize that the VLAN fields are only actually used by the bridges and switches and not by the user machines. Thus in 

Fig. 4-49

, it is not really essential that they are present on the lines going out to the end stations as long as they are on the line between the bridges or switches. Thus, to use VLANs, the bridges or switches have to be VLAN aware, but that was already a requirement. Now we are only introducing the additional requirement that they are 802.1Q aware, which new ones already are. 

As to throwing out all existing Ethernet cards, the answer is no. Remember that the 802.3 committee could not even get people to change the 

Type

 field into a 

Length

 field. You can imagine the reaction to an announcement that all existing Ethernet cards had to be thrown out. However, as new Ethernet cards come on the market, the hope is that they will be 802.1Q compliant and correctly fill in the VLAN fields. 

So if the originator does not generate the VLAN fields, who does? The answer is that the first VLAN-aware bridge or switch to touch a frame adds them and the last one down the road removes them. But how does it know which frame belongs to which VLAN? Well, the first 

251




bridge or switch could assign a VLAN number to a port, look at the MAC address, or (heaven forbid) examine the payload. Until Ethernet cards are all 802.1Q compliant, we are kind of back where we started. The real hope here is that all gigabit Ethernet cards will be 802.1Q compliant from the start and that as people upgrade to gigabit Ethernet, 802.1Q will be introduced automatically. As to the problem of frames longer than 1518 bytes, 802.1Q just raised the limit to 1522 bytes. 

During the transition process, many installations will have some legacy machines (typically classic or fast Ethernet) that are not VLAN aware and others (typically gigabit Ethernet) that are. This situation is illustrated in 

Fig. 4-50

, where the shaded symbols are VLAN aware and the empty ones are not. For simplicity, we assume that all the switches are VLAN aware. If this is not the case, the first VLAN-aware switch can add the tags based on MAC or IP addresses. 

Figure 4-50. Transition from legacy Ethernet to VLAN-aware Ethernet. The shaded symbols are VLAN aware. The empty ones are not. 

 

In this figure, VLAN-aware Ethernet cards generate tagged (i.e., 802.1Q) frames directly, and further switching uses these tags. To do this switching, the switches have to know which VLANs are reachable on each port, just as before. Knowing that a frame belongs to the gray VLAN does not help much until the switch knows which ports connect to machines on the gray VLAN. Thus, the switch needs a table indexed by VLAN telling which ports to use and whether they are VLAN aware or legacy. 

When a legacy PC sends a frame to a VLAN-aware switch, the switch builds a new tagged frame based on its knowledge of the sender's VLAN (using the port, MAC address, or IP address). From that point on, it no longer matters that the sender was a legacy machine. Similarly, a switch that needs to deliver a tagged frame to a legacy machine has to reformat the frame in the legacy format before delivering it. 

Now let us take a look at the 802.1Q frame format. It is shown in 

Fig. 4-51

. The only change is the addition of a pair of 2-byte fields. The first one is the 

VLAN protocol ID

. It always has the value 0x8100. Since this number is greater than 1500, all Ethernet cards interpret it as a type rather than a length. What a legacy card does with such a frame is moot since such frames are not supposed to be sent to legacy cards. 

Figure 4-51. The 802.3 (legacy) and 802.1Q Ethernet frame formats. 

252




 

The second 2-byte field contains three subfields. The main one is the 

VLAN identifier

, occupying the low-order 12 bits. This is what the whole thing is about—which VLAN does the frame belong to? The 3-bit 

Priority

 field has nothing to do with VLANs at all, but since changing the Ethernet header is a once-in-a-decade event taking three years and featuring a hundred people, why not put in some other good things while you are at it? This field makes it possible to distinguish hard real-time traffic from soft real-time traffic from time-insensitive traffic in order to provide better quality of service over Ethernet. It is needed for voice over Ethernet (although in all fairness, IP has had a similar field for a quarter of a century and nobody ever used it). 

The last bit, 

CFI

 (

Canonical Format Indicator

) should have been called the 

CEI

 (

Corporate Ego Indicator

). It was originally intended to indicate little-endian MAC addresses versus big-endian MAC addresses, but that use got lost in other controversies. Its presence now indicates that the payload contains a freeze-dried 802.5 frame that is hoping to find another 802.5 LAN at the destination while being carried by Ethernet in between. This whole arrangement, of course, has nothing whatsoever to do with VLANs. But standards' committee politics is not unlike regular politics: if you vote for my bit, I will vote for your bit. 

As we mentioned above, when a tagged frame arrives at a VLAN-aware switch, the switch uses the VLAN ID as an index into a table to find out which ports to send it on. But where does the table come from? If it is manually constructed, we are back to square zero: manual configuration of bridges. The beauty of the transparent bridge is that it is plug-and-play and does not require any manual configuration. It would be a terrible shame to lose that property. Fortunately, VLAN-aware bridges can also autoconfigure themselves based on observing the tags that come by. If a frame tagged as VLAN 4 comes in on port 3, then apparently some machine on port 3 is on VLAN 4. The 802.1Q standard explains how to build the tables dynamically, mostly by referencing appropriate portions of Perlman's algorithm standardized in 802.1D. 

Before leaving the subject of VLAN routing, it is worth making one last observation. Many people in the Internet and Ethernet worlds are fanatically in favor of connectionless networking and violently opposed to anything smacking of connections in the data link or network layers. Yet VLANs introduce something that is surprisingly similar to a connection. To use VLANs properly, each frame carries a new special identifier that is used as an index into a table inside the switch to look up where the frame is supposed to be sent. That is precisely what happens in connection-oriented networks. In connectionless networks, it is the destination address that is used for routing, not some kind of connection identifier. We will see more of this creeping connectionism in 

Chap. 5

. 

4.8 Summary 

Some networks have a single channel that is used for all communication. In these networks, the key design issue is the allocation of this channel among the competing stations wishing to use it. Numerous channel allocation algorithms have been devised. A summary of some of the more important channel allocation methods is given in 

Fig. 4-52

. 

253




Figure 4-52. Channel allocation methods and systems for a common channel. 

 

The simplest allocation schemes are FDM and TDM. These are efficient when the number of stations is small and fixed and the traffic is continuous. Both are widely used under these circumstances, for example, for dividing up the bandwidth on telephone trunks. 

When the number of stations is large and variable or the traffic is fairly bursty, FDM and TDM are poor choices. The ALOHA protocol, with and without slotting, has been proposed as an alternative. ALOHA and its many variants and derivatives have been widely discussed, analyzed, and used in real systems. 

When the state of the channel can be sensed, stations can avoid starting a transmission while another station is transmitting. This technique, carrier sensing, has led to a variety of protocols that can be used on LANs and MANs. 

A class of protocols that eliminates contention altogether, or at least reduce it considerably, is well known. Binary countdown completely eliminates contention. The tree walk protocol reduces it by dynamically dividing the stations into two disjoint groups, one of which is permitted to transmit and one of which is not. It tries to make the division in such a way that only one station that is ready to send is permitted to do so. 

Wireless LANs have their own problems and solutions. The biggest problem is caused by hidden stations, so CSMA does not work. One class of solutions, typified by MACA and MACAW, attempts to stimulate transmissions around the destination, to make CSMA work better. Frequency hopping spread spectrum and direct sequence spread spectrum are also used. IEEE 802.11 combines CSMA and MACAW to produce CSMA/CA. 

Ethernet is the dominant form of local area networking. It uses CSMA/CD for channel allocation. Older versions used a cable that snaked from machine to machine, but now twisted pairs to hubs and switches are most common. Speeds have risen from 10 Mbps to 1 Gbps and are still rising. 

254




Wireless LANs are becoming common, with 802.11 dominating the field. Its physical layer allows five different transmission modes, including infrared, various spread spectrum schemes, and a multichannel FDM system. It can operate with a base station in each cell, but it can also operate without one. The protocol is a variant of MACAW, with virtual carrier sensing. 

Wireless MANs are starting to appear. These are broadband systems that use radio to replace the last mile on telephone connections. Traditional narrowband modulation techniques are used. Quality of service is important, with the 802.16 standard defining four classes (constant bit rate, two variable bit rate, and one best efforts). 

The Bluetooth system is also wireless but aimed more at the desktop, for connecting headsets and other peripherals to computers without wires. It is also intended to connect peripherals, such as fax machines, to mobile telephones. Like 801.11, it uses frequency hopping spread spectrum in the ISM band. Due to the expected noise level of many environments and need for real-time interaction, elaborate forward error correction is built into its various protocols. 

With so many different LANs, a way is needed to interconnect them all. Bridges and switches are used for this purpose. The spanning tree algorithm is used to build plug-and-play bridges. A new development in the LAN interconnection world is the VLAN, which separates the logical topology of the LANs from their physical topology. A new format for Ethernet frames (802.1Q) has been introduced to ease the introduction of VLANs into organizations. 

Problems 

1. For this problem, use a formula from this chapter, but first state the formula. Frames arrive randomly at a 100-Mbps channel for transmission. If the channel is busy when a frame arrives, it waits its turn in a queue. Frame length is exponentially distributed with a mean of 10,000 bits/frame. For each of the following frame arrival rates, give the delay experienced by the average frame, including both queueing time and transmission time. 

a. (a) 90 frames/sec. 

b. (b) 900 frames/sec. 

c. (c) 9000 frames/sec. 

2. A group of 

N

 stations share a 56-kbps pure ALOHA channel. Each station outputs a 1000-bit frame on an average of once every 100 sec, even if the previous one has not yet been sent (e.g., the stations can buffer outgoing frames). What is the maximum value of 

N

? 

3. Consider the delay of pure ALOHA versus slotted ALOHA at low load. Which one is less? Explain your answer. 

4. Ten thousand airline reservation stations are competing for the use of a single slotted ALOHA channel. The average station makes 18 requests/hour. A slot is 125 µsec. What is the approximate total channel load? 

5. A large population of ALOHA users manages to generate 50 requests/sec, including both originals and retransmissions. Time is slotted in units of 40 msec. 

a. (a) What is the chance of success on the first attempt? 

b. (b) What is the probability of exactly 

k

 collisions and then a success? 

c. (c) What is the expected number of transmission attempts needed? 

6. Measurements of a slotted ALOHA channel with an infinite number of users show that 10 percent of the slots are idle. 

a. (a) What is the channel load, 

G

? 

b. (b) What is the throughput? 

c. (c) Is the channel underloaded or overloaded? 

7. In an infinite-population slotted ALOHA system, the mean number of slots a station waits between a collision and its retransmission is 4. Plot the delay versus throughput curve for this system. 

8. How long does a station, 

s

, have to wait in the worst case before it can start transmitting its frame over a LAN that uses 

255




a. (a) the basic bit-map protocol? 

b. (b) Mok and Ward's protocol with permuting virtual station numbers? 

9. A LAN uses Mok and Ward's version of binary countdown. At a certain instant, the ten stations have the virtual station numbers 8, 2, 4, 5, 1, 7, 3, 6, 9, and 0. The next three stations to send are 4, 3, and 9, in that order. What are the new virtual station numbers after all three have finished their transmissions? 

10. Sixteen stations, numbered 1 through 16, are contending for the use of a shared channel by using the adaptive tree walk protocol. If all the stations whose addresses are prime numbers suddenly become ready at once, how many bit slots are needed to resolve the contention? 

11. A collection of 2

n

 stations uses the adaptive tree walk protocol to arbitrate access to a shared cable. At a certain instant, two of them become ready. What are the minimum, maximum, and mean number of slots to walk the tree if 2

n

 

1? 

12. The wireless LANs that we studied used protocols such as MACA instead of using CSMA/CD. Under what conditions, if any, would it be possible to use CSMA/CD instead? 

13. What properties do the WDMA and GSM channel access protocols have in common? See 

Chap. 2

 for GSM. 

14. Six stations, 

A

 through 

F

, communicate using the MACA protocol. Is it possible that two transmissions take place simultaneously? Explain your answer. 

15. A seven-story office building has 15 adjacent offices per floor. Each office contains a wall socket for a terminal in the front wall, so the sockets form a rectangular grid in the vertical plane, with a separation of 4 m between sockets, both horizontally and vertically. Assuming that it is feasible to run a straight cable between any pair of sockets, horizontally, vertically, or diagonally, how many meters of cable are needed to connect all sockets using 

a. (a) a star configuration with a single router in the middle? 

b. (b) an 802.3 LAN? 

16. What is the baud rate of the standard 10-Mbps Ethernet? 

17. Sketch the Manchester encoding for the bit stream: 0001110101. 

18. Sketch the differential Manchester encoding for the bit stream of the previous problem. Assume the line is initially in the low state. 

19. A 1-km-long, 10-Mbps CSMA/CD LAN (not 802.3) has a propagation speed of 200 m

/

µsec

.

 Repeaters are not allowed in this system. Data frames are 256 bits long, including 32 bits of header, checksum, and other overhead. The first bit slot after a successful transmission is reserved for the receiver to capture the channel in order to send a 32-bit acknowledgement frame. What is the effective data rate, excluding overhead, assuming that there are no collisions? 

20. Two CSMA/CD stations are each trying to transmit long (multiframe) files. After each frame is sent, they contend for the channel, using the binary exponential backoff algorithm. What is the probability that the contention ends on round 

k

, and what is the mean number of rounds per contention period? 

21. Consider building a CSMA/CD network running at 1 Gbps over a 1-km cable with no repeaters. The signal speed in the cable is 200,000 km/sec. What is the minimum frame size? 

22. An IP packet to be transmitted by Ethernet is 60 bytes long, including all its headers. If LLC is not in use, is padding needed in the Ethernet frame, and if so, how many bytes? 

23. Ethernet frames must be at least 64 bytes long to ensure that the transmitter is still going in the event of a collision at the far end of the cable. Fast Ethernet has the same 64-byte minimum frame size but can get the bits out ten times faster. How is it possible to maintain the same minimum frame size? 

24. Some books quote the maximum size of an Ethernet frame as 1518 bytes instead of 1500 bytes. Are they wrong? Explain your answer. 

25. The 1000Base-SX specification states that the clock shall run at 1250 MHz, even though gigabit Ethernet is only supposed to deliver 1 Gbps. Is this higher speed to provide for an extra margin of safety? If not, what is going on here? 

26. How many frames per second can gigabit Ethernet handle? Think carefully and take into account all the relevant cases. 

Hint

: the fact that it is 

gigabit

 Ethernet matters. 

256




27. Name two networks that allow frames to be packed back-to-back. Why is this feature worth having? 

28. In 

Fig. 4-27

, four stations, 

A

, 

B

, 

C

, and 

D

, are shown. Which of the last two stations do you think is closest to 

A

 and why? 

29. Suppose that an 11-Mbps 802.11b LAN is transmitting 64-byte frames back-to-back over a radio channel with a bit error rate of 10

-7

. How many frames per second will be damaged on average? 

30. An 802.16 network has a channel width of 20 MHz. How many bits/sec can be sent to a subscriber station? 

31. IEEE 802.16 supports four service classes. Which service class is the best choice for sending uncompressed video? 

32. Give two reasons why networks might use an error-correcting code instead of error detection and retransmission. 

33. From 

Fig. 4-35

, we see that a Bluetooth device can be in two piconets at the same time. Is there any reason why one device cannot be the master in both of them at the same time? 

34. 

Figure 4-25

 shows several physical layer protocols. Which of these is closest to the Bluetooth physical layer protocol? What is the biggest difference between the two? 

35. Bluetooth supports two types of links between a master and a slave. What are they and what is each one used for? 

36. Beacon frames in the frequency hopping spread spectrum variant of 802.11 contain the dwell time. Do you think the analogous beacon frames in Bluetooth also contain the dwell time? Discuss your answer. 

37. Consider the interconnected LANs showns in 

Fig. 4-44

. Assume that hosts 

a

 and 

b

 are on LAN 1, 

c

 is on LAN 2, and 

d

 is on LAN 8. Initially, hash tables in all bridges are empty and the spanning tree shown in 

Fig 4-44(b)

 is used. Show how the hash tables of different bridges change after each of the following events happen in sequence, first (a) then (b) and so on. 

a. (a) 

a

 sends to 

d

. 

b. (b) 

c

 sends to 

a

. 

c. (c) 

d

 sends to 

c

. 

d. (d) 

d

 moves to LAN 6. 

e. (e) 

d

 sends to 

a

. 

38. One consequence of using a spanning tree to forward frames in an extended LAN is that some bridges may not participate at all in forwarding frames. Identify three such bridges in 

Fig. 4-44

. Is there any reason for keeping these bridges, even though they are not used for forwarding? 

39. Imagine that a switch has line cards for four input lines. It frequently happens that a frame arriving on one of the lines has to exit on another line on the same card. What choices is the switch designer faced with as a result of this situation? 

40. A switch designed for use with fast Ethernet has a backplane that can move 10 Gbps. How many frames/sec can it handle in the worst case? 

41. Consider the network of 

Fig. 4-49(a)

. If machine 

J

 were to suddenly become white, would any changes be needed to the labeling? If so, what? 

42. Briefly describe the difference between store-and-forward and cut-through switches. 

43. Store-and-forward switches have an advantage over cut-through switches with respect to damaged frames. Explain what it is. 

44. To make VLANs work, configuration tables are needed in the switches and bridges. What if the VLANs of 

Fig. 4-49(a)

 use hubs rather than multidrop cables? Do the hubs need configuration tables, too? Why or why not? 

45. In 

Fig. 4-50

 the switch in the legacy end domain on the right is a VLAN-aware switch. Would it be possible to use a legacy switch there? If so, how would that work? If not, why not? 

46. Write a program to simulate the behavior of the CSMA/CD protocol over Ethernet when there are 

N

 stations ready to transmit while a frame is being transmitted. Your program should report the times when each station successfully starts sending its frame. Assume that a clock tick occurs once every slot time (51.2 microseconds) and a collision 

257




detection and sending of jamming sequence takes one slot time. All frames are the maximum length allowed. 

 

258




Chapter 5. The Network Layer 

The network layer is concerned with getting packets from the source all the way to the destination. Getting to the destination may require making many hops at intermediate routers along the way. This function clearly contrasts with that of the data link layer, which has the more modest goal of just moving frames from one end of a wire to the other. Thus, the network layer is the lowest layer that deals with end-to-end transmission. 

To achieve its goals, the network layer must know about the topology of the communication subnet (i.e., the set of all routers) and choose appropriate paths through it. It must also take care to choose routes to avoid overloading some of the communication lines and routers while leaving others idle. Finally, when the source and destination are in different networks, new problems occur. It is up to the network layer to deal with them. In this chapter we will study all these issues and illustrate them, primarily using the Internet and its network layer protocol, IP, although wireless networks will also be addressed. 

5.1 Network Layer Design Issues 

In the following sections we will provide an introduction to some of the issues that the designers of the network layer must grapple with. These issues include the service provided to the transport layer and the internal design of the subnet. 

5.1.1 Store-and-Forward Packet Switching 

But before starting to explain the details of the network layer, it is probably worth restating the context in which the network layer protocols operate. This context can be seen in 

Fig. 5-1

. The major components of the system are the carrier's equipment (routers connected by transmission lines), shown inside the shaded oval, and the customers' equipment, shown outside the oval. Host 

H1

 is directly connected to one of the carrier's routers, 

A

, by a leased line. In contrast, 

H2

 is on a LAN with a router, 

F

, owned and operated by the customer. This router also has a leased line to the carrier's equipment. We have shown 

F

 as being outside the oval because it does not belong to the carrier, but in terms of construction, software, and protocols, it is probably no different from the carrier's routers. Whether it belongs to the subnet is arguable, but for the purposes of this chapter, routers on customer premises are considered part of the subnet because they run the same algorithms as the carrier's routers (and our main concern here is algorithms). 

Figure 5-1. The environment of the network layer protocols. 

 

This equipment is used as follows. A host with a packet to send transmits it to the nearest router, either on its own LAN or over a point-to-point link to the carrier. The packet is stored there until it has fully arrived so the checksum can be verified. Then it is forwarded to the next 

259




router along the path until it reaches the destination host, where it is delivered. This mechanism is store-and-forward packet switching, as we have seen in previous chapters. 

5.1.2 Services Provided to the Transport Layer 

The network layer provides services to the transport layer at the network layer/transport layer interface. An important question is what kind of services the network layer provides to the transport layer. The network layer services have been designed with the following goals in mind. 

1. The services should be independent of the router technology. 

2. The transport layer should be shielded from the number, type, and topology of the routers present. 

3. The network addresses made available to the transport layer should use a uniform numbering plan, even across LANs and WANs. 

Given these goals, the designers of the network layer have a lot of freedom in writing detailed specifications of the services to be offered to the transport layer. This freedom often degenerates into a raging battle between two warring factions. The discussion centers on whether the network layer should provide connection-oriented service or connectionless service. 

One camp (represented by the Internet community) argues that the routers' job is moving packets around and nothing else. In their view (based on 30 years of actual experience with a real, working computer network), the subnet is inherently unreliable, no matter how it is designed. Therefore, the hosts should accept the fact that the network is unreliable and do error control (i.e., error detection and correction) and flow control themselves. 

This viewpoint leads quickly to the conclusion that the network service should be connectionless, with primitives SEND PACKET and RECEIVE PACKET and little else. In particular, no packet ordering and flow control should be done, because the hosts are going to do that anyway, and there is usually little to be gained by doing it twice. Furthermore, each packet must carry the full destination address, because each packet sent is carried independently of its predecessors, if any. 

The other camp (represented by the telephone companies) argues that the subnet should provide a reliable, connection-oriented service. They claim that 100 years of successful experience with the worldwide telephone system is an excellent guide. In this view, quality of service is the dominant factor, and without connections in the subnet, quality of service is very difficult to achieve, especially for real-time traffic such as voice and video. 

These two camps are best exemplified by the Internet and ATM. The Internet offers connectionless network-layer service; ATM networks offer connection-oriented network-layer service. However, it is interesting to note that as quality-of-service guarantees are becoming more and more important, the Internet is evolving. In particular, it is starting to acquire properties normally associated with connection-oriented service, as we will see later. Actually, we got an inkling of this evolution during our study of VLANs in 

Chap. 4

. 

5.1.3 Implementation of Connectionless Service 

Having looked at the two classes of service the network layer can provide to its users, it is time to see how this layer works inside. Two different organizations are possible, depending on the type of service offered. If connectionless service is offered, packets are injected into the subnet individually and routed independently of each other. No advance setup is needed. In this context, the packets are frequently called 

datagrams

 (in analogy with telegrams) and the subnet is called a 

datagram subnet

. If connection-oriented service is used, a path from the 

260




source router to the destination router must be established before any data packets can be sent. This connection is called a 

VC

 (

virtual circuit

), in analogy with the physical circuits set up by the telephone system, and the subnet is called a 

virtual-circuit subnet

. In this section we will examine datagram subnets; in the next one we will examine virtual-circuit subnets. 

Let us now see how a datagram subnet works. Suppose that the process 

P1

 in 

Fig. 5-2

 has a long message for 

P2

. It hands the message to the transport layer with instructions to deliver it to process 

P2

 on host 

H2

. The transport layer code runs on 

H1

, typically within the operating system. It prepends a transport header to the front of the message and hands the result to the network layer, probably just another procedure within the operating system. 

Figure 5-2. Routing within a datagram subnet. 

 

Let us assume that the message is four times longer than the maximum packet size, so the network layer has to break it into four packets, 1, 2, 3, and 4 and sends each of them in turn to router 

A

 using some point-to-point protocol, for example, PPP. At this point the carrier takes over. Every router has an internal table telling it where to send packets for each possible destination. Each table entry is a pair consisting of a destination and the outgoing line to use for that destination. Only directly-connected lines can be used. For example, in 

Fig. 5-2

, 

A

 has only two outgoing lines—to 

B

 and 

C

—so every incoming packet must be sent to one of these routers, even if the ultimate destination is some other router. 

A

's initial routing table is shown in the figure under the label ''initially.'' 

As they arrived at 

A

, packets 1, 2, and 3 were stored briefly (to verify their checksums). Then each was forwarded to 

C

 according to 

A

's table. Packet 1 was then forwarded to 

E

 and then to 

F

. When it got to 

F

, it was encapsulated in a data link layer frame and sent to 

H2

 over the LAN. Packets 2 and 3 follow the same route. 

However, something different happened to packet 4. When it got to 

A

 it was sent to router 

B

, even though it is also destined for 

F

. For some reason, 

A

 decided to send packet 4 via a different route than that of the first three. Perhaps it learned of a traffic jam somewhere along the 

ACE

 path and updated its routing table, as shown under the label ''later.'' The algorithm that manages the tables and makes the routing decisions is called the 

routing algorithm

. Routing algorithms are one of the main things we will study in this chapter. 

261




5.1.4 Implementation of Connection-Oriented Service 

For connection-oriented service, we need a virtual-circuit subnet. Let us see how that works. The idea behind virtual circuits is to avoid having to choose a new route for every packet sent, as in 

Fig. 5-2

. Instead, when a connection is established, a route from the source machine to the destination machine is chosen as part of the connection setup and stored in tables inside the routers. That route is used for all traffic flowing over the connection, exactly the same way that the telephone system works. When the connection is released, the virtual circuit is also terminated. With connection-oriented service, each packet carries an identifier telling which virtual circuit it belongs to. 

As an example, consider the situation of 

Fig. 5-3

. Here, host 

H1

 has established connection 1 with host 

H2

. It is remembered as the first entry in each of the routing tables. The first line of 

A

's table says that if a packet bearing connection identifier 1 comes in from 

H1

, it is to be sent to router 

C

 and given connection identifier 1. Similarly, the first entry at 

C

 routes the packet to 

E

, also with connection identifier 1. 

Figure 5-3. Routing within a virtual-circuit subnet. 

 

Now let us consider what happens if 

H3

 also wants to establish a connection to 

H2

. It chooses connection identifier 1 (because it is initiating the connection and this is its only connection) and tells the subnet to establish the virtual circuit. This leads to the second row in the tables. Note that we have a conflict here because although 

A

 can easily distinguish connection 1 packets from 

H1

 from connection 1 packets from 

H3

, 

C

 cannot do this. For this reason, 

A

 assigns a different connection identifier to the outgoing traffic for the second connection. Avoiding conflicts of this kind is why routers need the ability to replace connection identifiers in outgoing packets. In some contexts, this is called label switching. 

5.1.5 Comparison of Virtual-Circuit and Datagram Subnets 

Both virtual circuits and datagrams have their supporters and their detractors. We will now attempt to summarize the arguments both ways. The major issues are listed in 

Fig. 5-4

, although purists could probably find a counterexample for everything in the figure. 

Figure 5-4. Comparison of datagram and virtual-circuit subnets. 

262




 

Inside the subnet, several trade-offs exist between virtual circuits and datagrams. One trade-off is between router memory space and bandwidth. Virtual circuits allow packets to contain circuit numbers instead of full destination addresses. If the packets tend to be fairly short, a full destination address in every packet may represent a significant amount of overhead and hence, wasted bandwidth. The price paid for using virtual circuits internally is the table space within the routers. Depending upon the relative cost of communication circuits versus router memory, one or the other may be cheaper. 

Another trade-off is setup time versus address parsing time. Using virtual circuits requires a setup phase, which takes time and consumes resources. However, figuring out what to do with a data packet in a virtual-circuit subnet is easy: the router just uses the circuit number to index into a table to find out where the packet goes. In a datagram subnet, a more complicated lookup procedure is required to locate the entry for the destination. 

Yet another issue is the amount of table space required in router memory. A datagram subnet needs to have an entry for every possible destination, whereas a virtual-circuit subnet just needs an entry for each virtual circuit. However, this advantage is somewhat illusory since connection setup packets have to be routed too, and they use destination addresses, the same as datagrams do. 

Virtual circuits have some advantages in guaranteeing quality of service and avoiding congestion within the subnet because resources (e.g., buffers, bandwidth, and CPU cycles) can be reserved in advance, when the connection is established. Once the packets start arriving, the necessary bandwidth and router capacity will be there. With a datagram subnet, congestion avoidance is more difficult. 

For transaction processing systems (e.g., stores calling up to verify credit card purchases), the overhead required to set up and clear a virtual circuit may easily dwarf the use of the circuit. If the majority of the traffic is expected to be of this kind, the use of virtual circuits inside the subnet makes little sense. On the other hand, permanent virtual circuits, which are set up manually and last for months or years, may be useful here. 

Virtual circuits also have a vulnerability problem. If a router crashes and loses its memory, even if it comes back up a second later, all the virtual circuits passing through it will have to be aborted. In contrast, if a datagram router goes down, only those users whose packets were queued in the router at the time will suffer, and maybe not even all those, depending upon whether they have already been acknowledged. The loss of a communication line is fatal to 

263




virtual circuits using it but can be easily compensated for if datagrams are used. Datagrams also allow the routers to balance the traffic throughout the subnet, since routes can be changed partway through a long sequence of packet transmissions. 

5.2 Routing Algorithms 

The main function of the network layer is routing packets from the source machine to the destination machine. In most subnets, packets will require multiple hops to make the journey. The only notable exception is for broadcast networks, but even here routing is an issue if the source and destination are not on the same network. The algorithms that choose the routes and the data structures that they use are a major area of network layer design. 

The 

routing algorithm

 is that part of the network layer software responsible for deciding which output line an incoming packet should be transmitted on. If the subnet uses datagrams internally, this decision must be made anew for every arriving data packet since the best route may have changed since last time. If the subnet uses virtual circuits internally, routing decisions are made only when a new virtual circuit is being set up. Thereafter, data packets just follow the previously-established route. The latter case is sometimes called 

session routing

 because a route remains in force for an entire user session (e.g., a login session at a terminal or a file transfer). 

It is sometimes useful to make a distinction between routing, which is making the decision which routes to use, and forwarding, which is what happens when a packet arrives. One can think of a router as having two processes inside it. One of them handles each packet as it arrives, looking up the outgoing line to use for it in the routing tables. This process is 

forwarding

. The other process is responsible for filling in and updating the routing tables. That is where the routing algorithm comes into play. 

Regardless of whether routes are chosen independently for each packet or only when new connections are established, certain properties are desirable in a routing algorithm: correctness, simplicity, robustness, stability, fairness, and optimality. Correctness and simplicity hardly require comment, but the need for robustness may be less obvious at first. Once a major network comes on the air, it may be expected to run continuously for years without systemwide failures. During that period there will be hardware and software failures of all kinds. Hosts, routers, and lines will fail repeatedly, and the topology will change many times. The routing algorithm should be able to cope with changes in the topology and traffic without requiring all jobs in all hosts to be aborted and the network to be rebooted every time some router crashes. 

Stability is also an important goal for the routing algorithm. There exist routing algorithms that never converge to equilibrium, no matter how long they run. A stable algorithm reaches equilibrium and stays there. Fairness and optimality may sound obvious—surely no reasonable person would oppose them—but as it turns out, they are often contradictory goals. As a simple example of this conflict, look at 

Fig. 5-5

. Suppose that there is enough traffic between 

A

 and 

A

', between 

B

 and 

B

', and between 

C

 and 

C

' to saturate the horizontal links. To maximize the total flow, the 

X

 to 

X

' traffic should be shut off altogether. Unfortunately, 

X

 and 

X

' may not see it that way. Evidently, some compromise between global efficiency and fairness to individual connections is needed. 

Figure 5-5. Conflict between fairness and optimality. 

264




 

Before we can even attempt to find trade-offs between fairness and optimality, we must decide what it is we seek to optimize. Minimizing mean packet delay is an obvious candidate, but so is maximizing total network throughput. Furthermore, these two goals are also in conflict, since operating any queueing system near capacity implies a long queueing delay. As a compromise, many networks attempt to minimize the number of hops a packet must make, because reducing the number of hops tends to improve the delay and also reduce the amount of bandwidth consumed, which tends to improve the throughput as well. 

Routing algorithms can be grouped into two major classes: nonadaptive and adaptive. 

Nonadaptive algorithms

 do not base their routing decisions on measurements or estimates of the current traffic and topology. Instead, the choice of the route to use to get from 

I

 to 

J

 (for all 

I

 and 

J

) is computed in advance, off-line, and downloaded to the routers when the network is booted. This procedure is sometimes called 

static routing

. 

Adaptive algorithms

, in contrast, change their routing decisions to reflect changes in the topology, and usually the traffic as well. Adaptive algorithms differ in where they get their information (e.g., locally, from adjacent routers, or from all routers), when they change the routes (e.g., every ?

T

 sec, when the load changes or when the topology changes), and what metric is used for optimization (e.g., distance, number of hops, or estimated transit time). In the following sections we will discuss a variety of routing algorithms, both static and dynamic. 

5.2.1 The Optimality Principle 

Before we get into specific algorithms, it may be helpful to note that one can make a general statement about optimal routes without regard to network topology or traffic. This statement is known as the 

optimality principle

. It states that if router 

J

 is on the optimal path from router 

I

 to router 

K

, then the optimal path from 

J

 to 

K

 also falls along the same route. To see this, call the part of the route from 

I

 to 

Jr

1

 and the rest of the route 

r

2

.

 If a route better than 

r

2

 existed from 

J

 to 

K

, it could be concatenated with 

r

1

 to improve the route from 

I

 to 

K

, contradicting our statement that 

r

1

r

2

 is optimal. 

As a direct consequence of the optimality principle, we can see that the set of optimal routes from all sources to a given destination form a tree rooted at the destination. Such a tree is called a 

sink tree

 and is illustrated in 

Fig. 5-6

, where the distance metric is the number of hops. Note that a sink tree is not necessarily unique; other trees with the same path lengths may exist. The goal of all routing algorithms is to discover and use the sink trees for all routers. 

Figure 5-6. (a) A subnet. (b) A sink tree for router 

B

. 

265




 

Since a sink tree is indeed a tree, it does not contain any loops, so each packet will be delivered within a finite and bounded number of hops. In practice, life is not quite this easy. Links and routers can go down and come back up during operation, so different routers may have different ideas about the current topology. Also, we have quietly finessed the issue of whether each router has to individually acquire the information on which to base its sink tree computation or whether this information is collected by some other means. We will come back to these issues shortly. Nevertheless, the optimality principle and the sink tree provide a benchmark against which other routing algorithms can be measured. 

5.2.2 Shortest Path Routing 

Let us begin our study of feasible routing algorithms with a technique that is widely used in many forms because it is simple and easy to understand. The idea is to build a graph of the subnet, with each node of the graph representing a router and each arc of the graph representing a communication line (often called a link). To choose a route between a given pair of routers, the algorithm just finds the shortest path between them on the graph. 

The concept of a 

shortest path

 deserves some explanation. One way of measuring path length is the number of hops. Using this metric, the paths 

ABC

 and 

ABE

 in 

Fig. 5-7

 are equally long. Another metric is the geographic distance in kilometers, in which case 

ABC

 is clearly much longer than 

ABE

 (assuming the figure is drawn to scale). 

Figure 5-7. The first five steps used in computing the shortest path from 

A

 to 

D

. The arrows indicate the working node. 

266




 

However, many other metrics besides hops and physical distance are also possible. For example, each arc could be labeled with the mean queueing and transmission delay for some standard test packet as determined by hourly test runs. With this graph labeling, the shortest path is the fastest path rather than the path with the fewest arcs or kilometers. 

In the general case, the labels on the arcs could be computed as a function of the distance, bandwidth, average traffic, communication cost, mean queue length, measured delay, and other factors. By changing the weighting function, the algorithm would then compute the ''shortest'' path measured according to any one of a number of criteria or to a combination of criteria. 

Several algorithms for computing the shortest path between two nodes of a graph are known. This one is due to Dijkstra (1959). Each node is labeled (in parentheses) with its distance from the source node along the best known path. Initially, no paths are known, so all nodes are labeled with infinity. As the algorithm proceeds and paths are found, the labels may change, reflecting better paths. A label may be either tentative or permanent. Initially, all labels are tentative. When it is discovered that a label represents the shortest possible path from the source to that node, it is made permanent and never changed thereafter. 

To illustrate how the labeling algorithm works, look at the weighted, undirected graph of 

Fig. 

5-7(a)

, where the weights represent, for example, distance. We want to find the shortest path from 

A

 to 

D

. We start out by marking node 

A

 as permanent, indicated by a filled-in circle. Then we examine, in turn, each of the nodes adjacent to 

A

 (the working node), relabeling each one with the distance to 

A

. Whenever a node is relabeled, we also label it with the node from which the probe was made so that we can reconstruct the final path later. Having examined each of the nodes adjacent to 

A

, we examine all the tentatively labeled nodes in the whole graph and make the one with the smallest label permanent, as shown in 

Fig. 5-7(b)

. This one becomes the new working node. 

We now start at 

B

 and examine all nodes adjacent to it. If the sum of the label on 

B

 and the distance from 

B

 to the node being considered is less than the label on that node, we have a shorter path, so the node is relabeled. 

267




After all the nodes adjacent to the working node have been inspected and the tentative labels changed if possible, the entire graph is searched for the tentatively-labeled node with the smallest value. This node is made permanent and becomes the working node for the next round. 

Figure 5-7

 shows the first five steps of the algorithm. 

To see why the algorithm works, look at 

Fig. 5-7(c)

. At that point we have just made 

E

 permanent. Suppose that there were a shorter path than 

ABE

, say 

AXYZE

. There are two possibilities: either node 

Z

 has already been made permanent, or it has not been. If it has, then 

E

 has already been probed (on the round following the one when 

Z

 was made permanent), so the 

AXYZE

 path has not escaped our attention and thus cannot be a shorter path. 

Now consider the case where 

Z

 is still tentatively labeled. Either the label at 

Z

 is greater than or equal to that at 

E

, in which case 

AXYZE

 cannot be a shorter path than 

ABE

, or it is less than that of 

E

, in which case 

Z

 and not 

E

 will become permanent first, allowing 

E

 to be probed from 

Z

. 

This algorithm is given in 

Fig. 5-8

. The global variables 

n

 and 

dist

 describe the graph and are initialized before 

shortest

_

path

 is called. The only difference between the program and the algorithm described above is that in 

Fig. 5-8

, we compute the shortest path starting at the terminal node, 

t

, rather than at the source node, 

s

. Since the shortest path from 

t

 to 

s

 in an undirected graph is the same as the shortest path from 

s

 to 

t

, it does not matter at which end we begin (unless there are several shortest paths, in which case reversing the search might discover a different one). The reason for searching backward is that each node is labeled with its predecessor rather than its successor. When the final path is copied into the output variable, 

path

, the path is thus reversed. By reversing the search, the two effects cancel, and the answer is produced in the correct order. 

Figure 5-8. Dijkstra's algorithm to compute the shortest path through a graph. 

268




 

5.2.3 Flooding 

Another static algorithm is 

flooding

, in which every incoming packet is sent out on every outgoing line except the one it arrived on. Flooding obviously generates vast numbers of duplicate packets, in fact, an infinite number unless some measures are taken to damp the process. One such measure is to have a hop counter contained in the header of each packet, which is decremented at each hop, with the packet being discarded when the counter reaches zero. Ideally, the hop counter should be initialized to the length of the path from source to destination. If the sender does not know how long the path is, it can initialize the counter to the worst case, namely, the full diameter of the subnet. 

An alternative technique for damming the flood is to keep track of which packets have been flooded, to avoid sending them out a second time. achieve this goal is to have the source router put a sequence number in each packet it receives from its hosts. Each router then needs a list per source router telling which sequence numbers originating at that source have already been seen. If an incoming packet is on the list, it is not flooded. 

269




To prevent the list from growing without bound, each list should be augmented by a counter, 

k

, meaning that all sequence numbers through 

k

 have been seen. When a packet comes in, it is easy to check if the packet is a duplicate; if so, it is discarded. Furthermore, the full list below 

k

 is not needed, since 

k

 effectively summarizes it. 

A variation of flooding that is slightly more practical is 

selective flooding

.In this algorithm the routers do not send every incoming packet out on every line, only on those lines that are going approximately in the right direction. There is usually little point in sending a westbound packet on an eastbound line unless the topology is extremely peculiar and the router is sure of this fact. 

Flooding is not practical in most applications, but it does have some uses. For example, in military applications, where large numbers of routers may be blown to bits at any instant, the tremendous robustness of flooding is highly desirable. In distributed database applications, it is sometimes necessary to update all the databases concurrently, in which case flooding can be useful. In wireless networks, all messages transmitted by a station can be received by all other stations within its radio range, which is, in fact, flooding, and some algorithms utilize this property. A fourth possible use of flooding is as a metric against which other routing algorithms can be compared. Flooding always chooses the shortest path because it chooses every possible path in parallel. Consequently, no other algorithm can produce a shorter delay (if we ignore the overhead generated by the flooding process itself). 

5.2.4 Distance Vector Routing 

Modern computer networks generally use dynamic routing algorithms rather than the static ones described above because static algorithms do not take the current network load into account. Two dynamic algorithms in particular, distance vector routing and link state routing, are the most popular. In this section we will look at the former algorithm. In the following section we will study the latter algorithm. 

Distance vector routing

 algorithms operate by having each router maintain a table (i.e, a vector) giving the best known distance to each destination and which line to use to get there. These tables are updated by exchanging information with the neighbors. 

The distance vector routing algorithm is sometimes called by other names, most commonly the distributed 

Bellman-Ford

 routing algorithm and the 

Ford-Fulkerson

 algorithm, after the researchers who developed it (Bellman, 1957; and Ford and Fulkerson, 1962). It was the original ARPANET routing algorithm and was also used in the Internet under the name RIP. 

In distance vector routing, each router maintains a routing table indexed by, and containing one entry for, each router in the subnet. This entry contains two parts: the preferred outgoing line to use for that destination and an estimate of the time or distance to that destination. The metric used might be number of hops, time delay in milliseconds, total number of packets queued along the path, or something similar. 

The router is assumed to know the ''distance'' to each of its neighbors. If the metric is hops, the distance is just one hop. If the metric is queue length, the router simply examines each queue. If the metric is delay, the router can measure it directly with special ECHO packets that the receiver just timestamps and sends back as fast as it can. 

As an example, assume that delay is used as a metric and that the router knows the delay to each of its neighbors. Once every 

T

 msec each router sends to each neighbor a list of its estimated delays to each destination. It also receives a similar list from each neighbor. Imagine that one of these tables has just come in from neighbor 

X

, with 

X

i

 being 

X

's estimate of how long it takes to get to router 

i

. If the router knows that the delay to 

X

 is 

m

 msec, it also knows that it can reach router 

i

 via 

X

 in 

X

i

 + 

m

 msec. By performing this calculation for each neighbor, a router can find out which estimate seems the best and use that estimate and the 

270




corresponding line in its new routing table. Note that the old routing table is not used in the calculation. 

This updating process is illustrated in 

Fig. 5-9

. Part (a) shows a subnet. The first four columns of part (b) show the delay vectors received from the neighbors of router 

J

. 

A

 claims to have a 12-msec delay to 

B

, a 25-msec delay to 

C

, a 40-msec delay to 

D

, etc. Suppose that 

J

 has measured or estimated its delay to its neighbors, 

A

, 

I, H

, and 

K

 as 8, 10, 12, and 6 msec, respectively. 

Figure 5-9. (a) A subnet. (b) Input from 

A

, 

I

, 

H

, 

K

, and the new routing table for 

J

. 

 

Consider how 

J

 computes its new route to router 

G

. It knows that it can get to 

A

 in 8 msec, and 

A

 claims to be able to get to 

G

 in 18 msec, so 

J

 knows it can count on a delay of 26 msec to 

G

 if it forwards packets bound for 

G

 to 

A

. Similarly, it computes the delay to 

G

 via 

I

, 

H

, and 

K

 as 41 (31 + 10), 18 (6 + 12), and 37 (31 + 6) msec, respectively. The best of these values is 18, so it makes an entry in its routing table that the delay to 

G

 is 18 msec and that the route to use is via 

H

. The same calculation is performed for all the other destinations, with the new routing table shown in the last column of the figure. 

The Count-to-Infinity Problem 

Distance vector routing works in theory but has a serious drawback in practice: although it converges to the correct answer, it may do so slowly. In particular, it reacts rapidly to good news, but leisurely to bad news. Consider a router whose best route to destination 

X

 is large. If on the next exchange neighbor 

A

 suddenly reports a short delay to 

X

, the router just switches over to using the line to 

A

 to send traffic to 

X

. In one vector exchange, the good news is processed. 

To see how fast good news propagates, consider the five-node (linear) subnet of 

Fig. 5-10

, where the delay metric is the number of hops. Suppose 

A

 is down initially and all the other routers know this. In other words, they have all recorded the delay to 

A

 as infinity. 

Figure 5-10. The count-to-infinity problem. 

271




 

When 

A

 comes up, the other routers learn about it via the vector exchanges. For simplicity we will assume that there is a gigantic gong somewhere that is struck periodically to initiate a vector exchange at all routers simultaneously. At the time of the first exchange, 

B

 learns that its left neighbor has zero delay to 

A

. 

B

 now makes an entry in its routing table that 

A

 is one hop away to the left. All the other routers still think that 

A

 is down. At this point, the routing table entries for 

A

 are as shown in the second row of 

Fig. 5-10(a)

. On the next exchange, 

C

 learns that 

B

 has a path of length 1 to 

A

, so it updates its routing table to indicate a path of length 2, but 

D

 and 

E

 do not hear the good news until later. Clearly, the good news is spreading at the rate of one hop per exchange. In a subnet whose longest path is of length 

N

 hops, within 

N

 exchanges everyone will know about newly-revived lines and routers. 

Now let us consider the situation of 

Fig. 5-10(b)

, in which all the lines and routers are initially up. Routers 

B

, 

C

, 

D

, and 

E

 have distances to 

A

 of 1, 2, 3, and 4, respectively. Suddenly 

A

 goes down, or alternatively, the line between 

A

 and 

B

 is cut, which is effectively the same thing from 

B

's point of view. 

At the first packet exchange, 

B

 does not hear anything from 

A

. Fortunately, 

C

 says: Do not worry; I have a path to 

A

 of length 2. Little does 

B

 know that 

C

's path runs through 

B

 itself. For all 

B

 knows, 

C

 might have ten lines all with separate paths to 

A

 of length 2. As a result, 

B

 thinks it can reach 

A

 via 

C

, with a path length of 3. 

D

 and 

E

 do not update their entries for 

A

 on the first exchange. 

On the second exchange, 

C

 notices that each of its neighbors claims to have a path to 

A

 of length 3. It picks one of the them at random and makes its new distance to 

A

 4, as shown in the third row of 

Fig. 5-10(b)

. Subsequent exchanges produce the history shown in the rest of 

Fig. 5-10(b)

. 

From this figure, it should be clear why bad news travels slowly: no router ever has a value more than one higher than the minimum of all its neighbors. Gradually, all routers work their way up to infinity, but the number of exchanges required depends on the numerical value used for infinity. For this reason, it is wise to set infinity to the longest path plus 1. If the metric is time delay, there is no well-defined upper bound, so a high value is needed to prevent a path with a long delay from being treated as down. Not entirely surprisingly, this problem is known as the 

count-to-infinity

 problem. There have been a few attempts to solve it (such as split horizon with poisoned reverse in RFC 1058), but none of these work well in general. The core of the problem is that when 

X

 tells 

Y

 that it has a path somewhere, 

Y

 has no way of knowing whether it itself is on the path. 

5.2.5 Link State Routing 

Distance vector routing was used in the ARPANET until 1979, when it was replaced by link state routing. Two primary problems caused its demise. First, since the delay metric was queue length, it did not take line bandwidth into account when choosing routes. Initially, all the lines were 56 kbps, so line bandwidth was not an issue, but after some lines had been 

272




upgraded to 230 kbps and others to 1.544 Mbps, not taking bandwidth into account was a major problem. Of course, it would have been possible to change the delay metric to factor in line bandwidth, but a second problem also existed, namely, the algorithm often took too long to converge (the count-to-infinity problem). For these reasons, it was replaced by an entirely new algorithm, now called 

link state routing

. Variants of link state routing are now widely used. 

The idea behind link state routing is simple and can be stated as five parts. Each router must do the following: 

1. Discover its neighbors and learn their network addresses. 

2. Measure the delay or cost to each of its neighbors. 

3. Construct a packet telling all it has just learned. 

4. Send this packet to all other routers. 

5. Compute the shortest path to every other router. 

In effect, the complete topology and all delays are experimentally measured and distributed to every router. Then Dijkstra's algorithm can be run to find the shortest path to every other router. Below we will consider each of these five steps in more detail. 

Learning about the Neighbors 

When a router is booted, its first task is to learn who its neighbors are. It accomplishes this goal by sending a special HELLO packet on each point-to-point line. The router on the other end is expected to send back a reply telling who it is. These names must be globally unique because when a distant router later hears that three routers are all connected to 

F

, it is essential that it can determine whether all three mean the same 

F

. 

When two or more routers are connected by a LAN, the situation is slightly more complicated. 

Fig. 5-11(a)

 illustrates a LAN to which three routers, 

A

, 

C

, and 

F

, are directly connected. Each of these routers is connected to one or more additional routers, as shown. 

Figure 5-11. (a) Nine routers and a LAN. (b) A graph model of (a). 

 

One way to model the LAN is to consider it as a node itself, as shown in 

Fig. 5-11(b)

. Here we have introduced a new, artificial node, 

N

, to which 

A

, 

C

, and 

F

 are connected. The fact that it is possible to go from 

A

 to 

C

 on the LAN is represented by the path 

ANC

 here. 

Measuring Line Cost 

The link state routing algorithm requires each router to know, or at least have a reasonable estimate of, the delay to each of its neighbors. The most direct way to determine this delay is to send over the line a special ECHO packet that the other side is required to send back 

273




immediately. By measuring the round-trip time and dividing it by two, the sending router can get a reasonable estimate of the delay. For even better results, the test can be conducted several times, and the average used. Of course, this method implicitly assumes the delays are symmetric, which may not always be the case. 

An interesting issue is whether to take the load into account when measuring the delay. To factor the load in, the round-trip timer must be started when the ECHO packet is queued. To ignore the load, the timer should be started when the ECHO packet reaches the front of the queue. 

Arguments can be made both ways. Including traffic-induced delays in the measurements means that when a router has a choice between two lines with the same bandwidth, one of which is heavily loaded all the time and one of which is not, the router will regard the route over the unloaded line as a shorter path. This choice will result in better performance. 

Unfortunately, there is also an argument against including the load in the delay calculation. Consider the subnet of 

Fig. 5-12

, which is divided into two parts, East and West, connected by two lines, 

CF

 and 

EI

. 

Figure 5-12. A subnet in which the East and West parts are connected by two lines. 

 

Suppose that most of the traffic between East and West is using line 

CF

, and as a result, this line is heavily loaded with long delays. Including queueing delay in the shortest path calculation will make 

EI

 more attractive. After the new routing tables have been installed, most of the East-West traffic will now go over 

EI

, overloading this line. Consequently, in the next update, 

CF

 will appear to be the shortest path. As a result, the routing tables may oscillate wildly, leading to erratic routing and many potential problems. If load is ignored and only bandwidth is considered, this problem does not occur. Alternatively, the load can be spread over both lines, but this solution does not fully utilize the best path. Nevertheless, to avoid oscillations in the choice of best path, it may be wise to distribute the load over multiple lines, with some known fraction going over each line. 

Building Link State Packets 

Once the information needed for the exchange has been collected, the next step is for each router to build a packet containing all the data. The packet starts with the identity of the sender, followed by a sequence number and age (to be described later), and a list of neighbors. For each neighbor, the delay to that neighbor is given. An example subnet is given in 

Fig. 5-13(a)

 with delays shown as labels on the lines. The corresponding link state packets for all six routers are shown in 

Fig. 5-13(b)

. 

274




Figure 5-13. (a) A subnet. (b) The link state packets for this subnet. 

 

Building the link state packets is easy. The hard part is determining when to build them. One possibility is to build them periodically, that is, at regular intervals. Another possibility is to build them when some significant event occurs, such as a line or neighbor going down or coming back up again or changing its properties appreciably. 

Distributing the Link State Packets 

The trickiest part of the algorithm is distributing the link state packets reliably. As the packets are distributed and installed, the routers getting the first ones will change their routes. Consequently, the different routers may be using different versions of the topology, which can lead to inconsistencies, loops, unreachable machines, and other problems. 

First we will describe the basic distribution algorithm. Later we will give some refinements. The fundamental idea is to use flooding to distribute the link state packets. To keep the flood in check, each packet contains a sequence number that is incremented for each new packet sent. Routers keep track of all the (source router, sequence) pairs they see. When a new link state packet comes in, it is checked against the list of packets already seen. If it is new, it is forwarded on all lines except the one it arrived on. If it is a duplicate, it is discarded. If a packet with a sequence number lower than the highest one seen so far ever arrives, it is rejected as being obsolete since the router has more recent data. 

This algorithm has a few problems, but they are manageable. First, if the sequence numbers wrap around, confusion will reign. The solution here is to use a 32-bit sequence number. With one link state packet per second, it would take 137 years to wrap around, so this possibility can be ignored. 

Second, if a router ever crashes, it will lose track of its sequence number. If it starts again at 0, the next packet will be rejected as a duplicate. 

Third, if a sequence number is ever corrupted and 65,540 is received instead of 4 (a 1-bit error), packets 5 through 65,540 will be rejected as obsolete, since the current sequence number is thought to be 65,540. 

The solution to all these problems is to include the age of each packet after the sequence number and decrement it once per second. When the age hits zero, the information from that router is discarded. Normally, a new packet comes in, say, every 10 sec, so router information only times out when a router is down (or six consecutive packets have been lost, an unlikely event). The 

Age

 field is also decremented by each router during the initial flooding process, to make sure no packet can get lost and live for an indefinite period of time (a packet whose age is zero is discarded). 

Some refinements to this algorithm make it more robust. When a link state packet comes in to a router for flooding, it is not queued for transmission immediately. Instead it is first put in a holding area to wait a short while. If another link state packet from the same source comes in before the first packet is transmitted, their sequence numbers are compared. If they are equal, 

275




the duplicate is discarded. If they are different, the older one is thrown out. To guard against errors on the router-router lines, all link state packets are acknowledged. When a line goes idle, the holding area is scanned in round-robin order to select a packet or acknowledgement to send. 

The data structure used by router 

B

 for the subnet shown in 

Fig. 5-13(a)

 is depicted in 

Fig. 5-

14

. Each row here corresponds to a recently-arrived, but as yet not fully-processed, link state packet. The table records where the packet originated, its sequence number and age, and the data. In addition, there are send and acknowledgement flags for each of 

B

's three lines (to 

A

, 

C

, and 

F

, respectively). The send flags mean that the packet must be sent on the indicated line. The acknowledgement flags mean that it must be acknowledged there. 

Figure 5-14. The packet buffer for router 

B

 in 

Fig. 5-13

. 

 

In 

Fig. 5-14

, the link state packet from 

A

 arrives directly, so it must be sent to 

C

 and 

F

 and acknowledged to 

A

, as indicated by the flag bits. Similarly, the packet from 

F

 has to be forwarded to 

A

 and 

C

 and acknowledged to 

F

. 

However, the situation with the third packet, from 

E

, is different. It arrived twice, once via 

EAB

 and once via 

EFB

. Consequently, it has to be sent only to 

C

 but acknowledged to both 

A

 and 

F

, as indicated by the bits. 

If a duplicate arrives while the original is still in the buffer, bits have to be changed. For example, if a copy of 

C

's state arrives from 

F

 before the fourth entry in the table has been forwarded, the six bits will be changed to 100011 to indicate that the packet must be acknowledged to 

F

 but not sent there. 

Computing the New Routes 

Once a router has accumulated a full set of link state packets, it can construct the entire subnet graph because every link is represented. Every link is, in fact, represented twice, once for each direction. The two values can be averaged or used separately. 

Now Dijkstra's algorithm can be run locally to construct the shortest path to all possible destinations. The results of this algorithm can be installed in the routing tables, and normal operation resumed. 

For a subnet with 

n

 routers, each of which has 

k

 neighbors, the memory required to store the input data is proportional to 

kn

. For large subnets, this can be a problem. Also, the computation time can be an issue. Nevertheless, in many practical situations, link state routing works well. 

However, problems with the hardware or software can wreak havoc with this algorithm (also with other ones). For example, if a router claims to have a line it does not have or forgets a line it does have, the subnet graph will be incorrect. If a router fails to forward packets or 

276




corrupts them while forwarding them, trouble will arise. Finally, if it runs out of memory or does the routing calculation wrong, bad things will happen. As the subnet grows into the range of tens or hundreds of thousands of nodes, the probability of some router failing occasionally becomes nonnegligible. The trick is to try to arrange to limit the damage when the inevitable happens. Perlman (1988) discusses these problems and their solutions in detail. 

Link state routing is widely used in actual networks, so a few words about some example protocols using it are in order. The OSPF protocol, which is widely used in the Internet, uses a link state algorithm. We will describe OSPF in 

Sec. 5.6.4

. 

Another link state protocol is 

IS-IS

 (

Intermediate System-Intermediate System

), which was designed for DECnet and later adopted by ISO for use with its connectionless network layer protocol, CLNP. Since then it has been modified to handle other protocols as well, most notably, IP. IS-IS is used in some Internet backbones (including the old NSFNET backbone) and in some digital cellular systems such as CDPD. Novell NetWare uses a minor variant of IS-IS (NLSP) for routing IPX packets. 

Basically IS-IS distributes a picture of the router topology, from which the shortest paths are computed. Each router announces, in its link state information, which network layer addresses it can reach directly. These addresses can be IP, IPX, AppleTalk, or any other addresses. IS-IS can even support multiple network layer protocols at the same time. 

Many of the innovations designed for IS-IS were adopted by OSPF (OSPF was designed several years after IS-IS). These include a self-stabilizing method of flooding link state updates, the concept of a designated router on a LAN, and the method of computing and supporting path splitting and multiple metrics. As a consequence, there is very little difference between IS-IS and OSPF. The most important difference is that IS-IS is encoded in such a way that it is easy and natural to simultaneously carry information about multiple network layer protocols, a feature OSPF does not have. This advantage is especially valuable in large multiprotocol environments. 

5.2.6 Hierarchical Routing 

As networks grow in size, the router routing tables grow proportionally. Not only is router memory consumed by ever-increasing tables, but more CPU time is needed to scan them and more bandwidth is needed to send status reports about them. At a certain point the network may grow to the point where it is no longer feasible for every router to have an entry for every other router, so the routing will have to be done hierarchically, as it is in the telephone network. 

When hierarchical routing is used, the routers are divided into what we will call 

regions

, with each router knowing all the details about how to route packets to destinations within its own region, but knowing nothing about the internal structure of other regions. When different networks are interconnected, it is natural to regard each one as a separate region in order to free the routers in one network from having to know the topological structure of the other ones. 

For huge networks, a two-level hierarchy may be insufficient; it may be necessary to group the regions into clusters, the clusters into zones, the zones into groups, and so on, until we run out of names for aggregations. As an example of a multilevel hierarchy, consider how a packet might be routed from Berkeley, California, to Malindi, Kenya. The Berkeley router would know the detailed topology within California but would send all out-of-state traffic to the Los Angeles router. The Los Angeles router would be able to route traffic to other domestic routers but would send foreign traffic to New York. The New York router would be programmed to direct all traffic to the router in the destination country responsible for handling foreign traffic, say, in Nairobi. Finally, the packet would work its way down the tree in Kenya until it got to Malindi. 

277




Figure 5-15

 gives a quantitative example of routing in a two-level hierarchy with five regions. The full routing table for router 1

A

 has 17 entries, as shown in 

Fig. 5-15(b)

. When routing is done hierarchically, as in 

Fig. 5-15(c)

, there are entries for all the local routers as before, but all other regions have been condensed into a single router, so all traffic for region 2 goes via the 1

B

 -2

A

 line, but the rest of the remote traffic goes via the 1

C

 -3

B

 line. Hierarchical routing has reduced the table from 17 to 7 entries. As the ratio of the number of regions to the number of routers per region grows, the savings in table space increase. 

Figure 5-15. Hierarchical routing. 

 

Unfortunately, these gains in space are not free. There is a penalty to be paid, and this penalty is in the form of increased path length. For example, the best route from 1

A

 to 5

C

 is via region 2, but with hierarchical routing all traffic to region 5 goes via region 3, because that is better for most destinations in region 5. 

When a single network becomes very large, an interesting question is: How many levels should the hierarchy have? For example, consider a subnet with 720 routers. If there is no hierarchy, each router needs 720 routing table entries. If the subnet is partitioned into 24 regions of 30 routers each, each router needs 30 local entries plus 23 remote entries for a total of 53 entries. If a three-level hierarchy is chosen, with eight clusters, each containing 9 regions of 10 routers, each router needs 10 entries for local routers, 8 entries for routing to other regions within its own cluster, and 7 entries for distant clusters, for a total of 25 entries. Kamoun and Kleinrock (1979) discovered that the optimal number of levels for an 

N

 router subnet is ln 

N

, requiring a total of 

e

 ln 

N

 entries per router. They have also shown that the increase in effective mean path length caused by hierarchical routing is sufficiently small that it is usually acceptable. 

5.2.7 Broadcast Routing 

In some applications, hosts need to send messages to many or all other hosts. For example, a service distributing weather reports, stock market updates, or live radio programs might work best by broadcasting to all machines and letting those that are interested read the data. Sending a packet to all destinations simultaneously is called 

broadcasting

; various methods have been proposed for doing it. 

278




One broadcasting method that requires no special features from the subnet is for the source to simply send a distinct packet to each destination. Not only is the method wasteful of bandwidth, but it also requires the source to have a complete list of all destinations. In practice this may be the only possibility, but it is the least desirable of the methods. 

Flooding is another obvious candidate. Although flooding is ill-suited for ordinary point-to-point communication, for broadcasting it might rate serious consideration, especially if none of the methods described below are applicable. The problem with flooding as a broadcast technique is the same problem it has as a point-to-point routing algorithm: it generates too many packets and consumes too much bandwidth. 

A third algorithm is 

multidestination routing

. If this method is used, each packet contains either a list of destinations or a bit map indicating the desired destinations. When a packet arrives at a router, the router checks all the destinations to determine the set of output lines that will be needed. (An output line is needed if it is the best route to at least one of the destinations.) The router generates a new copy of the packet for each output line to be used and includes in each packet only those destinations that are to use the line. In effect, the destination set is partitioned among the output lines. After a sufficient number of hops, each packet will carry only one destination and can be treated as a normal packet. Multidestination routing is like separately addressed packets, except that when several packets must follow the same route, one of them pays full fare and the rest ride free. 

A fourth broadcast algorithm makes explicit use of the sink tree for the router initiating the broadcast—or any other convenient spanning tree for that matter. A 

spanning tree

 is a subset of the subnet that includes all the routers but contains no loops. If each router knows which of its lines belong to the spanning tree, it can copy an incoming broadcast packet onto all the spanning tree lines except the one it arrived on. This method makes excellent use of bandwidth, generating the absolute minimum number of packets necessary to do the job. The only problem is that each router must have knowledge of some spanning tree for the method to be applicable. Sometimes this information is available (e.g., with link state routing) but sometimes it is not (e.g., with distance vector routing). 

Our last broadcast algorithm is an attempt to approximate the behavior of the previous one, even when the routers do not know anything at all about spanning trees. The idea, called 

reverse path forwarding

, is remarkably simple once it has been pointed out. When a broadcast packet arrives at a router, the router checks to see if the packet arrived on the line that is normally used for sending packets 

to

 the source of the broadcast. If so, there is an excellent chance that the broadcast packet itself followed the best route from the router and is therefore the first copy to arrive at the router. This being the case, the router forwards copies of it onto all lines except the one it arrived on. If, however, the broadcast packet arrived on a line other than the preferred one for reaching the source, the packet is discarded as a likely duplicate. 

An example of reverse path forwarding is shown in 

Fig. 5-16

. Part (a) shows a subnet, part (b) shows a sink tree for router 

I

 of that subnet, and part (c) shows how the reverse path algorithm works. On the first hop, 

I

 sends packets to 

F

, 

H

, 

J

, and 

N

, as indicated by the second row of the tree. Each of these packets arrives on the preferred path to 

I

 (assuming that the preferred path falls along the sink tree) and is so indicated by a circle around the letter. On the second hop, eight packets are generated, two by each of the routers that received a packet on the first hop. As it turns out, all eight of these arrive at previously unvisited routers, and five of these arrive along the preferred line. Of the six packets generated on the third hop, only three arrive on the preferred path (at 

C

, 

E

, and 

K

); the others are duplicates. After five hops and 24 packets, the broadcasting terminates, compared with four hops and 14 packets had the sink tree been followed exactly. 

Figure 5-16. Reverse path forwarding. (a) A subnet. (b) A sink tree. (c) The tree built by reverse path forwarding. 

279




 

The principal advantage of reverse path forwarding is that it is both reasonably efficient and easy to implement. It does not require routers to know about spanning trees, nor does it have the overhead of a destination list or bit map in each broadcast packet as does multidestination addressing. Nor does it require any special mechanism to stop the process, as flooding does (either a hop counter in each packet and a priori knowledge of the subnet diameter, or a list of packets already seen per source). 

5.2.8 Multicast Routing 

Some applications require that widely-separated processes work together in groups, for example, a group of processes implementing a distributed database system. In these situations, it is frequently necessary for one process to send a message to all the other members of the group. If the group is small, it can just send each other member a point-to-point message. If the group is large, this strategy is expensive. Sometimes broadcasting can be used, but using broadcasting to inform 1000 machines on a million-node network is inefficient because most receivers are not interested in the message (or worse yet, they are definitely interested but are not supposed to see it). Thus, we need a way to send messages to well-defined groups that are numerically large in size but small compared to the network as a whole. 

Sending a message to such a group is called 

multicasting

, and its routing algorithm is called 

multicast routing

. In this section we will describe one way of doing multicast routing. For additional information, see (Chu et al., 2000; Costa et al. 2001; Kasera et al., 2000; Madruga and Garcia-Luna-Aceves, 2001; Zhang and Ryu, 2001). 

Multicasting requires group management. Some way is needed to create and destroy groups, and to allow processes to join and leave groups. How these tasks are accomplished is not of concern to the routing algorithm. What is of concern is that when a process joins a group, it informs its host of this fact. It is important that routers know which of their hosts belong to which groups. Either hosts must inform their routers about changes in group membership, or routers must query their hosts periodically. Either way, routers learn about which of their hosts are in which groups. Routers tell their neighbors, so the information propagates through the subnet. 

To do multicast routing, each router computes a spanning tree covering all other routers. For example, in 

Fig. 5-17(a)

 we have two groups, 1 and 2. Some routers are attached to hosts that belong to one or both of these groups, as indicated in the figure. A spanning tree for the leftmost router is shown in 

Fig. 5-17(b)

. 

Figure 5-17. (a) A network. (b) A spanning tree for the leftmost router. (c) A multicast tree for group 1. (d) A multicast tree for group 2. 

280




 

When a process sends a multicast packet to a group, the first router examines its spanning tree and prunes it, removing all lines that do not lead to hosts that are members of the group. In our example, 

Fig. 5-17(c)

 shows the pruned spanning tree for group 1. Similarly, 

Fig. 5-

17(d)

 shows the pruned spanning tree for group 2. Multicast packets are forwarded only along the appropriate spanning tree. 

Various ways of pruning the spanning tree are possible. The simplest one can be used if link state routing is used and each router is aware of the complete topology, including which hosts belong to which groups. Then the spanning tree can be pruned, starting at the end of each path, working toward the root, and removing all routers that do not belong to the group in question. 

With distance vector routing, a different pruning strategy can be followed. The basic algorithm is reverse path forwarding. However, whenever a router with no hosts interested in a particular group and no connections to other routers receives a multicast message for that group, it responds with a PRUNE message, telling the sender not to send it any more multicasts for that group. When a router with no group members among its own hosts has received such messages on all its lines, it, too, can respond with a PRUNE message. In this way, the subnet is recursively pruned. 

One potential disadvantage of this algorithm is that it scales poorly to large networks. Suppose that a network has 

n

 groups, each with an average of 

m

 members. For each group, 

m

 pruned spanning trees must be stored, for a total of 

mn

 trees. When many large groups exist, considerable storage is needed to store all the trees. 

An alternative design uses 

core-based trees

 (Ballardie et al., 1993). Here, a single spanning tree per group is computed, with the root (the core) near the middle of the group. To send a multicast message, a host sends it to the core, which then does the multicast along the spanning tree. Although this tree will not be optimal for all sources, the reduction in storage costs from 

m

 trees to one tree per group is a major saving. 

5.2.9 Routing for Mobile Hosts 

Millions of people have portable computers nowadays, and they generally want to read their e-mail and access their normal file systems wherever in the world they may be. These mobile 

281




hosts introduce a new complication: to route a packet to a mobile host, the network first has to find it. The subject of incorporating mobile hosts into a network is very young, but in this section we will sketch some of the issues and give a possible solution. 

The model of the world that network designers typically use is shown in 

Fig. 5-18

. Here we have a WAN consisting of routers and hosts. Connected to the WAN are LANs, MANs, and wireless cells of the type we studied in 

Chap. 2

. 

Figure 5-18. A WAN to which LANs, MANs, and wireless cells are attached. 

 

Hosts that never move are said to be stationary. They are connected to the network by copper wires or fiber optics. In contrast, we can distinguish two other kinds of hosts. Migratory hosts are basically stationary hosts who move from one fixed site to another from time to time but use the network only when they are physically connected to it. Roaming hosts actually compute on the run and want to maintain their connections as they move around. We will use the term 

mobile hosts

 to mean either of the latter two categories, that is, all hosts that are away from home and still want to be connected. 

All hosts are assumed to have a permanent 

home location

 that never changes. Hosts also have a permanent home address that can be used to determine their home locations, analogous to the way the telephone number 1-212-5551212 indicates the United States (country code 1) and Manhattan (212). The routing goal in systems with mobile hosts is to make it possible to send packets to mobile hosts using their home addresses and have the packets efficiently reach them wherever they may be. The trick, of course, is to find them. 

In the model of 

Fig. 5-18

, the world is divided up (geographically) into small units. Let us call them areas, where an area is typically a LAN or wireless cell. Each area has one or more 

foreign agents

, which are processes that keep track of all mobile hosts visiting the area. In addition, each area has a 

home agent

, which keeps track of hosts whose home is in the area, but who are currently visiting another area. 

When a new host enters an area, either by connecting to it (e.g., plugging into the LAN) or just wandering into the cell, his computer must register itself with the foreign agent there. The registration procedure typically works like this: 

1. Periodically, each foreign agent broadcasts a packet announcing its existence and address. A newly-arrived mobile host may wait for one of these messages, but if none arrives quickly enough, the mobile host can broadcast a packet saying: Are there any foreign agents around? 

2. The mobile host registers with the foreign agent, giving its home address, current data link layer address, and some security information. 

282




3. The foreign agent contacts the mobile host's home agent and says: One of your hosts is over here. The message from the foreign agent to the home agent contains the foreign agent's network address. It also includes the security information to convince the home agent that the mobile host is really there. 

4. The home agent examines the security information, which contains a timestamp, to prove that it was generated within the past few seconds. If it is happy, it tells the foreign agent to proceed. 

5. When the foreign agent gets the acknowledgement from the home agent, it makes an entry in its tables and informs the mobile host that it is now registered. 

Ideally, when a host leaves an area, that, too, should be announced to allow deregistration, but many users abruptly turn off their computers when done. 

When a packet is sent to a mobile host, it is routed to the host's home LAN because that is what the address says should be done, as illustrated in step 1 of 

Fig. 5-19

. Here the sender, in the northwest city of Seattle, wants to send a packet to a host normally across the United States in New York. Packets sent to the mobile host on its home LAN in New York are intercepted by the home agent there. The home agent then looks up the mobile host's new (temporary) location and finds the address of the foreign agent handling the mobile host, in Los Angeles. 

Figure 5-19. Packet routing for mobile hosts. 

 

The home agent then does two things. First, it encapsulates the packet in the payload field of an outer packet and sends the latter to the foreign agent (step 2 in 

Fig. 5-19

). This mechanism is called tunneling; we will look at it in more detail later. After getting the encapsulated packet, the foreign agent removes the original packet from the payload field and sends it to the mobile host as a data link frame. 

Second, the home agent tells the sender to henceforth send packets to the mobile host by encapsulating them in the payload of packets explicitly addressed to the foreign agent instead of just sending them to the mobile host's home address (step 3). Subsequent packets can now be routed directly to the host via the foreign agent (step 4), bypassing the home location entirely. 

The various schemes that have been proposed differ in several ways. First, there is the issue of how much of this protocol is carried out by the routers and how much by the hosts, and in the 

283




latter case, by which layer in the hosts. Second, in a few schemes, routers along the way record mapped addresses so they can intercept and redirect traffic even before it gets to the home location. Third, in some schemes each visitor is given a unique temporary address; in others, the temporary address refers to an agent that handles traffic for all visitors. 

Fourth, the schemes differ in how they actually manage to arrange for packets that are addressed to one destination to be delivered to a different one. One choice is changing the destination address and just retransmitting the modified packet. Alternatively, the whole packet, home address and all, can be encapsulated inside the payload of another packet sent to the temporary address. Finally, the schemes differ in their security aspects. In general, when a host or router gets a message of the form ''Starting right now, please send all of Stephany's mail to me,'' it might have a couple of questions about whom it was talking to and whether this is a good idea. Several mobile host protocols are discussed and compared in (Hac and Guo, 2000; Perkins, 1998a; Snoeren and Balakrishnan, 2000; Solomon, 1998; and Wang and Chen, 2001). 

5.2.10 Routing in Ad Hoc Networks 

We have now seen how to do routing when the hosts are mobile but the routers are fixed. An even more extreme case is one in which the routers themselves are mobile. Among the possibilities are: 

1. Military vehicles on a battlefield with no existing infrastructure. 

2. A fleet of ships at sea. 

3. Emergency workers at an earthquake that destroyed the infrastructure. 

4. A gathering of people with notebook computers in an area lacking 802.11. 

In all these cases, and others, each node consists of a router and a host, usually on the same computer. Networks of nodes that just happen to be near each other are called 

ad hoc networks

 or 

MANETs

 (

Mobile Ad hoc NETworks

). Let us now examine them briefly. More information can be found in (Perkins, 2001). 

What makes ad hoc networks different from wired networks is that all the usual rules about fixed topologies, fixed and known neighbors, fixed relationship between IP address and location, and more are suddenly tossed out the window. Routers can come and go or appear in new places at the drop of a bit. With a wired network, if a router has a valid path to some destination, that path continues to be valid indefinitely (barring a failure somewhere in the system). With an ad hoc network, the topology may be changing all the time, so desirability and even validity of paths can change spontaneously, without warning. Needless to say, these circumstances make routing in ad hoc networks quite different from routing in their fixed counterparts. 

A variety of routing algorithms for ad hoc networks have been proposed. One of the more interesting ones is the 

AODV

 (

Ad hoc On-demand Distance Vector

) routing algorithm (Perkins and Royer, 1999). It is a distant relative of the Bellman-Ford distance vector algorithm but adapted to work in a mobile environment and takes into account the limited bandwidth and low battery life found in this environment. Another unusual characteristic is that it is an on-demand algorithm, that is, it determines a route to some destination only when somebody wants to send a packet to that destination. Let us now see what that means. 

Route Discovery 

At any instant of time, an ad hoc network can be described by a graph of the nodes (routers + hosts). Two nodes are connected (i.e., have an arc between them in the graph) if they can communicate directly using their radios. Since one of the two may have a more powerful transmitter than the other, it is possible that 

A

 is connected to 

B

 but 

B

 is not connected to 

A

. 

284




However, for simplicity, we will assume all connections are symmetric. It should also be noted that the mere fact that two nodes are within radio range of each other does not mean that they are connected. There may be buildings, hills, or other obstacles that block their communication. 

To describe the algorithm, consider the ad hoc network of 

Fig. 5-20

, in which a process at node 

A

 wants to send a packet to node 

I

. The AODV algorithm maintains a table at each node, keyed by destination, giving information about that destination, including which neighbor to send packets to in order to reach the destination. Suppose that 

A

 looks in its table and does not find an entry for 

I

. It now has to discover a route to 

I

. This property of discovering routes only when they are needed is what makes this algorithm ''on demand.'' 

Figure 5-20. (a) Range of 

A

's broadcast. (b) After 

B

 and 

D

 have received 

A

's broadcast. (c) After 

C

, 

F

, and 

G

 have received 

A

's broadcast. (d) After 

E

, 

H

, and 

I

 have received 

A

's broadcast. The shaded nodes are new recipients. The arrows show the possible reverse routes. 

 

To locate 

I

, 

A

 constructs a special ROUTE REQUEST packet and broadcasts it. The packet reaches 

B

 and 

D

, as illustrated in 

Fig. 5-20(a)

. In fact, the reason 

B

 and 

D

 are connected to 

A

 in the graph is that they can receive communication from 

A

. 

F

, for example, is not shown with an arc to 

A

 because it cannot receive 

A

's radio signal. Thus, 

F

 is not connected to 

A

. 

The format of the ROUTE REQUEST packet is shown in 

Fig. 5-21

. It contains the source and destination addresses, typically their IP addresses, which identify who is looking for whom. It also contains a 

Request ID

, which is a local counter maintained separately by each node and incremented each time a ROUTE REQUEST is broadcast. Together, the 

Source address

 and 

Request ID

 fields uniquely identify the ROUTE REQUEST packet to allow nodes to discard any duplicates they may receive. 

Figure 5-21. Format of a ROUTE REQUEST packet. 

 

In addition to the 

Request ID

 counter, each node also maintains a second sequence counter incremented whenever a ROUTE REQUEST is sent (or a reply to someone else's ROUTE REQUEST). It functions a little bit like a clock and is used to tell new routes from old routes. The fourth field of 

Fig. 5-21

 is 

A

's sequence counter; the fifth field is the most recent value of 

I

's sequence number that 

A

 has seen (0 if it has never seen it). The use of these fields will become clear shortly. The final field, 

Hop count

, will keep track of how many hops the packet has made. It is initialized to 0. 

285




When a ROUTE REQUEST packet arrives at a node (

B

 and 

D

 in this case), it is processed in the following steps. 

1. The (

Source address

, 

Request ID

) pair is looked up in a local history table to see if this request has already been seen and processed. If it is a duplicate, it is discarded and processing stops. If it is not a duplicate, the pair is entered into the history table so future duplicates can be rejected, and processing continues. 

2. The receiver looks up the destination in its route table. If a fresh route to the destination is known, a ROUTE REPLY packet is sent back to the source telling it how to get to the destination (basically: Use me). Fresh means that the 

Destination sequence number

 stored in the routing table is greater than or equal to the 

Destination sequence number

 in the ROUTE REQUEST packet. If it is less, the stored route is older than the previous route the source had for the destination, so step 3 is executed. 

3. Since the receiver does not know a fresh route to the destination, it increments the 

Hop count

 field and rebroadcasts the ROUTE REQUEST packet. It also extracts the data from the packet and stores it as a new entry in its reverse route table. This information will be used to construct the reverse route so that the reply can get back to the source later. The arrows in 

Fig. 5-20

 are used for building the reverse route. A timer is also started for the newly-made reverse route entry. If it expires, the entry is deleted. 

Neither 

B

 nor 

D

 knows where 

I

 is, so each of them creates a reverse route entry pointing back to 

A

, as shown by the arrows in 

Fig. 5-20

, and broadcasts the packet with 

Hop count

 set to 1. The broadcast from 

B

 reaches 

C

 and 

D

. 

C

 makes an entry for it in its reverse route table and rebroadcasts it. In contrast, 

D

 rejects it as a duplicate. Similarly, 

D

's broadcast is rejected by 

B

. However, 

D

's broadcast is accepted by 

F

 and 

G

 and stored, as shown in 

Fig. 5-20(c)

. After 

E

, 

H

, and 

I

 receive the broadcast, the ROUTE REQUEST finally reaches a destination that knows where 

I

 is, namely, 

I

 itself, as illustrated in 

Fig. 5-20(d)

. Note that although we have shown the broadcasts in three discrete steps here, the broadcasts from different nodes are not coordinated in any way. 

In response to the incoming request, 

I

 builds a ROUTE REPLY packet, as shown in 

Fig. 5-22

. The 

Source address

, 

Destination address

, and 

Hop count

 are copied from the incoming request, but the 

Destination sequence number

 taken from its counter in memory. The 

Hop count

 field is set to 0. The 

Lifetime

 field controls how long the route is valid. This packet is unicast to the node that the ROUTE REQUEST packet came from, in this case, 

G

. It then follows the reverse path to 

D

 and finally to 

A

. At each node, 

Hop count

 is incremented so the node can see how far from the destination (

I

) it is. 

Figure 5-22. Format of a ROUTE REPLY packet. 

 

At each intermediate node on the way back, the packet is inspected. It is entered into the local routing table as a route to 

I

 if one or more of the following three conditions are met: 

1. No route to 

I

 is known. 

2. The sequence number for 

I

 in the ROUTE REPLY packet is greater than the value in the routing table. 

3. The sequence numbers are equal but the new route is shorter. 

In this way, all the nodes on the reverse route learn the route to 

I

 for free, as a byproduct of 

A

's route discovery. Nodes that got the original REQUEST ROUTE packet but were not on the reverse path (

B

, 

C

, 

E

, 

F

, and 

H

 in this example) discard the reverse route table entry when the associated timer expires. 

286




In a large network, the algorithm generates many broadcasts, even for destinations that are close by. The number of broadcasts can be reduced as follows. The IP packet's 

Time to live

 is initialized by the sender to the expected diameter of the network and decremented on each hop. If it hits 0, the packet is discarded instead of being broadcast. 

The discovery process is then modified as follows. To locate a destination, the sender broadcasts a ROUTE REQUEST packet with 

Time to live

 set to 1. If no response comes back within a reasonable time, another one is sent, this time with 

Time to live

 set to 2. Subsequent attempts use 3, 4, 5, etc. In this way, the search is first attempted locally, then in increasingly wider rings. 

Route Maintenance 

Because nodes can move or be switched off, the topology can change spontaneously. For example, in 

Fig. 5-20

, if 

G

 is switched off, 

A

 will not realize that the route it was using to 

I

 (

ADGI

) is no longer valid. The algorithm needs to be able to deal with this. Periodically, each node broadcasts a 

Hello

 message. Each of its neighbors is expected to respond to it. If no response is forthcoming, the broadcaster knows that that neighbor has moved out of range and is no longer connected to it. Similarly, if it tries to send a packet to a neighbor that does not respond, it learns that the neighbor is no longer available. 

This information is used to purge routes that no longer work. For each possible destination, each node, 

N

, keeps track of its neighbors that have fed it a packet for that destination during the last ?

T

 seconds. These are called 

N

's 

active neighbors

 for that destination. 

N

 does this by having a routing table keyed by destination and containing the outgoing node to use to reach the destination, the hop count to the destination, the most recent destination sequence number, and the list of active neighbors for that destination. A possible routing table for node 

D

 in our example topology is shown in 

Fig. 5-23(a)

. 

Figure 5-23. (a) 

D

's routing table before 

G

 goes down. (b) The graph after 

G

 has gone down. 

 

When any of 

N

's neighbors becomes unreachable, it checks its routing table to see which destinations have routes using the now-gone neighbor. For each of these routes, the active neighbors are informed that their route via 

N

 is now invalid and must be purged from their routing tables. The active neighbors then tell their active neighbors, and so on, recursively, until all routes depending on the now-gone node are purged from all routing tables. 

As an example of route maintenance, consider our previous example, but now with 

G

 suddenly switched off. The changed topology is illustrated in 

Fig. 5-23(b)

. When 

D

 discovers that 

G

 is gone, it looks at its routing table and sees that 

G

 was used on routes to 

E

, 

G

, and 

I

. The union of the active neighbors for these destinations is the set {

A

, 

B

}. In other words, 

A

 and 

B

 depend on 

G

 for some of their routes, so they have to be informed that these routes no longer 

287




work. 

D

 tells them by sending them packets that cause them to update their own routing tables accordingly. 

D

 also purges the entries for 

E

, 

G

, and 

I

 from its routing table. 

It may not have been obvious from our description, but a critical difference between AODV and Bellman-Ford is that nodes do not send out periodic broadcasts containing their entire routing table. This difference saves both bandwidth and battery life. 

AODV is also capable of doing broadcast and multicast routing. For details, consult (Perkins and Royer, 2001). Ad hoc routing is a red-hot research area. A great deal has been published on the topic. A few of the papers include (Chen et al., 2002; Hu and Johnson, 2001; Li et al., 2001; Raju and Garcia-Luna-Aceves, 2001; Ramanathan and Redi, 2002; Royer and Toh, 1999; Spohn and Garcia-Luna-Aceves, 2001; Tseng et al., 2001; and Zadeh et al., 2002). 

5.2.11 Node Lookup in Peer-to-Peer Networks 

A relatively new phenomenon is peer-to-peer networks, in which a large number of people, usually with permanent wired connections to the Internet, are in contact to share resources. The first widespread application of peer-to-peer technology was for mass crime: 50 million Napster users were exchanging copyrighted songs without the copyright owners' permission until Napster was shut down by the courts amid great controversy. Nevertheless, peer-to-peer technology has many interesting and legal uses. It also has something similar to a routing problem, although it is not quite the same as the ones we have studied so far. Nevertheless, it is worth a quick look. 

What makes peer-to-peer systems interesting is that they are totally distributed. All nodes are symmetric and there is no central control or hierarchy. In a typical peer-to-peer system the users each have some information that may be of interest to other users. This information may be free software, (public domain) music, photographs, and so on. If there are large numbers of users, they will not know each other and will not know where to find what they are looking for. One solution is a big central database, but this may not be feasible for some reason (e.g., nobody is willing to host and maintain it). Thus, the problem comes down to how a user finds a node that contains what he is looking for in the absence of a centralized database or even a centralized index. 

Let us assume that each user has one or more data items such as songs, photographs, programs, files, and so on that other users might want to read. Each item has an ASCII string naming it. A potential user knows just the ASCII string and wants to find out if one or more people have copies and, if so, what their IP addresses are. 

As an example, consider a distributed genealogical database. Each genealogist has some on-line records for his or her ancestors and relatives, possibly with photos, audio, or even video clips of the person. Multiple people may have the same great grandfather, so an ancestor may have records at multiple nodes. The name of the record is the person's name in some canonical form. At some point, a genealogist discovers his great grandfather's will in an archive, in which the great grandfather bequeaths his gold pocket watch to his nephew. The genealogist now knows the nephew's name and wants to find out if any other genealogist has a record for him. How, without a central database, do we find out who, if anyone, has records? 

Various algorithms have been proposed to solve this problem. The one we will examine is Chord (Dabek et al., 2001a; and Stoica et al., 2001). A simplified explanation of how it works is as follows. The Chord system consists of 

n

 participating users, each of whom may have some stored records and each of whom is prepared to store bits and pieces of the index for use by other users. Each user node has an IP address that can be hashed to an 

m

-bit number using a hash function, 

hash

. Chord uses SHA-1 for 

hash

. SHA-1 is used in cryptography; we will look at it in 

Chap. 8

. For now, it is just a function that takes a variable-length byte string as argument and produces a highly-random 160-bit number. Thus, we can convert any IP address to a 160-bit number called the 

node identifier

. 

288




Conceptually, all the 2

160

 node identifiers are arranged in ascending order in a big circle. Some of them correspond to participating nodes, but most of them do not. In 

Fig. 5-24(a)

 we show the node identifier circle for 

m

 = 5 (just ignore the arcs in the middle for the moment). In this example, the nodes with identifiers 1, 4, 7, 12, 15, 20, and 27 correspond to actual nodes and are shaded in the figure; the rest do not exist. 

Figure 5-24. (a) A set of 32 node identifiers arranged in a circle. The shaded ones correspond to actual machines. The arcs show the fingers from nodes 1, 4, and 12. The labels on the arcs are the table indices. (b) Examples of the finger tables. 

 

Let us now define the function 

successor

(

k

) as the node identifier of the first actual node following 

k

 around the circle clockwise. For example, 

successor

 (6) = 7, 

successor

 (8) = 12, and 

successor

 (22) = 27. 

The names of the records (song names, ancestors' names, and so on) are also hashed with 

hash

 (i.e., SHA-1) to generate a 160-bit number, called the 

key

. Thus, to convert 

name

 (the ASCII name of the record) to its key, we use 

key

 = 

hash

(

name

). This computation is just a local procedure call to 

hash.

 If a person holding a genealogical record for 

name

 wants to make it available to everyone, he first builds a tuple consisting of (

name

, 

my-IP-address

) and then asks 

successor

(

hash

(

name

)) to store the tuple. If multiple records (at different nodes) exist for this name, their tuple will all be stored at the same node. In this way, the index is distributed over the nodes at random. For fault tolerance, 

p

 different hash functions could be used to store each tuple at 

p

 nodes, but we will not consider that further here. 

If some user later wants to look up 

name

, he hashes it to get 

key

 and then uses 

successor

 (

key

) to find the IP address of the node storing its index tuples. The first step is easy; the second one is not. To make it possible to find the IP address of the node corresponding to a certain key, each node must maintain certain administrative data structures. One of these is 

289




the IP address of its successor node along the node identifier circle. For example, in 

Fig. 5-24

, node 4's successor is 7 and node 7's successor is 12. 

Lookup can now proceed as follows. The requesting node sends a packet to its successor containing its IP address and the key it is looking for. The packet is propagated around the ring until it locates the successor to the node identifier being sought. That node checks to see if it has any information matching the key, and if so, returns it directly to the requesting node, whose IP address it has. 

As a first optimization, each node could hold the IP addresses of both its successor and its predecessor, so that queries could be sent either clockwise or counterclockwise, depending on which path is thought to be shorter. For example, node 7 in 

Fig. 5-24

 could go clockwise to find node identifier 10 but counterclockwise to find node identifier 3. 

Even with two choices of direction, linearly searching all the nodes is very inefficient in a large peer-to-peer system since the mean number of nodes required per search is 

n/

2. To greatly speed up the search, each node also maintains what Chord calls a 

finger table

. The finger table has 

m

 entries, indexed by 0 through 

m

 - 1, each one pointing to a different actual node. Each of the entries has two fields: 

start

 and the IP address of 

successor

(

start

), as shown for three example nodes in 

Fig. 5-24(b)

. The values of the fields for entry 

i

 at node 

k

 are: 

 

Note that each node stores the IP addresses of a relatively small number of nodes and that most of these are fairly close by in terms of node identifier. 

Using the finger table, the lookup of 

key

 at node 

k

 proceeds as follows. If 

key

 falls between 

k

 and 

successor

 (

k

), then the node holding information about 

key

 is 

successor

 (

k

) and the search terminates. Otherwise, the finger table is searched to find the entry whose 

start

 field is the closest predecessor of 

key

. A request is then sent directly to the IP address in that finger table entry to ask it to continue the search. Since it is closer to 

key

 but still below it, chances are good that it will be able to return the answer with only a small number of additional queries. In fact, since every lookup halves the remaining distance to the target, it can be shown that the average number of lookups is log

2

n

. 

As a first example, consider looking up 

key

 = 3 at node 1. Since node 1 knows that 3 lies between it and its successor, 4, the desired node is 4 and the search terminates, returning node 4's IP address. 

As a second example, consider looking up 

key

 = 14 at node 1. Since 14 does not lie between 1 and 4, the finger table is consulted. The closest predecessor to 14 is 9, so the request is forwarded to the IP address of 9's entry, namely, that of node 12. Node 12 sees that 14 falls between it and its successor (15), so it returns the IP address of node 15. 

As a third example, consider looking up 

key

 = 16 at node 1. Again a query is sent to node 12, but this time node 12 does not know the answer itself. It looks for the node most closely preceding 16 and finds 14, which yields the IP address of node 15. A query is then sent there. Node 15 observes that 16 lies between it and its successor (20), so it returns the IP address of 20 to the caller, which works its way back to node 1. 

Since nodes join and leave all the time, Chord needs a way to handle these operations. We assume that when the system began operation it was small enough that the nodes could just exchange information directly to build the first circle and finger tables. After that an automated procedure is needed, as follows. When a new node, 

r

, wants to join, it must contact some existing node and ask it to look up the IP address of 

successor

 (

r

) for it. The new node then asks 

successor

 (

r

) for its predecessor. The new node then asks both of these to insert 

r

 in 

290




between them in the circle. For example, if 24 in 

Fig. 5-24

 wants to join, it asks any node to look up 

successor

 (24), which is 27. Then it asks 27 for its predecessor (20). After it tells both of those about its existence, 20 uses 24 as its successor and 27 uses 24 as its predecessor. In addition, node 27 hands over those keys in the range 21–24, which now belong to 24. At this point, 24 is fully inserted. 

However, many finger tables are now wrong. To correct them, every node runs a background process that periodically recomputes each finger by calling 

successor

. When one of these queries hits a new node, the corresponding finger entry is updated. 

When a node leaves gracefully, it hands its keys over to its successor and informs its predecessor of its departure so the predecessor can link to the departing node's successor. When a node crashes, a problem arises because its predecessor no longer has a valid successor. To alleviate this problem, each node keeps track not only of its direct successor but also its 

s

 direct successors, to allow it to skip over up to 

s

 - 1 consecutive failed nodes and reconnect the circle. 

Chord has been used to construct a distributed file system (Dabek et al., 2001b) and other applications, and research is ongoing. A different peer-to-peer system, Pastry, and its applications are described in (Rowstron and Druschel, 2001a; and Rowstron and Druschel, 2001b). A third peer-to-peer system, Freenet, is discussed in (Clarke et al., 2002). A fourth system of this type is described in (Ratnasamy et al., 2001). 

5.3 Congestion Control Algorithms 

When too many packets are present in (a part of) the subnet, performance degrades. This situation is called 

congestion

. 

Figure 5-25

 depicts the symptom. When the number of packets dumped into the subnet by the hosts is within its carrying capacity, they are all delivered (except for a few that are afflicted with transmission errors) and the number delivered is proportional to the number sent. However, as traffic increases too far, the routers are no longer able to cope and they begin losing packets. This tends to make matters worse. At very high trafffic, performance collapses completely and almost no packets are delivered. 

Figure 5-25. When too much traffic is offered, congestion sets in and performance degrades sharply. 

 

Congestion can be brought on by several factors. If all of a sudden, streams of packets begin arriving on three or four input lines and all need the same output line, a queue will build up. If there is insufficient memory to hold all of them, packets will be lost. Adding more memory may help up to a point, but Nagle (1987) discovered that if routers have an infinite amount of memory, congestion gets worse, not better, because by the time packets get to the front of the queue, they have already timed out (repeatedly) and duplicates have been sent. All these 

291




packets will be dutifully forwarded to the next router, increasing the load all the way to the destination. 

Slow processors can also cause congestion. If the routers' CPUs are slow at performing the bookkeeping tasks required of them (queueing buffers, updating tables, etc.), queues can build up, even though there is excess line capacity. Similarly, low-bandwidth lines can also cause congestion. Upgrading the lines but not changing the processors, or vice versa, often helps a little, but frequently just shifts the bottleneck. Also, upgrading part, but not all, of the system, often just moves the bottleneck somewhere else. The real problem is frequently a mismatch between parts of the system. This problem will persist until all the components are in balance. 

It is worth explicitly pointing out the difference between congestion control and flow control, as the relationship is subtle. Congestion control has to do with making sure the subnet is able to carry the offered traffic. It is a global issue, involving the behavior of all the hosts, all the routers, the store-and-forwarding processing within the routers, and all the other factors that tend to diminish the carrying capacity of the subnet. 

Flow control, in contrast, relates to the point-to-point traffic between a given sender and a given receiver. Its job is to make sure that a fast sender cannot continually transmit data faster than the receiver is able to absorb it. Flow control frequently involves some direct feedback from the receiver to the sender to tell the sender how things are doing at the other end. 

To see the difference between these two concepts, consider a fiber optic network with a capacity of 1000 gigabits/sec on which a supercomputer is trying to transfer a file to a personal computer at 1 Gbps. Although there is no congestion (the network itself is not in trouble), flow control is needed to force the supercomputer to stop frequently to give the personal computer a chance to breathe. 

At the other extreme, consider a store-and-forward network with 1-Mbps lines and 1000 large computers, half of which are trying to transfer files at 100 kbps to the other half. Here the problem is not that of fast senders overpowering slow receivers, but that the total offered traffic exceeds what the network can handle. 

The reason congestion control and flow control are often confused is that some congestion control algorithms operate by sending messages back to the various sources telling them to slow down when the network gets into trouble. Thus, a host can get a ''slow down'' message either because the receiver cannot handle the load or because the network cannot handle it. We will come back to this point later. 

We will start our study of congestion control by looking at a general model for dealing with it. Then we will look at broad approaches to preventing it in the first place. After that, we will look at various dynamic algorithms for coping with it once it has set in. 

5.3.1 General Principles of Congestion Control 

Many problems in complex systems, such as computer networks, can be viewed from a control theory point of view. This approach leads to dividing all solutions into two groups: open loop and closed loop. Open loop solutions attempt to solve the problem by good design, in essence, to make sure it does not occur in the first place. Once the system is up and running, midcourse corrections are not made. 

Tools for doing open-loop control include deciding when to accept new traffic, deciding when to discard packets and which ones, and making scheduling decisions at various points in the network. All of these have in common the fact that they make decisions without regard to the current state of the network. 

292




In contrast, closed loop solutions are based on the concept of a feedback loop. This approach has three parts when applied to congestion control: 

1. Monitor the system to detect when and where congestion occurs. 

2. Pass this information to places where action can be taken. 

3. Adjust system operation to correct the problem. 

A variety of metrics can be used to monitor the subnet for congestion. Chief among these are the percentage of all packets discarded for lack of buffer space, the average queue lengths, the number of packets that time out and are retransmitted, the average packet delay, and the standard deviation of packet delay. In all cases, rising numbers indicate growing congestion. 

The second step in the feedback loop is to transfer the information about the congestion from the point where it is detected to the point where something can be done about it. The obvious way is for the router detecting the congestion to send a packet to the traffic source or sources, announcing the problem. Of course, these extra packets increase the load at precisely the moment that more load is not needed, namely, when the subnet is congested. 

However, other possibilities also exist. For example, a bit or field can be reserved in every packet for routers to fill in whenever congestion gets above some threshold level. When a router detects this congested state, it fills in the field in all outgoing packets, to warn the neighbors. 

Still another approach is to have hosts or routers periodically send probe packets out to explicitly ask about congestion. This information can then be used to route traffic around problem areas. Some radio stations have helicopters flying around their cities to report on road congestion to make it possible for their mobile listeners to route their packets (cars) around hot spots. 

In all feedback schemes, the hope is that knowledge of congestion will cause the hosts to take appropriate action to reduce the congestion. For a scheme to work correctly, the time scale must be adjusted carefully. If every time two packets arrive in a row, a router yells STOP and every time a router is idle for 20 µsec, it yells GO, the system will oscillate wildly and never converge. On the other hand, if it waits 30 minutes to make sure before saying anything, the congestion control mechanism will react too sluggishly to be of any real use. To work well, some kind of averaging is needed, but getting the time constant right is a nontrivial matter. 

Many congestion control algorithms are known. To provide a way to organize them in a sensible way, Yang and Reddy (1995) have developed a taxonomy for congestion control algorithms. They begin by dividing all algorithms into open loop or closed loop, as described above. They further divide the open loop algorithms into ones that act at the source versus ones that act at the destination. The closed loop algorithms are also divided into two subcategories: explicit feedback versus implicit feedback. In explicit feedback algorithms, packets are sent back from the point of congestion to warn the source. In implicit algorithms, the source deduces the existence of congestion by making local observations, such as the time needed for acknowledgements to come back. 

The presence of congestion means that the load is (temporarily) greater than the resources (in part of the system) can handle. Two solutions come to mind: increase the resources or decrease the load. For example, the subnet may start using dial-up telephone lines to temporarily increase the bandwidth between certain points. On satellite systems, increasing transmission power often gives higher bandwidth. Splitting traffic over multiple routes instead of always using the best one may also effectively increase the bandwidth. Finally, spare routers that are normally used only as backups (to make the system fault tolerant) can be put on-line to give more capacity when serious congestion appears. 

293




However, sometimes it is not possible to increase the capacity, or it has already been increased to the limit. The only way then to beat back the congestion is to decrease the load. Several ways exist to reduce the load, including denying service to some users, degrading service to some or all users, and having users schedule their demands in a more predictable way. 

Some of these methods, which we will study shortly, can best be applied to virtual circuits. For subnets that use virtual circuits internally, these methods can be used at the network layer. For datagram subnets, they can nevertheless sometimes be used on transport layer connections. In this chapter, we will focus on their use in the network layer. In the next one, we will see what can be done at the transport layer to manage congestion. 

5.3.2 Congestion Prevention Policies 

Let us begin our study of methods to control congestion by looking at open loop systems. These systems are designed to minimize congestion in the first place, rather than letting it happen and reacting after the fact. They try to achieve their goal by using appropriate policies at various levels. In 

Fig. 5-26

 we see different data link, network, and transport policies that can affect congestion (Jain, 1990). 

Figure 5-26. Policies that affect congestion. 

 

Let us start at the data link layer and work our way upward. The retransmission policy is concerned with how fast a sender times out and what it transmits upon timeout. A jumpy sender that times out quickly and retransmits all outstanding packets using go back n will put a heavier load on the system than will a leisurely sender that uses selective repeat. Closely related to this is the buffering policy. If receivers routinely discard all out-of-order packets, these packets will have to be transmitted again later, creating extra load. With respect to congestion control, selective repeat is clearly better than go back n. 

Acknowledgement policy also affects congestion. If each packet is acknowledged immediately, the acknowledgement packets generate extra traffic. However, if acknowledgements are saved up to piggyback onto reverse traffic, extra timeouts and retransmissions may result. A tight flow control scheme (e.g., a small window) reduces the data rate and thus helps fight congestion. 

At the network layer, the choice between using virtual circuits and using datagrams affects congestion since many congestion control algorithms work only with virtual-circuit subnets. Packet queueing and service policy relates to whether routers have one queue per input line, one queue per output line, or both. It also relates to the order in which packets are processed 

294




(e.g., round robin or priority based). Discard policy is the rule telling which packet is dropped when there is no space. A good policy can help alleviate congestion and a bad one can make it worse. 

A good routing algorithm can help avoid congestion by spreading the traffic over all the lines, whereas a bad one can send too much traffic over already congested lines. Finally, packet lifetime management deals with how long a packet may live before being discarded. If it is too long, lost packets may clog up the works for a long time, but if it is too short, packets may sometimes time out before reaching their destination, thus inducing retransmissions. 

In the transport layer, the same issues occur as in the data link layer, but in addition, determining the timeout interval is harder because the transit time across the network is less predictable than the transit time over a wire between two routers. If the timeout interval is too short, extra packets will be sent unnecessarily. If it is too long, congestion will be reduced but the response time will suffer whenever a packet is lost. 

5.3.3 Congestion Control in Virtual-Circuit Subnets 

The congestion control methods described above are basically open loop: they try to prevent congestion from occurring in the first place, rather than dealing with it after the fact. In this section we will describe some approaches to dynamically controlling congestion in virtual-circuit subnets. In the next two, we will look at techniques that can be used in any subnet. 

One technique that is widely used to keep congestion that has already started from getting worse is 

admission control

. The idea is simple: once congestion has been signaled, no more virtual circuits are set up until the problem has gone away. Thus, attempts to set up new transport layer connections fail. Letting more people in just makes matters worse. While this approach is crude, it is simple and easy to carry out. In the telephone system, when a switch gets overloaded, it also practices admission control by not giving dial tones. 

An alternative approach is to allow new virtual circuits but carefully route all new virtual circuits around problem areas. For example, consider the subnet of 

Fig. 5-27(a)

, in which two routers are congested, as indicated. 

Figure 5-27. (a) A congested subnet. (b) A redrawn subnet that eliminates the congestion. A virtual circuit from 

A

 to 

B

 is also shown. 

 

Suppose that a host attached to router 

A

 wants to set up a connection to a host attached to router 

B

. Normally, this connection would pass through one of the congested routers. To avoid this situation, we can redraw the subnet as shown in 

Fig. 5-27(b)

, omitting the congested routers and all of their lines. The dashed line shows a possible route for the virtual circuit that avoids the congested routers. 

295




Another strategy relating to virtual circuits is to negotiate an agreement between the host and subnet when a virtual circuit is set up. This agreement normally specifies the volume and shape of the traffic, quality of service required, and other parameters. To keep its part of the agreement, the subnet will typically reserve resources along the path when the circuit is set up. These resources can include table and buffer space in the routers and bandwidth on the lines. In this way, congestion is unlikely to occur on the new virtual circuits because all the necessary resources are guaranteed to be available. 

This kind of reservation can be done all the time as standard operating procedure or only when the subnet is congested. A disadvantage of doing it all the time is that it tends to waste resources. If six virtual circuits that might use 1 Mbps all pass through the same physical 6-Mbps line, the line has to be marked as full, even though it may rarely happen that all six virtual circuits are transmitting full blast at the same time. Consequently, the price of the congestion control is unused (i.e., wasted) bandwidth in the normal case. 

5.3.4 Congestion Control in Datagram Subnets 

Let us now turn to some approaches that can be used in datagram subnets (and also in virtual-circuit subnets). Each router can easily monitor the utilization of its output lines and other resources. For example, it can associate with each line a real variable, 

u

, whose value, between 0.0 and 1.0, reflects the recent utilization of that line. To maintain a good estimate of 

u

, a sample of the instantaneous line utilization, 

f

 (either 0 or 1), can be made periodically and 

u

 updated according to 

 

 

where the constant 

a

 determines how fast the router forgets recent history. 

Whenever 

u

 moves above the threshold, the output line enters a ''warning'' state. Each newly-arriving packet is checked to see if its output line is in warning state. If it is, some action is taken. The action taken can be one of several alternatives, which we will now discuss. 

The Warning Bit 

The old DECNET architecture signaled the warning state by setting a special bit in the packet's header. So does frame relay. When the packet arrived at its destination, the transport entity copied the bit into the next acknowledgement sent back to the source. The source then cut back on traffic. 

As long as the router was in the warning state, it continued to set the warning bit, which meant that the source continued to get acknowledgements with it set. The source monitored the fraction of acknowledgements with the bit set and adjusted its transmission rate accordingly. As long as the warning bits continued to flow in, the source continued to decrease its transmission rate. When they slowed to a trickle, it increased its transmission rate. Note that since every router along the path could set the warning bit, traffic increased only when no router was in trouble. 

Choke Packets 

The previous congestion control algorithm is fairly subtle. It uses a roundabout means to tell the source to slow down. Why not just tell it directly? In this approach, the router sends a 

choke packet

 back to the source host, giving it the destination found in the packet. The 

296




original packet is tagged (a header bit is turned on) so that it will not generate any more choke packets farther along the path and is then forwarded in the usual way. 

When the source host gets the choke packet, it is required to reduce the traffic sent to the specified destination by 

X

 percent. Since other packets aimed at the same destination are probably already under way and will generate yet more choke packets, the host should ignore choke packets referring to that destination for a fixed time interval. After that period has expired, the host listens for more choke packets for another interval. If one arrives, the line is still congested, so the host reduces the flow still more and begins ignoring choke packets again. If no choke packets arrive during the listening period, the host may increase the flow again. The feedback implicit in this protocol can help prevent congestion yet not throttle any flow unless trouble occurs. 

Hosts can reduce traffic by adjusting their policy parameters, for example, their window size. Typically, the first choke packet causes the data rate to be reduced to 0.50 of its previous rate, the next one causes a reduction to 0.25, and so on. Increases are done in smaller increments to prevent congestion from reoccurring quickly. 

Several variations on this congestion control algorithm have been proposed. For one, the routers can maintain several thresholds. Depending on which threshold has been crossed, the choke packet can contain a mild warning, a stern warning, or an ultimatum. 

Another variation is to use queue lengths or buffer utilization instead of line utilization as the trigger signal. The same exponential weighting can be used with this metric as with 

u

, of course. 

Hop-by-Hop Choke Packets 

At high speeds or over long distances, sending a choke packet to the source hosts does not work well because the reaction is so slow. Consider, for example, a host in San Francisco (router 

A

 in 

Fig. 5-28

) that is sending traffic to a host in New York (router 

D

 in 

Fig. 5-28

) at 155 Mbps. If the New York host begins to run out of buffers, it will take about 30 msec for a choke packet to get back to San Francisco to tell it to slow down. The choke packet propagation is shown as the second, third, and fourth steps in 

Fig. 5-28(a)

. In those 30 msec, another 4.6 megabits will have been sent. Even if the host in San Francisco completely shuts down immediately, the 4.6 megabits in the pipe will continue to pour in and have to be dealt with. Only in the seventh diagram in 

Fig. 5-28(a)

 will the New York router notice a slower flow. 

Figure 5-28. (a) A choke packet that affects only the source. (b) A choke packet that affects each hop it passes through. 

297




 

An alternative approach is to have the choke packet take effect at every hop it passes through, as shown in the sequence of 

Fig. 5-28(b)

. Here, as soon as the choke packet reaches 

F

, 

F

 is required to reduce the flow to 

D

. Doing so will require 

F

 to devote more buffers to the flow, since the source is still sending away at full blast, but it gives 

D

 immediate relief, like a headache remedy in a television commercial. In the next step, the choke packet reaches 

E

, which tells 

E

 to reduce the flow to 

F

. This action puts a greater demand on 

E

's buffers but gives 

F

 immediate relief. Finally, the choke packet reaches 

A

 and the flow genuinely slows down. 

The net effect of this hop-by-hop scheme is to provide quick relief at the point of congestion at the price of using up more buffers upstream. In this way, congestion can be nipped in the bud without losing any packets. The idea is discussed in detail and simulation results are given in (Mishra and Kanakia, 1992). 

298




5.3.5 Load Shedding 

When none of the above methods make the congestion disappear, routers can bring out the heavy artillery: load shedding. 

Load shedding

 is a fancy way of saying that when routers are being inundated by packets that they cannot handle, they just throw them away. The term comes from the world of electrical power generation, where it refers to the practice of utilities intentionally blacking out certain areas to save the entire grid from collapsing on hot summer days when the demand for electricity greatly exceeds the supply. 

A router drowning in packets can just pick packets at random to drop, but usually it can do better than that. Which packet to discard may depend on the applications running. For file transfer, an old packet is worth more than a new one because dropping packet 6 and keeping packets 7 through 10 will cause a gap at the receiver that may force packets 6 through 10 to be retransmitted (if the receiver routinely discards out-of-order packets). In a 12-packet file, dropping 6 may require 7 through 12 to be retransmitted, whereas dropping 10 may require only 10 through 12 to be retransmitted. In contrast, for multimedia, a new packet is more important than an old one. The former policy (old is better than new) is often called 

wine

 and the latter (new is better than old) is often called 

milk

. 

A step above this in intelligence requires cooperation from the senders. For many applications, some packets are more important than others. For example, certain algorithms for compressing video periodically transmit an entire frame and then send subsequent frames as differences from the last full frame. In this case, dropping a packet that is part of a difference is preferable to dropping one that is part of a full frame. As another example, consider transmitting a document containing ASCII text and pictures. Losing a line of pixels in some image is far less damaging than losing a line of readable text. 

To implement an intelligent discard policy, applications must mark their packets in priority classes to indicate how important they are. If they do this, then when packets have to be discarded, routers can first drop packets from the lowest class, then the next lowest class, and so on. Of course, unless there is some significant incentive to mark packets as anything other than VERY IMPORTANT— NEVER, EVER DISCARD, nobody will do it. 

The incentive might be in the form of money, with the low-priority packets being cheaper to send than the high-priority ones. Alternatively, senders might be allowed to send high-priority packets under conditions of light load, but as the load increased they would be discarded, thus encouraging the users to stop sending them. 

Another option is to allow hosts to exceed the limits specified in the agreement negotiated when the virtual circuit was set up (e.g., use a higher bandwidth than allowed), but subject to the condition that all excess traffic be marked as low priority. Such a strategy is actually not a bad idea, because it makes more efficient use of idle resources, allowing hosts to use them as long as nobody else is interested, but without establishing a right to them when times get tough. 

Random Early Detection 

It is well known that dealing with congestion after it is first detected is more effective than letting it gum up the works and then trying to deal with it. This observation leads to the idea of discarding packets before all the buffer space is really exhausted. A popular algorithm for doing this is called 

RED

 (

Random Early Detection

) (Floyd and Jacobson, 1993). In some transport protocols (including TCP), the response to lost packets is for the source to slow down. The reasoning behind this logic is that TCP was designed for wired networks and wired networks are very reliable, so lost packets are mostly due to buffer overruns rather than transmission errors. This fact can be exploited to help reduce congestion. 

299




By having routers drop packets before the situation has become hopeless (hence the ''early'' in the name), the idea is that there is time for action to be taken before it is too late. To determine when to start discarding, routers maintain a running average of their queue lengths. When the average queue length on some line exceeds a threshold, the line is said to be congested and action is taken. 

Since the router probably cannot tell which source is causing most of the trouble, picking a packet at random from the queue that triggered the action is probably as good as it can do. 

How should the router tell the source about the problem? One way is to send it a choke packet, as we have described. A problem with that approach is that it puts even more load on the already congested network. A different strategy is to just discard the selected packet and not report it. The source will eventually notice the lack of acknowledgement and take action. Since it knows that lost packets are generally caused by congestion and discards, it will respond by slowing down instead of trying harder. This implicit form of feedback only works when sources respond to lost packets by slowing down their transmission rate. In wireless networks, where most losses are due to noise on the air link, this approach cannot be used. 

5.3.6 Jitter Control 

For applications such as audio and video streaming, it does not matter much if the packets take 20 msec or 30 msec to be delivered, as long as the transit time is constant. The variation (i.e., standard deviation) in the packet arrival times is called 

jitter

. High jitter, for example, having some packets taking 20 msec and others taking 30 msec to arrive will give an uneven quality to the sound or movie. Jitter is illustrated in 

Fig. 5-29

. In contrast, an agreement that 99 percent of the packets be delivered with a delay in the range of 24.5 msec to 25.5 msec might be acceptable. 

Figure 5-29. (a) High jitter. (b) Low jitter. 

 

The range chosen must be feasible, of course. It must take into account the speed-of-light transit time and the minimum delay through the routers and perhaps leave a little slack for some inevitable delays. 

The jitter can be bounded by computing the expected transit time for each hop along the path. When a packet arrives at a router, the router checks to see how much the packet is behind or ahead of its schedule. This information is stored in the packet and updated at each hop. If the packet is ahead of schedule, it is held just long enough to get it back on schedule. If it is behind schedule, the router tries to get it out the door quickly. 

In fact, the algorithm for determining which of several packets competing for an output line should go next can always choose the packet furthest behind in its schedule. In this way, 

300




packets that are ahead of schedule get slowed down and packets that are behind schedule get speeded up, in both cases reducing the amount of jitter. 

In some applications, such as video on demand, jitter can be eliminated by buffering at the receiver and then fetching data for display from the buffer instead of from the network in real time. However, for other applications, especially those that require real-time interaction between people such as Internet telephony and videoconferencing, the delay inherent in buffering is not acceptable. 

Congestion control is an active area of research. The state-of-the-art is summarized in (Gevros et al., 2001). 

5.4 Quality of Service 

The techniques we looked at in the previous sections are designed to reduce congestion and improve network performance. However, with the growth of multimedia networking, often these ad hoc measures are not enough. Serious attempts at guaranteeing quality of service through network and protocol design are needed. In the following sections we will continue our study of network performance, but now with a sharper focus on ways to provide a quality of service matched to application needs. It should be stated at the start, however, that many of these ideas are in flux and are subject to change. 

5.4.1 Requirements 

A stream of packets from a source to a destination is called a 

flow

.Ina connection-oriented network, all the packets belonging to a flow follow the same route; in a connectionless network, they may follow different routes. The needs of each flow can be characterized by four primary parameters: reliability, delay, jitter, and bandwidth. Together these determine the 

QoS

 (

Quality of Service

) the flow requires. Several common applications and the stringency of their requirements are listed in 

Fig. 5-30

. 

Figure 5-30. How stringent the quality-of-service requirements are. 

 

The first four applications have stringent requirements on reliability. No bits may be delivered incorrectly. This goal is usually achieved by checksumming each packet and verifying the checksum at the destination. If a packet is damaged in transit, it is not acknowledged and will be retransmitted eventually. This strategy gives high reliability. The four final (audio/video) applications can tolerate errors, so no checksums are computed or verified. 

File transfer applications, including e-mail and video, are not delay sensitive. If all packets are delayed uniformly by a few seconds, no harm is done. Interactive applications, such as Web surfing and remote login, are more delay sensitive. Real-time applications, such as telephony and videoconferencing have strict delay requirements. If all the words in a telephone call are 

301




each delayed by exactly 2.000 seconds, the users will find the connection unacceptable. On the other hand, playing audio or video files from a server does not require low delay. 

The first three applications are not sensitive to the packets arriving with irregular time intervals between them. Remote login is somewhat sensitive to that, since characters on the screen will appear in little bursts if the connection suffers much jitter. Video and especially audio are extremely sensitive to jitter. If a user is watching a video over the network and the frames are all delayed by exactly 2.000 seconds, no harm is done. But if the transmission time varies randomly between 1 and 2 seconds, the result will be terrible. For audio, a jitter of even a few milliseconds is clearly audible. 

Finally, the applications differ in their bandwidth needs, with e-mail and remote login not needing much, but video in all forms needing a great deal. 

ATM networks classify flows in four broad categories with respect to their QoS demands as follows: 

1. Constant bit rate (e.g., telephony). 

2. Real-time variable bit rate (e.g., compressed videoconferencing). 

3. Non-real-time variable bit rate (e.g., watching a movie over the Internet). 

4. Available bit rate (e.g., file transfer). 

These categories are also useful for other purposes and other networks. Constant bit rate is an attempt to simulate a wire by providing a uniform bandwidth and a uniform delay. Variable bit rate occurs when video is compressed, some frames compressing more than others. Thus, sending a frame with a lot of detail in it may require sending many bits whereas sending a shot of a white wall may compress extremely well. Available bit rate is for applications, such as e-mail, that are not sensitive to delay or jitter. 

5.4.2 Techniques for Achieving Good Quality of Service 

Now that we know something about QoS requirements, how do we achieve them? Well, to start with, there is no magic bullet. No single technique provides efficient, dependable QoS in an optimum way. Instead, a variety of techniques have been developed, with practical solutions often combining multiple techniques. We will now examine some of the techniques system designers use to achieve QoS. 

Overprovisioning 

An easy solution is to provide so much router capacity, buffer space, and bandwidth that the packets just fly through easily. The trouble with this solution is that it is expensive. As time goes on and designers have a better idea of how much is enough, this technique may even become practical. To some extent, the telephone system is overprovisioned. It is rare to pick up a telephone and not get a dial tone instantly. There is simply so much capacity available there that demand can always be met. 

Buffering 

Flows can be buffered on the receiving side before being delivered. Buffering them does not affect the reliability or bandwidth, and increases the delay, but it smooths out the jitter. For audio and video on demand, jitter is the main problem, so this technique helps a lot. 

We saw the difference between high jitter and low jitter in 

Fig. 5-29

. In 

Fig. 5-31

 we see a stream of packets being delivered with substantial jitter. Packet 1 is sent from the server at 

t

 = 0 sec and arrives at the client at 

t

 = 1 sec. Packet 2 undergoes more delay and takes 2 sec to arrive. As the packets arrive, they are buffered on the client machine. 

302




Figure 5-31. Smoothing the output stream by buffering packets. 

 

At 

t

 = 10 sec, playback begins. At this time, packets 1 through 6 have been buffered so that they can be removed from the buffer at uniform intervals for smooth play. Unfortunately, packet 8 has been delayed so much that it is not available when its play slot comes up, so playback must stop until it arrives, creating an annoying gap in the music or movie. This problem can be alleviated by delaying the starting time even more, although doing so also requires a larger buffer. Commercial Web sites that contain streaming audio or video all use players that buffer for about 10 seconds before starting to play. 

Traffic Shaping 

In the above example, the source outputs the packets with a uniform spacing between them, but in other cases, they may be emitted irregularly, which may cause congestion to occur in the network. Nonuniform output is common if the server is handling many streams at once, and it also allows other actions, such as fast forward and rewind, user authentication, and so on. Also, the approach we used here (buffering) is not always possible, for example, with videoconferencing. However, if something could be done to make the server (and hosts in general) transmit at a uniform rate, quality of service would be better. We will now examine a technique, 

traffic shaping

, which smooths out the traffic on the server side, rather than on the client side. 

Traffic shaping is about regulating the average 

rate

 (and burstiness) of data transmission. In contrast, the sliding window protocols we studied earlier limit the amount of data in transit at once, not the rate at which it is sent. When a connection is set up, the user and the subnet (i.e., the customer and the carrier) agree on a certain traffic pattern (i.e., shape) for that circuit. Sometimes this is called a 

service level agreement

. As long as the customer fulfills her part of the bargain and only sends packets according to the agreed-on contract, the carrier promises to deliver them all in a timely fashion. Traffic shaping reduces congestion and thus helps the carrier live up to its promise. Such agreements are not so important for file transfers but are of great importance for real-time data, such as audio and video connections, which have stringent quality-of-service requirements. 

In effect, with traffic shaping the customer says to the carrier: My transmission pattern will look like this; can you handle it? If the carrier agrees, the issue arises of how the carrier can tell if the customer is following the agreement and what to do if the customer is not. Monitoring a traffic flow is called 

traffic policing

. Agreeing to a traffic shape and policing it afterward are easier with virtual-circuit subnets than with datagram subnets. However, even with datagram subnets, the same ideas can be applied to transport layer connections. 

The Leaky Bucket Algorithm 

Imagine a bucket with a small hole in the bottom, as illustrated in 

Fig. 5-32(a)

. No matter the rate at which water enters the bucket, the outflow is at a constant rate, ?, when there is any water in the bucket and zero when the bucket is empty. Also, once the bucket is full, any 

303




additional water entering it spills over the sides and is lost (i.e., does not appear in the output stream under the hole). 

Figure 5-32. (a) A leaky bucket with water. (b) A leaky bucket with packets. 

 

The same idea can be applied to packets, as shown in 

Fig. 5-32(b)

. Conceptually, each host is connected to the network by an interface containing a leaky bucket, that is, a finite internal queue. If a packet arrives at the queue when it is full, the packet is discarded. In other words, if one or more processes within the host try to send a packet when the maximum number is already queued, the new packet is unceremoniously discarded. This arrangement can be built into the hardware interface or simulated by the host operating system. It was first proposed by Turner (1986) and is called the 

leaky bucket algorithm

. In fact, it is nothing other than a single-server queueing system with constant service time. 

The host is allowed to put one packet per clock tick onto the network. Again, this can be enforced by the interface card or by the operating system. This mechanism turns an uneven flow of packets from the user processes inside the host into an even flow of packets onto the network, smoothing out bursts and greatly reducing the chances of congestion. 

When the packets are all the same size (e.g., ATM cells), this algorithm can be used as described. However, when variable-sized packets are being used, it is often better to allow a fixed number of bytes per tick, rather than just one packet. Thus, if the rule is 1024 bytes per tick, a single 1024-byte packet can be admitted on a tick, two 512-byte packets, four 256-byte packets, and so on. If the residual byte count is too low, the next packet must wait until the next tick. 

Implementing the original leaky bucket algorithm is easy. The leaky bucket consists of a finite queue. When a packet arrives, if there is room on the queue it is appended to the queue; otherwise, it is discarded. At every clock tick, one packet is transmitted (unless the queue is empty). 

The byte-counting leaky bucket is implemented almost the same way. At each tick, a counter is initialized to 

n

. If the first packet on the queue has fewer bytes than the current value of the counter, it is transmitted, and the counter is decremented by that number of bytes. Additional packets may also be sent, as long as the counter is high enough. When the counter drops 

304




below the length of the next packet on the queue, transmission stops until the next tick, at which time the residual byte count is reset and the flow can continue. 

As an example of a leaky bucket, imagine that a computer can produce data at 25 million bytes/sec (200 Mbps) and that the network also runs at this speed. However, the routers can accept this data rate only for short intervals (basically, until their buffers fill up). For long intervals, they work best at rates not exceeding 2 million bytes/sec. Now suppose data comes in 1-million-byte bursts, one 40-msec burst every second. To reduce the average rate to 2 MB/sec, we could use a leaky bucket with ?=2 MB/sec and a capacity, 

C

, of 1 MB. This means that bursts of up to 1 MB can be handled without data loss and that such bursts are spread out over 500 msec, no matter how fast they come in. 

In 

Fig. 5-33(a)

 we see the input to the leaky bucket running at 25 MB/sec for 40 msec. In 

Fig. 

5-33(b)

 we see the output draining out at a uniform rate of 2 MB/sec for 500 msec. 

Figure 5-33. (a) Input to a leaky bucket. (b) Output from a leaky bucket. Output from a token bucket with capacities of (c) 250 KB, (d) 500 KB, and (e) 750 KB. (f) Output from a 500KB token bucket feeding a 10-MB/sec leaky bucket. 

 

305




The Token Bucket Algorithm 

The leaky bucket algorithm enforces a rigid output pattern at the average rate, no matter how bursty the traffic is. For many applications, it is better to allow the output to speed up somewhat when large bursts arrive, so a more flexible algorithm is needed, preferably one that never loses data. One such algorithm is the 

token bucket algorithm

. In this algorithm, the leaky bucket holds tokens, generated by a clock at the rate of one token every ?

T

 sec. In 

Fig. 

5-34(a)

 we see a bucket holding three tokens, with five packets waiting to be transmitted. For a packet to be transmitted, it must capture and destroy one token. In 

Fig. 5-34(b)

 we see that three of the five packets have gotten through, but the other two are stuck waiting for two more tokens to be generated. 

Figure 5-34. The token bucket algorithm. (a) Before. (b) After. 

 

The token bucket algorithm provides a different kind of traffic shaping than that of the leaky bucket algorithm. The leaky bucket algorithm does not allow idle hosts to save up permission to send large bursts later. The token bucket algorithm does allow saving, up to the maximum size of the bucket, 

n

. This property means that bursts of up to 

n

 packets can be sent at once, allowing some burstiness in the output stream and giving faster response to sudden bursts of input. 

Another difference between the two algorithms is that the token bucket algorithm throws away tokens (i.e., transmission capacity) when the bucket fills up but never discards packets. In contrast, the leaky bucket algorithm discards packets when the bucket fills up. 

Here, too, a minor variant is possible, in which each token represents the right to send not one packet, but 

k

 bytes. A packet can only be transmitted if enough tokens are available to cover its length in bytes. Fractional tokens are kept for future use. 

The leaky bucket and token bucket algorithms can also be used to smooth traffic between routers, as well as to regulate host output as in our examples. However, one clear difference is that a token bucket regulating a host can make the host stop sending when the rules say it must. Telling a router to stop sending while its input keeps pouring in may result in lost data. 

The implementation of the basic token bucket algorithm is just a variable that counts tokens. The counter is incremented by one every ?

T

 and decremented by one whenever a packet is 

306




sent. When the counter hits zero, no packets may be sent. In the byte-count variant, the counter is incremented by 

k

 bytes every ?

T

 and decremented by the length of each packet sent. 

Essentially what the token bucket does is allow bursts, but up to a regulated maximum length. Look at 

Fig. 5-33(c)

 for example. Here we have a token bucket with a capacity of 250 KB. Tokens arrive at a rate allowing output at 2 MB/sec. Assuming the token bucket is full when the 1-MB burst arrives, the bucket can drain at the full 25 MB/sec for about 11 msec. Then it has to cut back to 2 MB/sec until the entire input burst has been sent. 

Calculating the length of the maximum rate burst is slightly tricky. It is not just 1 MB divided by 25 MB/sec because while the burst is being output, more tokens arrive. If we call the burst length 

S

 sec, the token bucket capacity 

C

 bytes, the token arrival rate ? bytes/sec, and the maximum output rate 

M

 bytes/sec, we see that an output burst contains a maximum of 

C

 + ?

S

 bytes. We also know that the number of bytes in a maximum-speed burst of length 

S

 seconds is 

MS

. Hence we have 

 

 

We can solve this equation to get 

S

 = 

C/

(

M

 - ?). For our parameters of 

C

 = 250 KB, 

M

 = 25 MB/sec, and ?=2 MB/sec, we get a burst time of about 11 msec. 

Figure 5-33(d)

 and 

Fig. 5-

33(e)

 show the token bucket for capacities of 500 KB and 750 KB, respectively. 

A potential problem with the token bucket algorithm is that it allows large bursts again, even though the maximum burst interval can be regulated by careful selection of ? and 

M

. It is frequently desirable to reduce the peak rate, but without going back to the low value of the original leaky bucket. 

One way to get smoother traffic is to insert a leaky bucket after the token bucket. The rate of the leaky bucket should be higher than the token bucket's ? but lower than the maximum rate of the network. 

Figure 5-33(f)

 shows the output for a 500-KB token bucket followed by a 10-MB/sec leaky bucket. 

Policing all these schemes can be a bit tricky. Essentially, the network has to simulate the algorithm and make sure that no more packets or bytes are being sent than are permitted. Nevertheless, these tools provide ways to shape the network traffic into more manageable forms to assist meeting quality-of-service requirements. 

Resource Reservation 

Being able to regulate the shape of the offered traffic is a good start to guaranteeing the quality of service. However, effectively using this information implicitly means requiring all the packets of a flow to follow the same route. Spraying them over routers at random makes it hard to guarantee anything. As a consequence, something similar to a virtual circuit has to be set up from the source to the destination, and all the packets that belong to the flow must follow this route. 

Once we have a specific route for a flow, it becomes possible to reserve resources along that route to make sure the needed capacity is available. Three different kinds of resources can potentially be reserved: 

1. Bandwidth. 

2. Buffer space. 

3. CPU cycles. 

307




The first one, bandwidth, is the most obvious. If a flow requires 1 Mbps and the outgoing line has a capacity of 2 Mbps, trying to direct three flows through that line is not going to work. Thus, reserving bandwidth means not oversubscribing any output line. 

A second resource that is often in short supply is buffer space. When a packet arrives, it is usually deposited on the network interface card by the hardware itself. The router software then has to copy it to a buffer in RAM and queue that buffer for transmission on the chosen outgoing line. If no buffer is available, the packet has to be discarded since there is no place to put it. For a good quality of service, some buffers can be reserved for a specific flow so that flow does not have to compete for buffers with other flows. There will always be a buffer available when the flow needs one, up to some maximum. 

Finally, CPU cycles are also a scarce resource. It takes router CPU time to process a packet, so a router can process only a certain number of packets per second. Making sure that the CPU is not overloaded is needed to ensure timely processing of each packet. 

At first glance, it might appear that if it takes, say, 1 µsec to process a packet, a router can process 1 million packets/sec. This observation is not true because there will always be idle periods due to statistical fluctuations in the load. If the CPU needs every single cycle to get its work done, losing even a few cycles due to occasional idleness creates a backlog it can never get rid of. 

However, even with a load slightly below the theoretical capacity, queues can build up and delays can occur. Consider a situation in which packets arrive at random with a mean arrival rate of ? packets/sec. The CPU time required by each one is also random, with a mean processing capacity of µ packets/sec. Under the assumption that both the arrival and service distributions are Poisson distributions, it can be proven using queueing theory that the mean delay experienced by a packet, 

T

, is 

 

 

where ? = ?

/

µ is the CPU utilization. The first factor, 1

/

µ, is what the service time would be in the absence of competition. The second factor is the slowdown due to competition with other flows. For example, if ? = 950,000 packets/sec and µ = 1,000,000 packets/sec, then ? = 0.95 and the mean delay experienced by each packet will be 20 µsec instead of 1 µsec. This time accounts for both the queueing time and the service time, as can be seen when the load is very low (?

/

µ 

0). If there are, say, 30 routers along the flow's route, queueing delay alone will account for 600 µsec of delay. 

Admission Control 

Now we are at the point where the incoming traffic from some flow is well shaped and can potentially follow a single route in which capacity can be reserved in advance on the routers along the path. When such a flow is offered to a router, it has to decide, based on its capacity and how many commitments it has already made for other flows, whether to admit or reject the flow. 

The decision to accept or reject a flow is not a simple matter of comparing the (bandwidth, buffers, cycles) requested by the flow with the router's excess capacity in those three dimensions. It is a little more complicated than that. To start with, although some applications may know about their bandwidth requirements, few know about buffers or CPU cycles, so at the minimum, a different way is needed to describe flows. Next, some applications are far more tolerant of an occasional missed deadline than others. Finally, some applications may be 

308




willing to haggle about the flow parameters and others may not. For example, a movie viewer that normally runs at 30 frames/sec may be willing to drop back to 25 frames/sec if there is not enough free bandwidth to support 30 frames/sec. Similarly, the number of pixels per frame, audio bandwidth, and other properties may be adjustable. 

Because many parties may be involved in the flow negotiation (the sender, the receiver, and all the routers along the path between them), flows must be described accurately in terms of specific parameters that can be negotiated. A set of such parameters is called a 

flow specification

. Typically, the sender (e.g., the video server) produces a flow specification proposing the parameters it would like to use. As the specification propagates along the route, each router examines it and modifies the parameters as need be. The modifications can only reduce the flow, not increase it (e.g., a lower data rate, not a higher one). When it gets to the other end, the parameters can be established. 

As an example of what can be in a flow specification, consider the example of 

Fig. 5-35

, which is based on RFCs 2210 and 2211. It has five parameters, the first of which, the 

Token bucket rate

, is the number of bytes per second that are put into the bucket. This is the maximum sustained rate the sender may transmit, averaged over a long time interval. 

Figure 5-35. An example flow specification. 

 

The second parameter is the size of the bucket in bytes. If, for example, the 

Token bucket rate

 is 1 Mbps and the 

Token bucket size

 is 500 KB, the bucket can fill continuously for 4 sec before it fills up (in the absence of any transmissions). Any tokens sent after that are lost. 

The third parameter, the 

Peak data rate

, is the maximum tolerated transmission rate, even for brief time intervals. The sender must never exceed this rate. 

The last two parameters specify the minimum and maximum packet sizes, including the transport and network layer headers (e.g., TCP and IP). The minimum size is important because processing each packet takes some fixed time, no matter how short. A router may be prepared to handle 10,000 packets/sec of 1 KB each, but not be prepared to handle 100,000 packets/sec of 50 bytes each, even though this represents a lower data rate. The maximum packet size is important due to internal network limitations that may not be exceeded. For example, if part of the path goes over an Ethernet, the maximum packet size will be restricted to no more than 1500 bytes no matter what the rest of the network can handle. 

An interesting question is how a router turns a flow specification into a set of specific resource reservations. That mapping is implementation specific and is not standardized. Suppose that a router can process 100,000 packets/sec. If it is offered a flow of 1 MB/sec with minimum and maximum packet sizes of 512 bytes, the router can calculate that it might get 2048 packets/sec from that flow. In that case, it must reserve 2% of its CPU for that flow, preferably more to avoid long queueing delays. If a router's policy is never to allocate more than 50% of its CPU (which implies a factor of two delay, and it is already 49% full, then this flow must be rejected. Similar calculations are needed for the other resources. 

The tighter the flow specification, the more useful it is to the routers. If a flow specification states that it needs a 

Token bucket rate

 of 5 MB/sec but packets can vary from 50 bytes to 

309




1500 bytes, then the packet rate will vary from about 3500 packets/sec to 105,000 packets/sec. The router may panic at the latter number and reject the flow, whereas with a minimum packet size of 1000 bytes, the 5 MB/sec flow might have been accepted. 

Proportional Routing 

Most routing algorithms try to find the best path for each destination and send all traffic to that destination over the best path. A different approach that has been proposed to provide a higher quality of service is to split the traffic for each destination over multiple paths. Since routers generally do not have a complete overview of network-wide traffic, the only feasible way to split traffic over multiple routes is to use locally-available information. A simple method is to divide the traffic equally or in proportion to the capacity of the outgoing links. However, more sophisticated algorithms are also available (Nelakuditi and Zhang, 2002). 

Packet Scheduling 

If a router is handling multiple flows, there is a danger that one flow will hog too much of its capacity and starve all the other flows. Processing packets in the order of their arrival means that an aggressive sender can capture most of the capacity of the routers its packets traverse, reducing the quality of service for others. To thwart such attempts, various packet scheduling algorithms have been devised (Bhatti and Crowcroft, 2000). 

One of the first ones was the 

fair queueing

 algorithm (Nagle, 1987). The essence of the algorithm is that routers have separate queues for each output line, one for each flow. When a line becomes idle, the router scans the queues round robin, taking the first packet on the next queue. In this way, with 

n

 hosts competing for a given output line, each host gets to send one out of every 

n

 packets. Sending more packets will not improve this fraction. 

Although a start, the algorithm has a problem: it gives more bandwidth to hosts that use large packets than to hosts that use small packets. Demers et al. (1990) suggested an improvement in which the round robin is done in such a way as to simulate a byte-by-byte round robin, instead of a packet-by-packet round robin. In effect, it scans the queues repeatedly, byte-for-byte, until it finds the tick on which each packet will be finished. The packets are then sorted in order of their finishing and sent in that order. The algorithm is illustrated in 

Fig. 5-36

. 

Figure 5-36. (a) A router with five packets queued for line 

O

. (b) Finishing times for the five packets. 

 

In 

Fig. 5-36(a)

 we see packets of length 2 to 6 bytes. At (virtual) clock tick 1, the first byte of the packet on line 

A

 is sent. Then goes the first byte of the packet on line 

B

, and so on. The first packet to finish is 

C

, after eight ticks. The sorted order is given in 

Fig. 5-36(b)

. In the absence of new arrivals, the packets will be sent in the order listed, from 

C

 to 

A

. 

One problem with this algorithm is that it gives all hosts the same priority. In many situations, it is desirable to give video servers more bandwidth than regular file servers so that they can 

310




be given two or more bytes per tick. This modified algorithm is called 

weighted fair queueing

 and is widely used. Sometimes the weight is equal to the number of flows coming out of a machine, so each process gets equal bandwidth. An efficient implementation of the algorithm is discussed in (Shreedhar and Varghese, 1995). Increasingly, the actual forwarding of packets through a router or switch is being done in hardware (Elhanany et al., 2001). 

5.4.3 Integrated Services 

Between 1995 and 1997, IETF put a lot of effort into devising an architecture for streaming multimedia. This work resulted in over two dozen RFCs, starting with RFCs 2205–2210. The generic name for this work is 

flow-based algorithms

 or 

integrated services

. It was aimed at both unicast and multicast applications. An example of the former is a single user streaming a video clip from a news site. An example of the latter is a collection of digital television stations broadcasting their programs as streams of IP packets to many receivers at various locations. Below we will concentrate on multicast, since unicast is a special case of multicast. 

In many multicast applications, groups can change membership dynamically, for example, as people enter a video conference and then get bored and switch to a soap opera or the croquet channel. Under these conditions, the approach of having the senders reserve bandwidth in advance does not work well, since it would require each sender to track all entries and exits of its audience. For a system designed to transmit television with millions of subscribers, it would not work at all. 

RSVP—The Resource reSerVation Protocol 

The main IETF protocol for the integrated services architecture is 

RSVP

. It is described in RFC 2205 and others. This protocol is used for making the reservations; other protocols are used for sending the data. RSVP allows multiple senders to transmit to multiple groups of receivers, permits individual receivers to switch channels freely, and optimizes bandwidth use while at the same time eliminating congestion. 

In its simplest form, the protocol uses multicast routing using spanning trees, as discussed earlier. Each group is assigned a group address. To send to a group, a sender puts the group's address in its packets. The standard multicast routing algorithm then builds a spanning tree covering all group members. The routing algorithm is not part of RSVP. The only difference from normal multicasting is a little extra information that is multicast to the group periodically to tell the routers along the tree to maintain certain data structures in their memories. 

As an example, consider the network of 

Fig. 5-37(a)

. Hosts 1 and 2 are multicast senders, and hosts 3, 4, and 5 are multicast receivers. In this example, the senders and receivers are disjoint, but in general, the two sets may overlap. The multicast trees for hosts 1 and 2 are shown in 

Fig. 5-37(b)

 and 

Fig. 5-37(c)

, respectively. 

Figure 5-37. (a) A network. (b) The multicast spanning tree for host 1. (c) The multicast spanning tree for host 2. 

311




 

To get better reception and eliminate congestion, any of the receivers in a group can send a reservation message up the tree to the sender. The message is propagated using the reverse path forwarding algorithm discussed earlier. At each hop, the router notes the reservation and reserves the necessary bandwidth. If insufficient bandwidth is available, it reports back failure. By the time the message gets back to the source, bandwidth has been reserved all the way from the sender to the receiver making the reservation request along the spanning tree. 

An example of such a reservation is shown in 

Fig. 5-38(a)

. Here host 3 has requested a channel to host 1. Once it has been established, packets can flow from 1 to 3 without congestion. Now consider what happens if host 3 next reserves a channel to the other sender, host 2, so the user can watch two television programs at once. A second path is reserved, as illustrated in 

Fig. 5-38(b)

. Note that two separate channels are needed from host 3 to router 

E

 because two independent streams are being transmitted. 

Figure 5-38. (a) Host 3 requests a channel to host 1. (b) Host 3 then requests a second channel, to host 2. (c) Host 5 requests a channel to host 1. 

 

312




Finally, in 

Fig. 5-38(c)

, host 5 decides to watch the program being transmitted by host 1 and also makes a reservation. First, dedicated bandwidth is reserved as far as router 

H

. However, this router sees that it already has a feed from host 1, so if the necessary bandwidth has already been reserved, it does not have to reserve any more. Note that hosts 3 and 5 might have asked for different amounts of bandwidth (e.g., 3 has a black-and-white television set, so it does not want the color information), so the capacity reserved must be large enough to satisfy the greediest receiver. 

When making a reservation, a receiver can (optionally) specify one or more sources that it wants to receive from. It can also specify whether these choices are fixed for the duration of the reservation or whether the receiver wants to keep open the option of changing sources later. The routers use this information to optimize bandwidth planning. In particular, two receivers are only set up to share a path if they both agree not to change sources later on. 

The reason for this strategy in the fully dynamic case is that reserved bandwidth is decoupled from the choice of source. Once a receiver has reserved bandwidth, it can switch to another source and keep that portion of the existing path that is valid for the new source. If host 2 is transmitting several video streams, for example, host 3 may switch between them at will without changing its reservation: the routers do not care what program the receiver is watching. 

5.4.4 Differentiated Services 

Flow-based algorithms have the potential to offer good quality of service to one or more flows because they reserve whatever resources are needed along the route. However, they also have a downside. They require an advance setup to establish each flow, something that does not scale well when there are thousands or millions of flows. Also, they maintain internal per-flow state in the routers, making them vulnerable to router crashes. Finally, the changes required to the router code are substantial and involve complex router-to-router exchanges for setting up the flows. As a consequence, few implementations of RSVP or anything like it exist yet. 

For these reasons, IETF has also devised a simpler approach to quality of service, one that can be largely implemented locally in each router without advance setup and without having the whole path involved. This approach is known as 

class-based

 (as opposed to flow-based) quality of service. IETF has standardized an architecture for it, called 

differentiated services

, which is described in RFCs 2474, 2475, and numerous others. We will now describe it. 

Differentiated services (DS) can be offered by a set of routers forming an administrative domain (e.g., an ISP or a telco). The administration defines a set of service classes with corresponding forwarding rules. If a customer signs up for DS, customer packets entering the domain may carry a 

Type of Service

 field in them, with better service provided to some classes (e.g., premium service) than to others. Traffic within a class may be required to conform to some specific shape, such as a leaky bucket with some specified drain rate. An operator with a nose for business might charge extra for each premium packet transported or might allow up to 

N

 premium packets per month for a fixed additional monthly fee. Note that this scheme requires no advance setup, no resource reservation, and no time-consuming end-to-end negotiation for each flow, as with integrated services. This makes DS relatively easy to implement. 

Class-based service also occurs in other industries. For example, package delivery companies often offer overnight, two-day, and three-day service. Airlines offer first class, business class, and cattle class service. Long-distance trains often have multiple service classes. Even the Paris subway has two service classes. For packets, the classes may differ in terms of delay, jitter, and probability of being discarded in the event of congestion, among other possibilities (but probably not roomier Ethernet frames). 

313




To make the difference between flow-based quality of service and class-based quality of service clearer, consider an example: Internet telephony. With a flow-based scheme, each telephone call gets its own resources and guarantees. With a class-based scheme, all the telephone calls together get the resources reserved for the class telephony. These resources cannot be taken away by packets from the file transfer class or other classes, but no telephone call gets any private resources reserved for it alone. 

Expedited Forwarding 

The choice of service classes is up to each operator, but since packets are often forwarded between subnets run by different operators, IETF is working on defining network-independent service classes. The simplest class is 

expedited forwarding

, so let us start with that one. It is described in RFC 3246. 

The idea behind expedited forwarding is very simple. Two classes of service are available: regular and expedited. The vast majority of the traffic is expected to be regular, but a small fraction of the packets are expedited. The expedited packets should be able to transit the subnet as though no other packets were present. A symbolic representation of this ''two-tube'' system is given in 

Fig. 5-39

. Note that there is still just one physical line. The two logical pipes shown in the figure represent a way to reserve bandwidth, not a second physical line. 

Figure 5-39. Expedited packets experience a traffic-free network. 

 

One way to implement this strategy is to program the routers to have two output queues for each outgoing line, one for expedited packets and one for regular packets. When a packet arrives, it is queued accordingly. Packet scheduling should use something like weighted fair queueing. For example, if 10% of the traffic is expedited and 90% is regular, 20% of the bandwidth could be dedicated to expedited traffic and the rest to regular traffic. Doing so would give the expedited traffic twice as much bandwidth as it needs in order to provide low delay for it. This allocation can be achieved by transmitting one expedited packet for every four regular packets (assuming the size distribution for both classes is similar). In this way, it is hoped that expedited packets see an unloaded subnet, even when there is, in fact, a heavy load. 

Assured Forwarding 

A somewhat more elaborate scheme for managing the service classes is called 

assured forwarding

. It is described in RFC 2597. It specifies that there shall be four priority classes, each class having its own resources. In addition, it defines three discard probabilities for packets that are undergoing congestion: low, medium, and high. Taken together, these two factors define 12 service classes. 

Figure 5-40

 shows one way packets might be processed under assured forwarding. Step 1 is to classify the packets into one of the four priority classes. This step might be done on the sending host (as shown in the figure) or in the ingress (first) router. The advantage of doing 

314




classification on the sending host is that more information is available about which packets belong to which flows there. 

Figure 5-40. A possible implementation of the data flow for assured forwarding. 

 

Step 2 is to mark the packets according to their class. A header field is needed for this purpose. Fortunately, an 8-bit 

Type of service

 field is available in the IP header, as we will see shortly. RFC 2597 specifies that six of these bits are to be used for the service class, leaving coding room for historical service classes and future ones. 

Step 3 is to pass the packets through a shaper/dropper filter that may delay or drop some of them to shape the four streams into acceptable forms, for example, by using leaky or token buckets. If there are too many packets, some of them may be discarded here, by discard category. More elaborate schemes involving metering or feedback are also possible. 

In this example, these three steps are performed on the sending host, so the output stream is now fed into the ingress router. It is worth noting that these steps may be performed by special networking software or even the operating system, to avoid having to change existing applications. 

5.4.5 Label Switching and MPLS 

While IETF was working out integrated services and differentiated services, several router vendors were working on better forwarding methods. This work focused on adding a label in front of each packet and doing the routing based on the label rather than on the destination address. Making the label an index into an internal table makes finding the correct output line becomes just a matter of table lookup. Using this technique, routing can be done very quickly and any necessary resources can be reserved along the path. 

Of course, labeling flows this way comes perilously close to virtual circuits. X.25, ATM, frame relay, and all other networks with a virtual-circuit subnet also put a label (i.e., virtual-circuit identifier) in each packet, look it up in a table, and route based on the table entry. Despite the fact that many people in the Internet community have an intense dislike for connection-oriented networking, the idea seems to keep coming back, this time to provide fast routing and quality of service. However, there are essential differences between the way the Internet handles route construction and the way connection-oriented networks do it, so the technique is certainly not traditional circuit switching. 

This ''new'' switching idea goes by various (proprietary) names, including 

label switching

 and 

tag switching

. Eventually, IETF began to standardize the idea under the name 

MPLS

 (

MultiProtocol Label Switching

). We will call it MPLS below. It is described in RFC 3031 and many other RFCs. 

315




As an aside, some people make a distinction between 

routing

 and 

switching

. Routing is the process of looking up a destination address in a table to find where to send it. In contrast, switching uses a label taken from the packet as an index into a forwarding table. These definitions are far from universal, however. 

The first problem is where to put the label. Since IP packets were not designed for virtual circuits, there is no field available for virtual-circuit numbers within the IP header. For this reason, a new MPLS header had to be added in front of the IP header. On a router-to-router line using PPP as the framing protocol, the frame format, including the PPP, MPLS, IP, and TCP headers, is as shown in 

Fig. 5-41

. In a sense, MPLS is thus layer 2.5. 

Figure 5-41. Transmitting a TCP segment using IP, MPLS, and PPP. 

 

The generic MPLS header has four fields, the most important of which is the 

Label

 field, which holds the index. The 

QoS

 field indicates the class of service. The 

S

 field relates to stacking multiple labels in hierarchical networks (discussed below). If it hits 0, the packet is discarded. This feature prevents infinite looping in the case of routing instability. 

Because the MPLS headers are not part of the network layer packet or the data link layer frame, MPLS is to a large extent independent of both layers. Among other things, this property means it is possible to build MPLS switches that can forward both IP packets and ATM cells, depending on what shows up. This feature is where the ''multiprotocol'' in the name MPLS came from. 

When an MPLS-enhanced packet (or cell) arrives at an MPLS-capable router, the label is used as an index into a table to determine the outgoing line to use and also the new label to use. This label swapping is used in all virtual-circuit subnets because labels have only local significance and two different routers can feed unrelated packets with the same label into another router for transmission on the same outgoing line. To be distinguishable at the other end, labels have to be remapped at every hop. We saw this mechanism in action in 

Fig. 5-3

. MPLS uses the same technique. 

One difference from traditional virtual circuits is the level of aggregation. It is certainly possible for each flow to have its own set of labels through the subnet. However, it is more common for routers to group multiple flows that end at a particular router or LAN and use a single label for them. The flows that are grouped together under a single label are said to belong to the same 

FEC

 (

Forwarding Equivalence Class

). This class covers not only where the packets are going, but also their service class (in the differentiated services sense) because all their packets are treated the same way for forwarding purposes. 

With traditional virtual-circuit routing, it is not possible to group several distinct paths with different end points onto the same virtual-circuit identifier because there would be no way to distinguish them at the final destination. With MPLS, the packets still contain their final destination address, in addition to the label, so that at the end of the labeled route the label header can be removed and forwarding can continue the usual way, using the network layer destination address. 

316




One major difference between MPLS and conventional VC designs is how the forwarding table is constructed. In traditional virtual-circuit networks, when a user wants to establish a connection, a setup packet is launched into the network to create the path and make the forwarding table entries. MPLS does not work that way because there is no setup phase for each connection (because that would break too much existing Internet software). 

Instead, there are two ways for the forwarding table entries to be created. In the 

data-driven

 approach, when a packet arrives, the first router it hits contacts the router downstream where the packet has to go and asks it to generate a label for the flow. This method is applied recursively. Effectively, this is on-demand virtual-circuit creation. 

The protocols that do this spreading are very careful to avoid loops. They often use a technique called 

colored threads

. The backward propagation of an FEC can be compared to pulling a uniquely colored thread back into the subnet. If a router ever sees a color it already has, it knows there is a loop and takes remedial action. The data-driven approach is primarily used on networks in which the underlying transport is ATM (such as much of the telephone system). 

The other way, used on networks not based on ATM, is the 

control-driven

 approach. It has several variants. One of these works like this. When a router is booted, it checks to see for which routes it is the final destination (e.g., which hosts are on its LAN). It then creates one or more FECs for them, allocates a label for each one, and passes the labels to its neighbors. They, in turn, enter the labels in their forwarding tables and send new labels to their neighbors, until all the routers have acquired the path. Resources can also be reserved as the path is constructed to guarantee an appropriate quality of service. 

MPLS can operate at multiple levels at once. At the highest level, each carrier can be regarded as a kind of metarouter, with there being a path through the metarouters from source to destination. This path can use MPLS. However, within each carrier's network, MPLS can also be used, leading to a second level of labeling. In fact, a packet may carry an entire stack of labels with it. The 

S

 bit in 

Fig. 5-41

 allows a router removing a label to know if there are any additional labels left. It is set to 1 for the bottom label and 0 for all the other labels. In practice, this facility is mostly used to implement virtual private networks and recursive tunnels. 

Although the basic ideas behind MPLS are straightforward, the details are extremely complicated, with many variations and optimizations, so we will not pursue this topic further. For more information, see (Davie and Rekhter, 2000; Lin et al., 2002; Pepelnjak and Guichard, 2001; and Wang, 2001). 

5.5 Internetworking 

Until now, we have implicitly assumed that there is a single homogeneous network, with each machine using the same protocol in each layer. Unfortunately, this assumption is wildly optimistic. Many different networks exist, including LANs, MANs, and WANs. Numerous protocols are in widespread use in every layer. In the following sections we will take a careful look at the issues that arise when two or more networks are connected to form an 

internet

. 

Considerable controversy exists about the question of whether today's abundance of network types is a temporary condition that will go away as soon as everyone realizes how wonderful [fill in your favorite network] is or whether it is an inevitable, but permanent, feature of the world that is here to stay. Having different networks invariably means having different protocols. 

We believe that a variety of different networks (and thus protocols) will always be around, for the following reasons. First of all, the installed base of different networks is large. Nearly all 

317




personal computers run TCP/IP. Many large businesses have mainframes running IBM's SNA. A substantial number of telephone companies operate ATM networks. Some personal computer LANs still use Novell NCP/IPX or AppleTalk. Finally, wireless is an up-and-coming area with a variety of protocols. This trend will continue for years due to legacy problems, new technology, and the fact that not all vendors perceive it in their interest for their customers to be able to easily migrate to another vendor's system. 

Second, as computers and networks get cheaper, the place where decisions get made moves downward in organizations. Many companies have a policy to the effect that purchases costing over a million dollars have to be approved by top management, purchases costing over 100,000 dollars have to be approved by middle management, but purchases under 100,000 dollars can be made by department heads without any higher approval. This can easily lead to the engineering department installing UNIX workstations running TCP/IP and the marketing department installing Macs with AppleTalk. 

Third, different networks (e.g., ATM and wireless) have radically different technology, so it should not be surprising that as new hardware developments occur, new software will be created to fit the new hardware. For example, the average home now is like the average office ten years ago: it is full of computers that do not talk to one another. In the future, it may be commonplace for the telephone, the television set, and other appliances all to be networked together so that they can be controlled remotely. This new technology will undoubtedly bring new networks and new protocols. 

As an example of how different networks might be connected, consider the example of 

Fig. 5-

42

. Here we see a corporate network with multiple locations tied together by a wide area ATM network. At one of the locations, an FDDI optical backbone is used to connect an Ethernet, an 802.11 wireless LAN, and the corporate data center's SNA mainframe network. 

Figure 5-42. A collection of interconnected networks. 

 

The purpose of interconnecting all these networks is to allow users on any of them to communicate with users on all the other ones and also to allow users on any of them to access data on any of them. Accomplishing this goal means sending packets from one network to another. Since networks often differ in important ways, getting packets from one network to another is not always so easy, as we will now see. 

5.5.1 How Networks Differ 

Networks can differ in many ways. Some of the differences, such as different modulation techniques or frame formats, are in the physical and data link layers, These differences will not concern us here. Instead, in 

Fig. 5-43

 we list some of the differences that can occur in the network layer. It is papering over these differences that makes internetworking more difficult than operating within a single network. 

318




Figure 5-43. Some of the many ways networks can differ. 

 

When packets sent by a source on one network must transit one or more foreign networks before reaching the destination network (which also may be different from the source network), many problems can occur at the interfaces between networks. To start with, when packets from a connection-oriented network must transit a connectionless one, they may be reordered, something the sender does not expect and the receiver is not prepared to deal with. Protocol conversions will often be needed, which can be difficult if the required functionality cannot be expressed. Address conversions will also be needed, which may require some kind of directory system. Passing multicast packets through a network that does not support multicasting requires generating separate packets for each destination. 

The differing maximum packet sizes used by different networks can be a major nuisance. How do you pass an 8000-byte packet through a network whose maximum size is 1500 bytes? Differing qualities of service is an issue when a packet that has real-time delivery constraints passes through a network that does not offer any real-time guarantees. 

Error, flow, and congestion control often differ among different networks. If the source and destination both expect all packets to be delivered in sequence without error but an intermediate network just discards packets whenever it smells congestion on the horizon, many applications will break. Also, if packets can wander around aimlessly for a while and then suddenly emerge and be delivered, trouble will occur if this behavior was not anticipated and dealt with. Different security mechanisms, parameter settings, and accounting rules, and even national privacy laws also can cause problems. 

5.5.2 How Networks Can Be Connected 

Networks can be interconnected by different devices, as we saw in 

Chap 4

. Let us briefly review that material. In the physical layer, networks can be connected by repeaters or hubs, which just move the bits from one network to an identical network. These are mostly analog devices and do not understand anything about digital protocols (they just regenerate signals). 

One layer up we find bridges and switches, which operate at the data link layer. They can accept frames, examine the MAC addresses, and forward the frames to a different network while doing minor protocol translation in the process, for example, from Ethernet to FDDI or to 802.11. 

In the network layer, we have routers that can connect two networks. If two networks have dissimilar network layers, the router may be able to translate between the packet formats, 

319




although packet translation is now increasingly rare. A router that can handle multiple protocols is called a 

multiprotocol router

. 

In the transport layer we find transport gateways, which can interface between two transport connections. For example, a transport gateway could allow packets to flow between a TCP network and an SNA network, which has a different transport protocol, by essentially gluing a TCP connection to an SNA connection. 

Finally, in the application layer, application gateways translate message semantics. As an example, gateways between Internet e-mail (RFC 822) and X.400 e-mail must parse the e-mail messages and change various header fields. 

In this chapter we will focus on internetworking in the network layer. To see how that differs from switching in the data link layer, examine 

Fig. 5-44

. In 

Fig. 5-44(a)

, the source machine, 

S

, wants to send a packet to the destination machine, 

D

. These machines are on different Ethernets, connected by a switch. 

S

 encapsulates the packet in a frame and sends it on its way. The frame arrives at the switch, which then determines that the frame has to go to LAN 2 by looking at its MAC address. The switch just removes the frame from LAN 1 and deposits it on LAN 2. 

Figure 5-44. (a) Two Ethernets connected by a switch. (b) Two Ethernets connected by routers. 

 

Now let us consider the same situation but with the two Ethernets connected by a pair of routers instead of a switch. The routers are connected by a point-to-point line, possibly a leased line thousands of kilometers long. Now the frame is picked up by the router and the packet removed from the frame's data field. The router examines the address in the packet (e.g., an IP address) and looks up this address in its routing table. Based on this address, it decides to send the packet to the remote router, potentially encapsulated in a different kind of frame, depending on the line protocol. At the far end, the packet is put into the data field of an Ethernet frame and deposited onto LAN 2. 

An essential difference between the switched (or bridged) case and the routed case is this. With a switch (or bridge), the entire frame is transported on the basis of its MAC address. With a router, the packet is extracted from the frame and the address in the packet is used for deciding where to send it. Switches do not have to understand the network layer protocol being used to switch packets. Routers do. 

5.5.3 Concatenated Virtual Circuits 

Two styles of internetworking are possible: a connection-oriented concatenation of virtual-circuit subnets, and a datagram internet style. We will now examine these in turn, but first a word of caution. In the past, most (public) networks were connection oriented (and frame relay, SNA, 802.16, and ATM still are). Then with the rapid acceptance of the Internet, datagrams became fashionable. However, it would be a mistake to think that datagrams are 

320




forever. In this business, the only thing that is forever is change. With the growing importance of multimedia networking, it is likely that connection-orientation will make a come-back in one form or another since it is easier to guarantee quality of service with connections than without them. Therefore, we will devote some space to connection-oriented networking below 

In the concatenated virtual-circuit model, shown in 

Fig. 5-45

, a connection to a host in a distant network is set up in a way similar to the way connections are normally established. The subnet sees that the destination is remote and builds a virtual circuit to the router nearest the destination network. Then it constructs a virtual circuit from that router to an external 

gateway

 (multiprotocol router). This gateway records the existence of the virtual circuit in its tables and proceeds to build another virtual circuit to a router in the next subnet. This process continues until the destination host has been reached. 

Figure 5-45. Internetworking using concatenated virtual circuits. 

 

Once data packets begin flowing along the path, each gateway relays incoming packets, converting between packet formats and virtual-circuit numbers as needed. Clearly, all data packets must traverse the same sequence of gateways. Consequently, packets in a flow are never reordered by the network. 

The essential feature of this approach is that a sequence of virtual circuits is set up from the source through one or more gateways to the destination. Each gateway maintains tables telling which virtual circuits pass through it, where they are to be routed, and what the new virtual-circuit number is. 

This scheme works best when all the networks have roughly the same properties. For example, if all of them guarantee reliable delivery of network layer packets, then barring a crash somewhere along the route, the flow from source to destination will also be reliable. Similarly, if none of them guarantee reliable delivery, then the concatenation of the virtual circuits is not reliable either. On the other hand, if the source machine is on a network that does guarantee reliable delivery but one of the intermediate networks can lose packets, the concatenation has fundamentally changed the nature of the service. 

Concatenated virtual circuits are also common in the transport layer. In particular, it is possible to build a bit pipe using, say, SNA, which terminates in a gateway, and have a TCP connection go from the gateway to the next gateway. In this manner, an end-to-end virtual circuit can be built spanning different networks and protocols. 

5.5.4 Connectionless Internetworking 

The alternative internetwork model is the datagram model, shown in 

Fig. 5-46

. In this model, the only service the network layer offers to the transport layer is the ability to inject datagrams into the subnet and hope for the best. There is no notion of a virtual circuit at all in 

321




the network layer, let alone a concatenation of them. This model does not require all packets belonging to one connection to traverse the same sequence of gateways. In 

Fig. 5-46

 datagrams from host 1 to host 2 are shown taking different routes through the internetwork. A routing decision is made separately for each packet, possibly depending on the traffic at the moment the packet is sent. This strategy can use multiple routes and thus achieve a higher bandwidth than the concatenated virtual-circuit model. On the other hand, there is no guarantee that the packets arrive at the destination in order, assuming that they arrive at all. 

Figure 5-46. A connectionless internet. 

 

The model of 

Fig. 5-46

 is not quite as simple as it looks. For one thing, if each network has its own network layer protocol, it is not possible for a packet from one network to transit another one. One could imagine the multiprotocol routers actually trying to translate from one format to another, but unless the two formats are close relatives with the same information fields, such conversions will always be incomplete and often doomed to failure. For this reason, conversion is rarely attempted. 

A second, and more serious, problem is addressing. Imagine a simple case: a host on the Internet is trying to send an IP packet to a host on an adjoining SNA network. The IP and SNA addresses are different. One would need a mapping between IP and SNA addresses in both directions. Furthermore, the concept of what is addressable is different. In IP, hosts (actually, interface cards) have addresses. In SNA, entities other than hosts (e.g., hardware devices) can also have addresses. At best, someone would have to maintain a database mapping everything to everything to the extent possible, but it would constantly be a source of trouble. 

Another idea is to design a universal ''internet'' packet and have all routers recognize it. This approach is, in fact, what IP is—a packet designed to be carried through many networks. Of course, it may turn out that IPv4 (the current Internet protocol) drives all other formats out of the market, IPv6 (the future Internet protocol) does not catch on, and nothing new is ever invented, but history suggests otherwise. Getting everybody to agree to a single format is difficult when companies perceive it to their commercial advantage to have a proprietary format that they control. 

Let us now briefly recap the two ways internetworking can be approached. The concatenated virtual-circuit model has essentially the same advantages as using virtual circuits within a single subnet: buffers can be reserved in advance, sequencing can be guaranteed, short headers can be used, and the troubles caused by delayed duplicate packets can be avoided. 

It also has the same disadvantages: table space required in the routers for each open connection, no alternate routing to avoid congested areas, and vulnerability to router failures along the path. It also has the disadvantage of being difficult, if not impossible, to implement if one of the networks involved is an unreliable datagram network. 

322




The properties of the datagram approach to internetworking are pretty much the same as those of datagram subnets: more potential for congestion, but also more potential for adapting to it, robustness in the face of router failures, and longer headers needed. Various adaptive routing algorithms are possible in an internet, just as they are within a single datagram network. 

A major advantage of the datagram approach to internetworking is that it can be used over subnets that do not use virtual circuits inside. Many LANs, mobile networks (e.g., aircraft and naval fleets), and even some WANs fall into this category. When an internet includes one of these, serious problems occur if the internetworking strategy is based on virtual circuits. 

5.5.5 Tunneling 

Handling the general case of making two different networks interwork is exceedingly difficult. However, there is a common special case that is manageable. This case is where the source and destination hosts are on the same type of network, but there is a different network in between. As an example, think of an international bank with a TCP/IP-based Ethernet in Paris, a TCP/IP-based Ethernet in London, and a non-IP wide area network (e.g., ATM) in between, as shown in 

Fig. 5-47

. 

Figure 5-47. Tunneling a packet from Paris to London. 

 

The solution to this problem is a technique called 

tunneling

. To send an IP packet to host 2, host 1 constructs the packet containing the IP address of host 2, inserts it into an Ethernet frame addressed to the Paris multiprotocol router, and puts it on the Ethernet. When the multiprotocol router gets the frame, it removes the IP packet, inserts it in the payload field of the WAN network layer packet, and addresses the latter to the WAN address of the London multiprotocol router. When it gets there, the London router removes the IP packet and sends it to host 2 inside an Ethernet frame. 

The WAN can be seen as a big tunnel extending from one multiprotocol router to the other. The IP packet just travels from one end of the tunnel to the other, snug in its nice box. It does not have to worry about dealing with the WAN at all. Neither do the hosts on either Ethernet. Only the multiprotocol router has to understand IP and WAN packets. In effect, the entire distance from the middle of one multiprotocol router to the middle of the other acts like a serial line. 

An analogy may make tunneling clearer. Consider a person driving her car from Paris to London. Within France, the car moves under its own power, but when it hits the English Channel, it is loaded into a high-speed train and transported to England through the Chunnel (cars are not permitted to drive through the Chunnel). Effectively, the car is being carried as freight, as depicted in 

Fig. 5-48

. At the far end, the car is let loose on the English roads and 

323




once again continues to move under its own power. Tunneling of packets through a foreign network works the same way. 

Figure 5-48. Tunneling a car from France to England. 

 

5.5.6 Internetwork Routing 

Routing through an internetwork is similar to routing within a single subnet, but with some added complications. Consider, for example, the internetwork of 

Fig. 5-49(a)

 in which five networks are connected by six (possibly multiprotocol) routers. Making a graph model of this situation is complicated by the fact that every router can directly access (i.e., send packets to) every other router connected to any network to which it is connected. For example, 

B

 in 

Fig. 5-

49(a)

 can directly access 

A

 and 

C

 via network 2 and also 

D

 via network 3. This leads to the graph of 

Fig. 5-49(b)

. 

Figure 5-49. (a) An internetwork. (b) A graph of the internetwork. 

 

Once the graph has been constructed, known routing algorithms, such as the distance vector and link state algorithms, can be applied to the set of multiprotocol routers. This gives a two-level routing algorithm: within each network an 

interior gateway protocol

 is used, but between the networks, an 

exterior gateway protocol

 is used (''gateway'' is an older term for ''router''). In fact, since each network is independent, they may all use different algorithms. Because each network in an internetwork is independent of all the others, it is often referred to as an 

Autonomous System

 (

AS

). 

A typical internet packet starts out on its LAN addressed to the local multiprotocol router (in the MAC layer header). After it gets there, the network layer code decides which multiprotocol router to forward the packet to, using its own routing tables. If that router can be reached using the packet's native network protocol, the packet is forwarded there directly. Otherwise it is tunneled there, encapsulated in the protocol required by the intervening network. This process is repeated until the packet reaches the destination network. 

One of the differences between internetwork routing and intranetwork routing is that internetwork routing may require crossing international boundaries. Various laws suddenly come into play, such as Sweden's strict privacy laws about exporting personal data about Swedish citizens from Sweden. Another example is the Canadian law saying that data traffic originating in Canada and ending in Canada may not leave the country. This law means that 

324




traffic from Windsor, Ontario to Vancouver may not be routed via nearby Detroit, even if that route is the fastest and cheapest. 

Another difference between interior and exterior routing is the cost. Within a single network, a single charging algorithm normally applies. However, different networks may be under different managements, and one route may be less expensive than another. Similarly, the quality of service offered by different networks may be different, and this may be a reason to choose one route over another. 

5.5.7 Fragmentation 

Each network imposes some maximum size on its packets. These limits have various causes, among them: 

1. Hardware (e.g., the size of an Ethernet frame). 

2. Operating system (e.g., all buffers are 512 bytes). 

3. Protocols (e.g., the number of bits in the packet length field). 

4. Compliance with some (inter)national standard. 

5. Desire to reduce error-induced retransmissions to some level. 

6. Desire to prevent one packet from occupying the channel too long. 

The result of all these factors is that the network designers are not free to choose any maximum packet size they wish. Maximum payloads range from 48 bytes (ATM cells) to 65,515 bytes (IP packets), although the payload size in higher layers is often larger. 

An obvious problem appears when a large packet wants to travel through a network whose maximum packet size is too small. One solution is to make sure the problem does not occur in the first place. In other words, the internet should use a routing algorithm that avoids sending packets through networks that cannot handle them. However, this solution is no solution at all. What happens if the original source packet is too large to be handled by the destination network? The routing algorithm can hardly bypass the destination. 

Basically, the only solution to the problem is to allow gateways to break up packets into 

fragments

, sending each fragment as a separate internet packet. However, as every parent of a small child knows, converting a large object into small fragments is considerably easier than the reverse process. (Physicists have even given this effect a name: the second law of thermodynamics.) Packet-switching networks, too, have trouble putting the fragments back together again. 

Two opposing strategies exist for recombining the fragments back into the original packet. The first strategy is to make fragmentation caused by a ''small-packet'' network transparent to any subsequent networks through which the packet must pass on its way to the ultimate destination. This option is shown in 

Fig. 5-50(a)

. In this approach, the small-packet network has gateways (most likely, specialized routers) that interface to other networks. When an oversized packet arrives at a gateway, the gateway breaks it up into fragments. Each fragment is addressed to the same exit gateway, where the pieces are recombined. In this way passage through the small-packet network has been made transparent. Subsequent networks are not even aware that fragmentation has occurred. ATM networks, for example, have special hardware to provide transparent fragmentation of packets into cells and then reassembly of cells into packets. In the ATM world, fragmentation is called segmentation; the concept is the same, but some of the details are different. 

Figure 5-50. (a) Transparent fragmentation. (b) Nontransparent fragmentation. 

325




 

Transparent fragmentation is straightforward but has some problems. For one thing, the exit gateway must know when it has received all the pieces, so either a count field or an ''end of packet'' bit must be provided. For another thing, all packets must exit via the same gateway. By not allowing some fragments to follow one route to the ultimate destination and other fragments a disjoint route, some performance may be lost. A last problem is the overhead required to repeatedly reassemble and then refragment a large packet passing through a series of small-packet networks. ATM requires transparent fragmentation. 

The other fragmentation strategy is to refrain from recombining fragments at any intermediate gateways. Once a packet has been fragmented, each fragment is treated as though it were an original packet. All fragments are passed through the exit gateway (or gateways), as shown in 

Fig. 5-50(b)

. Recombination occurs only at the destination host. IP works this way. 

Nontransparent fragmentation also has some problems. For example, it requires 

every

 host to be able to do reassembly. Yet another problem is that when a large packet is fragmented, the total overhead increases because each fragment must have a header. Whereas in the first method this overhead disappears as soon as the small-packet network is exited, in this method the overhead remains for the rest of the journey. An advantage of nontransparent fragmentation, however, is that multiple exit gateways can now be used and higher performance can be achieved. Of course, if the concatenated virtual-circuit model is being used, this advantage is of no use. 

When a packet is fragmented, the fragments must be numbered in such a way that the original data stream can be reconstructed. One way of numbering the fragments is to use a tree. If packet 0 must be split up, the pieces are called 0.0, 0.1, 0.2, etc. If these fragments themselves must be fragmented later on, the pieces are numbered 0.0.0, 0.0.1, 0.0.2, . . . , 0.1.0, 0.1.1, 0.1.2, etc. If enough fields have been reserved in the header for the worst case and no duplicates are generated anywhere, this scheme is sufficient to ensure that all the pieces can be correctly reassembled at the destination, no matter what order they arrive in. 

However, if even one network loses or discards packets, end-to-end retransmissions are needed, with unfortunate effects for the numbering system. Suppose that a 1024-bit packet is initially fragmented into four equal-sized fragments, 0.0, 0.1, 0.2, and 0.3. Fragment 0.1 is lost, but the other parts arrive at the destination. Eventually, the source times out and retransmits the original packet again. Only this time Murphy's law strikes and the route taken passes through a network with a 512-bit limit, so two fragments are generated. When the new fragment 0.1 arrives at the destination, the receiver will think that all four pieces are now accounted for and reconstruct the packet incorrectly. 

A completely different (and better) numbering system is for the internetwork protocol to define an elementary fragment size small enough that the elementary fragment can pass through 

326




every network. When a packet is fragmented, all the pieces are equal to the elementary fragment size except the last one, which may be shorter. An internet packet may contain several fragments, for efficiency reasons. The internet header must provide the original packet number and the number of the (first) elementary fragment contained in the packet. As usual, there must also be a bit indicating that the last elementary fragment contained within the internet packet is the last one of the original packet. 

This approach requires two sequence fields in the internet header: the original packet number and the fragment number. There is clearly a trade-off between the size of the elementary fragment and the number of bits in the fragment number. Because the elementary fragment size is presumed to be acceptable to every network, subsequent fragmentation of an internet packet containing several fragments causes no problem. The ultimate limit here is to have the elementary fragment be a single bit or byte, with the fragment number then being the bit or byte offset within the original packet, as shown in 

Fig. 5-51

. 

Figure 5-51. Fragmentation when the elementary data size is 1 byte. (a) Original packet, containing 10 data bytes. (b) Fragments after passing through a network with maximum packet size of 8 payload bytes plus header. (c) Fragments after passing through a size 5 gateway. 

 

Some internet protocols take this method even further and consider the entire transmission on a virtual circuit to be one giant packet, so that each fragment contains the absolute byte number of the first byte within the fragment. 

5.6 The Network Layer in the Internet 

Before getting into the specifics of the network layer in the Internet, it is worth taking at look at the principles that drove its design in the past and made it the success that it is today. All too often, nowadays, people seem to have forgotten them. These principles are enumerated and discussed in RFC 1958, which is well worth reading (and should be mandatory for all protocol designers—with a final exam at the end). This RFC draws heavily on ideas found in (Clark, 1988; and Saltzer et al., 1984). We will now summarize what we consider to be the top 10 principles (from most important to least important). 

1. Make sure it works. Do not finalize the design or standard until multiple prototypes have successfully communicated with each other. All too often designers first write a 

327




1000-page standard, get it approved, then discover it is deeply flawed and does not work. Then they write version 1.1 of the standard. This is not the way to go. 

2. Keep it simple. When in doubt, use the simplest solution. William of Occam stated this principle (Occam's razor) in the 14th century. Put in modern terms: fight features. If a feature is not absolutely essential, leave it out, especially if the same effect can be achieved by combining other features. 

3. Make clear choices. If there are several ways of doing the same thing, choose one. Having two or more ways to do the same thing is looking for trouble. Standards often have multiple options or modes or parameters because several powerful parties insist that their way is best. Designers should strongly resist this tendency. Just say no. 

4. Exploit modularity. This principle leads directly to the idea of having protocol stacks, each of whose layers is independent of all the other ones. In this way, if circumstances that require one module or layer to be changed, the other ones will not be affected. 

5. Expect heterogeneity. Different types of hardware, transmission facilities, and applications will occur on any large network. To handle them, the network design must be simple, general, and flexible. 

6. Avoid static options and parameters. If parameters are unavoidable (e.g., maximum packet size), it is best to have the sender and receiver negotiate a value than defining fixed choices. 

7. Look for a good design; it need not be perfect. Often the designers have a good design but it cannot handle some weird special case. Rather than messing up the design, the designers should go with the good design and put the burden of working around it on the people with the strange requirements. 

8. Be strict when sending and tolerant when receiving. In other words, only send packets that rigorously comply with the standards, but expect incoming packets that may not be fully conformant and try to deal with them. 

9. Think about scalability. If the system is to handle millions of hosts and billions of users effectively, no centralized databases of any kind are tolerable and load must be spread as evenly as possible over the available resources. 

10. Consider performance and cost. If a network has poor performance or outrageous costs, nobody will use it. 

Let us now leave the general principles and start looking at the details of the Internet's network layer. At the network layer, the Internet can be viewed as a collection of subnetworks or 

Autonomous Systems

 (

ASes

) that are interconnected. There is no real structure, but several major backbones exist. These are constructed from high-bandwidth lines and fast routers. Attached to the backbones are regional (midlevel) networks, and attached to these regional networks are the LANs at many universities, companies, and Internet service providers. A sketch of this quasi-hierarchical organization is given in 

Fig. 5-52

. 

Figure 5-52. The Internet is an interconnected collection of many networks. 

328




 

The glue that holds the whole Internet together is the network layer protocol, 

IP

 (

Internet Protocol

). Unlike most older network layer protocols, it was designed from the beginning with internetworking in mind. A good way to think of the network layer is this. Its job is to provide a best-efforts (i.e., not guaranteed) way to transport datagrams from source to destination, without regard to whether these machines are on the same network or whether there are other networks in between them. 

Communication in the Internet works as follows. The transport layer takes data streams and breaks them up into datagrams. In theory, datagrams can be up to 64 Kbytes each, but in practice they are usually not more than 1500 bytes (so they fit in one Ethernet frame). Each datagram is transmitted through the Internet, possibly being fragmented into smaller units as it goes. When all the pieces finally get to the destination machine, they are reassembled by the network layer into the original datagram. This datagram is then handed to the transport layer, which inserts it into the receiving process' input stream. As can be seen from 

Fig. 5-52

, a packet originating at host 1 has to traverse six networks to get to host 2. In practice, it is often much more than six. 

5.6.1 The IP Protocol 

An appropriate place to start our study of the network layer in the Internet is the format of the IP datagrams themselves. An IP datagram consists of a header part and a text part. The header has a 20-byte fixed part and a variable length optional part. The header format is shown in 

Fig. 5-53

. It is transmitted in big-endian order: from left to right, with the high-order bit of the 

Version

 field going first. (The SPARC is big endian; the Pentium is little-endian.) On little endian machines, software conversion is required on both transmission and reception. 

Figure 5-53. The IPv4 (Internet Protocol) header. 

329




 

The 

Version

 field keeps track of which version of the protocol the datagram belongs to. By including the version in each datagram, it becomes possible to have the transition between versions take years, with some machines running the old version and others running the new one. Currently a transition between IPv4 and IPv6 is going on, has already taken years, and is by no means close to being finished (Durand, 2001; Wiljakka, 2002; and Waddington and Chang, 2002). Some people even think it will never happen (Weiser, 2001). As an aside on numbering, IPv5 was an experimental real-time stream protocol that was never widely used. 

Since the header length is not constant, a field in the header, 

IHL

, is provided to tell how long the header is, in 32-bit words. The minimum value is 5, which applies when no options are present. The maximum value of this 4-bit field is 15, which limits the header to 60 bytes, and thus the 

Options

 field to 40 bytes. For some options, such as one that records the route a packet has taken, 40 bytes is far too small, making that option useless. 

The 

Type of service

 field is one of the few fields that has changed its meaning (slightly) over the years. It was and is still intended to distinguish between different classes of service. Various combinations of reliability and speed are possible. For digitized voice, fast delivery beats accurate delivery. For file transfer, error-free transmission is more important than fast transmission. 

Originally, the 6-bit field contained (from left to right), a three-bit 

Precedence

 field and three flags, 

D

, 

T

, and 

R

. The 

Precedence

 field was a priority, from 0 (normal) to 7 (network control packet). The three flag bits allowed the host to specify what it cared most about from the set {Delay, Throughput, Reliability}. In theory, these fields allow routers to make choices between, for example, a satellite link with high throughput and high delay or a leased line with low throughput and low delay. In practice, current routers often ignore the 

Type of service

 field altogether. 

Eventually, IETF threw in the towel and changed the field slightly to accommodate differentiated services. Six of the bits are used to indicate which of the service classes discussed earlier each packet belongs to. These classes include the four queueing priorities, three discard probabilities, and the historical classes. 

The 

Total length

 includes everything in the datagram—both header and data. The maximum length is 65,535 bytes. At present, this upper limit is tolerable, but with future gigabit networks, larger datagrams may be needed. 

The 

Identification

 field is needed to allow the destination host to determine which datagram a newly arrived fragment belongs to. All the fragments of a datagram contain the same 

Identification

 value. 

330




Next comes an unused bit and then two 1-bit fields. 

DF

 stands for Don't Fragment. It is an order to the routers not to fragment the datagram because the destination is incapable of putting the pieces back together again. For example, when a computer boots, its ROM might ask for a memory image to be sent to it as a single datagram. By marking the datagram with the 

DF

 bit, the sender knows it will arrive in one piece, even if this means that the datagram must avoid a small-packet network on the best path and take a suboptimal route. All machines are required to accept fragments of 576 bytes or less. 

MF

 stands for More Fragments. All fragments except the last one have this bit set. It is needed to know when all fragments of a datagram have arrived. 

The 

Fragment offset

 tells where in the current datagram this fragment belongs. All fragments except the last one in a datagram must be a multiple of 8 bytes, the elementary fragment unit. Since 13 bits are provided, there is a maximum of 8192 fragments per datagram, giving a maximum datagram length of 65,536 bytes, one more than the 

Total length

 field. 

The 

Time to live

 field is a counter used to limit packet lifetimes. It is supposed to count time in seconds, allowing a maximum lifetime of 255 sec. It must be decremented on each hop and is supposed to be decremented multiple times when queued for a long time in a router. In practice, it just counts hops. When it hits zero, the packet is discarded and a warning packet is sent back to the source host. This feature prevents datagrams from wandering around forever, something that otherwise might happen if the routing tables ever become corrupted. 

When the network layer has assembled a complete datagram, it needs to know what to do with it. The 

Protocol

 field tells it which transport process to give it to. TCP is one possibility, but so are UDP and some others. The numbering of protocols is global across the entire Internet. Protocols and other assigned numbers were formerly listed in RFC 1700, but nowadays they are contained in an on-line data base located at 

www.iana.org

. 

The 

Header checksum

 verifies the header only. Such a checksum is useful for detecting errors generated by bad memory words inside a router. The algorithm is to add up all the 16-bit halfwords as they arrive, using one's complement arithmetic and then take the one's complement of the result. For purposes of this algorithm, the 

Header checksum

 is assumed to be zero upon arrival. This algorithm is more robust than using a normal add. Note that the 

Header checksum

 must be recomputed at each hop because at least one field always changes (the 

Time to live

 field), but tricks can be used to speed up the computation. 

The 

Source address

 and 

Destination address

 indicate the network number and host number. We will discuss Internet addresses in the next section. The 

Options

 field was designed to provide an escape to allow subsequent versions of the protocol to include information not present in the original design, to permit experimenters to try out new ideas, and to avoid allocating header bits to information that is rarely needed. The options are variable length. Each begins with a 1-byte code identifying the option. Some options are followed by a 1-byte option length field, and then one or more data bytes. The 

Options

 field is padded out to a multiple of four bytes. Originally, five options were defined, as listed in 

Fig. 5-54

, but since then some new ones have been added. The current complete list is now maintained on-line at 

www.iana.org/assignments/ip-parameters

. 

Figure 5-54. Some of the IP options. 

331




 

The 

Security

 option tells how secret the information is. In theory, a military router might use this field to specify not to route through certain countries the military considers to be ''bad guys.'' In practice, all routers ignore it, so its only practical function is to help spies find the good stuff more easily. 

The 

Strict source routing

 option gives the complete path from source to destination as a sequence of IP addresses. The datagram is required to follow that exact route. It is most useful for system managers to send emergency packets when the routing tables are corrupted, or for making timing measurements. 

The 

Loose source routing

 option requires the packet to traverse the list of routers specified, and in the order specified, but it is allowed to pass through other routers on the way. Normally, this option would only provide a few routers, to force a particular path. For example, to force a packet from London to Sydney to go west instead of east, this option might specify routers in New York, Los Angeles, and Honolulu. This option is most useful when political or economic considerations dictate passing through or avoiding certain countries. 

The 

Record route

 option tells the routers along the path to append their IP address to the option field. This allows system managers to track down bugs in the routing algorithms (''Why are packets from Houston to Dallas visiting Tokyo first?''). When the ARPANET was first set up, no packet ever passed through more than nine routers, so 40 bytes of option was ample. As mentioned above, now it is too small. 

Finally, the 

Timestamp

 option is like the 

Record route

 option, except that in addition to recording its 32-bit IP address, each router also records a 32-bit timestamp. This option, too, is mostly for debugging routing algorithms. 

5.6.2 IP Addresses 

Every host and router on the Internet has an IP address, which encodes its network number and host number. The combination is unique: in principle, no two machines on the Internet have the same IP address. All IP addresses are 32 bits long and are used in the 

Source address

 and 

Destination address

 fields of IP packets. It is important to note that an IP address does not actually refer to a host. It really refers to a network interface, so if a host is on two networks, it must have two IP addresses. However, in practice, most hosts are on one network and thus have one IP address. 

For several decades, IP addresses were divided into the five categories listed in 

Fig. 5-55

. This allocation has come to be called 

classful addressing

.Itisno longer used, but references to it in the literature are still common. We will discuss the replacement of classful addressing shortly. 

Figure 5-55. IP address formats. 

332




 

The class A, B, C, and D formats allow for up to 128 networks with 16 million hosts each, 16,384 networks with up to 64K hosts, and 2 million networks (e.g., LANs) with up to 256 hosts each (although a few of these are special). Also supported is multicast, in which a datagram is directed to multiple hosts. Addresses beginning with 1111 are reserved for future use. Over 500,000 networks are now connected to the Internet, and the number grows every year. Network numbers are managed by a nonprofit corporation called 

ICANN

 (

Internet Corporation for Assigned Names and Numbers

) to avoid conflicts. In turn, ICANN has delegated parts of the address space to various regional authorities, which then dole out IP addresses to ISPs and other companies. 

Network addresses, which are 32-bit numbers, are usually written in 

dotted decimal notation

. In this format, each of the 4 bytes is written in decimal, from 0 to 255. For example, the 32-bit hexadecimal address C0290614 is written as 192.41.6.20. The lowest IP address is 0.0.0.0 and the highest is 255.255.255.255. 

The values 0 and -1 (all 1s) have special meanings, as shown in 

Fig. 5-56

. The value 0 means this network or this host. The value of -1 is used as a broadcast address to mean all hosts on the indicated network. 

Figure 5-56. Special IP addresses. 

 

The IP address 0.0.0.0 is used by hosts when they are being booted. IP addresses with 0 as network number refer to the current network. These addresses allow machines to refer to their own network without knowing its number (but they have to know its class to know how many 0s to include). The address consisting of all 1s allows broadcasting on the local network, typically a LAN. The addresses with a proper network number and all 1s in the host field allow machines to send broadcast packets to distant LANs anywhere in the Internet (although many network administrators disable this feature). Finally, all addresses of the form 127.

xx.yy.zz

 are reserved for loopback testing. Packets sent to that address are not put out onto the wire; they are processed locally and treated as incoming packets. This allows packets to be sent to the local network without the sender knowing its number. 

333




Subnets 

As we have seen, all the hosts in a network must have the same network number. This property of IP addressing can cause problems as networks grow. For example, consider a university that started out with one class B network used by the Computer Science Dept. for the computers on its Ethernet. A year later, the Electrical Engineering Dept. wanted to get on the Internet, so they bought a repeater to extend the CS Ethernet to their building. As time went on, many other departments acquired computers and the limit of four repeaters per Ethernet was quickly reached. A different organization was required. 

Getting a second network address would be hard to do since network addresses are scarce and the university already had enough addresses for over 60,000 hosts. The problem is the rule that a single class A, B, or C address refers to one network, not to a collection of LANs. As more and more organizations ran into this situation, a small change was made to the addressing system to deal with it. 

The solution is to allow a network to be split into several parts for internal use but still act like a single network to the outside world. A typical campus network nowadays might look like that of 

Fig. 5-57

, with a main router connected to an ISP or regional network and numerous Ethernets spread around campus in different departments. Each of the Ethernets has its own router connected to the main router (possibly via a backbone LAN, but the nature of the interrouter connection is not relevant here). 

Figure 5-57. A campus network consisting of LANs for various departments. 

 

In the Internet literature, the parts of the network (in this case, Ethernets) are called 

subnets

. As we mentioned in 

Chap. 1

, this usage conflicts with ''subnet'' to mean the set of all routers and communication lines in a network. Hopefully, it will be clear from the context which meaning is intended. In this section and the next one, the new definition will be the one used exclusively. 

When a packet comes into the main router, how does it know which subnet (Ethernet) to give it to? One way would be to have a table with 65,536 entries in the main router telling which router to use for each host on campus. This idea would work, but it would require a very large table in the main router and a lot of manual maintenance as hosts were added, moved, or taken out of service. 

Instead, a different scheme was invented. Basically, instead of having a single class B address with 14 bits for the network number and 16 bits for the host number, some bits are taken away from the host number to create a subnet number. For example, if the university has 35 departments, it could use a 6-bit subnet number and a 10-bit host number, allowing for up to 

334




64 Ethernets, each with a maximum of 1022 hosts (0 and -1 are not available, as mentioned earlier). This split could be changed later if it turns out to be the wrong one. 

To implement subnetting, the main router needs a 

subnet mask

 that indicates the split between network + subnet number and host, as shown in 

Fig. 5-58

. Subnet masks are also written in dotted decimal notation, with the addition of a slash followed by the number of bits in the network + subnet part. For the example of 

Fig. 5-58

, the subnet mask can be written as 255.255.252.0. An alternative notation is /22 to indicate that the subnet mask is 22 bits long. 

Figure 5-58. A class B network subnetted into 64 subnets. 

 

Outside the network, the subnetting is not visible, so allocating a new subnet does not require contacting ICANN or changing any external databases. In this example, the first subnet might use IP addresses starting at 130.50.4.1; the second subnet might start at 130.50.8.1; the third subnet might start at 130.50.12.1; and so on. To see why the subnets are counting by fours, note that the corresponding binary addresses are as follows: 

Subnet 1: 10000010  00110010  000001|00  00000001  

Subnet 2: 10000010  00110010  000010|00  00000001  

Subnet 3: 10000010  00110010  000011|00  00000001  

Here the vertical bar (|) shows the boundary between the subnet number and the host number. To its left is the 6-bit subnet number; to its right is the 10-bit host number. 

To see how subnets work, it is necessary to explain how IP packets are processed at a router. Each router has a table listing some number of (network, 0) IP addresses and some number of (this-network, host) IP addresses. The first kind tells how to get to distant networks. The second kind tells how to get to local hosts. Associated with each table is the network interface to use to reach the destination, and certain other information. 

When an IP packet arrives, its destination address is looked up in the routing table. If the packet is for a distant network, it is forwarded to the next router on the interface given in the table. If it is a local host (e.g., on the router's LAN), it is sent directly to the destination. If the network is not present, the packet is forwarded to a default router with more extensive tables. This algorithm means that each router only has to keep track of other networks and local hosts, not (network, host) pairs, greatly reducing the size of the routing table. 

When subnetting is introduced, the routing tables are changed, adding entries of the form (this-network, subnet, 0) and (this-network, this-subnet, host). Thus, a router on subnet 

k

 knows how to get to all the other subnets and also how to get to all the hosts on subnet 

k

. It does not have to know the details about hosts on other subnets. In fact, all that needs to be changed is to have each router do a Boolean AND with the network's subnet mask to get rid of the host number and look up the resulting address in its tables (after determining which network class it is). For example, a packet addressed to 130.50.15.6 and arriving at the main router is ANDed with the subnet mask 255.255.252.0/22 to give the address 130.50.12.0. This address is looked up in the routing tables to find out which output line to use to get to the router for subnet 3. Subnetting thus reduces router table space by creating a three-level hierarchy consisting of network, subnet, and host. 

335




CIDR—Classless InterDomain Routing 

IP has been in heavy use for decades. It has worked extremely well, as demonstrated by the exponential growth of the Internet. Unfortunately, IP is rapidly becoming a victim of its own popularity: it is running out of addresses. This looming disaster has sparked a great deal of discussion and controversy within the Internet community about what to do about it. In this section we will describe both the problem and several proposed solutions. 

Back in 1987, a few visionaries predicted that some day the Internet might grow to 100,000 networks. Most experts pooh-poohed this as being decades in the future, if ever. The 100,000th network was connected in 1996. The problem, as mentioned above, is that the Internet is rapidly running out of IP addresses. In principle, over 2 billion addresses exist, but the practice of organizing the address space by classes (see 

Fig. 5-55

) wastes millions of them. In particular, the real villain is the class B network. For most organizations, a class A network, with 16 million addresses is too big, and a class C network, with 256 addresses is too small. A class B network, with 65,536, is just right. In Internet folklore, this situation is known as the 

three bears problem

 (as in 

Goldilocks and the Three Bears

). 

In reality, a class B address is far too large for most organizations. Studies have shown that more than half of all class B networks have fewer than 50 hosts. A class C network would have done the job, but no doubt every organization that asked for a class B address thought that one day it would outgrow the 8-bit host field. In retrospect, it might have been better to have had class C networks use 10 bits instead of eight for the host number, allowing 1022 hosts per network. Had this been the case, most organizations would have probably settled for a class C network, and there would have been half a million of them (versus only 16,384 class B networks). 

It is hard to fault the Internet designers for not having provided more (and smaller) class B addresses. At the time the decision was made to create the three classes, the Internet was a research network connecting the major research universities in the U.S. (plus a very small number of companies and military sites doing networking research). No one then perceived the Internet as becoming a mass market communication system rivaling the telephone network. At the time, someone no doubt said: ''The U.S. has about 2000 colleges and universities. Even if all of them connect to the Internet and many universities in other countries join, too, we are never going to hit 16,000 since there are not that many universities in the whole world. Furthermore, having the host number be an integral number of bytes speeds up packet processing.'' 

However, if the split had allocated 20 bits to the class B network number, another problem would have emerged: the routing table explosion. From the point of view of the routers, the IP address space is a two-level hierarchy, with network numbers and host numbers. Routers do not have to know about all the hosts, but they do have to know about all the networks. If half a million class C networks were in use, every router in the entire Internet would need a table with half a million entries, one per network, telling which line to use to get to that network, as well as providing other information. 

The actual physical storage of half a million entry tables is probably doable, although expensive for critical routers that keep the tables in static RAM on I/O boards. A more serious problem is that the complexity of various algorithms relating to management of the tables grows faster than linear. Worse yet, much of the existing router software and firmware was designed at a time when the Internet had 1000 connected networks and 10,000 networks seemed decades away. Design choices made then often are far from optimal now. 

In addition, various routing algorithms require each router to transmit its tables periodically (e.g., distance vector protocols). The larger the tables, the more likely it is that some parts will get lost underway, leading to incomplete data at the other end and possibly routing instabilities. 

336




The routing table problem could have been solved by going to a deeper hierarchy. For example, having each IP address contain a country, state/province, city, network, and host field might work. Then each router would only need to know how to get to each country, the states or provinces in its own country, the cities in its state or province, and the networks in its city. Unfortunately, this solution would require considerably more than 32 bits for IP addresses and would use addresses inefficiently (Liechtenstein would have as many bits as the United States). 

In short, some solutions solve one problem but create a new one. The solution that was implemented and that gave the Internet a bit of extra breathing room is 

CIDR

 (

Classless InterDomain Routing

). The basic idea behind CIDR, which is described in RFC 1519, is to allocate the remaining IP addresses in variable-sized blocks, without regard to the classes. If a site needs, say, 2000 addresses, it is given a block of 2048 addresses on a 2048-byte boundary. 

Dropping the classes makes forwarding more complicated. In the old classful system, forwarding worked like this. When a packet arrived at a router, a copy of the IP address was shifted right 28 bits to yield a 4-bit class number. A 16-way branch then sorted packets into A, B, C, and D (if supported), with eight of the cases for class A, four of the cases for class B, two of the cases for class C, and one each for D and E. The code for each class then masked off the 8-, 16-, or 24-bit network number and right aligned it in a 32-bit word. The network number was then looked up in the A, B, or C table, usually by indexing for A and B networks and hashing for C networks. Once the entry was found, the outgoing line could be looked up and the packet forwarded. 

With CIDR, this simple algorithm no longer works. Instead, each routing table entry is extended by giving it a 32-bit mask. Thus, there is now a single routing table for all networks consisting of an array of (IP address, subnet mask, outgoing line) triples. When a packet comes in, its destination IP address is first extracted. Then (conceptually) the routing table is scanned entry by entry, masking the destination address and comparing it to the table entry looking for a match. It is possible that multiple entries (with different subnet mask lengths) match, in which case the longest mask is used. Thus, if there is a match for a /20 mask and a /24 mask, the /24 entry is used. 

Complex algorithms have been devised to speed up the address matching process (Ruiz-Sanchez et al., 2001). Commercial routers use custom VLSI chips with these algorithms embedded in hardware. 

To make the forwarding algorithm easier to understand, let us consider an example in which millions of addresses are available starting at 194.24.0.0. Suppose that Cambridge University needs 2048 addresses and is assigned the addresses 194.24.0.0 through 194.24.7.255, along with mask 255.255.248.0. Next, Oxford University asks for 4096 addresses. Since a block of 4096 addresses must lie on a 4096-byte boundary, they cannot be given addresses starting at 194.24.8.0. Instead, they get 194.24.16.0 through 194.24.31.255 along with subnet mask 255.255.240.0. Now the University of Edinburgh asks for 1024 addresses and is assigned addresses 194.24.8.0 through 194.24.11.255 and mask 255.255.252.0. These assignments are summarized in 

Fig. 5-59

. 

Figure 5-59. A set of IP address assignments. 

 

337




The routing tables all over the world are now updated with the three assigned entries. Each entry contains a base address and a subnet mask. These entries (in binary) are: 

   Address                             Mask  

C: 11000010 00011000 00000000 00000000 11111111 11111111 11111000 00000000  

E: 11000010 00011000 00001000 00000000 11111111 11111111 11111100 00000000  

O: 11000010 00011000 00010000 00000000 11111111 11111111 11110000 00000000  

Now consider what happens when a packet comes in addressed to 194.24.17.4, which in binary is represented as the following 32-bit string 

11000010 00011000 00010001 00000100  

First it is Boolean ANDed with the Cambridge mask to get 

11000010 00011000 00010000 00000000  

This value does not match the Cambridge base address, so the original address is next ANDed with the Edinburgh mask to get 

11000010 00011000 00010000 00000000  

This value does not match the Edinburgh base address, so Oxford is tried next, yielding 

11000010 00011000 00010000 00000000  

This value does match the Oxford base. If no longer matches are found farther down the table, the Oxford entry is used and the packet is sent along the line named in it. 

Now let us look at these three universities from the point of view of a router in Omaha, Nebraska, that has only four outgoing lines: Minneapolis, New York, Dallas, and Denver. When the router software there gets the three new entries, it notices that it can combine all three entries into a single 

aggregate entry

 194.24.0.0/19 with a binary address and submask as follows: 

11000010 0000000 00000000 00000000 11111111 11111111 11100000 00000000  

This entry sends all packets destined for any of the three universities to New York. By aggregating the three entries, the Omaha router has reduced its table size by two entries. 

If New York has a single line to London for all U.K. traffic, it can use an aggregated entry as well. However, if it has separate lines for London and Edinburgh, then it has to have three separate entries. Aggregation is heavily used throughout the Internet to reduce the size of the router tables. 

As a final note on this example, the aggregate route entry in Omaha also sends packets for the unassigned addresses to New York. As long as the addresses are truly unassigned, this does not matter because they are not supposed to occur. However, if they are later assigned to a company in California, an additional entry, 194.24.12.0/22, will be needed to deal with them. 

NAT—Network Address Translation 

IP addresses are scarce. An ISP might have a /16 (formerly class B) address, giving it 65,534 host numbers. If it has more customers than that, it has a problem. For home customers with dial-up connections, one way around the problem is to dynamically assign an IP address to a computer when it calls up and logs in and take the IP address back when the session ends. In 

338




this way, a single /16 address can handle up to 65,534 active users, which is probably good enough for an ISP with several hundred thousand customers. When the session is terminated, the IP address is reassigned to another caller. While this strategy works well for an ISP with a moderate number of home users, it fails for ISPs that primarily serve business customers. 

The problem is that business customers expect to be on-line continuously during business hours. Both small businesses, such as three-person travel agencies, and large corporations have multiple computers connected by a LAN. Some computers are employee PCs; others may be Web servers. Generally, there is a router on the LAN that is connected to the ISP by a leased line to provide continuous connectivity. This arrangement means that each computer must have its own IP address all day long. In effect, the total number of computers owned by all its business customers combined cannot exceed the number of IP addresses the ISP has. For a /16 address, this limits the total number of computers to 65,534. For an ISP with tens of thousands of business customers, this limit will quickly be exceeded. 

To make matters worse, more and more home users are subscribing to ADSL or Internet over cable. Two of the features of these services are (1) the user gets a permanent IP address and (2) there is no connect charge (just a monthly flat rate charge), so many ADSL and cable users just stay logged in permanently. This development just adds to the shortage of IP addresses. Assigning IP addresses on-the-fly as is done with dial-up users is of no use because the number of IP addresses in use at any one instant may be many times the number the ISP owns. 

And just to make it a bit more complicated, many ADSL and cable users have two or more computers at home, often one for each family member, and they all want to be on-line all the time using the single IP address their ISP has given them. The solution here is to connect all the PCs via a LAN and put a router on it. From the ISP's point of view, the family is now the same as a small business with a handful of computers. Welcome to Jones, Inc. 

The problem of running out of IP addresses is not a theoretical problem that might occur at some point in the distant future. It is happening right here and right now. The long-term solution is for the whole Internet to migrate to IPv6, which has 128-bit addresses. This transition is slowly occurring, but it will be years before the process is complete. As a consequence, some people felt that a quick fix was needed for the short term. This quick fix came in the form of 

NAT

 (

Network Address Translation

), which is described in RFC 3022 and which we will summarize below. For additional information, see (Dutcher, 2001). 

The basic idea behind NAT is to assign each company a single IP address (or at most, a small number of them) for Internet traffic. 

Within

 the company, every computer gets a unique IP address, which is used for routing intramural traffic. However, when a packet exits the company and goes to the ISP, an address translation takes place. To make this scheme possible, three ranges of IP addresses have been declared as private. Companies may use them internally as they wish. The only rule is that no packets containing these addresses may appear on the Internet itself. The three reserved ranges are: 

10.0.0.0    – 10.255.255.255/8    (16,777,216 hosts)  

172.16.0.0  – 172.31.255.255/12   (1,048,576 hosts)  

192.168.0.0 – 192.168.255.255/16  (65,536 hosts)  

The first range provides for 16,777,216 addresses (except for 0 and -1, as usual) and is the usual choice of most companies, even if they do not need so many addresses. 

The operation of NAT is shown in 

Fig. 5-60

. Within the company premises, every machine has a unique address of the form 10.

x.y.z

. However, when a packet leaves the company premises, it passes through a 

NAT box

 that converts the internal IP source address, 10.0.0.1 in the figure, to the company's true IP address, 198.60.42.12 in this example. The NAT box is often combined in a single device with a firewall, which provides security by carefully controlling 

339




what goes into the company and what comes out. We will study firewalls in 

Chap. 8

. It is also possible to integrate the NAT box into the company's router. 

Figure 5-60. Placement and operation of a NAT box. 

 

So far we have glossed over one tiny little detail: when the reply comes back (e.g., from a Web server), it is naturally addressed to 198.60.42.12, so how does the NAT box know which address to replace it with? Herein lies the problem with NAT. If there were a spare field in the IP header, that field could be used to keep track of who the real sender was, but only 1 bit is still unused. In principle, a new option could be created to hold the true source address, but doing so would require changing the IP code on all the machines on the entire Internet to handle the new option. This is not a promising alternative for a quick fix. 

What actually happened is as follows. The NAT designers observed that most IP packets carry either TCP or UDP payloads. When we study TCP and UDP in 

Chap. 6

, we will see that both of these have headers containing a source port and a destination port. Below we will just discuss TCP ports, but exactly the same story holds for UDP ports. The ports are 16-bit integers that indicate where the TCP connection begins and ends. These ports provide the field needed to make NAT work. 

When a process wants to establish a TCP connection with a remote process, it attaches itself to an unused TCP port on its own machine. This is called the 

source port

 and tells the TCP code where to send incoming packets belonging to this connection. The process also supplies a 

destination port

 to tell who to give the packets to on the remote side. Ports 0–1023 are reserved for well-known services. For example, port 80 is the port used by Web servers, so remote clients can locate them. Each outgoing TCP message contains both a source port and a destination port. Together, these ports serve to identify the processes using the connection on both ends. 

An analogy may make the use of ports clearer. Imagine a company with a single main telephone number. When people call the main number, they reach an operator who asks which extension they want and then puts them through to that extension. The main number is analogous to the company's IP address and the extensions on both ends are analogous to the ports. Ports are an extra 16-bits of addressing that identify which process gets which incoming packet. 

Using the 

Source port

 field, we can solve our mapping problem. Whenever an outgoing packet enters the NAT box, the 10.

x.y.z

 source address is replaced by the company's true IP address. In addition, the TCP 

Source port

 field is replaced by an index into the NAT box's 65,536-entry translation table. This table entry contains the original IP address and the original source port. Finally, both the IP and TCP header checksums are recomputed and inserted into the packet. It is necessary to replace the 

Source port

 because connections from machines 10.0.0.1 and 

340




10.0.0.2 may both happen to use port 5000, for example, so the 

Source port

 alone is not enough to identify the sending process. 

When a packet arrives at the NAT box from the ISP, the 

Source port

 in the TCP header is extracted and used as an index into the NAT box's mapping table. From the entry located, the internal IP address and original TCP 

Source port

 are extracted and inserted into the packet. Then both the IP and TCP checksums are recomputed and inserted into the packet. The packet is then passed to the company router for normal delivery using the 10.

x.y.z

 address. 

NAT can also be used to alleviate the IP shortage for ADSL and cable users. When the ISP assigns each user an address, it uses 10.

x.y.z

 addresses. When packets from user machines exit the ISP and enter the main Internet, they pass through a NAT box that translates them to the ISP's true Internet address. On the way back, packets undergo the reverse mapping. In this respect, to the rest of the Internet, the ISP and its home ADSL/cable users just looks like a big company. 

Although this scheme sort of solves the problem, many people in the IP community regard it as an abomination-on-the-face-of-the-earth. Briefly summarized, here are some of the objections. First, NAT violates the architectural model of IP, which states that every IP address uniquely identifies a single machine worldwide. The whole software structure of the Internet is built on this fact. With NAT, thousands of machines may (and do) use address 10.0.0.1. 

Second, NAT changes the Internet from a connectionless network to a kind of connection-oriented network. The problem is that the NAT box must maintain information (the mapping) for each connection passing through it. Having the network maintain connection state is a property of connection-oriented networks, not connectionless ones. If the NAT box crashes and its mapping table is lost, all its TCP connections are destroyed. In the absence of NAT, router crashes have no effect on TCP. The sending process just times out within a few seconds and retransmits all unacknowledged packets. With NAT, the Internet becomes as vulnerable as a circuit-switched network. 

Third, NAT violates the most fundamental rule of protocol layering: layer 

k

 may not make any assumptions about what layer 

k

 + 1 has put into the payload field. This basic principle is there to keep the layers independent. If TCP is later upgraded to TCP-2, with a different header layout (e.g., 32-bit ports), NAT will fail. The whole idea of layered protocols is to ensure that changes in one layer do not require changes in other layers. NAT destroys this independence. 

Fourth, processes on the Internet are not required to use TCP or UDP. If a user on machine 

A

 decides to use some new transport protocol to talk to a user on machine 

B

 (for example, for a multimedia application), introduction of a NAT box will cause the application to fail because the NAT box will not be able to locate the TCP 

Source port

 correctly. 

Fifth, some applications insert IP addresses in the body of the text. The receiver then extracts these addresses and uses them. Since NAT knows nothing about these addresses, it cannot replace them, so any attempt to use them on the remote side will fail. 

FTP

, the standard 

File Transfer Protocol

 works this way and can fail in the presence of NAT unless special precautions are taken. Similarly, the H.323 Internet telephony protocol (which we will study in 

Chap. 7

) has this property and can fail in the presence of NAT. It may be possible to patch NAT to work with H.323, but having to patch the code in the NAT box every time a new application comes along is not a good idea. 

Sixth, since the TCP 

Source port

 field is 16 bits, at most 65,536 machines can be mapped onto an IP address. Actually, the number is slightly less because the first 4096 ports are reserved for special uses. However, if multiple IP addresses are available, each one can handle up to 61,440 machines. 

341




These and other problems with NAT are discussed in RFC 2993. In general, the opponents of NAT say that by fixing the problem of insufficient IP addresses with a temporary and ugly hack, the pressure to implement the real solution, that is, the transition to IPv6, is reduced, and this is a bad thing. 

5.6.3 Internet Control Protocols 

In addition to IP, which is used for data transfer, the Internet has several control protocols used in the network layer, including ICMP, ARP, RARP, BOOTP, and DHCP. In this section we will look at each of these in turn. 

The Internet Control Message Protocol 

The operation of the Internet is monitored closely by the routers. When something unexpected occurs, the event is reported by the 

ICMP

 (

Internet Control Message Protocol

), which is also used to test the Internet. About a dozen types of ICMP messages are defined. The most important ones are listed in 

Fig. 5-61

. Each ICMP message type is encapsulated in an IP packet. 

Figure 5-61. The principal ICMP message types. 

 

The DESTINATION UNREACHABLE message is used when the subnet or a router cannot locate the destination or when a packet with the 

DF

 bit cannot be delivered because a ''small-packet'' network stands in the way. 

The TIME EXCEEDED message is sent when a packet is dropped because its counter has reached zero. This event is a symptom that packets are looping, that there is enormous congestion, or that the timer values are being set too low. 

The PARAMETER PROBLEM message indicates that an illegal value has been detected in a header field. This problem indicates a bug in the sending host'sIP software or possibly in the software of a router transited. 

The SOURCE QUENCH message was formerly used to throttle hosts that were sending too many packets. When a host received this message, it was expected to slow down. It is rarely used any more because when congestion occurs, these packets tend to add more fuel to the fire. Congestion control in the Internet is now done largely in the transport layer; we will study it in detail in 

Chap. 6

. 

The REDIRECT message is used when a router notices that a packet seems to be routed wrong. It is used by the router to tell the sending host about the probable error. 

342




The ECHO and ECHO REPLY messages are used to see if a given destination is reachable and alive. Upon receiving the ECHO message, the destination is expected to send an ECHO REPLY message back. The TIMESTAMP REQUEST and TIMESTAMP REPLY messages are similar, except that the arrival time of the message and the departure time of the reply are recorded in the reply. This facility is used to measure network performance. 

In addition to these messages, others have been defined. The on-line list is now kept at 

www.iana.org/assignments/icmp-parameters

. 

ARP—The Address Resolution Protocol 

Although every machine on the Internet has one (or more) IP addresses, these cannot actually be used for sending packets because the data link layer hardware does not understand Internet addresses. Nowadays, most hosts at companies and universities are attached to a LAN by an interface board that only understands LAN addresses. For example, every Ethernet board ever manufactured comes equipped with a 48-bit Ethernet address. Manufacturers of Ethernet boards request a block of addresses from a central authority to ensure that no two boards have the same address (to avoid conflicts should the two boards ever appear on the same LAN). The boards send and receive frames based on 48-bit Ethernet addresses. They know nothing at all about 32-bit IP addresses. 

The question now arises: How do IP addresses get mapped onto data link layer addresses, such as Ethernet? To explain how this works, let us use the example of 

Fig. 5-62

, in which a small university with several class C (now called /24) networks is illustrated. Here we have two Ethernets, one in the Computer Science Dept., with IP address 192.31.65.0 and one in Electrical Engineering, with IP address 192.31.63.0. These are connected by a campus backbone ring (e.g., FDDI) with IP address 192.31.60.0. Each machine on an Ethernet has a unique Ethernet address, labeled 

E1

 through 

E6

, and each machine on the FDDI ring has an FDDI address, labeled 

F1

 through 

F3

. 

Figure 5-62. Three interconnected /24 networks: two Ethernets and an FDDI ring. 

 

Let us start out by seeing how a user on host 1 sends a packet to a user on host 2. Let us assume the sender knows the name of the intended receiver, possibly something like 

mary@eagle.cs.uni.edu

. The first step is to find the IP address for host 2, known as 

eagle.cs.uni.edu

. This lookup is performed by the Domain Name System, which we will study in 

Chap. 7

. For the moment, we will just assume that DNS returns the IP address for host 2 (192.31.65.5). 

The upper layer software on host 1 now builds a packet with 192.31.65.5 in the 

Destination address

 field and gives it to the IP software to transmit. The IP software can look at the address and see that the destination is on its own network, but it needs some way to find the 

343




destination's Ethernet address. One solution is to have a configuration file somewhere in the system that maps IP addresses onto Ethernet addresses. While this solution is certainly possible, for organizations with thousands of machines, keeping all these files up to date is an error-prone, time-consuming job. 

A better solution is for host 1 to output a broadcast packet onto the Ethernet asking: Who owns IP address 192.31.65.5? The broadcast will arrive at every machine on Ethernet 192.31.65.0, and each one will check its IP address. Host 2 alone will respond with its Ethernet address (

E2

). In this way host 1 learns that IP address 192.31.65.5 is on the host with Ethernet address 

E2

. The protocol used for asking this question and getting the reply is called 

ARP

 (

Address Resolution Protocol

). Almost every machine on the Internet runs it. ARP is defined in RFC 826. 

The advantage of using ARP over configuration files is the simplicity. The system manager does not have to do much except assign each machine an IP address and decide about subnet masks. ARP does the rest. 

At this point, the IP software on host 1 builds an Ethernet frame addressed to 

E2

, puts the IP packet (addressed to 192.31.65.5) in the payload field, and dumps it onto the Ethernet. The Ethernet board of host 2 detects this frame, recognizes it as a frame for itself, scoops it up, and causes an interrupt. The Ethernet driver extracts the IP packet from the payload and passes it to the IP software, which sees that it is correctly addressed and processes it. 

Various optimizations are possible to make ARP work more efficiently. To start with, once a machine has run ARP, it caches the result in case it needs to contact the same machine shortly. Next time it will find the mapping in its own cache, thus eliminating the need for a second broadcast. In many cases host 2 will need to send back a reply, forcing it, too, to run ARP to determine the sender's Ethernet address. This ARP broadcast can be avoided by having host 1 include its IP-to-Ethernet mapping in the ARP packet. When the ARP broadcast arrives at host 2, the pair (192.31.65.7, E1) is entered into host 2's ARP cache for future use. In fact, all machines on the Ethernet can enter this mapping into their ARP caches. 

Yet another optimization is to have every machine broadcast its mapping when it boots. This broadcast is generally done in the form of an ARP looking for its own IP address. There should not be a response, but a side effect of the broadcast is to make an entry in everyone's ARP cache. If a response does (unexpectedly) arrive, two machines have been assigned the same IP address. The new one should inform the system manager and not boot. 

To allow mappings to change, for example, when an Ethernet board breaks and is replaced with a new one (and thus a new Ethernet address), entries in the ARP cache should time out after a few minutes. 

Now let us look at 

Fig. 5-62

 again, only this time host 1 wants to send a packet to host 4 (192.31.63.8). Using ARP will fail because host 4 will not see the broadcast (routers do not forward Ethernet-level broadcasts). There are two solutions. First, the CS router could be configured to respond to ARP requests for network 192.31.63.0 (and possibly other local networks). In this case, host 1 will make an ARP cache entry of (192.31.63.8, E3) and happily send all traffic for host 4 to the local router. This solution is called 

proxy ARP

. The second solution is to have host 1 immediately see that the destination is on a remote network and just send all such traffic to a default Ethernet address that handles all remote traffic, in this case 

E3

. This solution does not require having the CS router know which remote networks it is serving. 

Either way, what happens is that host 1 packs the IP packet into the payload field of an Ethernet frame addressed to 

E3

. When the CS router gets the Ethernet frame, it removes the IP packet from the payload field and looks up the IP address in its routing tables. It discovers that packets for network 192.31.63.0 are supposed to go to router 192.31.60.7. If it does not 

344




already know the FDDI address of 192.31.60.7, it broadcasts an ARP packet onto the ring and learns that its ring address is 

F3

. It then inserts the packet into the payload field of an FDDI frame addressed to 

F3

 and puts it on the ring. 

At the EE router, the FDDI driver removes the packet from the payload field and gives it to the IP software, which sees that it needs to send the packet to 192.31.63.8. If this IP address is not in its ARP cache, it broadcasts an ARP request on the EE Ethernet and learns that the destination address is 

E6

,soit builds an Ethernet frame addressed to 

E6

, puts the packet in the payload field, and sends it over the Ethernet. When the Ethernet frame arrives at host 4, the packet is extracted from the frame and passed to the IP software for processing. 

Going from host 1 to a distant network over a WAN works essentially the same way, except that this time the CS router's tables tell it to use the WAN router whose FDDI address is 

F2

. 

RARP, BOOTP, and DHCP 

ARP solves the problem of finding out which Ethernet address corresponds to a given IP address. Sometimes the reverse problem has to be solved: Given an Ethernet address, what is the corresponding IP address? In particular, this problem occurs when a diskless workstation is booted. Such a machine will normally get the binary image of its operating system from a remote file server. But how does it learn its IP address? 

The first solution devised was to use 

RARP

 (

Reverse Address Resolution Protocol

) (defined in RFC 903). This protocol allows a newly-booted workstation to broadcast its Ethernet address and say: My 48-bit Ethernet address is 14.04.05.18.01.25. Does anyone out there know my IP address? The RARP server sees this request, looks up the Ethernet address in its configuration files, and sends back the corresponding IP address. 

Using RARP is better than embedding an IP address in the memory image because it allows the same image to be used on all machines. If the IP address were buried inside the image, each workstation would need its own image. 

A disadvantage of RARP is that it uses a destination address of all 1s (limited broadcasting) to reach the RARP server. However, such broadcasts are not forwarded by routers, so a RARP server is needed on each network. To get around this problem, an alternative bootstrap protocol called 

BOOTP

 was invented. Unlike RARP, BOOTP uses UDP messages, which are forwarded over routers. It also provides a diskless workstation with additional information, including the IP address of the file server holding the memory image, the IP address of the default router, and the subnet mask to use. BOOTP is described in RFCs 951, 1048, and 1084. 

A serious problem with BOOTP is that it requires manual configuration of tables mapping IP address to Ethernet address. When a new host is added to a LAN, it cannot use BOOTP until an administrator has assigned it an IP address and entered its (Ethernet address, IP address) into the BOOTP configuration tables by hand. To eliminate this error-prone step, BOOTP was extended and given a new name: 

DHCP

 (

Dynamic Host Configuration Protocol

). DHCP allows both manual IP address assignment and automatic assignment. It is described in RFCs 2131 and 2132. In most systems, it has largely replaced RARP and BOOTP. 

Like RARP and BOOTP, DHCP is based on the idea of a special server that assigns IP addresses to hosts asking for one. This server need not be on the same LAN as the requesting host. Since the DHCP server may not be reachable by broadcasting, a 

DHCP relay agent

 is needed on each LAN, as shown in 

Fig. 5-63

. 

Figure 5-63. Operation of DHCP. 

345




 

To find its IP address, a newly-booted machine broadcasts a DHCP DISCOVER packet. The DHCP relay agent on its LAN intercepts all DHCP broadcasts. When it finds a DHCP DISCOVER packet, it sends the packet as a unicast packet to the DHCP server, possibly on a distant network. The only piece of information the relay agent needs is the IP address of the DHCP server. 

An issue that arises with automatic assignment of IP addresses from a pool is how long an IP address should be allocated. If a host leaves the network and does not return its IP address to the DHCP server, that address will be permanently lost. After a period of time, many addresses may be lost. To prevent that from happening, IP address assignment may be for a fixed period of time, a technique called 

leasing

. Just before the lease expires, the host must ask the DHCP for a renewal. If it fails to make a request or the request is denied, the host may no longer use the IP address it was given earlier. 

5.6.4 OSPF—The Interior Gateway Routing Protocol 

We have now finished our study of Internet control protocols. It is time to move on the next topic: routing in the Internet. As we mentioned earlier, the Internet is made up of a large number of autonomous systems. Each AS is operated by a different organization and can use its own routing algorithm inside. For example, the internal networks of companies 

X

, 

Y

, and 

Z

 are usually seen as three ASes if all three are on the Internet. All three may use different routing algorithms internally. Nevertheless, having standards, even for internal routing, simplifies the implementation at the boundaries between ASes and allows reuse of code. In this section we will study routing within an AS. In the next one, we will look at routing between ASes. A routing algorithm within an AS is called an 

interior gateway protocol

; an algorithm for routing between ASes is called an 

exterior gateway protocol

. 

The original Internet interior gateway protocol was a distance vector protocol (RIP) based on the Bellman-Ford algorithm inherited from the ARPANET. It worked well in small systems, but less well as ASes got larger. It also suffered from the count-to-infinity problem and generally slow convergence, so it was replaced in May 1979 by a link state protocol. In 1988, the Internet Engineering Task Force began work on a successor. That successor, called 

OSPF

 (

Open Shortest Path First

), became a standard in 1990. Most router vendors now support it, and it has become the main interior gateway protocol. Below we will give a sketch of how OSPF works. For the complete story, see RFC 2328. 

Given the long experience with other routing protocols, the group designing the new protocol had a long list of requirements that had to be met. First, the algorithm had to be published in the open literature, hence the ''O'' in OSPF. A proprietary solution owned by one company would not do. Second, the new protocol had to support a variety of distance metrics, including physical distance, delay, and so on. Third, it had to be a dynamic algorithm, one that adapted to changes in the topology automatically and quickly. 

Fourth, and new for OSPF, it had to support routing based on type of service. The new protocol had to be able to route real-time traffic one way and other traffic a different way. The IP protocol has a 

Type of Service

 field, but no existing routing protocol used it. This field was included in OSPF but still nobody used it, and it was eventually removed. 

346




Fifth, and related to the above, the new protocol had to do load balancing, splitting the load over multiple lines. Most previous protocols sent all packets over the best route. The second-best route was not used at all. In many cases, splitting the load over multiple lines gives better performance. 

Sixth, support for hierarchical systems was needed. By 1988, the Internet had grown so large that no router could be expected to know the entire topology. The new routing protocol had to be designed so that no router would have to. 

Seventh, some modicum of security was required to prevent fun-loving students from spoofing routers by sending them false routing information. Finally, provision was needed for dealing with routers that were connected to the Internet via a tunnel. Previous protocols did not handle this well. 

OSPF supports three kinds of connections and networks: 

1. Point-to-point lines between exactly two routers. 

2. Multiaccess networks with broadcasting (e.g., most LANs). 

3. Multiaccess networks without broadcasting (e.g., most packet-switched WANs). 

A 

multiaccess

 network is one that can have multiple routers on it, each of which can directly communicate with all the others. All LANs and WANs have this property. 

Figure 5-64(a)

 shows an AS containing all three kinds of networks. Note that hosts do not generally play a role in OSPF. 

Figure 5-64. (a) An autonomous system. (b) A graph representation of (a). 

 

347




OSPF operates by abstracting the collection of actual networks, routers, and lines into a directed graph in which each arc is assigned a cost (distance, delay, etc.). It then computes the shortest path based on the weights on the arcs. A serial connection between two routers is represented by a pair of arcs, one in each direction. Their weights may be different. A multiaccess network is represented by a node for the network itself plus a node for each router. The arcs from the network node to the routers have weight 0 and are omitted from the graph. 

Figure 5-64(b)

 shows the graph representation of the network of 

Fig. 5-64(a)

. Weights are symmetric, unless marked otherwise. What OSPF fundamentally does is represent the actual network as a graph like this and then compute the shortest path from every router to every other router. 

Many of the ASes in the Internet are themselves large and nontrivial to manage. OSPF allows them to be divided into numbered 

areas

, where an area is a network or a set of contiguous networks. Areas do not overlap but need not be exhaustive, that is, some routers may belong to no area. An area is a generalization of a subnet. Outside an area, its topology and details are not visible. 

Every AS has a 

backbone

 area, called area 0. All areas are connected to the backbone, possibly by tunnels, so it is possible to go from any area in the AS to any other area in the AS via the backbone. A tunnel is represented in the graph as an arc and has a cost. Each router that is connected to two or more areas is part of the backbone. As with other areas, the topology of the backbone is not visible outside the backbone. 

Within an area, each router has the same link state database and runs the same shortest path algorithm. Its main job is to calculate the shortest path from itself to every other router in the area, including the router that is connected to the backbone, of which there must be at least one. A router that connects to two areas needs the databases for both areas and must run the shortest path algorithm for each one separately. 

During normal operation, three kinds of routes may be needed: intra-area, interarea, and inter-AS. Intra-area routes are the easiest, since the source router already knows the shortest path to the destination router. Interarea routing always proceeds in three steps: go from the source to the backbone; go across the backbone to the destination area; go to the destination. This algorithm forces a star configuration on OSPF with the backbone being the hub and the other areas being spokes. Packets are routed from source to destination ''as is.'' They are not encapsulated or tunneled, unless going to an area whose only connection to the backbone is a tunnel. 

Figure 5-65

 shows part of the Internet with ASes and areas. 

Figure 5-65. The relation between ASes, backbones, and areas in OSPF. 

348




 

OSPF distinguishes four classes of routers: 

1. Internal routers are wholly within one area. 

2. Area border routers connect two or more areas. 

3. Backbone routers are on the backbone. 

4. AS boundary routers talk to routers in other ASes. 

These classes are allowed to overlap. For example, all the border routers are automatically part of the backbone. In addition, a router that is in the backbone but not part of any other area is also an internal router. Examples of all four classes of routers are illustrated in 

Fig. 5-

65

. 

When a router boots, it sends HELLO messages on all of its point-to-point lines and multicasts them on LANs to the group consisting of all the other routers. On WANs, it needs some configuration information to know who to contact. From the responses, each router learns who its neighbors are. Routers on the same LAN are all neighbors. 

OSPF works by exchanging information between adjacent routers, which is not the same as between neighboring routers. In particular, it is inefficient to have every router on a LAN talk to every other router on the LAN. To avoid this situation, one router is elected as the 

designated router

. It is said to be 

adjacent

 to all the other routers on its LAN, and exchanges information with them. Neighboring routers that are not adjacent do not exchange information with each other. A backup designated router is always kept up to date to ease the transition should the primary designated router crash and need to replaced immediately. 

During normal operation, each router periodically floods LINK STATE UPDATE messages to each of its adjacent routers. This message gives its state and provides the costs used in the topological database. The flooding messages are acknowledged, to make them reliable. Each message has a sequence number, so a router can see whether an incoming LINK STATE UPDATE is older or newer than what it currently has. Routers also send these messages when a line goes up or down or its cost changes. 

349




DATABASE DESCRIPTION messages give the sequence numbers of all the link state entries currently held by the sender. By comparing its own values with those of the sender, the receiver can determine who has the most recent values. These messages are used when a line is brought up. 

Either partner can request link state information from the other one by using LINK STATE REQUEST messages. The result of this algorithm is that each pair of adjacent routers checks to see who has the most recent data, and new information is spread throughout the area this way. All these messages are sent as raw IP packets. The five kinds of messages are summarized in 

Fig. 5-66

. 

Figure 5-66. The five types of OSPF messages. 

 

Finally, we can put all the pieces together. Using flooding, each router informs all the other routers in its area of its neighbors and costs. This information allows each router to construct the graph for its area(s) and compute the shortest path. The backbone area does this too. In addition, the backbone routers accept information from the area border routers in order to compute the best route from each backbone router to every other router. This information is propagated back to the area border routers, which advertise it within their areas. Using this information, a router about to send an interarea packet can select the best exit router to the backbone. 

5.6.5 BGP—The Exterior Gateway Routing Protocol 

Within a single AS, the recommended routing protocol is OSPF (although it is certainly not the only one in use). Between ASes, a different protocol, 

BGP

 (

Border Gateway Protocol

), is used. A different protocol is needed between ASes because the goals of an interior gateway protocol and an exterior gateway protocol are not the same. All an interior gateway protocol has to do is move packets as efficiently as possible from the source to the destination. It does not have to worry about politics. 

Exterior gateway protocol routers have to worry about politics a great deal (Metz, 2001). For example, a corporate AS might want the ability to send packets to any Internet site and receive packets from any Internet site. However, it might be unwilling to carry transit packets originating in a foreign AS and ending in a different foreign AS, even if its own AS was on the shortest path between the two foreign ASes (''That's their problem, not ours''). On the other hand, it might be willing to carry transit traffic for its neighbors or even for specific other ASes that paid it for this service. Telephone companies, for example, might be happy to act as a carrier for their customers, but not for others. Exterior gateway protocols in general, and BGP in particular, have been designed to allow many kinds of routing policies to be enforced in the interAS traffic. 

Typical policies involve political, security, or economic considerations. A few examples of routing constraints are: 

1. No transit traffic through certain ASes. 

2. Never put Iraq on a route starting at the Pentagon. 

3. Do not use the United States to get from British Columbia to Ontario. 

350




4. Only transit Albania if there is no alternative to the destination. 

5. Traffic starting or ending at IBM should not transit Microsoft. 

Policies are typically manually configured into each BGP router (or included using some kind of script). They are not part of the protocol itself. 

From the point of view of a BGP router, the world consists of ASes and the lines connecting them. Two ASes are considered connected if there is a line between a border router in each one. Given BGP's special interest in transit traffic, networks are grouped into one of three categories. The first category is the 

stub networks

, which have only one connection to the BGP graph. These cannot be used for transit traffic because there is no one on the other side. Then come the 

multiconnected networks

. These could be used for transit traffic, except that they refuse. Finally, there are the 

transit networks

, such as backbones, which are willing to handle third-party packets, possibly with some restrictions, and usually for pay. 

Pairs of BGP routers communicate with each other by establishing TCP connections. Operating this way provides reliable communication and hides all the details of the network being passed through. 

BGP is fundamentally a distance vector protocol, but quite different from most others such as RIP. Instead of maintaining just the cost to each destination, each BGP router keeps track of the path used. Similarly, instead of periodically giving each neighbor its estimated cost to each possible destination, each BGP router tells its neighbors the exact path it is using. 

As an example, consider the BGP routers shown in 

Fig. 5-67(a)

. In particular, consider 

F

's routing table. Suppose that it uses the path 

FGCD

 to get to 

D

. When the neighbors give it routing information, they provide their complete paths, as shown in 

Fig. 5-67(b)

 (for simplicity, only destination 

D

 is shown here). 

Figure 5-67. (a) A set of BGP routers. (b) Information sent to 

F

. 

 

After all the paths come in from the neighbors, 

F

 examines them to see which is the best. It quickly discards the paths from 

I

 and 

E

, since these paths pass through 

F

 itself. The choice is then between using 

B

 and 

G

. Every BGP router contains a module that examines routes to a given destination and scores them, returning a number for the ''distance'' to that destination for each route. Any route violating a policy constraint automatically gets a score of infinity. The router then adopts the route with the shortest distance. The scoring function is not part of the BGP protocol and can be any function the system managers want. 

BGP easily solves the count-to-infinity problem that plagues other distance vector routing algorithms. For example, suppose 

G

 crashes or the line 

FG

 goes down. 

F

 then receives routes from its three remaining neighbors. These routes are 

BCD

, 

IFGCD

, and 

EFGCD

. It can immediately see that the two latter routes are pointless, since they pass through 

F

 itself, so it chooses 

FBCD

 as its new route. Other distance vector algorithms often make the wrong choice 

351




because they cannot tell which of their neighbors have independent routes to the destination and which do not. The definition of BGP is in RFCs 1771 to 1774. 

5.6.6 Internet Multicasting 

Normal IP communication is between one sender and one receiver. However, for some applications it is useful for a process to be able to send to a large number of receivers simultaneously. Examples are updating replicated, distributed databases, transmitting stock quotes to multiple brokers, and handling digital conference (i.e., multiparty) telephone calls. 

IP supports multicasting, using class D addresses. Each class D address identifies a group of hosts. Twenty-eight bits are available for identifying groups, so over 250 million groups can exist at the same time. When a process sends a packet to a class D address, a best-efforts attempt is made to deliver it to all the members of the group addressed, but no guarantees are given. Some members may not get the packet. 

Two kinds of group addresses are supported: permanent addresses and temporary ones. A permanent group is always there and does not have to be set up. Each permanent group has a permanent group address. Some examples of permanent group addresses are: 

224.0.0.1 All systems on a LAN 

224.0.0.2 All routers on a LAN 

224.0.0.5 All OSPF routers on a LAN 

224.0.0.6 All designated OSPF routers on a LAN 

Temporary groups must be created before they can be used. A process can ask its host to join a specific group. It can also ask its host to leave the group. When the last process on a host leaves a group, that group is no longer present on the host. Each host keeps track of which groups its processes currently belong to. 

Multicasting is implemented by special multicast routers, which may or may not be colocated with the standard routers. About once a minute, each multicast router sends a hardware (i.e., data link layer) multicast to the hosts on its LAN (address 224.0.0.1) asking them to report back on the groups their processes currently belong to. Each host sends back responses for all the class D addresses it is interested in. 

These query and response packets use a protocol called 

IGMP

 (

Internet Group Management Protocol

), which is vaguely analogous to ICMP. It has only two kinds of packets: query and response, each with a simple, fixed format containing some control information in the first word of the payload field and a class D address in the second word. It is described in RFC 1112. 

Multicast routing is done using spanning trees. Each multicast router exchanges information with its neighbors, using a modified distance vector protocol in order for each one to construct a spanning tree per group covering all group members. Various optimizations are used to prune the tree to eliminate routers and networks not interested in particular groups. The protocol makes heavy use of tunneling to avoid bothering nodes not in a spanning tree. 

5.6.7 Mobile IP 

Many users of the Internet have portable computers and want to stay connected to the Internet when they visit a distant Internet site and even on the road in between. Unfortunately, the IP addressing system makes working far from home easier said than done. 

352




In this section we will examine the problem and the solution. A more detailed description is given in (Perkins, 1998a). 

The real villain is the addressing scheme itself. Every IP address contains a network number and a host number. For example, consider the machine with IP address 160.80.40.20/16. The 160.80 gives the network number (8272 in decimal); the 40.20 is the host number (10260 in decimal). Routers all over the world have routing tables telling which line to use to get to network 160.80. Whenever a packet comes in with a destination IP address of the form 160.80.xxx.yyy, it goes out on that line. 

If all of a sudden, the machine with that address is carted off to some distant site, the packets for it will continue to be routed to its home LAN (or router). The owner will no longer get e-mail, and so on. Giving the machine a new IP address corresponding to its new location is unattractive because large numbers of people, programs, and databases would have to be informed of the change. 

Another approach is to have the routers use complete IP addresses for routing, instead of just the network. However, this strategy would require each router to have millions of table entries, at astronomical cost to the Internet. 

When people began demanding the ability to connect their notebook computers to the Internet wherever they were, IETF set up a Working Group to find a solution. The Working Group quickly formulated a number of goals considered desirable in any solution. The major ones were: 

1. Each mobile host must be able to use its home IP address anywhere. 

2. Software changes to the fixed hosts were not permitted. 

3. Changes to the router software and tables were not permitted. 

4. Most packets for mobile hosts should not make detours on the way. 

5. No overhead should be incurred when a mobile host is at home. 

The solution chosen was the one described in 

Sec. 5.2.8

. To review it briefly, every site that wants to allow its users to roam has to create a home agent. Every site that wants to allow visitors has to create a foreign agent. When a mobile host shows up at a foreign site, it contacts the foreign host there and registers. The foreign host then contacts the user's home agent and gives it a 

care-of address

, normally the foreign agent's own IP address. 

When a packet arrives at the user's home LAN, it comes in at some router attached to the LAN. The router then tries to locate the host in the usual way, by broadcasting an ARP packet asking, for example: What is the Ethernet address of 160.80.40.20? The home agent responds to this query by giving its own Ethernet address. The router then sends packets for 160.80.40.20 to the home agent. It, in turn, tunnels them to the care-of address by encapsulating them in the payload field of an IP packet addressed to the foreign agent. The foreign agent then decapsulates and delivers them to the data link address of the mobile host. In addition, the home agent gives the care-of address to the sender, so future packets can be tunneled directly to the foreign agent. This solution meets all the requirements stated above. 

One small detail is probably worth mentioning. At the time the mobile host moves, the router probably has its (soon-to-be-invalid) Ethernet address cached. Replacing that Ethernet address with the home agent's is done by a trick called 

gratuitous ARP

. This is a special, unsolicited message to the router that causes it to replace a specific cache entry, in this case, that of the mobile host about to leave. When the mobile host returns later, the same trick is used to update the router's cache again. 

Nothing in the design prevents a mobile host from being its own foreign agent, but that approach only works if the mobile host (in its capacity as foreign agent) is logically connected to the Internet at its current site. Also, the mobile host must be able to acquire a (temporary) 

353




care-of IP address to use. That IP address must belong to the LAN to which it is currently attached. 

The IETF solution for mobile hosts solves a number of other problems not mentioned so far. For example, how are agents located? The solution is for each agent to periodically broadcast its address and the type of services it is willing to provide (e.g., home, foreign, or both). When a mobile host arrives somewhere, it can just listen for these broadcasts, called 

advertisements

. Alternatively, it can broadcast a packet announcing its arrival and hope that the local foreign agent responds to it. 

Another problem that had to be solved is what to do about impolite mobile hosts that leave without saying goodbye. The solution is to make registration valid only for a fixed time interval. If it is not refreshed periodically, it times out, so the foreign host can clear its tables. 

Yet another issue is security. When a home agent gets a message asking it to please forward all of Roberta's packets to some IP address, it had better not comply unless it is convinced that Roberta is the source of this request, and not somebody trying to impersonate her. Cryptographic authentication protocols are used for this purpose. We will study such protocols in 

Chap. 8

. 

A final point addressed by the Working Group relates to levels of mobility. Imagine an airplane with an on-board Ethernet used by the navigation and avionics computers. On this Ethernet is a standard router that talks to the wired Internet on the ground over a radio link. One fine day, some clever marketing executive gets the idea to install Ethernet connectors in all the arm rests so passengers with mobile computers can also plug in. 

Now we have two levels of mobility: the aircraft's own computers, which are stationary with respect to the Ethernet, and the passengers' computers, which are mobile with respect to it. In addition, the on-board router is mobile with respect to routers on the ground. Being mobile with respect to a system that is itself mobile can be handled using recursive tunneling. 

5.6.8 IPv6 

While CIDR and NAT may buy a few more years' time, everyone realizes that the days of IP in its current form (IPv4) are numbered. In addition to these technical problems, another issue looms in the background. In its early years, the Internet was largely used by universities, high-tech industry, and the U.S. Government (especially the Dept. of Defense). With the explosion of interest in the Internet starting in the mid-1990s, it began to be used by a different group of people, especially people with different requirements. For one thing, numerous people with wireless portables use it to keep in contact with their home bases. For another, with the impending convergence of the computer, communication, and entertainment industries, it may not be that long before every telephone and television set in the world is an Internet node, producing a billion machines being used audio and video on demand. Under these circumstances, it became apparent that IP had to evolve and become more flexible. 

Seeing these problems on the horizon, in 1990, IETF started work on a new version of IP, one which would never run out of addresses, would solve a variety of other problems, and be more flexible and efficient as well. Its major goals were: 

1. Support billions of hosts, even with inefficient address space allocation. 

2. Reduce the size of the routing tables. 

3. Simplify the protocol, to allow routers to process packets faster. 

4. Provide better security (authentication and privacy) than current IP. 

5. Pay more attention to type of service, particularly for real-time data. 

6. Aid multicasting by allowing scopes to be specified. 

7. Make it possible for a host to roam without changing its address. 

8. Allow the protocol to evolve in the future. 

354




9. Permit the old and new protocols to coexist for years. 

To develop a protocol that met all these requirements, IETF issued a call for proposals and discussion in RFC 1550. Twenty-one responses were received, not all of them full proposals. By December 1992, seven serious proposals were on the table. They ranged from making minor patches to IP, to throwing it out altogether and replacing with a completely different protocol. 

One proposal was to run TCP over CLNP, which, with its 160-bit addresses would have provided enough address space forever and would have unified two major network layer protocols. However, many people felt that this would have been an admission that something in the OSI world was actually done right, a statement considered Politically Incorrect in Internet circles. CLNP was patterned closely on IP, so the two are not really that different. In fact, the protocol ultimately chosen differs from IP far more than CLNP does. Another strike against CLNP was its poor support for service types, something required to transmit multimedia efficiently. 

Three of the better proposals were published in 

IEEE Network

 (Deering, 1993; Francis, 1993; and Katz and Ford, 1993). After much discussion, revision, and jockeying for position, a modified combined version of the Deering and Francis proposals, by now called 

SIPP

 (

Simple Internet Protocol Plus

) was selected and given the designation 

IPv6.

IPv6 meets the goals fairly well. It maintains the good features of IP, discards or deemphasizes the bad ones, and adds new ones where needed. In general, IPv6 is not compatible with IPv4, but it is compatible with the other auxiliary Internet protocols, including TCP, UDP, ICMP, IGMP, OSPF, BGP, and DNS, sometimes with small modifications being required (mostly to deal with longer addresses). The main features of IPv6 are discussed below. More information about it can be found in RFCs 2460 through 2466. 

First and foremost, IPv6 has longer addresses than IPv4. They are 16 bytes long, which solves the problem that IPv6 set out to solve: provide an effectively unlimited supply of Internet addresses. We will have more to say about addresses shortly. 

The second major improvement of IPv6 is the simplification of the header. It contains only seven fields (versus 13 in IPv4). This change allows routers to process packets faster and thus improve throughput and delay. We will discuss the header shortly, too. 

The third major improvement was better support for options. This change was essential with the new header because fields that previously were required are now optional. In addition, the way options are represented is different, making it simple for routers to skip over options not intended for them. This feature speeds up packet processing time. 

A fourth area in which IPv6 represents a big advance is in security. IETF had its fill of newspaper stories about precocious 12-year-olds using their personal computers to break into banks and military bases all over the Internet. There was a strong feeling that something had to be done to improve security. Authentication and privacy are key features of the new IP. These were later retrofitted to IPv4, however, so in the area of security the differences are not so great any more. 

Finally, more attention has been paid to quality of service. Various half-hearted efforts have been made in the past, but now with the growth of multimedia on the Internet, the sense of urgency is greater. 

The Main IPv6 Header 

The IPv6 header is shown in 

Fig. 5-68

. The 

Version

 field is always 6 for IPv6 (and 4 for IPv4). During the transition period from IPv4, which will probably take a decade, routers will be able 

355




to examine this field to tell what kind of packet they have. As an aside, making this test wastes a few instructions in the critical path, so many implementations are likely to try to avoid it by using some field in the data link header to distinguish IPv4 packets from IPv6 packets. In this way, packets can be passed to the correct network layer handler directly. However, having the data link layer be aware of network packet types completely violates the design principle that each layer should not be aware of the meaning of the bits given to it from the layer above. The discussions between the ''Do it right'' and ''Make it fast'' camps will no doubt be lengthy and vigorous. 

Figure 5-68. The IPv6 fixed header (required). 

 

The 

Traffic class

 field is used to distinguish between packets with different real-time delivery requirements. A field designed for this purpose has been in IP since the beginning, but it has been only sporadically implemented by routers. Experiments are now underway to determine how best it can be used for multimedia delivery. 

The 

Flow label

 field is also still experimental but will be used to allow a source and destination to set up a pseudoconnection with particular properties and requirements. For example, a stream of packets from one process on a certain source host to a certain process on a certain destination host might have stringent delay requirements and thus need reserved bandwidth. The flow can be set up in advance and given an identifier. When a packet with a nonzero 

Flow label

 shows up, all the routers can look it up in internal tables to see what kind of special treatment it requires. In effect, flows are an attempt to have it both ways: the flexibility of a datagram subnet and the guarantees of a virtual-circuit subnet. 

Each flow is designated by the source address, destination address, and flow number, so many flows may be active at the same time between a given pair of IP addresses. Also, in this way, even if two flows coming from different hosts but with the same flow label pass through the same router, the router will be able to tell them apart using the source and destination addresses. It is expected that flow labels will be chosen randomly, rather than assigned sequentially starting at 1, so routers as expected to hash them. 

The 

Payload length

 field tells how many bytes follow the 40-byte header of 

Fig. 5-68

. The name was changed from the IPv4 

Total length

 field because the meaning was changed slightly: the 40 header bytes are no longer counted as part of the length (as they used to be). 

356




The 

Next header

 field lets the cat out of the bag. The reason the header could be simplified is that there can be additional (optional) extension headers. This field tells which of the (currently) six extension headers, if any, follow this one. If this header is the last IP header, the 

Next header

 field tells which transport protocol handler (e.g., TCP, UDP) to pass the packet to. 

The 

Hop limit

 field is used to keep packets from living forever. It is, in practice, the same as the 

Time to live

 field in IPv4, namely, a field that is decremented on each hop. In theory, in IPv4 it was a time in seconds, but no router used it that way, so the name was changed to reflect the way it is actually used. 

Next come the 

Source address

 and 

Destination address

 fields. Deering's original proposal, SIP, used 8-byte addresses, but during the review process many people felt that with 8-byte addresses IPv6 would run out of addresses within a few decades, whereas with 16-byte addresses it would never run out. Other people argued that 16 bytes was overkill, whereas still others favored using 20-byte addresses to be compatible with the OSI datagram protocol. Still another faction wanted variable-sized addresses. After much debate, it was decided that fixed-length 16-byte addresses were the best compromise. 

A new notation has been devised for writing 16-byte addresses. They are written as eight groups of four hexadecimal digits with colons between the groups, like this: 

8000:0000:0000:0000:0123:4567:89AB:CDEF 

Since many addresses will have many zeros inside them, three optimizations have been authorized. First, leading zeros within a group can be omitted, so 0123 can be written as 123. Second, one or more groups of 16 zero bits can be replaced by a pair of colons. Thus, the above address now becomes 

8000::123:4567:89AB:CDEF 

Finally, IPv4 addresses can be written as a pair of colons and an old dotted decimal number, for example 

::192.31.20.46 

Perhaps it is unnecessary to be so explicit about it, but there are a lot of 16-byte addresses. Specifically, there are 2

128

 of them, which is approximately 3 x 10

38

. If the entire earth, land and water, were covered with computers, IPv6 would allow 7 x 10

23

 IP addresses per square meter. Students of chemistry will notice that this number is larger than Avogadro's number. While it was not the intention to give every molecule on the surface of the earth its own IP address, we are not that far off. 

In practice, the address space will not be used efficiently, just as the telephone number address space is not (the area code for Manhattan, 212, is nearly full, but that for Wyoming, 307, is nearly empty). In RFC 3194, Durand and Huitema calculated that, using the allocation of telephone numbers as a guide, even in the most pessimistic scenario there will still be well over 1000 IP addresses per square meter of the entire earth's surface (land and water). In any likely scenario, there will be trillions of them per square meter. In short, it seems unlikely that we will run out in the foreseeable future. 

It is instructive to compare the IPv4 header (

Fig. 5-53

) with the IPv6 header (

Fig. 5-68

) to see what has been left out in IPv6. The 

IHL

 field is gone because the IPv6 header has a fixed length. The 

Protocol

 field was taken out because the 

Next header

 field tells what follows the last IP header (e.g., a UDP or TCP segment). 

357




All the fields relating to fragmentation were removed because IPv6 takes a different approach to fragmentation. To start with, all IPv6-conformant hosts are expected to dynamically determine the datagram size to use. This rule makes fragmentation less likely to occur in the first place. Also, the minimum has been raised from 576 to 1280 to allow 1024 bytes of data and many headers. In addition, when a host sends an IPv6 packet that is too large, instead of fragmenting it, the router that is unable to forward it sends back an error message. This message tells the host to break up all future packets to that destination. Having the host send packets that are the right size in the first place is ultimately much more efficient than having the routers fragment them on the fly. 

Finally, the 

Checksum

 field is gone because calculating it greatly reduces performance. With the reliable networks now used, combined with the fact that the data link layer and transport layers normally have their own checksums, the value of yet another checksum was not worth the performance price it extracted. Removing all these features has resulted in a lean and mean network layer protocol. Thus, the goal of IPv6—a fast, yet flexible, protocol with plenty of address space—has been met by this design. 

Extension Headers 

Some of the missing IPv4 fields are occasionally still needed, so IPv6 has introduced the concept of an (optional) 

extension header

. These headers can be supplied to provide extra information, but encoded in an efficient way. Six kinds of extension headers are defined at present, as listed in 

Fig. 5-69

. Each one is optional, but if more than one is present, they must appear directly after the fixed header, and preferably in the order listed. 

Figure 5-69. IPv6 extension headers. 

 

Some of the headers have a fixed format; others contain a variable number of variable-length fields. For these, each item is encoded as a (Type, Length, Value) tuple. The 

Type

 is a 1-byte field telling which option this is. The 

Type

 values have been chosen so that the first 2 bits tell routers that do not know how to process the option what to do. The choices are: skip the option; discard the packet; discard the packet and send back an ICMP packet; and the same as the previous one, except do not send ICMP packets for multicast addresses (to prevent one bad multicast packet from generating millions of ICMP reports). 

The 

Length

 is also a 1-byte field. It tells how long the value is (0 to 255 bytes). The 

Value

 is any information required, up to 255 bytes. 

The hop-by-hop header is used for information that all routers along the path must examine. So far, one option has been defined: support of datagrams exceeding 64K. The format of this header is shown in 

Fig. 5-70

. When it is used, the 

Payload length

 field in the fixed header is set to zero. 

Figure 5-70. The hop-by-hop extension header for large datagrams (jumbograms). 

358




 

As with all extension headers, this one starts out with a byte telling what kind of header comes next. This byte is followed by one telling how long the hop-by-hop header is in bytes, excluding the first 8 bytes, which are mandatory. All extensions begin this way. 

The next 2 bytes indicate that this option defines the datagram size (code 194) and that the size is a 4-byte number. The last 4 bytes give the size of the datagram. Sizes less than 65,536 bytes are not permitted and will result in the first router discarding the packet and sending back an ICMP error message. Datagrams using this header extension are called 

jumbograms

. The use of jumbograms is important for supercomputer applications that must transfer gigabytes of data efficiently across the Internet. 

The destination options header is intended for fields that need only be interpreted at the destination host. In the initial version of IPv6, the only options defined are null options for padding this header out to a multiple of 8 bytes, so initially it will not be used. It was included to make sure that new routing and host software can handle it, in case someone thinks of a destination option some day. 

The routing header lists one or more routers that must be visited on the way to the destination. It is very similar to the IPv4 loose source routing in that all addresses listed must be visited in order, but other routers not listed may be visited in between. The format of the routing header is shown in 

Fig. 5-71

. 

Figure 5-71. The extension header for routing. 

 

The first 4 bytes of the routing extension header contain four 1-byte integers. The 

Next header

 and 

Header entension length

 fields were described above. The 

Routing type

 field gives the format of the rest of the header. Type 0 says that a reserved 32-bit word follows the first word, followed by some number of IPv6 addresses. Other types may be invented in the future as needed. Finally, the 

Segments left

 field keeps track of how many of the addresses in the list have not yet been visited. It is decremented every time one is visited. When it hits 0, the packet is on its own with no more guidance about what route to follow. Usually at this point it is so close to the destination that the best route is obvious. 

The fragment header deals with fragmentation similarly to the way IPv4 does. The header holds the datagram identifier, fragment number, and a bit telling whether more fragments will follow. In IPv6, unlike in IPv4, only the source host can fragment a packet. Routers along the way may not do this. Although this change is a major philosophical break with the past, it simplifies the routers' work and makes routing go faster. As mentioned above, if a router is confronted with a packet that is too big, it discards the packet and sends an ICMP packet back to the source. This information allows the source host to fragment the packet into smaller pieces using this header and try again. 

The authentication header provides a mechanism by which the receiver of a packet can be sure of who sent it. The encrypted security payload makes it possible to encrypt the contents of a 

359




packet so that only the intended recipient can read it. These headers use cryptographic techniques to accomplish their missions. 

Controversies 

Given the open design process and the strongly-held opinions of many of the people involved, it should come as no surprise that many choices made for IPv6 were highly controversial, to say the least. We will summarize a few of these briefly below. For all the gory details, see the RFCs. 

We have already mentioned the argument about the address length. The result was a compromise: 16-byte fixed-length addresses. 

Another fight developed over the length of the 

Hop limit

 field. One camp felt strongly that limiting the maximum number of hops to 255 (implicit in using an 8-bit field) was a gross mistake. After all, paths of 32 hops are common now, and 10 years from now much longer paths may be common. These people argued that using a huge address size was farsighted but using a tiny hop count was short-sighted. In their view, the greatest sin a computer scientist can commit is to provide too few bits somewhere. 

The response was that arguments could be made to increase every field, leading to a bloated header. Also, the function of the 

Hop limit

 field is to keep packets from wandering around for a long time and 65,535 hops is far too long. Finally, as the Internet grows, more and more long-distance links will be built, making it possible to get from any country to any other country in half a dozen hops at most. If it takes more than 125 hops to get from the source and destination to their respective international gateways, something is wrong with the national backbones. The 8-bitters won this one. 

Another hot potato was the maximum packet size. The supercomputer community wanted packets in excess of 64 KB. When a supercomputer gets started transferring, it really means business and does not want to be interrupted every 64 KB. The argument against large packets is that if a 1-MB packet hits a 1.5-Mbps T1 line, that packet will tie the line up for over 5 seconds, producing a very noticeable delay for interactive users sharing the line. A compromise was reached here: normal packets are limited to 64 KB, but the hop-by-hop extension header can be used to permit jumbograms. 

A third hot topic was removing the IPv4 checksum. Some people likened this move to removing the brakes from a car. Doing so makes the car lighter so it can go faster, but if an unexpected event happens, you have a problem. 

The argument against checksums was that any application that really cares about data integrity has to have a transport layer checksum anyway, so having another one in IP (in addition to the data link layer checksum) is overkill. Furthermore, experience showed that computing the IP checksum was a major expense in IPv4. The antichecksum camp won this one, and IPv6 does not have a checksum. 

Mobile hosts were also a point of contention. If a portable computer flies halfway around the world, can it continue operating at the destination with the same IPv6 address, or does it have to use a scheme with home agents and foreign agents? Mobile hosts also introduce asymmetries into the routing system. It may well be the case that a small mobile computer can easily hear the powerful signal put out by a large stationary router, but the stationary router cannot hear the feeble signal put out by the mobile host. Consequently, some people wanted to build explicit support for mobile hosts into IPv6. That effort failed when no consensus could be found for any specific proposal. 

360




Probably the biggest battle was about security. Everyone agreed it was essential, The war was about where and how. First where. The argument for putting it in the network layer is that it then becomes a standard service that all applications can use without any advance planning. The argument against it is that really secure applications generally want nothing less than end-to-end encryption, where the source application does the encryption and the destination application undoes it. With anything less, the user is at the mercy of potentially buggy network layer implementations over which he has no control. The response to this argument is that these applications can just refrain from using the IP security features and do the job themselves. The rejoinder to that is that the people who do not trust the network to do it right, do not want to pay the price of slow, bulky IP implementations that have this capability, even if it is disabled. 

Another aspect of where to put security relates to the fact that many (but not all) countries have stringent export laws concerning cryptography. Some, notably France and Iraq, also restrict its use domestically, so that people cannot have secrets from the police. As a result, any IP implementation that used a cryptographic system strong enough to be of much value could not be exported from the United States (and many other countries) to customers worldwide. Having to maintain two sets of software, one for domestic use and one for export, is something most computer vendors vigorously oppose. 

One point on which there was no controversy is that no one expects the IPv4 Internet to be turned off on a Sunday morning and come back up as an IPv6 Internet Monday morning. Instead, isolated ''islands'' of IPv6 will be converted, initially communicating via tunnels. As the IPv6 islands grow, they will merge into bigger islands. Eventually, all the islands will merge, and the Internet will be fully converted. Given the massive investment in IPv4 routers currently deployed, the conversion process will probably take a decade. For this reason, an enormous amount of effort has gone into making sure that this transition will be as painless as possible. For more information about IPv6, see (Loshin, 1999). 

5.7 Summary 

The network layer provides services to the transport layer. It can be based on either virtual circuits or datagrams. In both cases, its main job is routing packets from the source to the destination. In virtual-circuit subnets, a routing decision is made when the virtual circuit is set up. In datagram subnets, it is made on every packet. 

Many routing algorithms are used in computer networks. Static algorithms include shortest path routing and flooding. Dynamic algorithms include distance vector routing and link state routing. Most actual networks use one of these. Other important routing topics are hierarchical routing, routing for mobile hosts, broadcast routing, multicast routing, and routing in peer-to-peer networks. 

Subnets can easily become congested, increasing the delay and lowering the throughput for packets. Network designers attempt to avoid congestion by proper design. Techniques include retransmission policy, caching, flow control, and more. If congestion does occur, it must be dealt with. Choke packets can be sent back, load can be shed, and other methods applied. 

The next step beyond just dealing with congestion is to actually try to achieve a promised quality of service. The methods that can be used for this include buffering at the client, traffic shaping, resource reservation, and admission control. Approaches that have been designed for good quality of service include integrated services (including RSVP), differentiated services, and MPLS. 

Networks differ in various ways, so when multiple networks are interconnected problems can occur. Sometimes the problems can be finessed by tunneling a packet through a hostile 

361




network, but if the source and destination networks are different, this approach fails. When different networks have different maximum packet sizes, fragmentation may be called for. 

The Internet has a rich variety of protocols related to the network layer. These include the data transport protocol, IP, but also the control protocols ICMP, ARP, and RARP, and the routing protocols OSPF and BGP. The Internet is rapidly running out of IP addresses, so a new version of IP, IPv6, has been developed. 

Problems 

1. Give two example computer applications for which connection-oriented service is appropriate. Now give two examples for which connectionless service is best. 

2. Are there any circumstances when connection-oriented service will (or at least should) deliver packets out of order? Explain. 

3. Datagram subnets route each packet as a separate unit, independent of all others. Virtual-circuit subnets do not have to do this, since each data packet follows a predetermined route. Does this observation mean that virtual-circuit subnets do not need the capability to route isolated packets from an arbitrary source to an arbitrary destination? Explain your answer. 

4. Give three examples of protocol parameters that might be negotiated when a connection is set up. 

5. Consider the following design problem concerning implementation of virtual-circuit service. If virtual circuits are used internal to the subnet, each data packet must have a 3-byte header and each router must tie up 8 bytes of storage for circuit identification. If datagrams are used internally, 15-byte headers are needed but no router table space is required. Transmission capacity costs 1 cent per 10

6

 bytes, per hop. Very fast router memory can be purchased for 1 cent per byte and is depreciated over two years, assuming a 40-hour business week. The statistically average session runs for 1000 sec, in which time 200 packets are transmitted. The mean packet requires four hops. Which implementation is cheaper, and by how much? 

6. Assuming that all routers and hosts are working properly and that all software in both is free of all errors, is there any chance, however small, that a packet will be delivered to the wrong destination? 

7. Consider the network of 

Fig. 5-7

, but ignore the weights on the lines. Suppose that it uses flooding as the routing algorithm. If a packet sent by 

A

 to 

D

 has a maximum hop count of 3, list all the routes it will take. Also tell how many hops worth of bandwidth it consumes. 

8. Give a simple heuristic for finding two paths through a network from a given source to a given destination that can survive the loss of any communication line (assuming two such paths exist). The routers are considered reliable enough, so it is not necessary to worry about the possibility of router crashes. 

9. Consider the subnet of 

Fig. 5-13(a)

. Distance vector routing is used, and the following vectors have just come in to router 

C

: from 

B

: (5, 0, 8, 12, 6, 2); from 

D

: (16, 12, 6, 0, 9, 10); and from 

E

: (7, 6, 3, 9, 0, 4). The measured delays to 

B

, 

D

, and 

E

, are 6, 3, and 5, respectively. What is 

C

's new routing table? Give both the outgoing line to use and the expected delay. 

10. If delays are recorded as 8-bit numbers in a 50-router network, and delay vectors are exchanged twice a second, how much bandwidth per (full-duplex) line is chewed up by the distributed routing algorithm? Assume that each router has three lines to other routers. 

11. In 

Fig. 5-14

 the Boolean OR of the two sets of ACF bits are 111 in every row. Is this just an accident here, or does it hold for all subnets under all circumstances? 

12. For hierarchical routing with 4800 routers, what region and cluster sizes should be chosen to minimize the size of the routing table for a three-layer hierarchy? A good starting place is the hypothesis that a solution with 

k

 clusters of 

k

 regions of 

k

 routers is close to optimal, which means that 

k

 is about the cube root of 4800 (around 16). Use trial and error to check out combinations where all three parameters are in the general vicinity of 16. 

362




13. In the text it was stated that when a mobile host is not at home, packets sent to its home LAN are intercepted by its home agent on that LAN. For an IP network on an 802.3 LAN, how does the home agent accomplish this interception? 

14. Looking at the subnet of 

Fig. 5-6

, how many packets are generated by a broadcast from 

B

, using 

a. (a) reverse path forwarding? 

b. (b) the sink tree? 

15. Consider the network of 

Fig. 5-16(a)

. Imagine that one new line is added, between 

F

 and 

G

, but the sink tree of 

Fig. 5-16(b)

 remains unchanged. What changes occur to 

Fig. 

5-16(c)

? 

16. Compute a multicast spanning tree for router 

C

 in the following subnet for a group with members at routers 

A

, 

B

, 

C

, 

D

, 

E

, 

F

, 

I

, and 

K

. 

 

17. In 

Fig. 5-20

, do nodes 

H

 or 

I

 ever broadcast on the lookup shown starting at 

A

? 

18. Suppose that node 

B

 in 

Fig. 5-20

 has just rebooted and has no routing information in its tables. It suddenly needs a route to 

H

. It sends out broadcasts with 

TTL

 set to 1, 2, 3, and so on. How many rounds does it take to find a route? 

19. In the simplest version of the Chord algorithm for peer-to-peer lookup, searches do not use the finger table. Instead, they are linear around the circle, in either direction. Can a node accurately predict which direction it should search? Discuss your answer. 

20. Consider the Chord circle of 

Fig. 5-24

. Suppose that node 10 suddenly goes on line. Does this affect node 1's finger table, and if so, how? 

21. As a possible congestion control mechanism in a subnet using virtual circuits internally, a router could refrain from acknowledging a received packet until (1) it knows its last transmission along the virtual circuit was received successfully and (2) it has a free buffer. For simplicity, assume that the routers use a stop-and-wait protocol and that each virtual circuit has one buffer dedicated to it for each direction of traffic. If it takes 

T

 sec to transmit a packet (data or acknowledgement) and there are 

n

 routers on the path, what is the rate at which packets are delivered to the destination host? Assume that transmission errors are rare and that the host-router connection is infinitely fast. 

22. A datagram subnet allows routers to drop packets whenever they need to. The probability of a router discarding a packet is 

p

. Consider the case of a source host connected to the source router, which is connected to the destination router, and then to the destination host. If either of the routers discards a packet, the source host eventually times out and tries again. If both host-router and router-router lines are counted as hops, what is the mean number of 

a. (a) hops a packet makes per transmission? 

b. (b) transmissions a packet makes? 

c. (c) hops required per received packet? 

23. Describe two major differences between the warning bit method and the RED method. 

24. Give an argument why the leaky bucket algorithm should allow just one packet per tick, independent of how large the packet is. 

25. The byte-counting variant of the leaky bucket algorithm is used in a particular system. The rule is that one 1024-byte packet, or two 512-byte packets, etc., may be sent on each tick. Give a serious restriction of this system that was not mentioned in the text. 

363




26. An ATM network uses a token bucket scheme for traffic shaping. A new token is put into the bucket every 5 µsec. Each token is good for one cell, which contains 48 bytes of data. What is the maximum sustainable data rate? 

27. A computer on a 6-Mbps network is regulated by a token bucket. The token bucket is filled at a rate of 1 Mbps. It is initially filled to capacity with 8 megabits. How long can the computer transmit at the full 6 Mbps? 

28. Imagine a flow specification that has a maximum packet size of 1000 bytes, a token bucket rate of 10 million bytes/sec, a token bucket size of 1 million bytes, and a maximum transmission rate of 50 million bytes/sec. How long can a burst at maximum speed last? 

29. The network of 

Fig. 5-37

 uses RSVP with multicast trees for hosts 1 and 2 as shown. Suppose that host 3 requests a channel of bandwidth 2 MB/sec for a flow from host 1 and another channel of bandwidth 1 MB/sec for a flow from host 2. At the same time, host 4 requests a channel of bandwidth 2 MB/sec for a flow from host 1 and host 5 requests a channel of bandwidth 1 MB/sec for a flow from host 2. How much total bandwidth will be reserved for these requests at routers 

A

, 

B

, 

C

, 

E

, 

H

, 

J

, 

K

, and 

L

? 

30. The CPU in a router can process 2 million packets/sec. The load offered to it is 1.5 million packets/sec. If a route from source to destination contains 10 routers, how much time is spent being queued and serviced by the CPUs? 

31. Consider the user of differentiated services with expedited forwarding. Is there a guarantee that expedited packets experience a shorter delay than regular packets? Why or why not? 

32. Is fragmentation needed in concatenated virtual-circuit internets or only in datagram systems? 

33. Tunneling through a concatenated virtual-circuit subnet is straightforward: the multiprotocol router at one end just sets up a virtual circuit to the other end and passes packets through it. Can tunneling also be used in datagram subnets? If so, how? 

34. Suppose that host 

A

 is connected to a router 

R

 1, 

R

 1 is connected to another router, 

R

 2, and 

R

 2 is connected to host 

B

. Suppose that a TCP message that contains 900 bytes of data and 20 bytes of TCP header is passed to the IP code at host 

A

 for delivery to 

B

. Show the 

Total length, Identification

, 

DF

, 

MF

, and 

Fragment offset

 fields of the IP header in each packet transmitted over the three links. Assume that link 

A-R1

 can support a maximum frame size of 1024 bytes including a 14-byte frame header, link 

R1-R2

 can support a maximum frame size of 512 bytes, including an 8-byte frame header, and link 

R2-B

 can support a maximum frame size of 512 bytes including a 12-byte frame header. 

35. A router is blasting out IP packets whose total length (data plus header) is 1024 bytes. Assuming that packets live for 10 sec, what is the maximum line speed the router can operate at without danger of cycling through the IP datagram ID number space? 

36. An IP datagram using the 

Strict source routing

 option has to be fragmented. Do you think the option is copied into each fragment, or is it sufficient to just put it in the first fragment? Explain your answer. 

37. Suppose that instead of using 16 bits for the network part of a class B address originally, 20 bits had been used. How many class B networks would there have been? 

38. Convert the IP address whose hexadecimal representation is C22F1582 to dotted decimal notation. 

39. A network on the Internet has a subnet mask of 255.255.240.0. What is the maximum number of hosts it can handle? 

40. A large number of consecutive IP address are available starting at 198.16.0.0. Suppose that four organizations, 

A

, 

B

, 

C

, and 

D

, request 4000, 2000, 4000, and 8000 addresses, respectively, and in that order. For each of these, give the first IP address assigned, the last IP address assigned, and the mask in the 

w.x.y.z

/

s

 notation. 

41. A router has just received the following new IP addresses: 57.6.96.0/21, 57.6.104.0/21, 57.6.112.0/21, and 57.6.120.0/21. If all of them use the same outgoing line, can they be aggregated? If so, to what? If not, why not? 

42. The set of IP addresses from 29.18.0.0 to 19.18.128.255 has been aggregated to 29.18.0.0/17. However, there is a gap of 1024 unassigned addresses from 29.18.60.0 to 29.18.63.255 that are now suddenly assigned to a host using a different outgoing 

364




line. Is it now necessary to split up the aggregate address into its constituent blocks, add the new block to the table, and then see if any reaggregation is possible? If not, what can be done instead? 

43. A router has the following (CIDR) entries in its routing table: 

Address/mask

Next hop

135.46.56.0/22 

Interface 0 

135.46.60.0/22 

Interface 1 

192.53.40.0/23 

Router 1 

default 

Router 2 

44. For each of the following IP addresses, what does the router do if a packet with that address arrives? 

a. (a) 135.46.63.10 

b. (b) 135.46.57.14 

c. (c) 135.46.52.2 

d. (d) 192.53.40.7 

e. (e) 192.53.56.7 

45. Many companies have a policy of having two (or more) routers connecting the company to the Internet to provide some redundancy in case one of them goes down. Is this policy still possible with NAT? Explain your answer. 

46. You have just explained the ARP protocol to a friend. When you are all done, he says: ''I've got it. ARP provides a service to the network layer, so it is part of the data link layer.'' What do you say to him? 

47. ARP and RARP both map addresses from one space to another. In this respect, they are similar. However, their implementations are fundamentally different. In what major way do they differ? 

48. Describe a way to reassemble IP fragments at the destination. 

49. Most IP datagram reassembly algorithms have a timer to avoid having a lost fragment tie up reassembly buffers forever. Suppose that a datagram is fragmented into four fragments. The first three fragments arrive, but the last one is delayed. Eventually, the timer goes off and the three fragments in the receiver's memory are discarded. A little later, the last fragment stumbles in. What should be done with it? 

50. In both IP and ATM, the checksum covers only the header and not the data. Why do you suppose this design was chosen? 

51. A person who lives in Boston travels to Minneapolis, taking her portable computer with her. To her surprise, the LAN at her destination in Minneapolis is a wireless IP LAN, so she does not have to plug in. Is it still necessary to go through the entire business with home agents and foreign agents to make e-mail and other traffic arrive correctly? 

52. IPv6 uses 16-byte addresses. If a block of 1 million addresses is allocated every picosecond, how long will the addresses last? 

53. The 

Protocol

 field used in the IPv4 header is not present in the fixed IPv6 header. Why not? 

54. When the IPv6 protocol is introduced, does the ARP protocol have to be changed? If so, are the changes conceptual or technical? 

55. Write a program to simulate routing using flooding. Each packet should contain a counter that is decremented on each hop. When the counter gets to zero, the packet is discarded. Time is discrete, with each line handling one packet per time interval. Make three versions of the program: all lines are flooded, all lines except the input line are flooded, and only the (statically chosen) best 

k

 lines are flooded. Compare flooding with deterministic routing (

k

 = 1) in terms of both delay and the bandwidth used. 

56. Write a program that simulates a computer network using discrete time. The first packet on each router queue makes one hop per time interval. Each router has only a finite number of buffers. If a packet arrives and there is no room for it, it is discarded and not retransmitted. Instead, there is an end-to-end protocol, complete with timeouts and acknowledgement packets, that eventually regenerates the packet from the source 

365




router. Plot the throughput of the network as a function of the end-to-end timeout interval, parameterized by error rate. 

57. Write a function to do forwarding in an IP router. The procedure has one parameter, an IP address. It also has access to a global table consisting of an array of triples. Each triple contains three integers: an IP address, a subnet mask, and the outline line to use. The function looks up the IP address in the table using CIDR and returns the line to use as its value. 

58. Use the 

traceroute

 (UNIX) or 

tracert

 (Windows) programs to trace the route from your computer to various universities on other continents. Make a list of transoceanic links you have discovered. Some sites to try are 

www.berkeley.edu

 (California) 

www.mit.edu

 (Massachusetts) 

www.vu.nl

 (Amsterdam) 

www.ucl.ac.uk

 (London) 

www.usyd.edu.au

 (Sydney) 

www.u-tokyo.ac.jp

 (Tokyo) 

www.uct.ac.za

 (Cape Town) 

 

366




Chapter 6. The Transport Layer 

The transport layer is not just another layer. It is the heart of the whole protocol hierarchy. Its task is to provide reliable, cost-effective data transport from the source machine to the destination machine, independently of the physical network or networks currently in use. Without the transport layer, the whole concept of layered protocols would make little sense. In this chapter we will study the transport layer in detail, including its services, design, protocols, and performance. 

6.1 The Transport Service 

In the following sections we will provide an introduction to the transport service. We look at what kind of service is provided to the application layer. To make the issue of transport service more concrete, we will examine two sets of transport layer primitives. First comes a simple (but hypothetical) one to show the basic ideas. Then comes the interface commonly used in the Internet. 

6.1.1 Services Provided to the Upper Layers 

The ultimate goal of the transport layer is to provide efficient, reliable, and cost-effective service to its users, normally processes in the application layer. To achieve this goal, the transport layer makes use of the services provided by the network layer. The hardware and/or software within the transport layer that does the work is called the 

transport entity

. The transport entity can be located in the operating system kernel, in a separate user process, in a library package bound into network applications, or conceivably on the network interface card. The (logical) relationship of the network, transport, and application layers is illustrated in 

Fig. 

6-1

. 

Figure 6-1. The network, transport, and application layers. 

 

Just as there are two types of network service, connection-oriented and connectionless, there are also two types of transport service. The connection-oriented transport service is similar to the connection-oriented network service in many ways. In both cases, connections have three phases: establishment, data transfer, and release. Addressing and flow control are also similar in both layers. Furthermore, the connectionless transport service is also very similar to the connectionless network service. 

367




The obvious question is then this: If the transport layer service is so similar to the network layer service, why are there two distinct layers? Why is one layer not adequate? The answer is subtle, but crucial, and goes back to 

Fig. 1-9

. The transport code runs entirely on the users' machines, but the network layer mostly runs on the routers, which are operated by the carrier (at least for a wide area network). What happens if the network layer offers inadequate service? Suppose that it frequently loses packets? What happens if routers crash from time to time? 

Problems occur, that's what. The users have no real control over the network layer, so they cannot solve the problem of poor service by using better routers or putting more error handling in the data link layer. The only possibility is to put on top of the network layer another layer that improves the quality of the service. If, in a connection-oriented subnet, a transport entity is informed halfway through a long transmission that its network connection has been abruptly terminated, with no indication of what has happened to the data currently in transit, it can set up a new network connection to the remote transport entity. Using this new network connection, it can send a query to its peer asking which data arrived and which did not, and then pick up from where it left off. 

In essence, the existence of the transport layer makes it possible for the transport service to be more reliable than the underlying network service. Lost packets and mangled data can be detected and compensated for by the transport layer. Furthermore, the transport service primitives can be implemented as calls to library procedures in order to make them independent of the network service primitives. The network service calls may vary considerably from network to network (e.g., connectionless LAN service may be quite different from connection-oriented WAN service). By hiding the network service behind a set of transport service primitives, changing the network service merely requires replacing one set of library procedures by another one that does the same thing with a different underlying service. 

Thanks to the transport layer, application programmers can write code according to a standard set of primitives and have these programs work on a wide variety of networks, without having to worry about dealing with different subnet interfaces and unreliable transmission. If all real networks were flawless and all had the same service primitives and were guaranteed never, ever to change, the transport layer might not be needed. However, in the real world it fulfills the key function of isolating the upper layers from the technology, design, and imperfections of the subnet. 

For this reason, many people have traditionally made a distinction between layers 1 through 4 on the one hand and layer(s) above 4 on the other. The bottom four layers can be seen as the 

transport service provider

, whereas the upper layer(s) are the 

transport service user

. This distinction of provider versus user has a considerable impact on the design of the layers and puts the transport layer in a key position, since it forms the major boundary between the provider and user of the reliable data transmission service. 

6.1.2 Transport Service Primitives 

To allow users to access the transport service, the transport layer must provide some operations to application programs, that is, a transport service interface. Each transport service has its own interface. In this section, we will first examine a simple (hypothetical) transport service and its interface to see the bare essentials. In the following section we will look at a real example. 

The transport service is similar to the network service, but there are also some important differences. The main difference is that the network service is intended to model the service offered by real networks, warts and all. Real networks can lose packets, so the network service is generally unreliable. 

368




The (connection-oriented) transport service, in contrast, is reliable. Of course, real networks are not error-free, but that is precisely the purpose of the transport layer—to provide a reliable service on top of an unreliable network. 

As an example, consider two processes connected by pipes in UNIX. They assume the connection between them is perfect. They do not want to know about acknowledgements, lost packets, congestion, or anything like that. What they want is a 100 percent reliable connection. Process 

A

 puts data into one end of the pipe, and process 

B

 takes it out of the other. This is what the connection-oriented transport service is all about—hiding the imperfections of the network service so that user processes can just assume the existence of an error-free bit stream. 

As an aside, the transport layer can also provide unreliable (datagram) service. However, there is relatively little to say about that, so we will mainly concentrate on the connection-oriented transport service in this chapter. Nevertheless, there are some applications, such as client-server computing and streaming multimedia, which benefit from connectionless transport, so we will say a little bit about it later on. 

A second difference between the network service and transport service is whom the services are intended for. The network service is used only by the transport entities. Few users write their own transport entities, and thus few users or programs ever see the bare network service. In contrast, many programs (and thus programmers) see the transport primitives. Consequently, the transport service must be convenient and easy to use. 

To get an idea of what a transport service might be like, consider the five primitives listed in 

Fig. 6-2

. This transport interface is truly bare bones, but it gives the essential flavor of what a connection-oriented transport interface has to do. It allows application programs to establish, use, and then release connections, which is sufficient for many applications. 

Figure 6-2. The primitives for a simple transport service. 

 

To see how these primitives might be used, consider an application with a server and a number of remote clients. To start with, the server executes a LISTEN primitive, typically by calling a library procedure that makes a system call to block the server until a client turns up. When a client wants to talk to the server, it executes a CONNECT primitive. The transport entity carries out this primitive by blocking the caller and sending a packet to the server. Encapsulated in the payload of this packet is a transport layer message for the server's transport entity. 

A quick note on terminology is now in order. For lack of a better term, we will reluctantly use the somewhat ungainly acronym 

TPDU

 (

Transport Protocol Data Unit

) for messages sent from transport entity to transport entity. Thus, TPDUs (exchanged by the transport layer) are contained in packets (exchanged by the network layer). In turn, packets are contained in frames (exchanged by the data link layer). When a frame arrives, the data link layer processes the frame header and passes the contents of the frame payload field up to the network entity. The network entity processes the packet header and passes the contents of the packet payload up to the transport entity. This nesting is illustrated in 

Fig. 6-3

. 

369




Figure 6-3. Nesting of TPDUs, packets, and frames. 

 

Getting back to our client-server example, the client's CONNECT call causes a CONNECTION REQUEST TPDU to be sent to the server. When it arrives, the transport entity checks to see that the server is blocked on a LISTEN (i.e., is interested in handling requests). It then unblocks the server and sends a CONNECTION ACCEPTED TPDU back to the client. When this TPDU arrives, the client is unblocked and the connection is established. 

Data can now be exchanged using the SEND and RECEIVE primitives. In the simplest form, either party can do a (blocking) RECEIVE to wait for the other party to do a SEND. When the TPDU arrives, the receiver is unblocked. It can then process the TPDU and send a reply. As long as both sides can keep track of whose turn it is to send, this scheme works fine. 

Note that at the transport layer, even a simple unidirectional data exchange is more complicated than at the network layer. Every data packet sent will also be acknowledged (eventually). The packets bearing control TPDUs are also acknowledged, implicitly or explicitly. These acknowledgements are managed by the transport entities, using the network layer protocol, and are not visible to the transport users. Similarly, the transport entities will need to worry about timers and retransmissions. None of this machinery is visible to the transport users. To the transport users, a connection is a reliable bit pipe: one user stuffs bits in and they magically appear at the other end. This ability to hide complexity is the reason that layered protocols are such a powerful tool. 

When a connection is no longer needed, it must be released to free up table space within the two transport entities. Disconnection has two variants: asymmetric and symmetric. In the asymmetric variant, either transport user can issue a DISCONNECT primitive, which results in a DISCONNECT TPDU being sent to the remote transport entity. Upon arrival, the connection is released. 

In the symmetric variant, each direction is closed separately, independently of the other one. When one side does a DISCONNECT, that means it has no more data to send but it is still willing to accept data from its partner. In this model, a connection is released when both sides have done a DISCONNECT. 

A state diagram for connection establishment and release for these simple primitives is given in 

Fig. 6-4

. Each transition is triggered by some event, either a primitive executed by the local transport user or an incoming packet. For simplicity, we assume here that each TPDU is separately acknowledged. We also assume that a symmetric disconnection model is used, with the client going first. Please note that this model is quite unsophisticated. We will look at more realistic models later on. 

Figure 6-4. A state diagram for a simple connection management scheme. Transitions labeled in italics are caused by packet arrivals. The solid lines show the client's state sequence. The dashed lines show the server's state sequence. 

370




 

6.1.3 Berkeley Sockets 

Let us now briefly inspect another set of transport primitives, the socket primitives used in Berkeley UNIX for TCP. These primitives are widely used for Internet programming. They are listed in 

Fig. 6-5

. Roughly speaking, they follow the model of our first example but offer more features and flexibility. We will not look at the corresponding TPDUs here. That discussion will have to wait until we study TCP later in this chapter. 

Figure 6-5. The socket primitives for TCP. 

 

The first four primitives in the list are executed in that order by servers. The SOCKET primitive creates a new end point and allocates table space for it within the transport entity. The parameters of the call specify the addressing format to be used, the type of service desired (e.g., reliable byte stream), and the protocol. A successful SOCKET call returns an ordinary file descriptor for use in succeeding calls, the same way an OPEN call does. 

Newly-created sockets do not have network addresses. These are assigned using the BIND primitive. Once a server has bound an address to a socket, remote clients can connect to it. The reason for not having the SOCKET call create an address directly is that some processes care about their address (e.g., they have been using the same address for years and everyone knows this address), whereas others do not care. 

371




Next comes the LISTEN call, which allocates space to queue incoming calls for the case that several clients try to connect at the same time. In contrast to LISTEN in our first example, in the socket model LISTEN is not a blocking call. 

To block waiting for an incoming connection, the server executes an ACCEPT primitive. When a TPDU asking for a connection arrives, the transport entity creates a new socket with the same properties as the original one and returns a file descriptor for it. The server can then fork off a process or thread to handle the connection on the new socket and go back to waiting for the next connection on the original socket. ACCEPT returns a normal file descriptor, which can be used for reading and writing in the standard way, the same as for files. 

Now let us look at the client side. Here, too, a socket must first be created using the SOCKET primitive, but BIND is not required since the address used does not matter to the server. The CONNECT primitive blocks the caller and actively starts the connection process. When it completes (i.e., when the appropriate TPDU is received from the server), the client process is unblocked and the connection is established. Both sides can now use SEND and RECV to transmit and receive data over the full-duplex connection. The standard UNIX READ and WRITE system calls can also be used if none of the special options of SEND and RECV are required. 

Connection release with sockets is symmetric. When both sides have executed a CLOSE primitive, the connection is released. 

6.1.4 An Example of Socket Programming: An Internet File Server 

As an example of how the socket calls are used, consider the client and server code of 

Fig. 6-

6

. Here we have a very primitive Internet file server along with an example client that uses it. The code has many limitations (discussed below), but in principle the server code can be compiled and run on any UNIX system connected to the Internet. The client code can then be compiled and run on any other UNIX machine on the Internet, anywhere in the world. The client code can be executed with appropriate parameters to fetch any file to which the server has access on its machine. The file is written to standard output, which, of course, can be redirected to a file or pipe. 

Let us look at the server code first. It starts out by including some standard headers, the last three of which contain the main Internet-related definitions and data structures. Next comes a definition of 

SERVER

_

PORT

 as 12345. This number was chosen arbitrarily. Any number between 1024 and 65535 will work just as well as long as it is not in use by some other process. Of course, the client and server have to use the same port. If this server ever becomes a worldwide hit (unlikely, given how primitive it is), it will be assigned a permanent port below 1024 and appear on 

www.iana.org

. 

The next two lines in the server define two constants needed. The first one determines the chunk size used for the file transfer. The second one determines how many pending connections can be held before additional ones are discarded upon arrival. 

After the declarations of local variables, the server code begins. It starts out by initializing a data structure that will hold the server's IP address. This data structure will soon be bound to the server's socket. The call to 

memset

 sets the data structure to all 0s. The three assignments following it fill in three of its fields. The last of these contains the server's port. The functions 

htonl

 and 

htons

 have to do with converting values to a standard format so the code runs correctly on both big-endian machines (e.g., the SPARC) and little-endian machines (e.g., the Pentium). Their exact semantics are not relevant here. 

Next the server creates a socket and checks for errors (indicated by 

s <

 0). In a production version of the code, the error message could be a trifle more explanatory. The call to 

setsockopt

 is needed to allow the port to be reused so the server can run indefinitely, fielding 

372




request after request. Now the IP address is bound to the socket and a check is made to see if the call to 

bind

 succeeded. The final step in the initialization is the call to 

listen

 to announce the server's willingness to accept incoming calls and tell the system to hold up to 

QUEUE

_

SIZE

 of them in case new requests arrive while the server is still processing the current one. If the queue is full and additional requests arrive, they are quietly discarded. 

At this point the server enters its main loop, which it never leaves. The only way to stop it is to kill it from outside. The call to 

accept

 blocks the server until some client tries to establish a connection with it. If the 

accept

 call succeeds, it returns a file descriptor that can be used for reading and writing, analogous to how file descriptors can be used to read and write from pipes. However, unlike pipes, which are unidirectional, sockets are bidirectional, so 

sa

 (socket address) can be used for reading from the connection and also for writing to it. 

After the connection is established, the server reads the file name from it. If the name is not yet available, the server blocks waiting for it. After getting the file name, the server opens the file and then enters a loop that alternately reads blocks from the file and writes them to the socket until the entire file has been copied. Then the server closes the file and the connection and waits for the next connection to show up. It repeats this loop forever. 

Now let us look at the client code. To understand how it works, it is necessary to understand how it is invoked. Assuming it is called 

client

, a typical call is 

client flits.cs.vu.nl /usr/tom/filename >f  

This call only works if the server is already running on 

flits.cs.vu.nl

 and the file 

/usr/tom/filename

 exists and the server has read access to it. If the call is successful, the file is transferred over the Internet and written to 

f

, after which the client program exits. Since the server continues after a transfer, the client can be started again and again to get other files. 

The client code starts with some includes and declarations. Execution begins by checking to see if it has been called with the right number of arguments (

argc

 = 3 means the program name plus two arguments). Note that 

argv

 [1] contains the server's name (e.g., 

flits.cs.vu.nl

) and is converted to an IP address by 

gethostbyname

. This function uses DNS to look up the name. We will study DNS in 

Chap. 7

. 

Next a socket is created and initialized. After that, the client attempts to establish a TCP connection to the server, using 

connect

. If the server is up and running on the named machine and attached to 

SERVER

_

PORT

 and is either idle or has room in its 

listen

 queue, the connection will (eventually) be established. Using the connection, the client sends the name of the file by writing on the socket. The number of bytes sent is one larger than the name proper since the 0-byte terminating the name must also be sent to tell the server where the name ends. 

Figure 6.1 6-6. Client code using sockets. The server code is on the next page. 

[View full width]

 

/* This page contains a client program that can request a file from the server program  

 * on the next page. The server responds by sending the whole file.  

 */  

 

#include <sys/types.h>  

#include <sys/socket.h>  

#include <netinet/in.h>  

#include <netdb.h>  

 

373




#define SERVER_PORT 12345                      /* arbitrary, but client & server must  

agree */  

#define BUF_SIZE 4096                          /* block transfer size */  

 

int main(int argc, char **argv)  

{  

 int c, s, bytes;  

 char buf[BUF_SIZE];                           /* buffer for incoming file */  

 struct hostent *h;                            /* info about server */  

 struct sockaddr_in channel;                   /* holds IP address */  

 

 if (argc != 3) fatal("Usage: client server-name file-name");  

 h = gethostbyname(argv[1]);                   /* look up host's IP address */  

 if (!h) fatal("gethostbyname failed");  

 

 s = socket(PF_INET, SOCK_STREAM, IPPROTO_TCP);  

 i f (s <0) fatal("socket");  

 memset(&channel, 0, sizeof(channel));  

 channel.sin_family= AF_INET;  

 memcpy(&channel.sin_addr.s_addr, h->h_addr, h->h_length);  

 channel.sin_port= htons(SERVER_PORT);  

 

 c = connect(s, (struct sockaddr *) &channel, sizeof(channel));  

 if (c < 0) fatal("connect failed");  

 

 /* Connection is now established. Send file name including 0 byte at end. */  

 write(s, argv[2], strlen(argv[2])+1);  

 

 / * Go get the file and write it to standard output. */  

 while (1) {  

     bytes = read(s, buf, BUF_SIZE);             /* read from socket */  

     if (bytes <= 0) exit(0);                    /* check for end of file */  

     write(1, buf, bytes);                       /* write to standard output */  

 }  

}  

 

fatal(char *string)  

{  

 printf("%s\n", string);  

 exit(1);  

}  

[View full width]

 

#include <sys/types.h>                        /* This is the server code */  

#include <sys/fcntl.h>  

#include <sys/socket.h>  

#include <netinet/in.h>  

#include <netdb.h>  

 

#define SERVER_PORT 12345                     /* arbitrary, but client & server must  

agree */  

#define BUF_SIZE 4096                         /* block transfer size */  

#define QUEUE_SIZE 10  

 

int main(int argc, char *argv[])  

{  

 int s, b, l, fd, sa, bytes, on = 1;  

 char buf[BUF_SIZE];                          /* buffer for outgoing file */  

 struct sockaddr_in channel;                  /* holds IP address */  

 

 /* Build address structure to bind to socket. */  

 memset(&channel, 0, sizeof(channel));        /* zerochannel */  

374




 channel.sin_family = AF_INET;  

 channel.sin_addr.s_addr = htonl(INADDR_ANY);  

 channel.sin_port = htons(SERVER_PORT);  

 

 /* Passive open. Wait for connection. */  

 s = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);   /* createsocket */  

 if (s < 0) fatal("socket failed");  

 setsockopt(s, SOL_SOCKET, SO_REUSEADDR, (char *) &on, sizeof(on));  

 

 b = bind(s, (struct sockaddr *) &channel, sizeof(channel));  

 if (b < 0) fatal("bind failed");  

 

 l = listen(s, QUEUE_SIZE);                  /* specify queue size */  

 if (l < 0) fatal("listen failed");  

 

 /* Socket is now set up and bound. Wait for connection and process it. */  

 while (1) {  

     sa = accept(s, 0, 0);                  /* block for connection request */  

     if (sa < 0) fatal("accept failed");  

 

     read(sa, buf, BUF_SIZE);               /* read file name from socket */  

 

     /* Get and return the file. */  

     fd = open(buf, O_RDONLY);              /* open the file to be sent back */  

     if (fd < 0) fatal("open failed");  

   

     while (1) {  

          bytes = read(fd, buf, BUF_SIZE); /* read from file */  

          if (bytes <= 0) break;            /* check for end of file */  

          write(sa, buf, bytes);            /* write bytes to socket */  

     }  

     close(fd);                             /* closefile */  

     close(sa);                             /* close connection */  

 }  

}  

Now the client enters a loop, reading the file block by block from the socket and copying it to standard output. When it is done, it just exits. 

The procedure 

fatal

 prints an error message and exits. The server needs the same procedure, but it was omitted due to lack of space on the page. Since the client and server are compiled separately and normally run on different computers, they cannot share the code of 

fatal

. 

These two programs (as well as other material related to this book) can be fetched from the book's Web site 

http://www.prenhall.com/tanenbaum

 

by clicking on the Web Site link next to the photo of the cover. They can be downloaded and compiled on any UNIX system (e.g., Solaris, BSD, Linux) by 

cc –o client client.c –lsocket –lnsl  

cc –o server server.c –lsocket –lnsl  

The server is started by just typing 

server  

375




The client needs two arguments, as discussed above. A Windows version is also available on the Web site. 

Just for the record, this server is not the last word in serverdom. Its error checking is meager and its error reporting is mediocre. It has clearly never heard about security, and using bare UNIX system calls is not the last word in platform independence. It also makes some assumptions that are technically illegal, such as assuming the file name fits in the buffer and is transmitted atomically. Since it handles all requests strictly sequentially (because it has only a single thread), its performance is poor. These shortcomings notwithstanding, it is a complete, working Internet file server. In the exercises, the reader is invited to improve it. For more information about programming with sockets, see (Stevens, 1997). 

 

6.2 Elements of Transport Protocols 

The transport service is implemented by a 

transport protocol

 used between the two transport entities. In some ways, transport protocols resemble the data link protocols we studied in detail in 

Chap. 3

. Both have to deal with error control, sequencing, and flow control, among other issues. 

However, significant differences between the two also exist. These differences are due to major dissimilarities between the environments in which the two protocols operate, as shown in 

Fig. 6-7

. At the data link layer, two routers communicate directly via a physical channel, whereas at the transport layer, this physical channel is replaced by the entire subnet. This difference has many important implications for the protocols, as we shall see in this chapter. 

Figure 6-7. (a) Environment of the data link layer. (b) Environment of the transport layer. 

 

For one thing, in the data link layer, it is not necessary for a router to specify which router it wants to talk to—each outgoing line uniquely specifies a particular router. In the transport layer, explicit addressing of destinations is required. 

For another thing, the process of establishing a connection over the wire of 

Fig. 6-7(a)

 is simple: the other end is always there (unless it has crashed, in which case it is not there). Either way, there is not much to do. In the transport layer, initial connection establishment is more complicated, as we will see. 

Another, exceedingly annoying, difference between the data link layer and the transport layer is the potential existence of storage capacity in the subnet. When a router sends a frame, it may arrive or be lost, but it cannot bounce around for a while, go into hiding in a far corner of the world, and then suddenly emerge at an inopportune moment 30 sec later. If the subnet uses datagrams and adaptive routing inside, there is a nonnegligible probability that a packet may be stored for a number of seconds and then delivered later. The consequences of the subnet's ability to store packets can sometimes be disastrous and can require the use of special protocols. 

376




A final difference between the data link and transport layers is one of amount rather than of kind. Buffering and flow control are needed in both layers, but the presence of a large and dynamically varying number of connections in the transport layer may require a different approach than we used in the data link layer. In 

Chap. 3

, some of the protocols allocate a fixed number of buffers to each line, so that when a frame arrives a buffer is always available. In the transport layer, the larger number of connections that must be managed make the idea of dedicating many buffers to each one less attractive. In the following sections, we will examine all of these important issues and others. 

6.2.1 Addressing 

When an application (e.g., a user) process wishes to set up a connection to a remote application process, it must specify which one to connect to. (Connectionless transport has the same problem: To whom should each message be sent?) The method normally used is to define transport addresses to which processes can listen for connection requests. In the Internet, these end points are called 

ports

. In ATM networks, they are called 

AAL-SAPs

. We will use the generic term 

TSAP

, (

Transport Service Access Point

). The analogous end points in the network layer (i.e., network layer addresses) are then called 

NSAPs

. IP addresses are examples of NSAPs. 

Figure 6-8

 illustrates the relationship between the NSAP, TSAP and transport connection. Application processes, both clients and servers, can attach themselves to a TSAP to establish a connection to a remote TSAP. These connections run through NSAPs on each host, as shown. The purpose of having TSAPs is that in some networks, each computer has a single NSAP, so some way is needed to distinguish multiple transport end points that share that NSAP. 

Figure 6-8. TSAPs, NSAPs, and transport connections. 

 

A possible scenario for a transport connection is as follows. 

1. A time of day server process on host 2 attaches itself to TSAP 1522 to wait for an incoming call. How a process attaches itself to a TSAP is outside the networking model and depends entirely on the local operating system. A call such as our LISTEN might be used, for example. 

2. An application process on host 1 wants to find out the time-of-day, so it issues a CONNECT request specifying TSAP 1208 as the source and TSAP 1522 as the destination. This action ultimately results in a transport connection being established between the application process on host 1 and server 1 on host 2. 

377




3. The application process then sends over a request for the time. 

4. The time server process responds with the current time. 

5. The transport connection is then released. 

Note that there may well be other servers on host 2 that are attached to other TSAPs and waiting for incoming connections that arrive over the same NSAP. 

The picture painted above is fine, except we have swept one little problem under the rug: How does the user process on host 1 know that the time-of-day server is attached to TSAP 1522? One possibility is that the time-of-day server has been attaching itself to TSAP 1522 for years and gradually all the network users have learned this. In this model, services have stable TSAP addresses that are listed in files in well-known places, such as the 

/etc/services

 file on UNIX systems, which lists which servers are permanently attached to which ports. 

While stable TSAP addresses work for a small number of key services that never change (e.g. the Web server), user processes, in general, often want to talk to other user processes that only exist for a short time and do not have a TSAP address that is known in advance. Furthermore, if there are potentially many server processes, most of which are rarely used, it is wasteful to have each of them active and listening to a stable TSAP address all day long. In short, a better scheme is needed. 

One such scheme is shown in 

Fig. 6-9

 in a simplified form. It is known as the 

initial connection protocol

. Instead of every conceivable server listening at a well-known TSAP, each machine that wishes to offer services to remote users has a special 

process server

 that acts as a proxy for less heavily used servers. It listens to a set of ports at the same time, waiting for a connection request. Potential users of a service begin by doing a CONNECT request, specifying the TSAP address of the service they want. If no server is waiting for them, they get a connection to the process server, as shown in 

Fig. 6-9(a)

. 

Figure 6-9. How a user process in host 1 establishes a connection with a time-of-day server in host 2. 

 

After it gets the incoming request, the process server spawns the requested server, allowing it to inherit the existing connection with the user. The new server then does the requested work, while the process server goes back to listening for new requests, as shown in 

Fig. 6-9(b)

. 

378




While the initial connection protocol works fine for those servers that can be created as they are needed, there are many situations in which services do exist independently of the process server. A file server, for example, needs to run on special hardware (a machine with a disk) and cannot just be created on-the-fly when someone wants to talk to it. 

To handle this situation, an alternative scheme is often used. In this model, there exists a special process called a 

name server

 or sometimes a 

directory server

. To find the TSAP address corresponding to a given service name, such as ''time of day,'' a user sets up a connection to the name server (which listens to a well-known TSAP). The user then sends a message specifying the service name, and the name server sends back the TSAP address. Then the user releases the connection with the name server and establishes a new one with the desired service. 

In this model, when a new service is created, it must register itself with the name server, giving both its service name (typically, an ASCII string) and its TSAP. The name server records this information in its internal database so that when queries come in later, it will know the answers. 

The function of the name server is analogous to the directory assistance operator in the telephone system—it provides a mapping of names onto numbers. Just as in the telephone system, it is essential that the address of the well-known TSAP used by the name server (or the process server in the initial connection protocol) is indeed well known. If you do not know the number of the information operator, you cannot call the information operator to find it out. If you think the number you dial for information is obvious, try it in a foreign country sometime. 

6.2.2 Connection Establishment 

Establishing a connection sounds easy, but it is actually surprisingly tricky. At first glance, it would seem sufficient for one transport entity to just send a CONNECTION REQUEST TPDU to the destination and wait for a CONNECTION ACCEPTED reply. The problem occurs when the network can lose, store, and duplicate packets. This behavior causes serious complications. 

Imagine a subnet that is so congested that acknowledgements hardly ever get back in time and each packet times out and is retransmitted two or three times. Suppose that the subnet uses datagrams inside and that every packet follows a different route. Some of the packets might get stuck in a traffic jam inside the subnet and take a long time to arrive, that is, they are stored in the subnet and pop out much later. 

The worst possible nightmare is as follows. A user establishes a connection with a bank, sends messages telling the bank to transfer a large amount of money to the account of a not-entirely-trustworthy person, and then releases the connection. Unfortunately, each packet in the scenario is duplicated and stored in the subnet. After the connection has been released, all the packets pop out of the subnet and arrive at the destination in order, asking the bank to establish a new connection, transfer money (again), and release the connection. The bank has no way of telling that these are duplicates. It must assume that this is a second, independent transaction, and transfers the money again. For the remainder of this section we will study the problem of delayed duplicates, with special emphasis on algorithms for establishing connections in a reliable way, so that nightmares like the one above cannot happen. 

The crux of the problem is the existence of delayed duplicates. It can be attacked in various ways, none of them very satisfactory. One way is to use throw-away transport addresses. In this approach, each time a transport address is needed, a new one is generated. When a connection is released, the address is discarded and never used again. This strategy makes the process server model of 

Fig. 6-9

 impossible. 

379




Another possibility is to give each connection a connection identifier (i.e., a sequence number incremented for each connection established) chosen by the initiating party and put in each TPDU, including the one requesting the connection. After each connection is released, each transport entity could update a table listing obsolete connections as (peer transport entity, connection identifier) pairs. Whenever a connection request comes in, it could be checked against the table, to see if it belonged to a previously-released connection. 

Unfortunately, this scheme has a basic flaw: it requires each transport entity to maintain a certain amount of history information indefinitely. If a machine crashes and loses its memory, it will no longer know which connection identifiers have already been used. 

Instead, we need to take a different tack. Rather than allowing packets to live forever within the subnet, we must devise a mechanism to kill off aged packets that are still hobbling about. If we can ensure that no packet lives longer than some known time, the problem becomes somewhat more manageable. 

Packet lifetime can be restricted to a known maximum using one (or more) of the following techniques: 

1. Restricted subnet design. 

2. Putting a hop counter in each packet. 

3. Timestamping each packet. 

The first method includes any method that prevents packets from looping, combined with some way of bounding congestion delay over the (now known) longest possible path. The second method consists of having the hop count initialized to some appropriate value and decremented each time the packet is forwarded. The network protocol simply discards any packet whose hop counter becomes zero. The third method requires each packet to bear the time it was created, with the routers agreeing to discard any packet older than some agreed-upon time. This latter method requires the router clocks to be synchronized, which itself is a nontrivial task unless synchronization is achieved external to the network, for example by using GPS or some radio station that broadcasts the precise time periodically. 

In practice, we will need to guarantee not only that a packet is dead, but also that all acknowledgements to it are also dead, so we will now introduce 

T

, which is some small multiple of the true maximum packet lifetime. The multiple is protocol dependent and simply has the effect of making 

T

 longer. If we wait a time 

T

 after a packet has been sent, we can be sure that all traces of it are now gone and that neither it nor its acknowledgements will suddenly appear out of the blue to complicate matters. 

With packet lifetimes bounded, it is possible to devise a foolproof way to establish connections safely. The method described below is due to Tomlinson (1975). It solves the problem but introduces some peculiarities of its own. The method was further refined by Sunshine and Dalal (1978). Variants of it are widely used in practice, including in TCP. 

To get around the problem of a machine losing all memory of where it was after a crash, Tomlinson proposed equipping each host with a time-of-day clock. The clocks at different hosts need not be synchronized. Each clock is assumed to take the form of a binary counter that increments itself at uniform intervals. Furthermore, the number of bits in the counter must equal or exceed the number of bits in the sequence numbers. Last, and most important, the clock is assumed to continue running even if the host goes down. 

The basic idea is to ensure that two identically numbered TPDUs are never outstanding at the same time. When a connection is set up, the low-order 

k

 bits of the clock are used as the initial sequence number (also 

k

 bits). Thus, unlike our protocols of 

Chap. 3

, each connection starts numbering its TPDUs with a different initial sequence number. The sequence space should be so large that by the time sequence numbers wrap around, old TPDUs with the same sequence 

380




number are long gone. This linear relation between time and initial sequence numbers is shown in 

Fig. 6-10

. 

Figure 6-10. (a) TPDUs may not enter the forbidden region. (b) The resynchronization problem. 

 

Once both transport entities have agreed on the initial sequence number, any sliding window protocol can be used for data flow control. In reality, the initial sequence number curve (shown by the heavy line) is not linear, but a staircase, since the clock advances in discrete steps. For simplicity we will ignore this detail. 

A problem occurs when a host crashes. When it comes up again, its transport entity does not know where it was in the sequence space. One solution is to require transport entities to be idle for 

T

 sec after a recovery to let all old TPDUs die off. However, in a complex internetwork, 

T

 may be large, so this strategy is unattractive. 

To avoid requiring 

T

 sec of dead time after a crash, it is necessary to introduce a new restriction on the use of sequence numbers. We can best see the need for this restriction by means of an example. Let 

T

, the maximum packet lifetime, be 60 sec and let the clock tick once per second. As shown by the heavy line in 

Fig. 6-10(a)

, the initial sequence number for a connection opened at time 

x

 will be 

x

. Imagine that at 

t

 = 30 sec, an ordinary data TPDU being sent on (a previously opened) connection 5 is given sequence number 80. Call this TPDU 

X

. Immediately after sending TPDU 

X

, the host crashes and then quickly restarts. At 

t

 = 60, it begins reopening connections 0 through 4. At 

t

 = 70, it reopens connection 5, using initial sequence number 70 as required. Within the next 15 sec it sends data TPDUs 70 through 80. Thus, at 

t

 = 85 a new TPDU with sequence number 80 and connection 5 has been injected into the subnet. Unfortunately, TPDU 

X

 still exists. If it should arrive at the receiver before the new TPDU 80, TPDU 

X

 will be accepted and the correct TPDU 80 will be rejected as a duplicate. 

To prevent such problems, we must prevent sequence numbers from being used (i.e., assigned to new TPDUs) for a time 

T

 before their potential use as initial sequence numbers. The illegal combinations of time and sequence number are shown as the 

forbidden region

 in 

Fig. 6-

10(a)

. Before sending any TPDU on any connection, the transport entity must read the clock and check to see that it is not in the forbidden region. 

The protocol can get itself into trouble in two distinct ways. If a host sends too much data too fast on a newly-opened connection, the actual sequence number versus time curve may rise more steeply than the initial sequence number versus time curve. This means that the maximum data rate on any connection is one TPDU per clock tick. It also means that the transport entity must wait until the clock ticks before opening a new connection after a crash restart, lest the same number be used twice. Both of these points argue in favor of a short clock tick (a few µsec or less). 

381




Unfortunately, entering the forbidden region from underneath by sending too fast is not the only way to get into trouble. From 

Fig. 6-10(b)

, we see that at any data rate less than the clock rate, the curve of actual sequence numbers used versus time will eventually run into the forbidden region from the left. The greater the slope of the actual sequence number curve, the longer this event will be delayed. As we stated above, just before sending every TPDU, the transport entity must check to see if it is about to enter the forbidden region, and if so, either delay the TPDU for 

T

 sec or resynchronize the sequence numbers. 

The clock-based method solves the delayed duplicate problem for data TPDUs, but for this method to be useful, a connection must first be established. Since control TPDUs may also be delayed, there is a potential problem in getting both sides to agree on the initial sequence number. Suppose, for example, that connections are established by having host 1 send a CONNECTION REQUEST TPDU containing the proposed initial sequence number and destination port number to a remote peer, host 2. The receiver, host 2, then acknowledges this request by sending a CONNECTION ACCEPTED TPDU back. If the CONNECTION REQUEST TPDU is lost but a delayed duplicate CONNECTION REQUEST suddenly shows up at host 2, the connection will be established incorrectly. 

To solve this problem, Tomlinson (1975) introduced the 

three-way handshake

. This establishment protocol does not require both sides to begin sending with the same sequence number, so it can be used with synchronization methods other than the global clock method. The normal setup procedure when host 1 initiates is shown in 

Fig. 6-11(a)

. Host 1 chooses a sequence number, 

x

, and sends a CONNECTION REQUEST TPDU containing it to host 2. Host 2 replies with an ACK TPDU acknowledging 

x

 and announcing its own initial sequence number, 

y

. Finally, host 1 acknowledges host 2's choice of an initial sequence number in the first data TPDU that it sends. 

Figure 6-11. Three protocol scenarios for establishing a connection using a three-way handshake. CR denotes CONNECTION REQUEST. (a) Normal operation. (b) Old duplicate CONNECTION REQUEST appearing out of nowhere. (c) Duplicate CONNECTION REQUEST and duplicate ACK. 

382




 

Now let us see how the three-way handshake works in the presence of delayed duplicate control TPDUs. In 

Fig. 6-11(b)

, the first TPDU is a delayed duplicate CONNECTION REQUEST from an old connection. This TPDU arrives at host 2 without host 1's knowledge. Host 2 reacts to this TPDU by sending host 1 an 

ACK TPDU, in effect asking for verification that host 1 was indeed trying to set up a new connection. When host 1 rejects host 2's attempt to establish a connection, host 2 realizes that it was tricked by a delayed duplicate and abandons the connection. In this way, a delayed duplicate does no damage. 

The worst case is when both a delayed CONNECTION REQUEST and an ACK are floating around in the subnet. This case is shown in 

Fig. 6-11(c)

. As in the previous example, host 2 gets a delayed CONNECTION REQUEST and replies to it. At this point it is crucial to realize that host 2 has proposed using 

y

 as the initial sequence number for host 2 to host 1 traffic, knowing full well that no TPDUs containing sequence number 

y

 or acknowledgements to 

y

 are still in existence. When the second delayed TPDU arrives at host 2, the fact that 

z

 has been acknowledged rather than 

y

 tells host 2 that this, too, is an old duplicate. The important thing to realize here is that there is no combination of old TPDUs that can cause the protocol to fail and have a connection set up by accident when no one wants it. 

6.2.3 Connection Release 

Releasing a connection is easier than establishing one. Nevertheless, there are more pitfalls than one might expect. As we mentioned earlier, there are two styles of terminating a connection: asymmetric release and symmetric release. Asymmetric release is the way the telephone system works: when one party hangs up, the connection is broken. Symmetric 

383




release treats the connection as two separate unidirectional connections and requires each one to be released separately. 

Asymmetric release is abrupt and may result in data loss. Consider the scenario of 

Fig. 6-12

. After the connection is established, host 1 sends a TPDU that arrives properly at host 2. Then host 1 sends another TPDU. Unfortunately, host 2 issues a DISCONNECT before the second TPDU arrives. The result is that the connection is released and data are lost. 

Figure 6-12. Abrupt disconnection with loss of data. 

 

Clearly, a more sophisticated release protocol is needed to avoid data loss. One way is to use symmetric release, in which each direction is released independently of the other one. Here, a host can continue to receive data even after it has sent a DISCONNECT TPDU. 

Symmetric release does the job when each process has a fixed amount of data to send and clearly knows when it has sent it. In other situations, determining that all the work has been done and the connection should be terminated is not so obvious. One can envision a protocol in which host 1 says: I am done. Are you done too? If host 2 responds: I am done too. Goodbye, the connection can be safely released. 

Unfortunately, this protocol does not always work. There is a famous problem that illustrates this issue. It is called the 

two-army problem

. Imagine that a white army is encamped in a valley, as shown in 

Fig. 6-13

. On both of the surrounding hillsides are blue armies. The white army is larger than either of the blue armies alone, but together the blue armies are larger than the white army. If either blue army attacks by itself, it will be defeated, but if the two blue armies attack simultaneously, they will be victorious. 

Figure 6-13. The two-army problem. 

 

384




The blue armies want to synchronize their attacks. However, their only communication medium is to send messengers on foot down into the valley, where they might be captured and the message lost (i.e., they have to use an unreliable communication channel). The question is: Does a protocol exist that allows the blue armies to win? 

Suppose that the commander of blue army #1 sends a message reading: ''I propose we attack at dawn on March 29. How about it?'' Now suppose that the message arrives, the commander of blue army #2 agrees, and his reply gets safely back to blue army #1. Will the attack happen? Probably not, because commander #2 does not know if his reply got through. If it did not, blue army #1 will not attack, so it would be foolish for him to charge into battle. 

Now let us improve the protocol by making it a three-way handshake. The initiator of the original proposal must acknowledge the response. Assuming no messages are lost, blue army #2 will get the acknowledgement, but the commander of blue army #1 will now hesitate. After all, he does not know if his acknowledgement got through, and if it did not, he knows that blue army #2 will not attack. We could now make a four-way handshake protocol, but that does not help either. 

In fact, it can be proven that no protocol exists that works. Suppose that some protocol did exist. Either the last message of the protocol is essential or it is not. If it is not, remove it (and any other unessential messages) until we are left with a protocol in which every message is essential. What happens if the final message does not get through? We just said that it was essential, so if it is lost, the attack does not take place. Since the sender of the final message can never be sure of its arrival, he will not risk attacking. Worse yet, the other blue army knows this, so it will not attack either. 

To see the relevance of the two-army problem to releasing connections, just substitute ''disconnect'' for ''attack.'' If neither side is prepared to disconnect until it is convinced that the other side is prepared to disconnect too, the disconnection will never happen. 

In practice, one is usually prepared to take more risks when releasing connections than when attacking white armies, so the situation is not entirely hopeless. 

Figure 6-14

 illustrates four scenarios of releasing using a three-way handshake. While this protocol is not infallible, it is usually adequate. 

Figure 6-14. Four protocol scenarios for releasing a connection. (a) Normal case of three-way handshake. (b) Final ACK lost. (c) Response lost. (d) Response lost and subsequent DRs lost. 

385




 

In 

Fig. 6-14(a)

, we see the normal case in which one of the users sends a DR (DISCONNECTION REQUEST) TPDU to initiate the connection release. When it arrives, the recipient sends back a DR TPDU, too, and starts a timer, just in case its DR is lost. When this DR arrives, the original sender sends back an ACK TPDU and releases the connection. Finally, when the ACK TPDU arrives, the receiver also releases the connection. Releasing a connection means that the transport entity removes the information about the connection from its table of currently open connections and signals the connection's owner (the transport user) somehow. This action is different from a transport user issuing a DISCONNECT primitive. 

If the final ACK TPDU is lost, as shown in 

Fig. 6-14(b)

, the situation is saved by the timer. When the timer expires, the connection is released anyway. 

Now consider the case of the second DR being lost. The user initiating the disconnection will not receive the expected response, will time out, and will start all over again. In 

Fig. 6-14(c)

 we see how this works, assuming that the second time no TPDUs are lost and all TPDUs are delivered correctly and on time. 

Our last scenario, 

Fig. 6-14(d)

, is the same as 

Fig. 6-14(c)

 except that now we assume all the repeated attempts to retransmit the DR also fail due to lost TPDUs. After 

N

 retries, the sender just gives up and releases the connection. Meanwhile, the receiver times out and also exits. 

While this protocol usually suffices, in theory it can fail if the initial DR and 

N

 retransmissions are all lost. The sender will give up and release the connection, while the other side knows 

386




nothing at all about the attempts to disconnect and is still fully active. This situation results in a half-open connection. 

We could have avoided this problem by not allowing the sender to give up after 

N

 retries but forcing it to go on forever until it gets a response. However, if the other side is allowed to time out, then the sender will indeed go on forever, because no response will ever be forthcoming. If we do not allow the receiving side to time out, then the protocol hangs in 

Fig. 6-14(d)

. 

One way to kill off half-open connections is to have a rule saying that if no TPDUs have arrived for a certain number of seconds, the connection is then automatically disconnected. That way, if one side ever disconnects, the other side will detect the lack of activity and also disconnect. Of course, if this rule is introduced, it is necessary for each transport entity to have a timer that is stopped and then restarted whenever a TPDU is sent. If this timer expires, a dummy TPDU is transmitted, just to keep the other side from disconnecting. On the other hand, if the automatic disconnect rule is used and too many dummy TPDUs in a row are lost on an otherwise idle connection, first one side, then the other side will automatically disconnect. 

We will not belabor this point any more, but by now it should be clear that releasing a connection without data loss is not nearly as simple as it at first appears. 

6.2.4 Flow Control and Buffering 

Having examined connection establishment and release in some detail, let us now look at how connections are managed while they are in use. One of the key issues has come up before: flow control. In some ways the flow control problem in the transport layer is the same as in the data link layer, but in other ways it is different. The basic similarity is that in both layers a sliding window or other scheme is needed on each connection to keep a fast transmitter from overrunning a slow receiver. The main difference is that a router usually has relatively few lines, whereas a host may have numerous connections. This difference makes it impractical to implement the data link buffering strategy in the transport layer. 

In the data link protocols of 

Chap. 3

, frames were buffered at both the sending router and at the receiving router. In protocol 6, for example, both sender and receiver are required to dedicate 

MAX

_

SEQ

 + 1 buffers to each line, half for input and half for output. For a host with a maximum of, say, 64 connections, and a 4-bit sequence number, this protocol would require 1024 buffers. 

In the data link layer, the sending side must buffer outgoing frames because they might have to be retransmitted. If the subnet provides datagram service, the sending transport entity must also buffer, and for the same reason. If the receiver knows that the sender buffers all TPDUs until they are acknowledged, the receiver may or may not dedicate specific buffers to specific connections, as it sees fit. The receiver may, for example, maintain a single buffer pool shared by all connections. When a TPDU comes in, an attempt is made to dynamically acquire a new buffer. If one is available, the TPDU is accepted; otherwise, it is discarded. Since the sender is prepared to retransmit TPDUs lost by the subnet, no harm is done by having the receiver drop TPDUs, although some resources are wasted. The sender just keeps trying until it gets an acknowledgement. 

In summary, if the network service is unreliable, the sender must buffer all TPDUs sent, just as in the data link layer. However, with reliable network service, other trade-offs become possible. In particular, if the sender knows that the receiver always has buffer space, it need not retain copies of the TPDUs it sends. However, if the receiver cannot guarantee that every incoming TPDU will be accepted, the sender will have to buffer anyway. In the latter case, the sender cannot trust the network layer's acknowledgement, because the acknowledgement means only that the TPDU arrived, not that it was accepted. We will come back to this important point later. 

387




Even if the receiver has agreed to do the buffering, there still remains the question of the buffer size. If most TPDUs are nearly the same size, it is natural to organize the buffers as a pool of identically-sized buffers, with one TPDU per buffer, as in 

Fig. 6-15(a)

. However, if there is wide variation in TPDU size, from a few characters typed at a terminal to thousands of characters from file transfers, a pool of fixed-sized buffers presents problems. If the buffer size is chosen equal to the largest possible TPDU, space will be wasted whenever a short TPDU arrives. If the buffer size is chosen less than the maximum TPDU size, multiple buffers will be needed for long TPDUs, with the attendant complexity. 

Figure 6-15. (a) Chained fixed-size buffers. (b) Chained variable-sized buffers. (c) One large circular buffer per connection. 

 

Another approach to the buffer size problem is to use variable-sized buffers, as in 

Fig. 6-15(b)

. The advantage here is better memory utilization, at the price of more complicated buffer management. A third possibility is to dedicate a single large circular buffer per connection, as in 

Fig. 6-15(c)

. This system also makes good use of memory, provided that all connections are heavily loaded, but is poor if some connections are lightly loaded. 

The optimum trade-off between source buffering and destination buffering depends on the type of traffic carried by the connection. For low-bandwidth bursty traffic, such as that produced by an interactive terminal, it is better not to dedicate any buffers, but rather to acquire them dynamically at both ends. Since the sender cannot be sure the receiver will be able to acquire a buffer, the sender must retain a copy of the TPDU until it is acknowledged. On the other hand, for file transfer and other high-bandwidth traffic, it is better if the receiver does dedicate a full window of buffers, to allow the data to flow at maximum speed. Thus, for low-bandwidth bursty traffic, it is better to buffer at the sender, and for highbandwidth smooth traffic, it is better to buffer at the receiver. 

As connections are opened and closed and as the traffic pattern changes, the sender and receiver need to dynamically adjust their buffer allocations. Consequently, the transport protocol should allow a sending host to request buffer space at the other end. Buffers could be allocated per connection, or collectively, for all the connections running between the two hosts. Alternatively, the receiver, knowing its buffer situation (but not knowing the offered traffic) could tell the sender ''I have reserved 

X

 buffers for you.'' If the number of open connections should increase, it may be necessary for an allocation to be reduced, so the protocol should provide for this possibility. 

388




A reasonably general way to manage dynamic buffer allocation is to decouple the buffering from the acknowledgements, in contrast to the sliding window protocols of 

Chap. 3

. Dynamic buffer management means, in effect, a variable-sized window. Initially, the sender requests a certain number of buffers, based on its perceived needs. The receiver then grants as many of these as it can afford. Every time the sender transmits a TPDU, it must decrement its allocation, stopping altogether when the allocation reaches zero. The receiver then separately piggybacks both acknowledgements and buffer allocations onto the reverse traffic. 

Figure 6-16

 shows an example of how dynamic window management might work in a datagram subnet with 4-bit sequence numbers. Assume that buffer allocation information travels in separate TPDUs, as shown, and is not piggybacked onto reverse traffic. Initially, 

A

 wants eight buffers, but is granted only four of these. It then sends three TPDUs, of which the third is lost. TPDU 6 acknowledges receipt of all TPDUs up to and including sequence number 1, thus allowing 

A

 to release those buffers, and furthermore informs 

A

 that it has permission to send three more TPDUs starting beyond 1 (i.e., TPDUs 2, 3, and 4). 

A

 knows that it has already sent number 2, so it thinks that it may send TPDUs 3 and 4, which it proceeds to do. At this point it is blocked and must wait for more buffer allocation. Timeout-induced retransmissions (line 9), however, may occur while blocked, since they use buffers that have already been allocated. In line 10, 

B

 acknowledges receipt of all TPDUs up to and including 4 but refuses to let 

A

 continue. Such a situation is impossible with the fixed window protocols of 

Chap. 3

. The next TPDU from 

B

 to 

A

 allocates another buffer and allows 

A

 to continue. 

Figure 6-16. Dynamic buffer allocation. The arrows show the direction of transmission. An ellipsis (...) indicates a lost TPDU. 

 

Potential problems with buffer allocation schemes of this kind can arise in datagram networks if control TPDUs can get lost. Look at line 16. 

B

 has now allocated more buffers to 

A

, but the allocation TPDU was lost. Since control TPDUs are not sequenced or timed out, 

A

 is now deadlocked. To prevent this situation, each host should periodically send control TPDUs giving the acknowledgement and buffer status on each connection. That way, the deadlock will be broken, sooner or later. 

Until now we have tacitly assumed that the only limit imposed on the sender's data rate is the amount of buffer space available in the receiver. As memory prices continue to fall dramatically, it may become feasible to equip hosts with so much memory that lack of buffers is rarely, if ever, a problem. 

389




When buffer space no longer limits the maximum flow, another bottleneck will appear: the carrying capacity of the subnet. If adjacent routers can exchange at most 

x

 packets/sec and there are 

k

 disjoint paths between a pair of hosts, there is no way that those hosts can exchange more than 

kx

 TPDUs/sec, no matter how much buffer space is available at each end. If the sender pushes too hard (i.e., sends more than 

kx

 TPDUs/sec), the subnet will become congested because it will be unable to deliver TPDUs as fast as they are coming in. 

What is needed is a mechanism based on the subnet's carrying capacity rather than on the receiver's buffering capacity. Clearly, the flow control mechanism must be applied at the sender to prevent it from having too many unacknowledged TPDUs outstanding at once. Belsnes (1975) proposed using a sliding window flow control scheme in which the sender dynamically adjusts the window size to match the network's carrying capacity. If the network can handle 

c

 TPDUs/sec and the cycle time (including transmission, propagation, queueing, processing at the receiver, and return of the acknowledgement) is 

r

, then the sender's window should be 

cr

. With a window of this size the sender normally operates with the pipeline full. Any small decrease in network performance will cause it to block. 

In order to adjust the window size periodically, the sender could monitor both parameters and then compute the desired window size. The carrying capacity can be determined by simply counting the number of TPDUs acknowledged during some time period and then dividing by the time period. During the measurement, the sender should send as fast as it can, to make sure that the network's carrying capacity, and not the low input rate, is the factor limiting the acknowledgement rate. The time required for a transmitted TPDU to be acknowledged can be measured exactly and a running mean maintained. Since the network capacity available to any given flow varies in time, the window size should be adjusted frequently, to track changes in the carrying capacity. As we will see later, the Internet uses a similar scheme. 

6.2.5 Multiplexing 

Multiplexing several conversations onto connections, virtual circuits, and physical links plays a role in several layers of the network architecture. In the transport layer the need for multiplexing can arise in a number of ways. For example, if only one network address is available on a host, all transport connections on that machine have to use it. When a TPDU comes in, some way is needed to tell which process to give it to. This situation, called 

upward multiplexing

, is shown in 

Fig. 6-17(a)

. In this figure, four distinct transport connections all use the same network connection (e.g., IP address) to the remote host. 

Figure 6-17. (a) Upward multiplexing. (b) Downward multiplexing. 

 

Multiplexing can also be useful in the transport layer for another reason. Suppose, for example, that a subnet uses virtual circuits internally and imposes a maximum data rate on 

390




each one. If a user needs more bandwidth than one virtual circuit can provide, a way out is to open multiple network connections and distribute the traffic among them on a round-robin basis, as indicated in 

Fig. 6-17(b)

. This modus operandi is called 

downward multiplexing

. With 

k

 network connections open, the effective bandwidth is increased by a factor of 

k

. A common example of downward multiplexing occurs with home users who have an ISDN line. This line provides for two separate connections of 64 kbps each. Using both of them to call an Internet provider and dividing the traffic over both lines makes it possible to achieve an effective bandwidth of 128 kbps. 

6.2.6 Crash Recovery 

If hosts and routers are subject to crashes, recovery from these crashes becomes an issue. If the transport entity is entirely within the hosts, recovery from network and router crashes is straightforward. If the network layer provides datagram service, the transport entities expect lost TPDUs all the time and know how to cope with them. If the network layer provides connection-oriented service, then loss of a virtual circuit is handled by establishing a new one and then probing the remote transport entity to ask it which TPDUs it has received and which ones it has not received. The latter ones can be retransmitted. 

A more troublesome problem is how to recover from host crashes. In particular, it may be desirable for clients to be able to continue working when servers crash and then quickly reboot. To illustrate the difficulty, let us assume that one host, the client, is sending a long file to another host, the file server, using a simple stop-and-wait protocol. The transport layer on the server simply passes the incoming TPDUs to the transport user, one by one. Partway through the transmission, the server crashes. When it comes back up, its tables are reinitialized, so it no longer knows precisely where it was. 

In an attempt to recover its previous status, the server might send a broadcast TPDU to all other hosts, announcing that it had just crashed and requesting that its clients inform it of the status of all open connections. Each client can be in one of two states: one TPDU outstanding, 

S1

, or no TPDUs outstanding, 

S0

. Based on only this state information, the client must decide whether to retransmit the most recent TPDU. 

At first glance it would seem obvious: the client should retransmit only if and only if it has an unacknowledged TPDU outstanding (i.e., is in state 

S1

) when it learns of the crash. However, a closer inspection reveals difficulties with this naive approach. Consider, for example, the situation in which the server's transport entity first sends an acknowledgement, and then, when the acknowledgement has been sent, writes to the application process. Writing a TPDU onto the output stream and sending an acknowledgement are two distinct events that cannot be done simultaneously. If a crash occurs after the acknowledgement has been sent but before the write has been done, the client will receive the acknowledgement and thus be in state 

S0

 when the crash recovery announcement arrives. The client will therefore not retransmit, (incorrectly) thinking that the TPDU has arrived. This decision by the client leads to a missing TPDU. 

At this point you may be thinking: ''That problem can be solved easily. All you have to do is reprogram the transport entity to first do the write and then send the acknowledgement.'' Try again. Imagine that the write has been done but the crash occurs before the acknowledgement can be sent. The client will be in state 

S1

 and thus retransmit, leading to an undetected duplicate TPDU in the output stream to the server application process. 

No matter how the client and server are programmed, there are always situations where the protocol fails to recover properly. The server can be programmed in one of two ways: acknowledge first or write first. The client can be programmed in one of four ways: always retransmit the last TPDU, never retransmit the last TPDU, retransmit only in state 

S0

, or retransmit only in state 

S1

. This gives eight combinations, but as we shall see, for each combination there is some set of events that makes the protocol fail. 

391




Three events are possible at the server: sending an acknowledgement (

A

), writing to the output process (

W

), and crashing (

C

)

.

 The three events can occur in six different orderings: 

AC

(

W

), 

AWC

, 

C

(

AW

), 

C

(

WA

), 

WAC

, and 

WC

(

A

), where the parentheses are used to indicate that neither 

A

 nor 

W

 can follow 

C

 (i.e., once it has crashed, it has crashed). 

Figure 6-18

 shows all eight combinations of client and server strategy and the valid event sequences for each one. Notice that for each strategy there is some sequence of events that causes the protocol to fail. For example, if the client always retransmits, the 

AWC

 event will generate an undetected duplicate, even though the other two events work properly. 

Figure 6-18. Different combinations of client and server strategy. 

 

Making the protocol more elaborate does not help. Even if the client and server exchange several TPDUs before the server attempts to write, so that the client knows exactly what is about to happen, the client has no way of knowing whether a crash occurred just before or just after the write. The conclusion is inescapable: under our ground rules of no simultaneous events, host crash and recovery cannot be made transparent to higher layers. 

Put in more general terms, this result can be restated as recovery from a layer 

N

 crash can only be done by layer 

N

 + 1, and then only if the higher layer retains enough status information. As mentioned above, the transport layer can recover from failures in the network layer, provided that each end of a connection keeps track of where it is. 

This problem gets us into the issue of what a so-called end-to-end acknowledgement really means. In principle, the transport protocol is end-to-end and not chained like the lower layers. Now consider the case of a user entering requests for transactions against a remote database. Suppose that the remote transport entity is programmed to first pass TPDUs to the next layer up and then acknowledge. Even in this case, the receipt of an acknowledgement back at the user's machine does not necessarily mean that the remote host stayed up long enough to actually update the database. A truly end-to-end acknowledgement, whose receipt means that the work has actually been done and lack thereof means that it has not, is probably impossible to achieve. This point is discussed in more detail by Saltzer et al. (1984). 

 

6.3 A Simple Transport Protocol 

To make the ideas discussed so far more concrete, in this section we will study an example transport layer in detail. The abstract service primitives we will use are the connection-oriented primitives of 

Fig. 6-2

. The choice of these connection-oriented primitives makes the example similar to (but simpler than) the popular TCP protocol. 

392




6.3.1 The Example Service Primitives 

Our first problem is how to express these transport primitives concretely. CONNECT is easy: we will just have a library procedure 

connect

 that can be called with the appropriate parameters necessary to establish a connection. The parameters are the local and remote TSAPs. During the call, the caller is blocked (i.e., suspended) while the transport entity tries to set up the connection. If the connection succeeds, the caller is unblocked and can start transmitting data. 

When a process wants to be able to accept incoming calls, it calls 

listen

, specifying a particular TSAP to listen to. The process then blocks until some remote process attempts to establish a connection to its TSAP. 

Note that this model is highly asymmetric. One side is passive, executing a 

listen

 and waiting until something happens. The other side is active and initiates the connection. An interesting question arises of what to do if the active side begins first. One strategy is to have the connection attempt fail if there is no listener at the remote TSAP. Another strategy is to have the initiator block (possibly forever) until a listener appears. 

A compromise, used in our example, is to hold the connection request at the receiving end for a certain time interval. If a process on that host calls 

listen

 before the timer goes off, the connection is established; otherwise, it is rejected and the caller is unblocked and given an error return. 

To release a connection, we will use a procedure 

disconnect

. When both sides have disconnected, the connection is released. In other words, we are using a symmetric disconnection model. 

Data transmission has precisely the same problem as connection establishment: sending is active but receiving is passive. We will use the same solution for data transmission as for connection establishment: an active call 

send

 that transmits data and a passive call 

receive

 that blocks until a TPDU arrives. Our concrete service definition therefore consists of five primitives: CONNECT, LISTEN, DISCONNECT, SEND, and RECEIVE. Each primitive corresponds exactly to a library procedure that executes the primitive. The parameters for the service primitives and library procedures are as follows: 

connum = LISTEN(local)  

connum = CONNECT(local, remote)  

status = SEND(connum, buffer, bytes)  

status = RECEIVE(connum, buffer, bytes)  

status = DISCONNECT(connum)  

The LISTEN primitive announces the caller's willingness to accept connection requests directed at the indicated TSAP. The user of the primitive is blocked until an attempt is made to connect to it. There is no timeout. 

The CONNECT primitive takes two parameters, a local TSAP (i.e., transport address), 

local

, and a remote TSAP, 

remote

, and tries to establish a transport connection between the two. If it succeeds, it returns in 

connum

 a nonnegative number used to identify the connection on subsequent calls. If it fails, the reason for failure is put in 

connum

 as a negative number. In our simple model, each TSAP may participate in only one transport connection, so a possible reason for failure is that one of the transport addresses is currently in use. Some other reasons are remote host down, illegal local address, and illegal remote address. 

The SEND primitive transmits the contents of the buffer as a message on the indicated transport connection, in several units if need be. Possible errors, returned in 

status

, are no connection, illegal buffer address, or negative count. 

393




The RECEIVE primitive indicates the caller's desire to accept data. The size of the incoming message is placed in 

bytes

. If the remote process has released the connection or the buffer address is illegal (e.g., outside the user's program), 

status

 is set to an error code indicating the nature of the problem. 

The DISCONNECT primitive terminates a transport connection. The parameter 

connum

 tells which one. Possible errors are 

connum

 belongs to another process or 

connum

 is not a valid connection identifier. The error code, or 0 for success, is returned in 

status

. 

6.3.2 The Example Transport Entity 

Before looking at the code of the example transport entity, please be sure you realize that this example is analogous to the early examples presented in 

Chap. 3

: it is more for pedagogical purposes than a serious proposal. Many of the technical details (such as extensive error checking) that would be needed in a production system have been omitted here for the sake of simplicity. 

The transport layer makes use of the network service primitives to send and receive TPDUs. For this example, we need to choose network service primitives to use. One choice would have been unreliable datagram service. To keep the example simple, we have not made that choice. With unreliable datagram service, the transport code would have been large and complex, mostly dealing with lost and delayed packets. Furthermore, most of these ideas have already been discussed at length in 

Chap. 3

. 

Instead, we have chosen to use a connection-oriented, reliable network service. This way we can focus on transport issues that do not occur in the lower layers. These include connection establishment, connection release, and credit management, among others. A simple transport service built on top of an ATM network might look something like this. 

In general, the transport entity may be part of the host's operating system, or it may be a package of library routines running within the user's address space. For simplicity, our example has been programmed as though it were a library package, but the changes needed to make it part of the operating system are minimal (primarily how user buffers are accessed). 

It is worth noting, however, that in this example, the ''transport entity'' is not really a separate entity at all, but part of the user process. In particular, when the user executes a primitive that blocks, such as LISTEN, the entire transport entity blocks as well. While this design is fine for a host with only a single-user process, on a host with multiple users, it would be more natural to have the transport entity be a separate process, distinct from all the user processes. 

The interface to the network layer is via the procedures 

to

_

net

 and 

from

_

net

 (not shown). Each has six parameters. First comes the connection identifier, which maps one-to-one onto network virtual circuits. Next come the 

Q

 and 

M

 bits, which, when set to 1, indicate control message and that more data from this message follows in the next packet, respectively. After that we have the packet type, chosen from the set of six packet types listed in 

Fig. 6-19

. Finally, we have a pointer to the data itself, and an integer giving the number of bytes of data. 

Figure 6-19. The network layer packets used in our example. 

394




 

On calls to 

to

_

net

, the transport entity fills in all the parameters for the network layer to read; on calls to 

from

_

net

, the network layer dismembers an incoming packet for the transport entity. By passing information as procedure parameters rather than passing the actual outgoing or incoming packet itself, the transport layer is shielded from the details of the network layer protocol. If the transport entity should attempt to send a packet when the underlying virtual circuit's sliding window is full, it is suspended within 

to

_

net

 until there is room in the window. This mechanism is entirely transparent to the transport entity and is controlled by the network layer using commands analog ous to the 

enable

_

transport

_

layer

 and 

disable

_

transport

_

layer

 commands used in the protocols of 

Chap. 3

. The management of the packet layer window is also done by the network layer. 

In addition to this transparent suspension mechanism, explicit 

sleep

 and 

wakeup

 procedures (not shown) are also called by the transport entity. The procedure 

sleep

 is called when the transport entity is logically blocked waiting for an external event to happen, generally the arrival of a packet. After 

sleep

 has been called, the transport entity (and the user process, of course) stop executing. 

The actual code of the transport entity is shown in 

Fig. 6-20

. Each connection is always in one of seven states, as follows: 

Figure 6-20. An example transport entity. 

395




396




1. 

IDLE

— Connection not established yet. 

2. 

WAITING

— CONNECT has been executed and CALL REQUEST sent. 

3. 

QUEUED

— A CALL REQUEST has arrived; no LISTEN yet. 

4. 

ESTABLISHED

— The connection has been established. 

5. 

SENDING

— The user is waiting for permission to send a packet. 

6. 

RECEIVING

— A RECEIVE has been done. 

7. 

DISCONNECTING

— A DISCONNECT has been done locally. 

Transitions between states can occur when any of the following events occur: a primitive is executed, a packet arrives, or the timer expires. 

The procedures shown in 

Fig. 6-20

 are of two types. Most are directly callable by user programs. 

Packet

_

arrival

 and 

clock

 are different, however. They are spontaneously triggered by external events: the arrival of a packet and the clock ticking, respectively. In effect, they are interrupt routines. We will assume that they are never invoked while a transport entity procedure is running. Only when the user process is sleeping or executing outside the transport entity may they be called. This property is crucial to the correct functioning of the code. 

The existence of the 

Q

 (Qualifier) bit in the packet header allows us to avoid the overhead of a transport protocol header. Ordinary data messages are sent as data packets with 

Q

 = 0

.

 Transport protocol control messages, of which there is only one (CREDIT) in our example, are sent as data packets with 

Q

 = 1

.

 These control messages are detected and processed by the receiving transport entity. 

The main data structure used by the transport entity is the array 

conn

, which has one record for each potential connection. The record maintains the state of the connection, including the transport addresses at either end, the number of messages sent and received on the connection, the current state, the user buffer pointer, the number of bytes of the current messages sent or received so far, a bit indicating that the remote user has issued a DISCONNECT, a timer, and a permission counter used to enable sending of messages. Not all of these fields are used in our simple example, but a complete transport entity would need all of them, and perhaps more. Each 

conn

 entry is assumed initialized to the 

IDLE

 state. 

When the user calls CONNECT, the network layer is instructed to send a CALL REQUEST packet to the remote machine, and the user is put to sleep. When the CALL REQUEST packet arrives at the other side, the transport entity is interrupted to run 

packet

_

arrival

 to check whether the local user is listening on the specified address. If so, a CALL ACCEPTED packet is sent back and the remote user is awakened; if not, the CALL REQUEST is queued for 

TIMEOUT

 clock ticks. If a LISTEN is done within this period, the connection is established; otherwise, it times out and is rejected with a CLEAR REQUEST packet lest it block forever. 

Although we have eliminated the transport protocol header, we still need a way to keep track of which packet belongs to which transport connection, since multiple connections may exist simultaneously. The simplest approach is to use the network layer virtual circuit number as the transport connection number. Furthermore, the virtual circuit number can also be used as the index into the 

conn

 array. When a packet comes in on network layer virtual circuit 

k

, it belongs to transport connection 

k

, whose state is in the record 

conn

[

k

]. For connections initiated at a host, the connection number is chosen by the originating transport entity. For incoming calls, the network layer makes the choice, choosing any unused virtual circuit number. 

To avoid having to provide and manage buffers within the transport entity, here we use a flow control mechanism different from the normal sliding window. 

When a user calls RECEIVE, a special 

credit message

 is sent to the transport entity on the sending machine and is recorded in the 

conn

 array. When SEND is called, the transport entity checks to see if a credit has arrived on the specified connection. If so, the message is sent (in 

397




multiple packets if need be) and the credit decremented; if not, the transport entity puts itself to sleep until a credit arrives. This mechanism guarantees that no message is ever sent unless the other side has already done a RECEIVE. As a result, whenever a message arrives, there is guaranteed to be a buffer available into which the message can be put. The scheme can easily be generalized to allow receivers to provide multiple buffers and request multiple messages. 

You should keep the simplicity of 

Fig. 6-20

 in mind. A realistic transport entity would normally check all user-supplied parameters for validity, handle recovery from network layer crashes, deal with call collisions, and support a more general transport service including such facilities as interrupts, datagrams, and nonblocking versions of the SEND and RECEIVE primitives. 

6.3.3 The Example as a Finite State Machine 

Writing a transport entity is difficult and exacting work, especially for more realistic protocols. To reduce the chance of making an error, it is often useful to represent the state of the protocol as a finite state machine. 

We have already seen that our example protocol has seven states per connection. It is also possible to isolate 12 events that can move a connection from one state to another. Five of these events are the five service primitives. Another six are the arrivals of the six legal packet types. The last one is the expiration of the timer. 

Figure 6-21

 shows the main protocol actions in matrix form. The columns are the states and the rows are the 12 events. 

Figure 6-21. The example protocol as a finite state machine. Each entry has an optional predicate, an optional action, and the new state. The tilde indicates that no major action is taken. An overbar above a predicate indicates the negation of the predicate. Blank entries correspond to impossible or invalid events. 

398




 

Each entry in the matrix (i.e., the finite state machine) of 

Fig. 6-21

 has up to three fields: a predicate, an action, and a new state. The predicate indicates under which conditions the action is taken. For example, in the upper-left entry, if a LISTEN is executed and there is no more table space (predicate 

P1

), the LISTEN fails and the state does not change. On the other hand, if a CALL REQUEST packet has already arrived for the transport address being listened to (predicate 

P2

), the connection is established immediately. Another possibility is that 

P2

 is false, that is, no CALL REQUEST has come in, in which case the connection remains in the 

IDLE

 state, awaiting a CALL REQUEST packet. 

It is worth pointing out that the choice of states to use in the matrix is not entirely fixed by the protocol itself. In this example, there is no state 

LISTENING

, which might have been a reasonable thing to have following a LISTEN. There is no 

LISTENING

 state because a state is associated with a connection record entry, and no connection record is created by LISTEN. Why not? Because we have decided to use the network layer virtual circuit numbers as the connection identifiers, and for a LISTEN, the virtual circuit number is ultimately chosen by the network layer when the CALL REQUEST packet arrives. 

The actions 

A1

 through 

A12

 are the major actions, such as sending packets and starting timers. Not all the minor actions, such as initializing the fields of a connection record, are listed. If an action involves waking up a sleeping process, the actions following the wakeup also count. For example, if a CALL REQUEST packet comes in and a process was asleep waiting for it, the transmission of the CALL ACCEPT packet following the wakeup counts as part of the 

399




action for CALL REQUEST. After each action is performed, the connection may move to a new state, as shown in 

Fig. 6-21

. 

The advantage of representing the protocol as a matrix is threefold. First, in this form it is much easier for the programmer to systematically check each combination of state and event to see if an action is required. In production implementations, some of the combinations would be used for error handling. In 

Fig. 6-21

 no distinction is made between impossible situations and illegal ones. For example, if a connection is in 

waiting

 state, the DISCONNECT event is impossible because the user is blocked and cannot execute any primitives at all. On the other hand, in 

sending

 state, data packets are not expected because no credit has been issued. The arrival of a data packet is a protocol error. 

The second advantage of the matrix representation of the protocol is in implementing it. One could envision a two-dimensional array in which element 

a

[

i

][

j

 ] was a pointer or index to the procedure that handled the occurrence of event 

i

 when in state 

j

. One possible implementation is to write the transport entity as a short loop, waiting for an event at the top of the loop. When an event happens, the relevant connection is located and its state is extracted. With the event and state now known, the transport entity just indexes into the array 

a

 and calls the proper procedure. This approach gives a much more regular and systematic design than our transport entity. 

The third advantage of the finite state machine approach is for protocol description. In some standards documents, the protocols are given as finite state machines of the type of 

Fig. 6-21

. Going from this kind of description to a working transport entity is much easier if the transport entity is also driven by a finite state machine based on the one in the standard. 

The primary disadvantage of the finite state machine approach is that it may be more difficult to understand than the straight programming example we used initially. However, this problem may be partially solved by drawing the finite state machine as a graph, as is done in 

Fig. 6-22

. 

Figure 6-22. The example protocol in graphical form. Transitions that leave the connection state unchanged have been omitted for simplicity. 

400




 

6.4 The Internet Transport Protocols: UDP 

The Internet has two main protocols in the transport layer, a connectionless protocol and a connection-oriented one. In the following sections we will study both of them. The connectionless protocol is UDP. The connection-oriented protocol is TCP. Because UDP is basically just IP with a short header added, we will start with it. We will also look at two applications of UDP. 

6.4.1 Introduction to UDP 

The Internet protocol suite supports a connectionless transport protocol, 

UDP

 (

User Datagram Protocol

). UDP provides a way for applications to send encapsulated IP datagrams and send them without having to establish a connection. UDP is described in RFC 768. 

UDP transmits 

segments

 consisting of an 8-byte header followed by the payload. The header is shown in 

Fig. 6-23

. The two ports serve to identify the end points within the source and destination machines. When a UDP packet arrives, its payload is handed to the process attached to the destination port. This attachment occurs when BIND primitive or something similar is used, as we saw in 

Fig. 6-6

 for TCP (the binding process is the same for UDP). In fact, the main value of having UDP over just using raw IP is the addition of the source and destination ports. Without the port fields, the transport layer would not know what to do with the packet. With them, it delivers segments correctly. 

Figure 6-23. The UDP header. 

 

401




The source port is primarily needed when a reply must be sent back to the source. By copying the 

source port

 field from the incoming segment into the 

destination port

 field of the outgoing segment, the process sending the reply can specify which process on the sending machine is to get it. 

The 

UDP length

 field includes the 8-byte header and the data. The 

UDP checksum

 is optional and stored as 0 if not computed (a true computed 0 is stored as all 1s). Turning it off is foolish unless the quality of the data does not matter (e.g., digitized speech). 

It is probably worth mentioning explicitly some of the things that UDP does 

not

 do. It does not do flow control, error control, or retransmission upon receipt of a bad segment. All of that is up to the user processes. What it does do is provide an interface to the IP protocol with the added feature of demultiplexing multiple processes using the ports. That is all it does. For applications that need to have precise control over the packet flow, error control, or timing, UDP provides just what the doctor ordered. 

One area where UDP is especially useful is in client-server situations. Often, the client sends a short request to the server and expects a short reply back. If either the request or reply is lost, the client can just time out and try again. Not only is the code simple, but fewer messages are required (one in each direction) than with a protocol requiring an initial setup. 

An application that uses UDP this way is DNS (the Domain Name System), which we will study in 

Chap. 7

. In brief, a program that needs to look up the IP address of some host name, for example, 

www.cs.berkeley.edu

, can send a UDP packet containing the host name to a DNS server. The server replies with a UDP packet containing the host's IP address. No setup is needed in advance and no release is needed afterward. Just two messages go over the network. 

6.4.2 Remote Procedure Call 

In a certain sense, sending a message to a remote host and getting a reply back is a lot like making a function call in a programming language. In both cases you start with one or more parameters and you get back a result. This observation has led people to try to arrange request-reply interactions on networks to be cast in the form of procedure calls. Such an arrangement makes network applications much easier to program and more familiar to deal with. For example, just imagine a procedure named 

get

_

IP

_

address

 (

host

_

name

) that works by sending a UDP packet to a DNS server and waiting for the reply, timing out and trying again if one is not forthcoming quickly enough. In this way, all the details of networking can be hidden from the programmer. 

The key work in this area was done by Birrell and Nelson (1984). In a nutshell, what Birrell and Nelson suggested was allowing programs to call procedures located on remote hosts. When a process on machine 1 calls a procedure on machine 2, the calling process on 1 is suspended and execution of the called procedure takes place on 2. Information can be transported from the caller to the callee in the parameters and can come back in the procedure result. No message passing is visible to the programmer. This technique is known as 

RPC

 (

Remote Procedure Call

) and has become the basis for many networking applications. Traditionally, the calling procedure is known as the client and the called procedure is known as the server, and we will use those names here too. 

The idea behind RPC is to make a remote procedure call look as much as possible like a local one. In the simplest form, to call a remote procedure, the client program must be bound with a small library procedure, called the 

client stub

, that represents the server procedure in the client's address space. Similarly, the server is bound with a procedure called the 

server stub

. These procedures hide the fact that the procedure call from the client to the server is not local. 

402




The actual steps in making an RPC are shown in 

Fig. 6-24

. Step 1 is the client calling the client stub. This call is a local procedure call, with the parameters pushed onto the stack in the normal way. Step 2 is the client stub packing the parameters into a message and making a system call to send the message. Packing the parameters is called 

marshaling

. Step 3 is the kernel sending the message from the client machine to the server machine. Step 4 is the kernel passing the incoming packet to the server stub. Finally, step 5 is the server stub calling the server procedure with the unmarshaled parameters. The reply traces the same path in the other direction. 

Figure 6-24. Steps in making a remote procedure call. The stubs are shaded. 

 

The key item to note here is that the client procedure, written by the user, just makes a normal (i.e., local) procedure call to the client stub, which has the same name as the server procedure. Since the client procedure and client stub are in the same address space, the parameters are passed in the usual way. Similarly, the server procedure is called by a procedure in its address space with the parameters it expects. To the server procedure, nothing is unusual. In this way, instead of I/O being done on sockets, network communication is done by faking a normal procedure call. 

Despite the conceptual elegance of RPC, there are a few snakes hiding under the grass. A big one is the use of pointer parameters. Normally, passing a pointer to a procedure is not a problem. The called procedure can use the pointer in the same way the caller can because both procedures live in the same virtual address space. With RPC, passing pointers is impossible because the client and server are in different address spaces. 

In some cases, tricks can be used to make it possible to pass pointers. Suppose that the first parameter is a pointer to an integer, 

k

. The client stub can marshal 

k

 and send it along to the server. The server stub then creates a pointer to 

k

 and passes it to the server procedure, just as it expects. When the server procedure returns control to the server stub, the latter sends 

k

 back to the client where the new 

k

 is copied over the old one, just in case the server changed it. In effect, the standard calling sequence of call-by-reference has been replaced by copy-restore. Unfortunately, this trick does not always work, for example, if the pointer points to a graph or other complex data structure. For this reason, some restrictions must be placed on parameters to procedures called remotely. 

A second problem is that in weakly-typed languages, like C, it is perfectly legal to write a procedure that computes the inner product of two vectors (arrays), without specifying how large either one is. Each could be terminated by a special value known only to the calling and called procedure. Under these circumstances, it is essentially impossible for the client stub to marshal the parameters: it has no way of determining how large they are. 

A third problem is that it is not always possible to deduce the types of the parameters, not even from a formal specification or the code itself. An example is 

printf

, which may have any 

403




number of parameters (at least one), and the parameters can be an arbitrary mixture of integers, shorts, longs, characters, strings, floating-point numbers of various lengths, and other types. Trying to call 

printf

 as a remote procedure would be practically impossible because C is so permissive. However, a rule saying that RPC can be used provided that you do not program in C (or C++) would not be popular. 

A fourth problem relates to the use of global variables. Normally, the calling and called procedure can communicate by using global variables, in addition to communicating via parameters. If the called procedure is now moved to a remote machine, the code will fail because the global variables are no longer shared. 

These problems are not meant to suggest that RPC is hopeless. In fact, it is widely used, but some restrictions are needed to make it work well in practice. 

Of course, RPC need not use UDP packets, but RPC and UDP are a good fit and UDP is commonly used for RPC. However, when the parameters or results may be larger than the maximum UDP packet or when the operation requested is not idempotent (i.e., cannot be repeated safely, such as when incrementing a counter), it may be necessary to set up a TCP connection and send the request over it rather than use UDP. 

6.4.3 The Real-Time Transport Protocol 

Client-server RPC is one area in which UDP is widely used. Another one is real-time multimedia applications. In particular, as Internet radio, Internet telephony, music-on-demand, videoconferencing, video-on-demand, and other multimedia applications became more commonplace, people discovered that each application was reinventing more or less the same real-time transport protocol. It gradually became clear that having a generic real-time transport protocol for multiple applications would be a good idea. Thus was 

RTP

 (

Real-time Transport Protocol

) born. It is described in RFC 1889 and is now in widespread use. 

The position of RTP in the protocol stack is somewhat strange. It was decided to put RTP in user space and have it (normally) run over UDP. It operates as follows. The multimedia application consists of multiple audio, video, text, and possibly other streams. These are fed into the RTP library, which is in user space along with the application. This library then multiplexes the streams and encodes them in RTP packets, which it then stuffs into a socket. At the other end of the socket (in the operating system kernel), UDP packets are generated and embedded in IP packets. If the computer is on an Ethernet, the IP packets are then put in Ethernet frames for transmission. The protocol stack for this situation is shown in 

Fig. 6-25(a)

. The packet nesting is shown in 

Fig. 6-25(b)

. 

Figure 6-25. (a) The position of RTP in the protocol stack. (b) Packet nesting. 

 

As a consequence of this design, it is a little hard to say which layer RTP is in. Since it runs in user space and is linked to the application program, it certainly looks like an application 

404




protocol. On the other hand, it is a generic, application-independent protocol that just provides transport facilities, so it also looks like a transport protocol. Probably the best description is that it is a transport protocol that is implemented in the application layer. 

The basic function of RTP is to multiplex several real-time data streams onto a single stream of UDP packets. The UDP stream can be sent to a single destination (unicasting) or to multiple destinations (multicasting). Because RTP just uses normal UDP, its packets are not treated specially by the routers unless some normal IP quality-of-service features are enabled. In particular, there are no special guarantees about delivery, jitter, etc. 

Each packet sent in an RTP stream is given a number one higher than its predecessor. This numbering allows the destination to determine if any packets are missing. If a packet is missing, the best action for the destination to take is to approximate the missing value by interpolation. Retransmission is not a practical option since the retransmitted packet would probably arrive too late to be useful. As a consequence, RTP has no flow control, no error control, no acknowledgements, and no mechanism to request retransmissions. 

Each RTP payload may contain multiple samples, and they may be coded any way that the application wants. To allow for interworking, RTP defines several profiles (e.g., a single audio stream), and for each profile, multiple encoding formats may be allowed. For example, a single audio stream may be encoded as 8-bit PCM samples at 8 kHz, delta encoding, predictive encoding, GSM encoding, MP3, and so on. RTP provides a header field in which the source can specify the encoding but is otherwise not involved in how encoding is done. 

Another facility many real-time applications need is timestamping. The idea here is to allow the source to associate a timestamp with the first sample in each packet. The timestamps are relative to the start of the stream, so only the differences between timestamps are significant. The absolute values have no meaning. This mechanism allows the destination to do a small amount of buffering and play each sample the right number of milliseconds after the start of the stream, independently of when the packet containing the sample arrived. Not only does timestamping reduce the effects of jitter, but it also allows multiple streams to be synchronized with each other. For example, a digital television program might have a video stream and two audio streams. The two audio streams could be for stereo broadcasts or for handling films with an original language soundtrack and a soundtrack dubbed into the local language, giving the viewer a choice. Each stream comes from a different physical device, but if they are timestamped from a single counter, they can be played back synchronously, even if the streams are transmitted somewhat erratically. 

The RTP header is illustrated in 

Fig. 6-26

. It consists of three 32-bit words and potentially some extensions. The first word contains the 

Version

 field, which is already at 2. Let us hope this version is very close to the ultimate version since there is only one code point left (although 3 could be defined as meaning that the real version was in an extension word). 32 bits 

Figure 6-26. The RTP header. 

405




 

The 

P

 bit indicates that the packet has been padded to a multiple of 4 bytes. The last padding byte tells how many bytes were added. The 

X

 bit indicates that an extension header is present. The format and meaning of the extension header are not defined. The only thing that is defined is that the first word of the extension gives the length. This is an escape hatch for any unforeseen requirements. 

The 

CC

 field tells how many contributing sources are present, from 0 to 15 (see below). The 

M

 bit is an application-specific marker bit. It can be used to mark the start of a video frame, the start of a word in an audio channel, or something else that the application understands. The 

Payload type

 field tells which encoding algorithm has been used (e.g., uncompressed 8-bit audio, MP3, etc.). Since every packet carries this field, the encoding can change during transmission. The 

Sequence number

 is just a counter that is incremented on each RTP packet sent. It is used to detect lost packets. 

The timestamp is produced by the stream's source to note when the first sample in the packet was made. This value can help reduce jitter at the receiver by decoupling the playback from the packet arrival time. The 

Synchronization source identifier

 tells which stream the packet belongs to. It is the method used to multiplex and demultiplex multiple data streams onto a single stream of UDP packets. Finally, the 

Contributing source identifiers

, if any, are used when mixers are present in the studio. In that case, the mixer is the synchronizing source, and the streams being mixed are listed here. 

RTP has a little sister protocol (little sibling protocol?) called 

RTCP

 (

Realtime Transport Control Protocol

). It handles feedback, synchronization, and the user interface but does not transport any data. The first function can be used to provide feedback on delay, jitter, bandwidth, congestion, and other network properties to the sources. This information can be used by the encoding process to increase the data rate (and give better quality) when the network is functioning well and to cut back the data rate when there is trouble in the network. By providing continuous feedback, the encoding algorithms can be continuously adapted to provide the best quality possible under the current circumstances. For example, if the bandwidth increases or decreases during the transmission, the encoding may switch from MP3 to 8-bit PCM to delta encoding as required. The 

Payload type

 field is used to tell the destination what encoding algorithm is used for the current packet, making it possible to vary it on demand. 

RTCP also handles interstream synchronization. The problem is that different streams may use different clocks, with different granularities and different drift rates. RTCP can be used to keep them in sync. 

Finally, RTCP provides a way for naming the various sources (e.g., in ASCII text). This information can be displayed on the receiver's screen to indicate who is talking at the moment. 

More information about RTP can be found in (Perkins, 2002). 

406




6.5 The Internet Transport Protocols: TCP 

UDP is a simple protocol and it has some niche uses, such as client-server interactions and multimedia, but for most Internet applications, reliable, sequenced delivery is needed. UDP cannot provide this, so another protocol is required. It is called TCP and is the main workhorse of the Internet. Let us now study it in detail. 

6.5.1 Introduction to TCP 

TCP

 (

Transmission Control Protocol

) was specifically designed to provide a reliable end-to-end byte stream over an unreliable internetwork. An internetwork differs from a single network because different parts may have wildly different topologies, bandwidths, delays, packet sizes, and other parameters. TCP was designed to dynamically adapt to properties of the internetwork and to be robust in the face of many kinds of failures. 

TCP was formally defined in RFC 793. As time went on, various errors and inconsistencies were detected, and the requirements were changed in some areas. These clarifications and some bug fixes are detailed in RFC 1122. Extensions are given in RFC 1323. 

Each machine supporting TCP has a TCP transport entity, either a library procedure, a user process, or part of the kernel. In all cases, it manages TCP streams and interfaces to the IP layer. A TCP entity accepts user data streams from local processes, breaks them up into pieces not exceeding 64 KB (in practice, often 1460 data bytes in order to fit in a single Ethernet frame with the IP and TCP headers), and sends each piece as a separate IP datagram. When datagrams containing TCP data arrive at a machine, they are given to the TCP entity, which reconstructs the original byte streams. For simplicity, we will sometimes use just ''TCP'' to mean the TCP transport entity (a piece of software) or the TCP protocol (a set of rules). From the context it will be clear which is meant. For example, in ''The user gives TCP the data,'' the TCP transport entity is clearly intended. 

The IP layer gives no guarantee that datagrams will be delivered properly, so it is up to TCP to time out and retransmit them as need be. Datagrams that do arrive may well do so in the wrong order; it is also up to TCP to reassemble them into messages in the proper sequence. In short, TCP must furnish the reliability that most users want and that IP does not provide. 

6.5.2 The TCP Service Model 

TCP service is obtained by both the sender and receiver creating end points, called sockets, as discussed in 

Sec. 6.1.3

. Each socket has a socket number (address) consisting of the IP address of the host and a 16-bit number local to that host, called a 

port

. A port is the TCP name for a TSAP. For TCP service to be obtained, a connection must be explicitly established between a socket on the sending machine and a socket on the receiving machine. The socket calls are listed in 

Fig. 6-5

. 

A socket may be used for multiple connections at the same time. In other words, two or more connections may terminate at the same socket. Connections are identified by the socket identifiers at both ends, that is, (

socket1

, 

socket2

). No virtual circuit numbers or other identifiers are used. 

Port numbers below 1024 are called 

well-known ports

 and are reserved for standard services. For example, any process wishing to establish a connection to a host to transfer a file using FTP can connect to the destination host's port 21 to contact its FTP daemon. The list of well-known ports is given at 

www.iana.org

. Over 300 have been assigned. A few of the better known ones are listed in 

Fig. 6-27

. 

407




Figure 6-27. Some assigned ports. 

 

It would certainly be possible to have the FTP daemon attach itself to port 21 at boot time, the telnet daemon to attach itself to port 23 at boot time, and so on. However, doing so would clutter up memory with daemons that were idle most of the time. Instead, what is generally done is to have a single daemon, called 

inetd

 (

Internet daemon

) in UNIX, attach itself to multiple ports and wait for the first incoming connection. When that occurs, inetd forks off a new process and executes the appropriate daemon in it, letting that daemon handle the request. In this way, the daemons other than inetd are only active when there is work for them to do. Inetd learns which ports it is to use from a configuration file. Consequently, the system administrator can set up the system to have permanent daemons on the busiest ports (e.g., port 80) and inetd on the rest. 

All TCP connections are full duplex and point-to-point. Full duplex means that traffic can go in both directions at the same time. Point-to-point means that each connection has exactly two end points. TCP does not support multicasting or broadcasting. 

A TCP connection is a byte stream, not a message stream. Message boundaries are not preserved end to end. For example, if the sending process does four 512-byte writes to a TCP stream, these data may be delivered to the receiving process as four 512-byte chunks, two 1024-byte chunks, one 2048-byte chunk (see 

Fig. 6-28

), or some other way. There is no way for the receiver to detect the unit(s) in which the data were written. 

Figure 6-28. (a) Four 512-byte segments sent as separate IP datagrams. (b) The 2048 bytes of data delivered to the application in a single READ call. 

 

Files in UNIX have this property too. The reader of a file cannot tell whether the file was written a block at a time, a byte at a time, or all in one blow. As with a UNIX file, the TCP software has no idea of what the bytes mean and no interest in finding out. A byte is just a byte. 

When an application passes data to TCP, TCP may send it immediately or buffer it (in order to collect a larger amount to send at once), at its discretion. However, sometimes, the application really wants the data to be sent immediately. For example, suppose a user is logged in to a remote machine. After a command line has been finished and the carriage return typed, it is essential that the line be shipped off to the remote machine immediately and not buffered until 

408




the next line comes in. To force data out, applications can use the PUSH flag, which tells TCP not to delay the transmission. 

Some early applications used the PUSH flag as a kind of marker to delineate messages boundaries. While this trick sometimes works, it sometimes fails since not all implementations of TCP pass the PUSH flag to the application on the receiving side. Furthermore, if additional PUSHes come in before the first one has been transmitted (e.g., because the output line is busy), TCP is free to collect all the PUSHed data into a single IP datagram, with no separation between the various pieces. 

One last feature of the TCP service that is worth mentioning here is 

urgent data

. When an interactive user hits the DEL or CTRL-C key to break off a remote computation that has already begun, the sending application puts some control information in the data stream and gives it to TCP along with the URGENT flag. This event causes TCP to stop accumulating data and transmit everything it has for that connection immediately. 

When the urgent data are received at the destination, the receiving application is interrupted (e.g., given a signal in UNIX terms) so it can stop whatever it was doing and read the data stream to find the urgent data. The end of the urgent data is marked so the application knows when it is over. The start of the urgent data is not marked. It is up to the application to figure that out. This scheme basically provides a crude signaling mechanism and leaves everything else up to the application. 

6.5.3 The TCP Protocol 

In this section we will give a general overview of the TCP protocol. In the next one we will go over the protocol header, field by field. 

A key feature of TCP, and one which dominates the protocol design, is that every byte on a TCP connection has its own 32-bit sequence number. When the Internet began, the lines between routers were mostly 56-kbps leased lines, so a host blasting away at full speed took over 1 week to cycle through the sequence numbers. At modern network speeds, the sequence numbers can be consumed at an alarming rate, as we will see later. Separate 32-bit sequence numbers are used for acknowledgements and for the window mechanism, as discussed below. 

The sending and receiving TCP entities exchange data in the form of segments. A 

TCP segment

 consists of a fixed 20-byte header (plus an optional part) followed by zero or more data bytes. The TCP software decides how big segments should be. It can accumulate data from several writes into one segment or can split data from one write over multiple segments. Two limits restrict the segment size. First, each segment, including the TCP header, must fit in the 65,515-byte IP payload. Second, each network has a 

maximum transfer unit

, or 

MTU

, and each segment must fit in the MTU. In practice, the MTU is generally 1500 bytes (the Ethernet payload size) and thus defines the upper bound on segment size. 

The basic protocol used by TCP entities is the sliding window protocol. When a sender transmits a segment, it also starts a timer. When the segment arrives at the destination, the receiving TCP entity sends back a segment (with data if any exist, otherwise without data) bearing an acknowledgement number equal to the next sequence number it expects to receive. If the sender's timer goes off before the acknowledgement is received, the sender transmits the segment again. 

Although this protocol sounds simple, there are a number of sometimes subtle ins and outs, which we will cover below. Segments can arrive out of order, so bytes 3072–4095 can arrive but cannot be acknowledged because bytes 2048–-3071 have not turned up yet. Segments can also be delayed so long in transit that the sender times out and retransmits them. The retransmissions may include different byte ranges than the original transmission, requiring a 

409




careful administration to keep track of which bytes have been correctly received so far. However, since each byte in the stream has its own unique offset, it can be done. 

TCP must be prepared to deal with these problems and solve them in an efficient way. A considerable amount of effort has gone into optimizing the performance of TCP streams, even in the face of network problems. A number of the algorithms used by many TCP implementations will be discussed below. 

6.5.4 The TCP Segment Header 

Figure 6-29

 shows the layout of a TCP segment. Every segment begins with a fixed-format, 20-byte header. The fixed header may be followed by header options. After the options, if any, up to 65,535 - 20 - 20 = 65,495 data bytes may follow, where the first 20 refer to the IP header and the second to the TCP header. Segments without any data are legal and are commonly used for acknowledgements and control messages. 

Figure 6-29. The TCP header. 

 

Let us dissect the TCP header field by field. The 

Source port

 and 

Destination port

 fields identify the local end points of the connection. The well-known ports are defined at 

www.iana.org

 but each host can allocate the others as it wishes. A port plus its host's IP address forms a 48-bit unique end point. The source and destination end points together identify the connection. 

The 

Sequence number

 and 

Acknowledgement number

 fields perform their usual functions. Note that the latter specifies the next byte expected, not the last byte correctly received. Both are 32 bits long because every byte of data is numbered in a TCP stream. 

The 

TCP header length

 tells how many 32-bit words are contained in the TCP header. This information is needed because the 

Options

 field is of variable length, so the header is, too. Technically, this field really indicates the start of the data within the segment, measured in 32-bit words, but that number is just the header length in words, so the effect is the same. 

Next comes a 6-bit field that is not used. The fact that this field has survived intact for over a quarter of a century is testimony to how well thought out TCP is. Lesser protocols would have needed it to fix bugs in the original design. 

410




Now come six 1-bit flags. 

URG

 is set to 1 if the 

Urgent pointer

 is in use. The 

Urgent pointer

 is used to indicate a byte offset from the current sequence number at which urgent data are to be found. This facility is in lieu of interrupt messages. As we mentioned above, this facility is a bare-bones way of allowing the sender to signal the receiver without getting TCP itself involved in the reason for the interrupt. 

The 

ACK

 bit is set to 1 to indicate that the 

Acknowledgement number

 is valid. If 

ACK

 is 0, the segment does not contain an acknowledgement so the 

Acknowledgement number

 field is ignored. 

The 

PSH

 bit indicates PUSHed data. The receiver is hereby kindly requested to deliver the data to the application upon arrival and not buffer it until a full buffer has been received (which it might otherwise do for efficiency). 

The 

RST

 bit is used to reset a connection that has become confused due to a host crash or some other reason. It is also used to reject an invalid segment or refuse an attempt to open a connection. In general, if you get a segment with the 

RST

 bit on, you have a problem on your hands. 

The 

SYN

 bit is used to establish connections. The connection request has 

SYN

 = 1 and 

ACK

 = 0 to indicate that the piggyback acknowledgement field is not in use. The connection reply does bear an acknowledgement, so it has 

SYN

 = 1 and 

ACK

 = 1

.

 In essence the 

SYN

 bit is used to denote CONNECTION REQUEST and CONNECTION ACCEPTED, with the 

ACK

 bit used to distinguish between those two possibilities. 

The 

FIN

 bit is used to release a connection. It specifies that the sender has no more data to transmit. However, after closing a connection, the closing process may continue to receive data indefinitely. Both 

SYN

 and 

FIN

 segments have sequence numbers and are thus guaranteed to be processed in the correct order. 

Flow control in TCP is handled using a variable-sized sliding window. The 

Window size

 field tells how many bytes may be sent starting at the byte acknowledged. A 

Window size

 field of 0 is legal and says that the bytes up to and including 

Acknowledgement number

 - 1 have been received, but that the receiver is currently badly in need of a rest and would like no more data for the moment, thank you. The receiver can later grant permission to send by transmitting a segment with the same 

Acknowledgement number

 and a nonzero 

Window size

 field. 

In the protocols of 

Chap. 3

, acknowledgements of frames received and permission to send new frames were tied together. This was a consequence of a fixed window size for each protocol. In TCP, acknowledgements and permission to send additional data are completely decoupled. In effect, a receiver can say: I have received bytes up through 

k

 but I do not want any more just now. This decoupling (in fact, a variable-sized window) gives additional flexibility. We will study it in detail below. 

A 

Checksum

 is also provided for extra reliability. It checksums the header, the data, and the conceptual pseudoheader shown in 

Fig. 6-30

. When performing this computation, the TCP 

Checksum

 field is set to zero and the data field is padded out with an additional zero byte if its length is an odd number. The checksum algorithm is simply to add up all the 16-bit words in one's complement and then to take the one's complement of the sum. As a consequence, when the receiver performs the calculation on the entire segment, including the 

Checksum

 field, the result should be 0. 

Figure 6-30. The pseudoheader included in the TCP checksum. 

411




 

The pseudoheader contains the 32-bit IP addresses of the source and destination machines, the protocol number for TCP (6), and the byte count for the TCP segment (including the header). Including the pseudoheader in the TCP checksum computation helps detect misdelivered packets, but including it also violates the protocol hierarchy since the IP addresses in it belong to the IP layer, not to the TCP layer. UDP uses the same pseudoheader for its checksum. 

The 

Options

 field provides a way to add extra facilities not covered by the regular header. The most important option is the one that allows each host to specify the maximum TCP payload it is willing to accept. Using large segments is more efficient than using small ones because the 20-byte header can then be amortized over more data, but small hosts may not be able to handle big segments. During connection setup, each side can announce its maximum and see its partner's. If a host does not use this option, it defaults to a 536-byte payload. All Internet hosts are required to accept TCP segments of 536 + 20 = 556 bytes. The maximum segment size in the two directions need not be the same. 

For lines with high bandwidth, high delay, or both, the 64-KB window is often a problem. On a T3 line (44.736 Mbps), it takes only 12 msec to output a full 64-KB window. If the round-trip propagation delay is 50 msec (which is typical for a transcontinental fiber), the sender will be idle 3/4 of the time waiting for acknowledgements. On a satellite connection, the situation is even worse. A larger window size would allow the sender to keep pumping data out, but using the 16-bit 

Window size

 field, there is no way to express such a size. In RFC 1323, a 

Window scale

 option was proposed, allowing the sender and receiver to negotiate a window scale factor. This number allows both sides to shift the 

Window size

 field up to 14 bits to the left, thus allowing windows of up to 2

30

 bytes. Most TCP implementations now support this option. 

Another option proposed by RFC 1106 and now widely implemented is the use of the selective repeat instead of go back n protocol. If the receiver gets one bad segment and then a large number of good ones, the normal TCP protocol will eventually time out and retransmit all the unacknowledged segments, including all those that were received correctly (i.e., the go back n protocol). RFC 1106 introduced NAKs to allow the receiver to ask for a specific segment (or segments). After it gets these, it can acknowledge all the buffered data, thus reducing the amount of data retransmitted. 

6.5.5 TCP Connection Establishment 

Connections are established in TCP by means of the three-way handshake discussed in 

Sec. 

6.2.2

. To establish a connection, one side, say, the server, passively waits for an incoming connection by executing the LISTEN and ACCEPT primitives, either specifying a specific source or nobody in particular. 

The other side, say, the client, executes a CONNECT primitive, specifying the IP address and port to which it wants to connect, the maximum TCP segment size it is willing to accept, and optionally some user data (e.g., a password). The CONNECT primitive sends a TCP segment with the 

SYN

 bit on and 

ACK

 bit off and waits for a response. 

412




When this segment arrives at the destination, the TCP entity there checks to see if there is a process that has done a LISTEN on the port given in the 

Destination port

 field. If not, it sends a reply with the 

RST

 bit on to reject the connection. 

If some process is listening to the port, that process is given the incoming TCP segment. It can then either accept or reject the connection. If it accepts, an acknowledgement segment is sent back. The sequence of TCP segments sent in the normal case is shown in 

Fig. 6-31(a)

. Note that a 

SYN

 segment consumes 1 byte of sequence space so that it can be acknowledged unambiguously. 

Figure 6-31. (a) TCP connection establishment in the normal case. (b) Call collision. 

 

In the event that two hosts simultaneously attempt to establish a connection between the same two sockets, the sequence of events is as illustrated in 

Fig. 6-31(b)

. The result of these events is that just one connection is established, not two because connections are identified by their end points. If the first setup results in a connection identified by (

x

, 

y

) and the second one does too, only one table entry is made, namely, for (

x

, 

y

). 

The initial sequence number on a connection is not 0 for the reasons we discussed earlier. A clock-based scheme is used, with a clock tick every 4 µsec. For additional safety, when a host crashes, it may not reboot for the maximum packet lifetime to make sure that no packets from previous connections are still roaming around the Internet somewhere. 

6.5.6 TCP Connection Release 

Although TCP connections are full duplex, to understand how connections are released it is best to think of them as a pair of simplex connections. Each simplex connection is released independently of its sibling. To release a connection, either party can send a TCP segment with the 

FIN

 bit set, which means that it has no more data to transmit. When the 

FIN

 is acknowledged, that direction is shut down for new data. Data may continue to flow indefinitely in the other direction, however. When both directions have been shut down, the connection is released. Normally, four TCP segments are needed to release a connection, one 

FIN

 and one 

ACK

 for each direction. However, it is possible for the first 

ACK

 and the second 

FIN

 to be contained in the same segment, reducing the total count to three. 

Just as with telephone calls in which both people say goodbye and hang up the phone simultaneously, both ends of a TCP connection may send 

FIN

 segments at the same time. These are each acknowledged in the usual way, and the connection is shut down. There is, in fact, no essential difference between the two hosts releasing sequentially or simultaneously. 

413




To avoid the two-army problem, timers are used. If a response to a 

FIN

 is not forthcoming within two maximum packet lifetimes, the sender of the 

FIN

 releases the connection. The other side will eventually notice that nobody seems to be listening to it any more and will time out as well. While this solution is not perfect, given the fact that a perfect solution is theoretically impossible, it will have to do. In practice, problems rarely arise. 

6.5.7 TCP Connection Management Modeling 

The steps required to establish and release connections can be represented in a finite state machine with the 11 states listed in 

Fig. 6-32

. In each state, certain events are legal. When a legal event happens, some action may be taken. If some other event happens, an error is reported. 

Figure 6-32. The states used in the TCP connection management finite state machine. 

 

Each connection starts in the 

CLOSED

 state. It leaves that state when it does either a passive open (LISTEN), or an active open (CONNECT). If the other side does the opposite one, a connection is established and the state becomes 

ESTABLISHED

. Connection release can be initiated by either side. When it is complete, the state returns to 

CLOSED

. 

The finite state machine itself is shown in 

Fig. 6-33

. The common case of a client actively connecting to a passive server is shown with heavy lines—solid for the client, dotted for the server. The lightface lines are unusual event sequences. Each line in 

Fig. 6-33

 is marked by an 

event/action

 pair. The event can either be a user-initiated system call (CONNECT, LISTEN, SEND, or CLOSE), a segment arrival (

SYN

, 

FIN

, 

ACK

, or 

RST

), or, in one case, a timeout of twice the maximum packet lifetime. The action is the sending of a control segment (

SYN

, 

FIN

, or 

RST

) or nothing, indicated by —. Comments are shown in parentheses. 

Figure 6-33. TCP connection management finite state machine. The heavy solid line is the normal path for a client. The heavy dashed line is the normal path for a server. The light lines are unusual events. Each transition is labeled by the event causing it and the action resulting from it, separated by a slash. 

414




 

One can best understand the diagram by first following the path of a client (the heavy solid line), then later following the path of a server (the heavy dashed line). When an application program on the client machine issues a CONNECT request, the local TCP entity creates a connection record, marks it as being in the 

SYN SENT

 state, and sends a 

SYN

 segment. Note that many connections may be open (or being opened) at the same time on behalf of multiple applications, so the state is per connection and recorded in the connection record. When the 

SYN+ACK

 arrives, TCP sends the final 

ACK

 of the three-way handshake and switches into the 

ESTABLISHED

 state. Data can now be sent and received. 

When an application is finished, it executes a CLOSE primitive, which causes the local TCP entity to send a 

FIN

 segment and wait for the corresponding 

ACK

 (dashed box marked active close). When the 

ACK

 arrives, a transition is made to state 

FIN WAIT 2

 and one direction of the connection is now closed. When the other side closes, too, a 

FIN

 comes in, which is acknowledged. Now both sides are closed, but TCP waits a time equal to the maximum packet lifetime to guarantee that all packets from the connection have died off, just in case the acknowledgement was lost. When the timer goes off, TCP deletes the connection record. 

Now let us examine connection management from the server's viewpoint. The server does a LISTEN and settles down to see who turns up. When a 

SYN

 comes in, it is acknowledged and the server goes to the 

SYN RCVD

 state. When the server's 

SYN

 is itself acknowledged, the three-way handshake is complete and the server goes to the 

ESTABLISHED

 state. Data transfer can now occur. 

When the client is done, it does a CLOSE, which causes a 

FIN

 to arrive at the server (dashed box marked passive close). The server is then signaled. When it, too, does a CLOSE, a 

FIN

 is sent to the client. When the client's acknowledgement shows up, the server releases the connection and deletes the connection record. 

415




6.5.8 TCP Transmission Policy 

As mentioned earlier, window management in TCP is not directly tied to acknowledgements as it is in most data link protocols. For example, suppose the receiver has a 4096-byte buffer, as shown in 

Fig. 6-34

. If the sender transmits a 2048-byte segment that is correctly received, the receiver will acknowledge the segment. However, since it now has only 2048 bytes of buffer space (until the application removes some data from the buffer), it will advertise a window of 2048 starting at the next byte expected. 

Figure 6-34. Window management in TCP. 

 

Now the sender transmits another 2048 bytes, which are acknowledged, but the advertised window is 0. The sender must stop until the application process on the receiving host has removed some data from the buffer, at which time TCP can advertise a larger window. 

When the window is 0, the sender may not normally send segments, with two exceptions. First, urgent data may be sent, for example, to allow the user to kill the process running on the remote machine. Second, the sender may send a 1-byte segment to make the receiver reannounce the next byte expected and window size. The TCP standard explicitly provides this option to prevent deadlock if a window announcement ever gets lost. 

Senders are not required to transmit data as soon as they come in from the application. Neither are receivers required to send acknowledgements as soon as possible. For example, in 

Fig. 6-34

, when the first 2 KB of data came in, TCP, knowing that it had a 4-KB window available, would have been completely correct in just buffering the data until another 2 KB came in, to be able to transmit a segment with a 4-KB payload. This freedom can be exploited to improve performance. 

Consider a telnet connection to an interactive editor that reacts on every keystroke. In the worst case, when a character arrives at the sending TCP entity, TCP creates a 21-byte TCP 

416




segment, which it gives to IP to send as a 41-byte IP datagram. At the receiving side, TCP immediately sends a 40-byte acknowledgement (20 bytes of TCP header and 20 bytes of IP header). Later, when the editor has read the byte, TCP sends a window update, moving the window 1 byte to the right. This packet is also 40 bytes. Finally, when the editor has processed the character, it echoes the character as a 41-byte packet. In all, 162 bytes of bandwidth are used and four segments are sent for each character typed. When bandwidth is scarce, this method of doing business is not desirable. 

One approach that many TCP implementations use to optimize this situation is to delay acknowledgements and window updates for 500 msec in the hope of acquiring some data on which to hitch a free ride. Assuming the editor echoes within 500 msec, only one 41-byte packet now need be sent back to the remote user, cutting the packet count and bandwidth usage in half. 

Although this rule reduces the load placed on the network by the receiver, the sender is still operating inefficiently by sending 41-byte packets containing 1 byte of data. A way to reduce this usage is known as 

Nagle's algorithm

 (Nagle, 1984). What Nagle suggested is simple: when data come into the sender one byte at a time, just send the first byte and buffer all the rest until the outstanding byte is acknowledged. Then send all the buffered characters in one TCP segment and start buffering again until they are all acknowledged. If the user is typing quickly and the network is slow, a substantial number of characters may go in each segment, greatly reducing the bandwidth used. The algorithm additionally allows a new packet to be sent if enough data have trickled in to fill half the window or a maximum segment. 

Nagle's algorithm is widely used by TCP implementations, but there are times when it is better to disable it. In particular, when an X Windows application is being run over the Internet, mouse movements have to be sent to the remote computer. (The X Window system is the windowing system used on most UNIX systems.) Gathering them up to send in bursts makes the mouse cursor move erratically, which makes for unhappy users. 

Another problem that can degrade TCP performance is the 

silly window syndrome

 (Clark, 1982). This problem occurs when data are passed to the sending TCP entity in large blocks, but an interactive application on the receiving side reads data 1 byte at a time. To see the problem, look at 

Fig. 6-35

. Initially, the TCP buffer on the receiving side is full and the sender knows this (i.e., has a window of size 0). Then the interactive application reads one character from the TCP stream. This action makes the receiving TCP happy, so it sends a window update to the sender saying that it is all right to send 1 byte. The sender obliges and sends 1 byte. The buffer is now full, so the receiver acknowledges the 1-byte segment but sets the window to 0. This behavior can go on forever. 

Figure 6-35. Silly window syndrome. 

417




 

Clark's solution is to prevent the receiver from sending a window update for 1 byte. Instead it is forced to wait until it has a decent amount of space available and advertise that instead. Specifically, the receiver should not send a window update until it can handle the maximum segment size it advertised when the connection was established or until its buffer is half empty, whichever is smaller. 

Furthermore, the sender can also help by not sending tiny segments. Instead, it should try to wait until it has accumulated enough space in the window to send a full segment or at least one containing half of the receiver's buffer size (which it must estimate from the pattern of window updates it has received in the past). 

Nagle's algorithm and Clark's solution to the silly window syndrome are complementary. Nagle was trying to solve the problem caused by the sending application delivering data to TCP a byte at a time. Clark was trying to solve the problem of the receiving application sucking the data up from TCP a byte at a time. Both solutions are valid and can work together. The goal is for the sender not to send small segments and the receiver not to ask for them. 

The receiving TCP can go further in improving performance than just doing window updates in large units. Like the sending TCP, it can also buffer data, so it can block a 

READ

 request from the application until it has a large chunk of data to provide. Doing this reduces the number of calls to TCP, and hence the overhead. Of course, it also increases the response time, but for noninteractive applications like file transfer, efficiency may be more important than response time to individual requests. 

Another receiver issue is what to do with out-of-order segments. They can be kept or discarded, at the receiver's discretion. Of course, acknowledgements can be sent only when all the data up to the byte acknowledged have been received. If the receiver gets segments 0, 1, 2, 4, 5, 6, and 7, it can acknowledge everything up to and including the last byte in segment 2. When the sender times out, it then retransmits segment 3. If the receiver has buffered segments 4 through 7, upon receipt of segment 3 it can acknowledge all bytes up to the end of segment 7. 

6.5.9 TCP Congestion Control 

When the load offered to any network is more than it can handle, congestion builds up. The Internet is no exception. In this section we will discuss algorithms that have been developed over the past quarter of a century to deal with congestion. Although the network layer also 

418




tries to manage congestion, most of the heavy lifting is done by TCP because the real solution to congestion is to slow down the data rate. 

In theory, congestion can be dealt with by employing a principle borrowed from physics: the law of conservation of packets. The idea is to refrain from injecting a new packet into the network until an old one leaves (i.e., is delivered). TCP attempts to achieve this goal by dynamically manipulating the window size. 

The first step in managing congestion is detecting it. In the old days, detecting congestion was difficult. A timeout caused by a lost packet could have been caused by either (1) noise on a transmission line or (2) packet discard at a congested router. Telling the difference was difficult. 

Nowadays, packet loss due to transmission errors is relatively rare because most long-haul trunks are fiber (although wireless networks are a different story). Consequently, most transmission timeouts on the Internet are due to congestion. All the Internet TCP algorithms assume that timeouts are caused by congestion and monitor timeouts for signs of trouble the way miners watch their canaries. 

Before discussing how TCP reacts to congestion, let us first describe what it does to try to prevent congestion from occurring in the first place. When a connection is established, a suitable window size has to be chosen. The receiver can specify a window based on its buffer size. If the sender sticks to this window size, problems will not occur due to buffer overflow at the receiving end, but they may still occur due to internal congestion within the network. 

In 

Fig. 6-36

, we see this problem illustrated hydraulically. In 

Fig. 6-36(a)

, we see a thick pipe leading to a small-capacity receiver. As long as the sender does not send more water than the bucket can contain, no water will be lost. In 

Fig. 6-36(b)

, the limiting factor is not the bucket capacity, but the internal carrying capacity of the network. If too much water comes in too fast, it will back up and some will be lost (in this case by overflowing the funnel). 

Figure 6-36. (a) A fast network feeding a low-capacity receiver. (b) A slow network feeding a high-capacity receiver. 

 

419




The Internet solution is to realize that two potential problems exist—network capacity and receiver capacity—and to deal with each of them separately. To do so, each sender maintains two windows: the window the receiver has granted and a second window, the 

congestion window

. Each reflects the number of bytes the sender may transmit. The number of bytes that may be sent is the minimum of the two windows. Thus, the effective window is the minimum of what the sender thinks is all right and what the receiver thinks is all right. If the receiver says ''Send 8 KB'' but the sender knows that bursts of more than 4 KB clog the network, it sends 4 KB. On the other hand, if the receiver says ''Send 8 KB'' and the sender knows that bursts of up to 32 KB get through effortlessly, it sends the full 8 KB requested. 

When a connection is established, the sender initializes the congestion window to the size of the maximum segment in use on the connection. It then sends one maximum segment. If this segment is acknowledged before the timer goes off, it adds one segment's worth of bytes to the congestion window to make it two maximum size segments and sends two segments. As each of these segments is acknowledged, the congestion window is increased by one maximum segment size. When the congestion window is 

n

 segments, if all 

n

 are acknowledged on time, the congestion window is increased by the byte count corresponding to 

n

 segments. In effect, each burst acknowledged doubles the congestion window. 

The congestion window keeps growing exponentially until either a timeout occurs or the receiver's window is reached. The idea is that if bursts of size, say, 1024, 2048, and 4096 bytes work fine but a burst of 8192 bytes gives a timeout, the congestion window should be set to 4096 to avoid congestion. As long as the congestion window remains at 4096, no bursts longer than that will be sent, no matter how much window space the receiver grants. This algorithm is called 

slow start

, but it is not slow at all (Jacobson, 1988). It is exponential. All TCP implementations are required to support it. 

Now let us look at the Internet congestion control algorithm. It uses a third parameter, the 

threshold

, initially 64 KB, in addition to the receiver and congestion windows. When a timeout occurs, the threshold is set to half of the current congestion window, and the congestion window is reset to one maximum segment. Slow start is then used to determine what the network can handle, except that exponential growth stops when the threshold is hit. From that point on, successful transmissions grow the congestion window linearly (by one maximum segment for each burst) instead of one per segment. In effect, this algorithm is guessing that it is probably acceptable to cut the congestion window in half, and then it gradually works its way up from there. 

As an illustration of how the congestion algorithm works, see 

Fig. 6-37

. The maximum segment size here is 1024 bytes. Initially, the congestion window was 64 KB, but a timeout occurred, so the threshold is set to 32 KB and the congestion window to 1 KB for transmission 0 here. The congestion window then grows exponentially until it hits the threshold (32 KB). Starting then, it grows linearly. 

Figure 6-37. An example of the Internet congestion algorithm. 

420




 

Transmission 13 is unlucky (it should have known) and a timeout occurs. The threshold is set to half the current window (by now 40 KB, so half is 20 KB), and slow start is initiated all over again. When the acknowledgements from transmission 14 start coming in, the first four each double the congestion window, but after that, growth becomes linear again. 

If no more timeouts occur, the congestion window will continue to grow up to the size of the receiver's window. At that point, it will stop growing and remain constant as long as there are no more timeouts and the receiver's window does not change size. As an aside, if an ICMP SOURCE QUENCH packet comes in and is passed to TCP, this event is treated the same way as a timeout. An alternative (and more recent approach) is described in RFC 3168. 

6.5.10 TCP Timer Management 

TCP uses multiple timers (at least conceptually) to do its work. The most important of these is the 

retransmission timer

. When a segment is sent, a retransmission timer is started. If the segment is acknowledged before the timer expires, the timer is stopped. If, on the other hand, the timer goes off before the acknowledgement comes in, the segment is retransmitted (and the timer started again). The question that arises is: How long should the timeout interval be? 

This problem is much more difficult in the Internet transport layer than in the generic data link protocols of 

Chap. 3

. In the latter case, the expected delay is highly predictable (i.e., has a low variance), so the timer can be set to go off just slightly after the acknowledgement is expected, as shown in 

Fig. 6-38(a)

. Since acknowledgements are rarely delayed in the data link layer (due to lack of congestion), the absence of an acknowledgement at the expected time generally means either the frame or the acknowledgement has been lost. 

Figure 6-38. (a) Probability density of acknowledgement arrival times in the data link layer. (b) Probability density of acknowledgement arrival times for TCP. 

421




 

TCP is faced with a radically different environment. The probability density function for the time it takes for a TCP acknowledgement to come back looks more like 

Fig. 6-38(b)

 than 

Fig. 

6-38(a)

. Determining the round-trip time to the destination is tricky. Even when it is known, deciding on the timeout interval is also difficult. If the timeout is set too short, say, 

T

 

1

 in 

Fig. 

6-38(b)

, unnecessary retransmissions will occur, clogging the Internet with useless packets. If it is set too long, (e.g., 

T

 

2

), performance will suffer due to the long retransmission delay whenever a packet is lost. Furthermore, the mean and variance of the acknowledgement arrival distribution can change rapidly within a few seconds as congestion builds up or is resolved. 

The solution is to use a highly dynamic algorithm that constantly adjusts the timeout interval, based on continuous measurements of network performance. The algorithm generally used by TCP is due to Jacobson (1988) and works as follows. For each connection, TCP maintains a variable, 

RTT

, that is the best current estimate of the round-trip time to the destination in question. When a segment is sent, a timer is started, both to see how long the acknowledgement takes and to trigger a retransmission if it takes too long. If the acknowledgement gets back before the timer expires, TCP measures how long the acknowledgement took, say, 

M.

 It then updates 

RTT

 according to the formula 

 

 

where a is a smoothing factor that determines how much weight is given to the old value. Typically a = 7/8. 

Even given a good value of 

RTT

, choosing a suitable retransmission timeout is a nontrivial matter. Normally, TCP uses ß

RTT

, but the trick is choosing ß. In the initial implementations, ß was always 2, but experience showed that a constant value was inflexible because it failed to respond when the variance went up. 

In 1988, Jacobson proposed making ß roughly proportional to the standard deviation of the acknowledgement arrival time probability density function so that a large variance means a large ß, and vice versa. In particular, he suggested using the 

mean deviation

 as a cheap estimator of the 

standard deviation

. His algorithm requires keeping track of another smoothed variable, 

D

, the deviation. Whenever an acknowledgement comes in, the difference between the expected and observed values, | 

RTT

 - 

M

 |, is computed. A smoothed value of this is maintained in 

D

 by the formula 

 

422




 

where a may or may not be the same value used to smooth 

RTT

. While 

D

 is not exactly the same as the standard deviation, it is good enough and Jacobson showed how it could be computed using only integer adds, subtracts, and shifts—a big plus. Most TCP implementations now use this algorithm and set the timeout interval to 

 

 

The choice of the factor 4 is somewhat arbitrary, but it has two advantages. First, multiplication by 4 can be done with a single shift. Second, it minimizes unnecessary timeouts and retransmissions because less than 1 percent of all packets come in more than four standard deviations late. (Actually, Jacobson initially said to use 2, but later work has shown that 4 gives better performance.) 

One problem that occurs with the dynamic estimation of 

RTT

 is what to do when a segment times out and is sent again. When the acknowledgement comes in, it is unclear whether the acknowledgement refers to the first transmission or a later one. Guessing wrong can seriously contaminate the estimate of 

RTT

. Phil Karn discovered this problem the hard way. He is an amateur radio enthusiast interested in transmitting TCP/IP packets by ham radio, a notoriously unreliable medium (on a good day, half the packets get through). He made a simple proposal: do not update 

RTT

 on any segments that have been retransmitted. Instead, the timeout is doubled on each failure until the segments get through the first time. This fix is called 

Karn's algorithm

. Most TCP implementations use it. 

The retransmission timer is not the only timer TCP uses. A second timer is the 

persistence timer

. It is designed to prevent the following deadlock. The receiver sends an acknowledgement with a window size of 0, telling the sender to wait. Later, the receiver updates the window, but the packet with the update is lost. Now both the sender and the receiver are waiting for each other to do something. When the persistence timer goes off, the sender transmits a probe to the receiver. The response to the probe gives the window size. If it is still zero, the persistence timer is set again and the cycle repeats. If it is nonzero, data can now be sent. 

A third timer that some implementations use is the 

keepalive timer

. When a connection has been idle for a long time, the keepalive timer may go off to cause one side to check whether the other side is still there. If it fails to respond, the connection is terminated. This feature is controversial because it adds overhead and may terminate an otherwise healthy connection due to a transient network partition. 

The last timer used on each TCP connection is the one used in the 

TIMED WAIT

 state while closing. It runs for twice the maximum packet lifetime to make sure that when a connection is closed, all packets created by it have died off. 

6.5.11 Wireless TCP and UDP 

In theory, transport protocols should be independent of the technology of the underlying network layer. In particular, TCP should not care whether IP is running over fiber or over radio. In practice, it does matter because most TCP implementations have been carefully optimized based on assumptions that are true for wired networks but that fail for wireless networks. Ignoring the properties of wireless transmission can lead to a TCP implementation that is logically correct but has horrendous performance. 

423




The principal problem is the congestion control algorithm. Nearly all TCP implementations nowadays assume that timeouts are caused by congestion, not by lost packets. Consequently, when a timer goes off, TCP slows down and sends less vigorously (e.g., Jacobson's slow start algorithm). The idea behind this approach is to reduce the network load and thus alleviate the congestion. 

Unfortunately, wireless transmission links are highly unreliable. They lose packets all the time. The proper approach to dealing with lost packets is to send them again, and as quickly as possible. Slowing down just makes matters worse. If, say, 20 percent of all packets are lost, then when the sender transmits 100 packets/sec, the throughput is 80 packets/sec. If the sender slows down to 50 packets/sec, the throughput drops to 40 packets/sec. 

In effect, when a packet is lost on a wired network, the sender should slow down. When one is lost on a wireless network, the sender should try harder. When the sender does not know what the network is, it is difficult to make the correct decision. 

Frequently, the path from sender to receiver is heterogeneous. The first 1000 km might be over a wired network, but the last 1 km might be wireless. Now making the correct decision on a timeout is even harder, since it matters where the problem occurred. A solution proposed by Bakne and Badrinath (1995), 

indirect TCP

, is to split the TCP connection into two separate connections, as shown in 

Fig. 6-39

. The first connection goes from the sender to the base station. The second one goes from the base station to the receiver. The base station simply copies packets between the connections in both directions. 

Figure 6-39. Splitting a TCP connection into two connections. 

 

The advantage of this scheme is that both connections are now homogeneous. Timeouts on the first connection can slow the sender down, whereas timeouts on the second one can speed it up. Other parameters can also be tuned separately for the two connections. The disadvantage of the scheme is that it violates the semantics of TCP. Since each part of the connection is a full TCP connection, the base station acknowledges each TCP segment in the usual way. Only now, receipt of an acknowledgement by the sender does not mean that the receiver got the segment, only that the base station got it. 

A different solution, due to Balakrishnan et al. (1995), does not break the semantics of TCP. It works by making several small modifications to the network layer code in the base station. One of the changes is the addition of a snooping agent that observes and caches TCP segments going out to the mobile host and acknowledgements coming back from it. When the snooping agent sees a TCP segment going out to the mobile host but does not see an acknowledgement coming back before its (relatively short) timer goes off, it just retransmits that segment, without telling the source that it is doing so. It also retransmits when it sees duplicate acknowledgements from the mobile host go by, invariably meaning that the mobile host has missed something. Duplicate acknowledgements are discarded on the spot, to avoid having the source misinterpret them as congestion. 

One disadvantage of this transparency, however, is that if the wireless link is very lossy, the source may time out waiting for an acknowledgement and invoke the congestion control algorithm. With indirect TCP, the congestion control algorithm will never be started unless there really is congestion in the wired part of the network. 

424




The Balakrishnan et al. paper also has a solution to the problem of lost segments originating at the mobile host. When the base station notices a gap in the inbound sequence numbers, it generates a request for a selective repeat of the missing bytes by using a TCP option. 

Using these fixes, the wireless link is made more reliable in both directions, without the source knowing about it and without changing the TCP semantics. 

While UDP does not suffer from the same problems as TCP, wireless communication also introduces difficulties for it. The main trouble is that programs use UDP expecting it to be highly reliable. They know that no guarantees are given, but they still expect it to be near perfect. In a wireless environment, UDP will be far from perfect. For programs that can recover from lost UDP messages but only at considerable cost, suddenly going from an environment where messages theoretically can be lost but rarely are, to one in which they are constantly being lost can result in a performance disaster. 

Wireless communication also affects areas other than just performance. For example, how does a mobile host find a local printer to connect to, rather than use its home printer? Somewhat related to this is how to get the WWW page for the local cell, even if its name is not known. Also, WWW page designers tend to assume lots of bandwidth is available. Putting a large logo on every page becomes counterproductive if it is going to take 10 sec to transmit over a slow wireless link every time the page is referenced, irritating the users no end. 

As wireless networking becomes more common, the problems of running TCP over it become more acute. Additional work in this area is reported in (Barakat et al., 2000; Ghani and Dixit, 1999; Huston, 2001; and Xylomenos et al., 2001). 

6.5.12 Transactional TCP 

Earlier in this chapter we looked at remote procedure call as a way to implement client-server systems. If both the request and reply are small enough to fit into single packets and the operation is idempotent, UDP can simply be used, However, if these conditions are not met, using UDP is less attractive. For example, if the reply can be quite large, then the pieces must be sequenced and a mechanism must be devised to retransmit lost pieces. In effect, the application is required to reinvent TCP. 

Clearly, that is unattractive, but using TCP itself is also unattractive. The problem is the efficiency. The normal sequence of packets for doing an RPC over TCP is shown in 

Fig. 6-40(a)

. Nine packets are required in the best case. 

Figure 6-40. (a) RPC using normal TCP. (b) RPC using T/TCP. 

 

425




The nine packets are as follows: 

• 1. The client sends a 

SYN

 packet to establish a connection. 

• 2. The server sends an 

ACK

 packet to acknowledge the SYN packet. 

• 3. The client completes the three-way handshake. 

• 4. The client sends the actual request. 

• 5. The client sends a 

FIN

 packet to indicate that it is done sending. 

• 6. The server acknowledges the request and the 

FIN

. 

• 7. The server sends the reply back to the client. 

• 8. The server sends a FIN packet to indicate that it is also done. 

• 9. The client acknowledges the server's 

FIN

. 

Note that this is the best case. In the worst case, the client's request and 

FIN

 are acknowledged separately, as are the server's reply and 

FIN

. 

The question quickly arises of whether there is some way to combine the efficiency of RPC using UDP (just two messages) with the reliability of TCP. The answer is: Almost. It can be done with an experimental TCP variant called 

T/TCP

 (

Transactional TCP

), which is described in RFCs 1379 and 1644. 

The central idea here is to modify the standard connection setup sequence slightly to allow the transfer of data during setup. The T/TCP protocol is illustrated in 

Fig. 6-40(b)

. The client's first packet contains the 

SYN

 bit, the request itself, and the 

FIN

. In effect it says: I want to establish a connection, here is the data, and I am done. 

When the server gets the request, it looks up or computes the reply, and chooses how to respond. If the reply fits in one packet, it gives the reply of 

Fig. 6-40(b)

, which says: I acknowledge your 

FIN

, here is the answer, and I am done. The client then acknowledges the server's 

FIN

 and the protocol terminates in three messages. 

However, if the result is larger than 1 packet, the server also has the option of not turning on the 

FIN

 bit, in which case it can send multiple packets before closing its direction. 

It is probably worth mentioning that T/TCP is not the only proposed improvement to TCP. Another proposal is 

SCTP

 (

Stream Control Transmission Protocol

). Its features include message boundary preservation, multiple delivery modes (e.g., unordered delivery), multihoming (backup destinations), and selective acknowledgements (Stewart and Metz, 2001). However, whenever someone proposes changing something that has worked so well for so long, there is always a huge battle between the ''Users are demanding more features'' and ''If it ain't broken, don't fix it'' camps. 

6.6 Performance Issues 

Performance issues are very important in computer networks. When hundreds or thousands of computers are interconnected, complex interactions, with unforeseen consequences, are common. Frequently, this complexity leads to poor performance and no one knows why. In the following sections, we will examine many issues related to network performance to see what kinds of problems exist and what can be done about them. 

Unfortunately, understanding network performance is more an art than a science. There is little underlying theory that is actually of any use in practice. The best we can do is give rules of thumb gained from hard experience and present examples taken from the real world. We have intentionally delayed this discussion until we studied the transport layer in TCP in order to be able to use TCP as an example in various places. 

426




The transport layer is not the only place performance issues arise. We saw some of them in the network layer in the previous chapter. Nevertheless, the network layer tends to be largely concerned with routing and congestion control. The broader, system-oriented issues tend to be transport related, so this chapter is an appropriate place to examine them. 

In the next five sections, we will look at five aspects of network performance: 

1. Performance problems. 

2. Measuring network performance. 

3. System design for better performance. 

4. Fast TPDU processing. 

5. Protocols for future high-performance networks. 

As an aside, we need a generic name for the units exchanged by transport entities. The TCP term, segment, is confusing at best and is never used outside the TCP world in this context. The ATM terms (CS-PDU, SAR-PDU, and CPCS-PDU) are specific to ATM. Packets clearly refer to the network layer, and messages belong to the application layer. For lack of a standard term, we will go back to calling the units exchanged by transport entities TPDUs. When we mean both TPDU and packet together, we will use packet as the collective term, as in ''The CPU must be fast enough to process incoming packets in real time.'' By this we mean both the network layer packet and the TPDU encapsulated in it. 

6.6.1 Performance Problems in Computer Networks 

Some performance problems, such as congestion, are caused by temporary resource overloads. If more traffic suddenly arrives at a router than the router can handle, congestion will build up and performance will suffer. We studied congestion in detail in the previous chapter. 

Performance also degrades when there is a structural resource imbalance. For example, if a gigabit communication line is attached to a low-end PC, the poor CPU will not be able to process the incoming packets fast enough and some will be lost. These packets will eventually be retransmitted, adding delay, wasting bandwidth, and generally reducing performance. 

Overloads can also be synchronously triggered. For example, if a TPDU contains a bad parameter (e.g., the port for which it is destined), in many cases the receiver will thoughtfully send back an error notification. Now consider what could happen if a bad TPDU is broadcast to 10,000 machines: each one might send back an error message. The resulting 

broadcast storm

 could cripple the network. UDP suffered from this problem until the protocol was changed to cause hosts to refrain from responding to errors in UDP TPDUs sent to broadcast addresses. 

A second example of synchronous overload is what happens after an electrical power failure. When the power comes back on, all the machines simultaneously jump to their ROMs to start rebooting. A typical reboot sequence might require first going to some (DHCP) server to learn one's true identity, and then to some file server to get a copy of the operating system. If hundreds of machines all do this at once, the server will probably collapse under the load. 

Even in the absence of synchronous overloads and the presence of sufficient resources, poor performance can occur due to lack of system tuning. For example, if a machine has plenty of CPU power and memory but not enough of the memory has been allocated for buffer space, overruns will occur and TPDUs will be lost. Similarly, if the scheduling algorithm does not give a high enough priority to processing incoming TPDUs, some of them may be lost. 

Another tuning issue is setting timeouts correctly. When a TPDU is sent, a timer is typically set to guard against loss of the TPDU. If the timeout is set too short, unnecessary retransmissions 

427




will occur, clogging the wires. If the timeout is set too long, unnecessary delays will occur after a TPDU is lost. Other tunable parameters include how long to wait for data on which to piggyback before sending a separate acknowledgement, and how many retransmissions before giving up. 

Gigabit networks bring with them new performance problems. Consider, for example, sending a 64-KB burst of data from San Diego to Boston in order to fill the receiver's 64-KB buffer. Suppose that the link is 1 Gbps and the one-way speed-of-light-in-fiber delay is 20 msec. Initially, at 

t

 = 0, the pipe is empty, as illustrated in 

Fig. 6-41(a)

. Only 500 µsec later, in 

Fig. 

6-41(b)

, all the TPDUs are out on the fiber. The lead TPDU will now be somewhere in the vicinity of Brawley, still deep in Southern California. However, the transmitter must stop until it gets a window update. 

Figure 6-41. The state of transmitting one megabit from San Diego to Boston. (a) At 

t

 = 0. (b) After 500 µsec. (c) After 20 msec. (d) After 40 msec. 

 

After 20 msec, the lead TPDU hits Boston, as shown in 

Fig. 6-41(c)

 and is acknowledged. Finally, 40 msec after starting, the first acknowledgement gets back to the sender and the second burst can be transmitted. Since the transmission line was used for 0.5 msec out of 40, the efficiency is about 1.25 percent. This situation is typical of older protocols running over gigabit lines. 

A useful quantity to keep in mind when analyzing network performance is the 

bandwidth-delay product

. It is obtained by multiplying the bandwidth (in bits/sec) by the round-trip delay time (in sec). The product is the capacity of the pipe from the sender to the receiver and back (in bits). 

For the example of 

Fig. 6-41

 the bandwidth-delay product is 40 million bits. In other words, the sender would have to transmit a burst of 40 million bits to be able to keep going full speed until the first acknowledgement came back. It takes this many bits to fill the pipe (in both directions). This is why a burst of half a million bits only achieves a 1.25 percent efficiency: it is only 1.25 percent of the pipe's capacity. 

428




The conclusion that can be drawn here is that for good performance, the receiver's window must be at least as large as the bandwidth-delay product, preferably somewhat larger since the receiver may not respond instantly. For a transcontinental gigabit line, at least 5 megabytes are required. 

If the efficiency is terrible for sending a megabit, imagine what it is like for a short request of a few hundred bytes. Unless some other use can be found for the line while the first client is waiting for its reply, a gigabit line is no better than a megabit line, just more expensive. 

Another performance problem that occurs with time-critical applications like audio and video is jitter. Having a short mean transmission time is not enough. A small standard deviation is also required. Achieving a short mean transmission time along with a small standard deviation demands a serious engineering effort. 

6.6.2 Network Performance Measurement 

When a network performs poorly, its users often complain to the folks running it, demanding improvements. To improve the performance, the operators must first determine exactly what is going on. To find out what is really happening, the operators must make measurements. In this section we will look at network performance measurements. The discussion below is based on the work of Mogul (1993). 

The basic loop used to improve network performance contains the following steps: 

1. Measure the relevant network parameters and performance. 

2. Try to understand what is going on. 

3. Change one parameter. 

These steps are repeated until the performance is good enough or it is clear that the last drop of improvement has been squeezed out. 

Measurements can be made in many ways and at many locations (both physically and in the protocol stack). The most basic kind of measurement is to start a timer when beginning some activity and see how long that activity takes. For example, knowing how long it takes for a TPDU to be acknowledged is a key measurement. Other measurements are made with counters that record how often some event has happened (e.g., number of lost TPDUs). Finally, one is often interested in knowing the amount of something, such as the number of bytes processed in a certain time interval. 

Measuring network performance and parameters has many potential pitfalls. Below we list a few of them. Any systematic attempt to measure network performance should be careful to avoid these. 

Make Sure That the Sample Size Is Large Enough 

Do not measure the time to send one TPDU, but repeat the measurement, say, one million times and take the average. Having a large sample will reduce the uncertainty in the measured mean and standard deviation. This uncertainty can be computed using standard statistical formulas. 

Make Sure That the Samples Are Representative 

Ideally, the whole sequence of one million measurements should be repeated at different times of the day and the week to see the effect of different system loads on the measured quantity. Measurements of congestion, for example, are of little use if they are made at a moment when 

429




there is no congestion. Sometimes the results may be counterintuitive at first, such as heavy congestion at 10, 11, 1, and 2 o'clock, but no congestion at noon (when all the users are away at lunch). 

Be Careful When Using a Coarse-Grained Clock 

Computer clocks work by incrementing some counter at regular intervals. For example, a millisecond timer adds 1 to a counter every 1 msec. Using such a timer to measure an event that takes less than 1 msec is possible, but requires some care. (Some computers have more accurate clocks, of course.) 

To measure the time to send a TPDU, for example, the system clock (say, in milliseconds) should be read out when the transport layer code is entered and again when it is exited. If the true TPDU send time is 300 µsec, the difference between the two readings will be either 0 or 1, both wrong. However, if the measurement is repeated one million times and the total of all measurements added up and divided by one million, the mean time will be accurate to better than 1 µsec. 

Be Sure That Nothing Unexpected Is Going On during Your Tests 

Making measurements on a university system the day some major lab project has to be turned in may give different results than if made the next day. Likewise, if some researcher has decided to run a video conference over your network during your tests, you may get a biased result. It is best to run tests on an idle system and create the entire workload yourself. Even this approach has pitfalls though. While you might think nobody will be using the network at 3 A.M., that might be precisely when the automatic backup program begins copying all the disks to tape. Furthermore, there might be heavy traffic for your wonderful World Wide Web pages from distant time zones. 

Caching Can Wreak Havoc with Measurements 

The obvious way to measure file transfer times is to open a large file, read the whole thing, close it, and see how long it takes. Then repeat the measurement many more times to get a good average. The trouble is, the system may cache the file, so only the first measurement actually involves network traffic. The rest are just reads from the local cache. The results from such a measurement are essentially worthless (unless you want to measure cache performance). 

Often you can get around caching by simply overflowing the cache. For example, if the cache is 10 MB, the test loop could open, read, and close two 10-MB files on each pass, in an attempt to force the cache hit rate to 0. Still, caution is advised unless you are absolutely sure you understand the caching algorithm. 

Buffering can have a similar effect. One popular TCP/IP performance utility program has been known to report that UDP can achieve a performance substantially higher than the physical line allows. How does this occur? A call to UDP normally returns control as soon as the message has been accepted by the kernel and added to the transmission queue. If there is sufficient buffer space, timing 1000 UDP calls does not mean that all the data have been sent. Most of them may still be in the kernel, but the performance utility thinks they have all been transmitted. 

Understand What You Are Measuring 

When you measure the time to read a remote file, your measurements depend on the network, the operating systems on both the client and server, the particular hardware interface boards 

430




used, their drivers, and other factors. If the measurements are done carefully, you will ultimately discover the file transfer time for the configuration you are using. If your goal is to tune this particular configuration, these measurements are fine. 

However, if you are making similar measurements on three different systems in order to choose which network interface board to buy, your results could be thrown off completely by the fact that one of the network drivers is truly awful and is only getting 10 percent of the performance of the board. 

Be Careful about Extrapolating the Results 

Suppose that you make measurements of something with simulated network loads running from 0 (idle) to 0.4 (40 percent of capacity), as shown by the data points and solid line through them in 

Fig. 6-42

. It may be tempting to extrapolate linearly, as shown by the dotted line. However, many queueing results involve a factor of 1/(1 - ?), where ? is the load, so the true values may look more like the dashed line, which rises much faster than linearly. 

Figure 6-42. Response as a function of load. 

 

6.6.3 System Design for Better Performance 

Measuring and tinkering can often improve performance considerably, but they cannot substitute for good design in the first place. A poorly-designed network can be improved only so much. Beyond that, it has to be redesigned from scratch. 

In this section, we will present some rules of thumb based on hard experience with many networks. These rules relate to system design, not just network design, since the software and operating system are often more important than the routers and interface boards. Most of these ideas have been common knowledge to network designers for years and have been passed on from generation to generation by word of mouth. They were first stated explicitly by Mogul (1993); our treatment largely follows his. Another relevant source is (Metcalfe, 1993). 

Rule #1: CPU Speed Is More Important Than Network Speed 

Long experience has shown that in nearly all networks, operating system and protocol overhead dominate actual time on the wire. For example, in theory, the minimum RPC time on an Ethernet is 102 µsec, corresponding to a minimum (64-byte) request followed by a 

431




minimum (64-byte) reply. In practice, overcoming the software overhead and getting the RPC time anywhere near there is a substantial achievement. 

Similarly, the biggest problem in running at 1 Gbps is getting the bits from the user's buffer out onto the fiber fast enough and having the receiving CPU process them as fast as they come in. In short, if you double the CPU speed, you often can come close to doubling the throughput. Doubling the network capacity often has no effect since the bottleneck is generally in the hosts. 

Rule #2: Reduce Packet Count to Reduce Software Overhead 

Processing a TPDU has a certain amount of overhead per TPDU (e.g., header processing) and a certain amount of processing per byte (e.g., doing the checksum). When 1 million bytes are being sent, the per-byte overhead is the same no matter what the TPDU size is. However, using 128-byte TPDUs means 32 times as much per-TPDU overhead as using 4-KB TPDUs. This overhead adds up fast. 

In addition to the TPDU overhead, there is overhead in the lower layers to consider. Each arriving packet causes an interrupt. On a modern pipelined processor, each interrupt breaks the CPU pipeline, interferes with the cache, requires a change to the memory management context, and forces a substantial number of CPU registers to be saved. An 

n

-fold reduction in TPDUs sent thus reduces the interrupt and packet overhead by a factor of 

n

. 

This observation argues for collecting a substantial amount of data before transmission in order to reduce interrupts at the other side. Nagle's algorithm and Clark's solution to the silly window syndrome are attempts to do precisely this. 

Rule #3: Minimize Context Switches 

Context switches (e.g., from kernel mode to user mode) are deadly. They have the same bad properties as interrupts, the worst being a long series of initial cache misses. Context switches can be reduced by having the library procedure that sends data do internal buffering until it has a substantial amount of them. Similarly, on the receiving side, small incoming TPDUs should be collected together and passed to the user in one fell swoop instead of individually, to minimize context switches. 

In the best case, an incoming packet causes a context switch from the current user to the kernel, and then a switch to the receiving process to give it the newly-arrived data. Unfortunately, with many operating systems, additional context switches happen. For example, if the network manager runs as a special process in user space, a packet arrival is likely to cause a context switch from the current user to the kernel, then another one from the kernel to the network manager, followed by another one back to the kernel, and finally one from the kernel to the receiving process. This sequence is shown in 

Fig. 6-43

. All these context switches on each packet are very wasteful of CPU time and will have a devastating effect on network performance. 

Figure 6-43. Four context switches to handle one packet with a user-space network manager. 

432




 

Rule #4: Minimize Copying 

Even worse than multiple context switches are multiple copies. It is not unusual for an incoming packet to be copied three or four times before the TPDU enclosed in it is delivered. After a packet is received by the network interface in a special on-board hardware buffer, it is typically copied to a kernel buffer. From there it is copied to a network layer buffer, then to a transport layer buffer, and finally to the receiving application process. 

A clever operating system will copy a word at a time, but it is not unusual to require about five instructions per word (a load, a store, incrementing an index register, a test for end-of-data, and a conditional branch). Making three copies of each packet at five instructions per 32-bit word copied requires 15/4 or about four instructions per byte copied. On a 500-MIPS CPU, an instruction takes 2 nsec so each byte needs 8 nsec of processing time or about 1 nsec per bit, giving a maximum rate of about 1 Gbps. When overhead for header processing, interrupt handling, and context switches is factored in, 500 Mbps might be achievable, and we have not even considered the actual processing of the data. Clearly, handling a 10-Gbps Ethernet running at full blast is out of the question. 

In fact, probably a 500-Mbps line cannot be handled at full speed either. In the computation above, we have assumed that a 500-MIPS machine can execute any 500 million instructions/sec. In reality, machines can only run at such speeds if they are not referencing memory. Memory operations are often a factor of ten slower than register-register instructions (i.e., 20 nsec/instruction). If 20 percent of the instructions actually reference memory (i.e., are cache misses), which is likely when touching incoming packets, the average instruction execution time is 5.6 nsec (0.8 x 2 + 0.2 x 20). With four instructions/byte, we need 22.4 nsec/byte, or 2.8 nsec/bit), which gives about 357 Mbps. Factoring in 50 percent overhead gives us 178 Mbps. Note that hardware assistance will not help here. The problem is too much copying by the operating system. 

Rule #5: You Can Buy More Bandwidth but Not Lower Delay 

The next three rules deal with communication, rather than protocol processing. The first rule states that if you want more bandwidth, you can just buy it. Putting a second fiber next to the first one doubles the bandwidth but does nothing to reduce the delay. Making the delay shorter requires improving the protocol software, the operating system, or the network interface. Even if all of these improvements are made, the delay will not be reduced if the bottleneck is the transmission time. 

Rule #6: Avoiding Congestion Is Better Than Recovering from It 

The old maxim that an ounce of prevention is worth a pound of cure certainly holds for network congestion. When a network is congested, packets are lost, bandwidth is wasted, useless delays are introduced, and more. Recovering from congestion takes time and patience. Not having it occur in the first place is better. Congestion avoidance is like getting your DTP vaccination: it hurts a little at the time you get it, but it prevents something that would hurt a lot more in the future. 

433




Rule #7: Avoid Timeouts 

Timers are necessary in networks, but they should be used sparingly and timeouts should be minimized. When a timer goes off, some action is generally repeated. If it is truly necessary to repeat the action, so be it, but repeating it unnecessarily is wasteful. 

The way to avoid extra work is to be careful that timers are set a little bit on the conservative side. A timer that takes too long to expire adds a small amount of extra delay to one connection in the (unlikely) event of a TPDU being lost. A timer that goes off when it should not have uses up scarce CPU time, wastes bandwidth, and puts extra load on perhaps dozens of routers for no good reason. 

6.6.4 Fast TPDU Processing 

The moral of the story above is that the main obstacle to fast networking is protocol software. In this section we will look at some ways to speed up this software. For more information, see (Clark et al., 1989; and Chase et al., 2001). 

TPDU processing overhead has two components: overhead per TPDU and overhead per byte. Both must be attacked. The key to fast TPDU processing is to separate out the normal case (one-way data transfer) and handle it specially. Although a sequence of special TPDUs is needed to get into the 

ESTABLISHED

 state, once there, TPDU processing is straightforward until one side starts to close the connection. 

Let us begin by examining the sending side in the 

ESTABLISHED

 state when there are data to be transmitted. For the sake of clarity, we assume here that the transport entity is in the kernel, although the same ideas apply if it is a user-space process or a library inside the sending process. In 

Fig. 6-44

, the sending process traps into the kernel to do the SEND. The first thing the transport entity does is test to see if this is the normal case: the state is 

ESTABLISHED

, neither side is trying to close the connection, a regular (i.e., not an out-of-band) full TPDU is being sent, and enough window space is available at the receiver. If all conditions are met, no further tests are needed and the fast path through the sending transport entity can be taken. Typically, this path is taken most of the time. 

Figure 6-44. The fast path from sender to receiver is shown with a heavy line. The processing steps on this path are shaded. 

 

In the usual case, the headers of consecutive data TPDUs are almost the same. To take advantage of this fact, a prototype header is stored within the transport entity. At the start of the fast path, it is copied as fast as possible to a scratch buffer, word by word. Those fields that change from TPDU to TPDU are then overwritten in the buffer. Frequently, these fields are 

434




easily derived from state variables, such as the next sequence number. A pointer to the full TPDU header plus a pointer to the user data are then passed to the network layer. Here the same strategy can be followed (not shown in 

Fig. 6-44

). Finally, the network layer gives the resulting packet to the data link layer for transmission. 

As an example of how this principle works in practice, let us consider TCP/IP. 

Fig. 6-45(a)

 shows the TCP header. The fields that are the same between consecutive TPDUs on a one-way flow are shaded. All the sending transport entity has to do is copy the five words from the prototype header into the output buffer, fill in the next sequence number (by copying it from a word in memory), compute the checksum, and increment the sequence number in memory. It can then hand the header and data to a special IP procedure for sending a regular, maximum TPDU. IP then copies its five-word prototype header [see 

Fig. 6-45(b)

] into the buffer, fills in the 

Identification

 field, and computes its checksum. The packet is now ready for transmission. 

Figure 6-45. (a) TCP header. (b) IP header. In both cases, the shaded fields are taken from the prototype without change. 

 

Now let us look at fast path processing on the receiving side of 

Fig. 6-44

. Step 1 is locating the connection record for the incoming TPDU. For TCP, the connection record can be stored in a hash table for which some simple function of the two IP addresses and two ports is the key. Once the connection record has been located, both addresses and both ports must be compared to verify that the correct record has been found. 

An optimization that often speeds up connection record lookup even more is to maintain a pointer to the last one used and try that one first. Clark et al. (1989) tried this and observed a hit rate exceeding 90 percent. Other lookup heuristics are described in (McKenney and Dove, 1992). 

The TPDU is then checked to see if it is a normal one: the state is 

ESTABLISHED

, neither side is trying to close the connection, the TPDU is a full one, no special flags are set, and the sequence number is the one expected. These tests take just a handful of instructions. If all conditions are met, a special fast path TCP procedure is called. 

The fast path updates the connection record and copies the data to the user. While it is copying, it also computes the checksum, eliminating an extra pass over the data. If the checksum is correct, the connection record is updated and an acknowledgement is sent back. The general scheme of first making a quick check to see if the header is what is expected and then having a special procedure handle that case is called 

header prediction

. Many TCP implementations use it. When this optimization and all the other ones discussed in this chapter are used together, it is possible to get TCP to run at 90 percent of the speed of a local memory-to-memory copy, assuming the network itself is fast enough. 

Two other areas where major performance gains are possible are buffer management and timer management. The issue in buffer management is avoiding unnecessary copying, as mentioned above. Timer management is important because nearly all timers set do not expire. They are set to guard against TPDU loss, but most TPDUs arrive correctly and their 

435




acknowledgements also arrive correctly. Hence, it is important to optimize timer management for the case of timers rarely expiring. 

A common scheme is to use a linked list of timer events sorted by expiration time. The head entry contains a counter telling how many ticks away from expiry it is. Each successive entry contains a counter telling how many ticks after the previous entry it is. Thus, if timers expire in 3, 10, and 12 ticks, respectively, the three counters are 3, 7, and 2, respectively. 

At every clock tick, the counter in the head entry is decremented. When it hits zero, its event is processed and the next item on the list becomes the head. Its counter does not have to be changed. In this scheme, inserting and deleting timers are expensive operations, with execution times proportional to the length of the list. 

A more efficient approach can be used if the maximum timer interval is bounded and known in advance. Here an array, called a 

timing wheel

, can be used, as shown in 

Fig. 6-46

. Each slot corresponds to one clock tick. The current time shown is 

T

 = 4. Timers are scheduled to expire at 3, 10, and 12 ticks from now. If a new timer suddenly is set to expire in seven ticks, an entry is just made in slot 11. Similarly, if the timer set for 

T

 + 10 has to be canceled, the list starting in slot 14 has to be searched and the required entry removed. Note that the array of 

Fig. 6-46

 cannot accommodate timers beyond 

T

 + 15. 

Figure 6-46. A timing wheel. 

 

When the clock ticks, the current time pointer is advanced by one slot (circularly). If the entry now pointed to is nonzero, all of its timers are processed. Many variations on the basic idea are discussed in (Varghese and Lauck, 1987). 

6.6.5 Protocols for Gigabit Networks 

At the start of the 1990s, gigabit networks began to appear. People's first reaction was to use the old protocols on them, but various problems quickly arose. In this section we will discuss some of these problems and the directions new protocols are taking to solve them as we move toward ever faster networks. 

The first problem is that many protocols use 32-bit sequence numbers. When the Internet began, the lines between routers were mostly 56-kbps leased lines, so a host blasting away at full speed took over 1 week to cycle through the sequence numbers. To the TCP designers, 2

32

 was a pretty decent approximation of infinity because there was little danger of old packets 

436




still being around a week after they were transmitted. With 10-Mbps Ethernet, the wrap time became 57 minutes, much shorter, but still manageable. With a 1-Gbps Ethernet pouring data out onto the Internet, the wrap time is about 34 seconds, well under the 120 sec maximum packet lifetime on the Internet. All of a sudden, 2

32

 is not nearly as good an approximation to infinity since a sender can cycle through the sequence space while old packets still exist. RFC 1323 provides an escape hatch, though. 

The problem is that many protocol designers simply assumed, without stating it, that the time to use up the entire sequence space would greatly exceed the maximum packet lifetime. Consequently, there was no need to even worry about the problem of old duplicates still existing when the sequence numbers wrapped around. At gigabit speeds, that unstated assumption fails. 

A second problem is that communication speeds have improved much faster than computing speeds. (Note to computer engineers: Go out and beat those communication engineers! We are counting on you.) In the 1970s, the ARPANET ran at 56 kbps and had computers that ran at about 1 MIPS. Packets were 1008 bits, so the ARPANET was capable of delivering about 56 packets/sec. With almost 18 msec available per packet, a host could afford to spend 18,000 instructions processing a packet. Of course, doing so would soak up the entire CPU, but it could devote 9000 instructions per packet and still have half the CPU left to do real work. 

Compare these numbers to 1000-MIPS computers exchanging 1500-byte packets over a gigabit line. Packets can flow in at a rate of over 80,000 per second, so packet processing must be completed in 6.25 µsec if we want to reserve half the CPU for applications. In 6.25 µsec, a 1000-MIPS computer can execute 6250 instructions, only 1/3 of what the ARPANET hosts had available. Furthermore, modern RISC instructions do less per instruction than the old CISC instructions did, so the problem is even worse than it appears. The conclusion is this: there is less time available for protocol processing than there used to be, so protocols must become simpler. 

A third problem is that the go back n protocol performs poorly on lines with a large bandwidth-delay product. Consider, for example, a 4000-km line operating at 1 Gbps. The round-trip transmission time is 40 msec, in which time a sender can transmit 5 megabytes. If an error is detected, it will be 40 msec before the sender is told about it. If go back n is used, the sender will have to retransmit not just the bad packet, but also the 5 megabytes worth of packets that came afterward. Clearly, this is a massive waste of resources. 

A fourth problem is that gigabit lines are fundamentally different from megabit lines in that long gigabit lines are delay limited rather than bandwidth limited. In 

Fig. 6-47

 we show the time it takes to transfer a 1-megabit file 4000 km at various transmission speeds. At speeds up to 1 Mbps, the transmission time is dominated by the rate at which the bits can be sent. By 1 Gbps, the 40-msec roundtrip delay dominates the 1 msec it takes to put the bits on the fiber. Further increases in bandwidth have hardly any effect at all. 

Figure 6-47. Time to transfer and acknowledge a 1-megabit file over a 4000-km line. 

437




 

Figure 6-47

 has unfortunate implications for network protocols. It says that stop-and-wait protocols, such as RPC, have an inherent upper bound on their performance. This limit is dictated by the speed of light. No amount of technological progress in optics will ever improve matters (new laws of physics would help, though). 

A fifth problem that is worth mentioning is not a technological or protocol one like the others, but a result of new applications. Simply stated, it is that for many gigabit applications, such as multimedia, the variance in the packet arrival times is as important as the mean delay itself. A slow-but-uniform delivery rate is often preferable to a fast-but-jumpy one. 

Let us now turn from the problems to ways of dealing with them. We will first make some general remarks, then look at protocol mechanisms, packet layout, and protocol software. 

The basic principle that all gigabit network designers should learn by heart is: 

Design for speed, not for bandwidth optimization.

Old protocols were often designed to minimize the number of bits on the wire, frequently by using small fields and packing them together into bytes and words. Nowadays, there is plenty of bandwidth. Protocol processing is the problem, so protocols should be designed to minimize it. The IPv6 designers clearly understood this principle. 

A tempting way to go fast is to build fast network interfaces in hardware. The difficulty with this strategy is that unless the protocol is exceedingly simple, hardware just means a plug-in board with a second CPU and its own program. To make sure the network coprocessor is cheaper than the main CPU, it is often a slower chip. The consequence of this design is that much of the time the main (fast) CPU is idle waiting for the second (slow) CPU to do the critical work. It is a myth to think that the main CPU has other work to do while waiting. Furthermore, when two general-purpose CPUs communicate, race conditions can occur, so elaborate protocols are needed between the two processors to synchronize them correctly. Usually, the best approach is to make the protocols simple and have the main CPU do the work. 

Let us now look at the issue of feedback in high-speed protocols. Due to the (relatively) long delay loop, feedback should be avoided: it takes too long for the receiver to signal the sender. One example of feedback is governing the transmission rate by using a sliding window protocol. To avoid the (long) delays inherent in the receiver sending window updates to the sender, it is better to use a rate-based protocol. In such a protocol, the sender can send all it wants to, provided it does not send faster than some rate the sender and receiver have agreed upon in advance. 

438




A second example of feedback is Jacobson's slow start algorithm. This algorithm makes multiple probes to see how much the network can handle. With high-speed networks, making half a dozen or so small probes to see how the network responds wastes a huge amount of bandwidth. A more efficient scheme is to have the sender, receiver, and network all reserve the necessary resources at connection setup time. Reserving resources in advance also has the advantage of making it easier to reduce jitter. In short, going to high speeds inexorably pushes the design toward connection-oriented operation, or something fairly close to it. Of course, if bandwidth becomes so plentiful in the future that nobody cares about wasting lots of it, the design rules will become very different. 

Packet layout is an important consideration in gigabit networks. The header should contain as few fields as possible, to reduce processing time, and these fields should be big enough to do the job and be word aligned for ease of processing. In this context, ''big enough'' means that problems such as sequence numbers wrapping around while old packets still exist, receivers being unable to advertise enough window space because the window field is too small, and so on do not occur. 

The header and data should be separately checksummed, for two reasons. First, to make it possible to checksum the header but not the data. Second, to verify that the header is correct before copying the data into user space. It is desirable to do the data checksum at the time the data are copied to user space, but if the header is incorrect, the copy may go to the wrong process. To avoid an incorrect copy but to allow the data checksum to be done during copying, it is essential that the two checksums be separate. 

The maximum data size should be large, to permit efficient operation even in the face of long delays. Also, the larger the data block, the smaller the fraction of the total bandwidth devoted to headers. 1500 bytes is too small. 

Another valuable feature is the ability to send a normal amount of data along with the connection request. In this way, one round-trip time can be saved. 

Finally, a few words about the protocol software are appropriate. A key thought is concentrating on the successful case. Many older protocols tend to emphasize what to do when something goes wrong (e.g., a packet getting lost). To make the protocols run fast, the designer should aim for minimizing processing time when everything goes right. Minimizing processing time when an error occurs is secondary. 

A second software issue is minimizing copying time. As we saw earlier, copying data is often the main source of overhead. Ideally, the hardware should dump each incoming packet into memory as a contiguous block of data. The software should then copy this packet to the user buffer with a single block copy. Depending on how the cache works, it may even be desirable to avoid a copy loop. In other words, to copy 1024 words, the fastest way may be to have 1024 back-to-back 

MOVE

 instructions (or 1024 load-store pairs). The copy routine is so critical it should be carefully handcrafted in assembly code, unless there is a way to trick the compiler into producing precisely the optimal code. 

6.7 Summary 

The transport layer is the key to understanding layered protocols. It provides various services, the most important of which is an end-to-end, reliable, connection-oriented byte stream from sender to receiver. It is accessed through service primitives that permit the establishment, use, and release of connections. A common transport layer interface is the one provided by Berkeley sockets. 

Transport protocols must be able to do connection management over unreliable networks. Connection establishment is complicated by the existence of delayed duplicate packets that 

439




can reappear at inopportune moments. To deal with them, three-way handshakes are needed to establish connections. Releasing a connection is easier than establishing one but is still far from trivial due to the two-army problem. 

Even when the network layer is completely reliable, the transport layer has plenty of work to do. It must handle all the service primitives, manage connections and timers, and allocate and utilize credits. 

The Internet has two main transport protocols: UDP and TCP. UDP is a connectionless protocol that is mainly a wrapper for IP packets with the additional feature of multiplexing and demultiplexing multiple processes using a single IP address. UDP can be used for client-server interactions, for example, using RPC. It can also be used for building real-time protocols such as RTP. 

The main Internet transport protocol is TCP. It provides a reliable bidirectional byte stream. It uses a 20-byte header on all segments. Segments can be fragmented by routers within the Internet, so hosts must be prepared to do reassembly. A great deal of work has gone into optimizing TCP performance, using algorithms from Nagle, Clark, Jacobson, Karn, and others. Wireless links add a variety of complications to TCP. Transactional TCP is an extension to TCP that handles client-server interactions with a reduced number of packets. 

Network performance is typically dominated by protocol and TPDU processing overhead, and this situation gets worse at higher speeds. Protocols should be designed to minimize the number of TPDUs, context switches, and times each TPDU is copied. For gigabit networks, simple protocols are called for. 

Problems 

1. In our example transport primitives of 

Fig. 6-2

, LISTEN is a blocking call. Is this strictly necessary? If not, explain how a nonblocking primitive could be used. What advantage would this have over the scheme described in the text? 

2. In the model underlying 

Fig. 6-4

, it is assumed that packets may be lost by the network layer and thus must be individually acknowledged. Suppose that the network layer is 100 percent reliable and never loses packets. What changes, if any, are needed to 

Fig. 

6-4

? 

3. In both parts of 

Fig. 6-6

, there is a comment that the value of 

SERVER

_

PORT

 must be the same in both client and server. Why is this so important? 

4. Suppose that the clock-driven scheme for generating initial sequence numbers is used with a 15-bit wide clock counter. The clock ticks once every 100 msec, and the maximum packet lifetime is 60 sec. How often need resynchronization take place 

a. (a) in the worst case? 

b. (b) when the data consumes 240 sequence numbers/min? 

5. Why does the maximum packet lifetime, 

T

, have to be large enough to ensure that not only the packet but also its acknowledgements have vanished? 

6. Imagine that a two-way handshake rather than a three-way handshake were used to set up connections. In other words, the third message was not required. Are deadlocks now possible? Give an example or show that none exist. 

7. Imagine a generalized 

n

-army problem, in which the agreement of any two of the blue armies is sufficient for victory. Does a protocol exist that allows blue to win? 

8. Consider the problem of recovering from host crashes (i.e., 

Fig. 6-18

). If the interval between writing and sending an acknowledgement, or vice versa, can be made relatively small, what are the two best sender-receiver strategies for minimizing the chance of a protocol failure? 

9. Are deadlocks possible with the transport entity described in the text (

Fig. 6-20

)? 

10. Out of curiosity, the implementer of the transport entity of 

Fig. 6-20

 has decided to put counters inside the 

sleep

 procedure to collect statistics about the 

conn

 array. Among these are the number of connections in each of the seven possible states, 

n

i

 (

i

 = 1, ... 

440




,7). After writing a massive FORTRAN program to analyze the data, our implementer discovers that the relation S

n

i

 = 

MAX

_

CONN

 appears to always be true. Are there any other invariants involving only these seven variables? 

11. What happens when the user of the transport entity given in 

Fig. 6-20

 sends a zero-length message? Discuss the significance of your answer. 

12. For each event that can potentially occur in the transport entity of 

Fig. 6-20

, tell whether it is legal when the user is sleeping in 

sending

 state. 

13. Discuss the advantages and disadvantages of credits versus sliding window protocols. 

14. Why does UDP exist? Would it not have been enough to just let user processes send raw IP packets? 

15. Consider a simple application-level protocol built on top of UDP that allows a client to retrieve a file from a remote server residing at a well-known address. The client first sends a request with file name, and the server responds with a sequence of data packets containing different parts of the requested file. To ensure reliability and sequenced delivery, client and server use a stop-and-wait protocol. Ignoring the obvious performance issue, do you see a problem with this protocol? Think carefully about the possibility of processes crashing. 

16. A client sends a 128-byte request to a server located 100 km away over a 1-gigabit optical fiber. What is the efficiency of the line during the remote procedure call? 

17. Consider the situation of the previous problem again. Compute the minimum possible response time both for the given 1-Gbps line and for a 1-Mbps line. What conclusion can you draw? 

18. Both UDP and TCP use port numbers to identify the destination entity when delivering a message. Give two reasons for why these protocols invented a new abstract ID (port numbers), instead of using process IDs, which already existed when these protocols were designed. 

19. What is the total size of the minimum TCP MTU, including TCP and IP overhead but not including data link layer overhead? 

20. Datagram fragmentation and reassembly are handled by IP and are invisible to TCP. Does this mean that TCP does not have to worry about data arriving in the wrong order? 

21. RTP is used to transmit CD-quality audio, which makes a pair of 16-bit samples 44,100 times/sec, one sample for each of the stereo channels. How many packets per second must RTP transmit? 

22. Would it be possible to place the RTP code in the operating system kernel, along with the UDP code? Explain your answer. 

23. A process on host 1 has been assigned port 

p

, and a process on host 2 has been assigned port 

q

. Is it possible for there to be two or more TCP connections between these two ports at the same time? 

24. In 

Fig. 6-29

 we saw that in addition to the 32-bit 

Acknowledgement

 field, there is an 

ACK

 bit in the fourth word. Does this really add anything? Why or why not? 

25. The maximum payload of a TCP segment is 65,495 bytes. Why was such a strange number chosen? 

26. Describe two ways to get into the 

SYN RCVD

 state of 

Fig. 6-33

. 

27. Give a potential disadvantage when Nagle's algorithm is used on a badly-congested network. 

28. Consider the effect of using slow start on a line with a 10-msec round-trip time and no congestion. The receive window is 24 KB and the maximum segment size is 2 KB. How long does it take before the first full window can be sent? 

29. Suppose that the TCP congestion window is set to 18 KB and a timeout occurs. How big will the window be if the next four transmission bursts are all successful? Assume that the maximum segment size is 1 KB. 

30. If the TCP round-trip time, 

RTT

, is currently 30 msec and the following acknowledgements come in after 26, 32, and 24 msec, respectively, what is the new 

RTT

 estimate using the Jacobson algorithm? Use a = 0.9. 

31. A TCP machine is sending full windows of 65,535 bytes over a 1-Gbps channel that has a 10-msec one-way delay. What is the maximum throughput achievable? What is the line efficiency? 

441




32. What is the fastest line speed at which a host can blast out 1500-byte TCP payloads with a 120-sec maximum packet lifetime without having the sequence numbers wrap around? Take TCP, IP, and Ethernet overhead into consideration. Assume that Ethernet frames may be sent continuously. 

33. In a network that has a maximum TPDU size of 128 bytes, a maximum TPDU lifetime of 30 sec, and an 8-bit sequence number, what is the maximum data rate per connection? 

34. Suppose that you are measuring the time to receive a TPDU. When an interrupt occurs, you read out the system clock in milliseconds. When the TPDU is fully processed, you read out the clock again. You measure 0 msec 270,000 times and 1 msec 730,000 times. How long does it take to receive a TPDU? 

35. A CPU executes instructions at the rate of 1000 MIPS. Data can be copied 64 bits at a time, with each word copied costing 10 instructions. If an coming packet has to be copied four times, can this system handle a 1-Gbps line? For simplicity, assume that all instructions, even those instructions that read or write memory, run at the full 1000-MIPS rate. 

36. To get around the problem of sequence numbers wrapping around while old packets still exist, one could use 64-bit sequence numbers. However, theoretically, an optical fiber can run at 75 Tbps. What maximum packet lifetime is required to make sure that future 75 Tbps networks do not have wraparound problems even with 64-bit sequence numbers? Assume that each byte has its own sequence number, as TCP does. 

37. Give one advantage of RPC on UDP over transactional TCP. Give one advantage of T/TCP over RPC. 

38. In 

Fig. 6-40(a)

, we see that it takes 9 packets to complete the RPC. Are there any circumstances in which it takes exactly 10 packets? 

39. In 

Sec. 6.6.5

, we calculated that a gigabit line dumps 80,000 packets/sec on the host, giving it only 6250 instructions to process it and leaving half the CPU time for applications. This calculation assumed a 1500-byte packet. Redo the calculation for an ARPANET-sized packet (128 bytes). In both cases, assume that the packet sizes given include all overhead. 

40. For a 1-Gbps network operating over 4000 km, the delay is the limiting factor, not the bandwidth. Consider a MAN with the average source and destination 20 km apart. At what data rate does the round-trip delay due to the speed of light equal the transmission delay for a 1-KB packet? 

41. Calculate the bandwidth-delay product for the following networks: (1) T1 (1.5 Mbps), (2) Ethernet (10 Mbps), (3) T3 (45 Mbps), and (4) STS-3 (155 Mbps). Assume an RTT of 100 msec. Recall that a TCP header has 16 bits reserved for Window Size. What are its implications in light of your calculations? 

42. What is the bandwidth-delay product for a 50-Mbps channel on a geostationary satellite? If the packets are all 1500 bytes (including overhead), how big should the window be in packets? 

43. The file server of 

Fig. 6-6

 is far from perfect and could use a few improvements. Make the following modifications. 

a. (a) Give the client a third argument that specifies a byte range. 

b. (b) Add a client flag 

–w

 that allows the file to be written to the server. 

1. Modify the program of 

Fig. 6-20

 to do error recovery. Add a new packet type, 

reset

, that can arrive after a connection has been opened by both sides but closed by neither. This event, which happens simultaneously on both ends of the connection, means that any packets that were in transit have either been delivered or destroyed, but in either case are no longer in the subnet. 

2. Write a program that simulates buffer management in a transport entity, using a sliding window for flow control rather than the credit system of 

Fig. 6-20

. Let higher-layer processes randomly open connections, send data, and close connections. To keep it simple, have all the data travel from machine 

A

 to machine 

B

, and none the other way. Experiment with different buffer allocation strategies at 

B

, such as dedicating buffers to specific connections versus a common buffer pool, and measure the total throughput achieved by each one. 

442




3. Design and implement a chat system that allows multiple groups of users to chat. A chat coordinator resides at a well-known network address, uses UDP for communication with chat clients, sets up chat servers for each chat session, and maintains a chat session directory. There is one chat server per chat session. A chat server uses TCP for communication with clients. A chat client allows users to start, join, and leave a chat session. Design and implement the coordinator, server, and client code. 

 

443




Chapter 7. The Application Layer 

Having finished all the preliminaries, we now come to the layer where all the applications are found. The layers below the application layer are there to provide reliable transport, but they do not do real work for users. In this chapter we will study some real network applications. 

However, even in the application layer there is a need for support protocols, to allow the applications to function. Accordingly, we will look at one of these before starting with the applications themselves. The item in question is DNS, which handles naming within the Internet. After that, we will examine three real applications: electronic mail, the World Wide Web, and finally, multimedia. 

7.1 DNS—The Domain Name System 

Although programs theoretically could refer to hosts, mailboxes, and other resources by their network (e.g., IP) addresses, these addresses are hard for people to remember. Also, sending e-mail to 

tana@128.111.24.41

 means that if Tana's ISP or organization moves the mail server to a different machine with a different IP address, her e-mail address has to change. Consequently, ASCII names were introduced to decouple machine names from machine addresses. In this way, Tana's address might be something like 

tana@art.ucsb.edu

. Nevertheless, the network itself understands only numerical addresses, so some mechanism is required to convert the ASCII strings to network addresses. In the following sections we will study how this mapping is accomplished in the Internet. 

Way back in the ARPANET, there was simply a file, 

hosts.txt

, that listed all the hosts and their IP addresses. Every night, all the hosts would fetch it from the site at which it was maintained. For a network of a few hundred large timesharing machines, this approach worked reasonably well. 

However, when thousands of minicomputers and PCs were connected to the net, everyone realized that this approach could not continue to work forever. For one thing, the size of the file would become too large. However, even more important, host name conflicts would occur constantly unless names were centrally managed, something unthinkable in a huge international network due to the load and latency. To solve these problems, 

DNS

 (the 

Domain Name System

) was invented. 

The essence of DNS is the invention of a hierarchical, domain-based naming scheme and a distributed database system for implementing this naming scheme. It is primarily used for mapping host names and e-mail destinations to IP addresses but can also be used for other purposes. DNS is defined in RFCs 1034 and 1035. 

Very briefly, the way DNS is used is as follows. To map a name onto an IP address, an application program calls a library procedure called the 

resolver

, passing it the name as a parameter. We saw an example of a resolver, 

gethostbyname

, in 

Fig. 6-6

. The resolver sends a UDP packet to a local DNS server, which then looks up the name and returns the IP address to the resolver, which then returns it to the caller. Armed with the IP address, the program can then establish a TCP connection with the destination or send it UDP packets. 

7.1.1 The DNS Name Space 

Managing a large and constantly changing set of names is a nontrivial problem. In the postal system, name management is done by requiring letters to specify (implicitly or explicitly) the country, state or province, city, and street address of the addressee. By using this kind of hierarchical addressing, there is no confusion between the Marvin Anderson on Main St. in 

444




White Plains, N.Y. and the Marvin Anderson on Main St. in Austin, Texas. DNS works the same way. 

Conceptually, the Internet is divided into over 200 top-level 

domains

, where each domain covers many hosts. Each domain is partitioned into subdomains, and these are further partitioned, and so on. All these domains can be represented by a tree, as shown in 

Fig. 7-1

. The leaves of the tree represent domains that have no subdomains (but do contain machines, of course). A leaf domain may contain a single host, or it may represent a company and contain thousands of hosts. 

Figure 7-1. A portion of the Internet domain name space. 

 

The top-level domains come in two flavors: generic and countries. The original generic domains were 

com (commercial), edu

 (educational institutions), 

gov

 (the U.S. Federal Government), 

int

 (certain international organizations), 

mil

 (the U.S. armed forces), 

net

 (network providers), and 

org

 (nonprofit organizations). The country domains include one entry for every country, as defined in ISO 3166. 

In November 2000, ICANN approved four new, general-purpose, top-level domains, namely, 

biz

 (businesses), 

info

 (information), 

name

 (people's names), and 

pro

 (professions, such as doctors and lawyers). In addition, three more specialized top-level domains were introduced at the request of certain industries. These are 

aero

 (aerospace industry), 

coop

 (co-operatives), and 

museum

 (museums). Other top-level domains will be added in the future. 

As an aside, as the Internet becomes more commercial, it also becomes more contentious. Take 

pro

, for example. It was intended for certified professionals. But who is a professional? And certified by whom? Doctors and lawyers clearly are professionals. But what about freelance photographers, piano teachers, magicians, plumbers, barbers, exterminators, tattoo artists, mercenaries, and prostitutes? Are these occupations professional and thus eligible for 

pro

 domains? And if so, who certifies the individual practitioners? 

In general, getting a second-level domain, such as 

name-of-company

.com, is easy. It merely requires going to a registrar for the corresponding top-level domain (

com

 in this case) to check if the desired name is available and not somebody else's trademark. If there are no problems, the requester pays a small annual fee and gets the name. By now, virtually every common (English) word has been taken in the 

com

 domain. Try household articles, animals, plants, body parts, etc. Nearly all are taken. 

Each domain is named by the path upward from it to the (unnamed) root. The components are separated by periods (pronounced ''dot''). Thus, the engineering department at Sun Microsystems might be 

eng.sun.com.

, rather than a UNIX-style name such as 

/com/sun/eng.

 Notice that this hierarchical naming means that 

eng.sun.com.

 does not conflict with a potential use of 

eng

 in 

eng.yale.edu.

, which might be used by the Yale English department. 

445




Domain names can be either absolute or relative. An absolute domain name always ends with a period (e.g., 

eng.sun.com

.), whereas a relative one does not. Relative names have to be interpreted in some context to uniquely determine their true meaning. In both cases, a named domain refers to a specific node in the tree and all the nodes under it. 

Domain names are case insensitive, so 

edu

, 

Edu

, and 

EDU

 mean the same thing. Component names can be up to 63 characters long, and full path names must not exceed 255 characters. 

In principle, domains can be inserted into the tree in two different ways. For example, 

cs.yale.edu

 could equally well be listed under the 

us

 country domain as 

cs.yale.ct.us

. In practice, however, most organizations in the United States are under a generic domain, and most outside the United States are under the domain of their country. There is no rule against registering under two top-level domains, but few organizations except multinationals do it (e.g., 

sony.com

 and 

sony.nl

). 

Each domain controls how it allocates the domains under it. For example, Japan has domains 

ac.jp

 and 

co.jp

 that mirror 

edu

 and 

com

. The Netherlands does not make this distinction and puts all organizations directly under 

nl

. Thus, all three of the following are university computer science departments: 

1. 

cs.yale.edu

 (Yale University, in the United States) 

2. 

cs.vu.nl

 (Vrije Universiteit, in The Netherlands) 

3. 

cs.keio.ac.jp

 (Keio University, in Japan) 

To create a new domain, permission is required of the domain in which it will be included. For example, if a VLSI group is started at Yale and wants to be known as 

vlsi.cs.yale.edu

, it has to get permission from whoever manages 

cs.yale.edu

. Similarly, if a new university is chartered, say, the University of Northern South Dakota, it must ask the manager of the 

edu

 domain to assign it 

unsd.edu

. In this way, name conflicts are avoided and each domain can keep track of all its subdomains. Once a new domain has been created and registered, it can create subdomains, such as 

cs.unsd.edu

, without getting permission from anybody higher up the tree. 

Naming follows organizational boundaries, not physical networks. For example, if the computer science and electrical engineering departments are located in the same building and share the same LAN, they can nevertheless have distinct domains. Similarly, even if computer science is split over Babbage Hall and Turing Hall, the hosts in both buildings will normally belong to the same domain. 

7.1.2 Resource Records 

Every domain, whether it is a single host or a top-level domain, can have a set of 

resource records

 associated with it. For a single host, the most common resource record is just its IP address, but many other kinds of resource records also exist. When a resolver gives a domain name to DNS, what it gets back are the resource records associated with that name. Thus, the primary function of DNS is to map domain names onto resource records. 

A resource record is a five-tuple. Although they are encoded in binary for efficiency, in most expositions, resource records are presented as ASCII text, one line per resource record. The format we will use is as follows: 

Domain_name  Time_to_live  Class  Type  Value  

The 

Domain

_

name

 tells the domain to which this record applies. Normally, many records exist for each domain and each copy of the database holds information about multiple domains. This 

446




field is thus the primary search key used to satisfy queries. The order of the records in the database is not significant. 

The 

Time

_

to

_

live

 field gives an indication of how stable the record is. Information that is highly stable is assigned a large value, such as 86400 (the number of seconds in 1 day). Information that is highly volatile is assigned a small value, such as 60 (1 minute). We will come back to this point later when we have discussed caching. 

The third field of every resource record is the 

Class

. For Internet information, it is always 

IN

. For non-Internet information, other codes can be used, but in practice, these are rarely seen. 

The 

Type

 field tells what kind of record this is. The most important types are listed in 

Fig. 7-2

. 

Figure 7-2. . The principal DNS resource record types for IPv4. 

 

An 

SOA

 record provides the name of the primary source of information about the name server's zone (described below), the e-mail address of its administrator, a unique serial number, and various flags and timeouts. 

The most important record type is the 

A

 (Address) record. It holds a 32-bit IP address for some host. Every Internet host must have at least one IP address so that other machines can communicate with it. Some hosts have two or more network connections, in which case they will have one type 

A

 resource record per network connection (and thus per IP address). DNS can be configured to cycle through these, returning the first record on the first request, the second record on the second request, and so on. 

The next most important record type is the 

MX

 record. It specifies the name of the host prepared to accept e-mail for the specified domain. It is used because not every machine is prepared to accept e-mail. If someone wants to send e-mail to, for example, 

bill@microsoft.com

, the sending host needs to find a mail server at 

microsoft.com

 that is willing to accept e-mail. The 

MX

 record can provide this information. 

The 

NS

 records specify name servers. For example, every DNS database normally has an 

NS

 record for each of the top-level domains, so, for example, e-mail can be sent to distant parts of the naming tree. We will come back to this point later. 

CNAME

 records allow aliases to be created. For example, a person familiar with Internet naming in general and wanting to send a message to someone whose login name is 

paul

 in the computer science department at M.I.T. might guess that 

paul@cs.mit.edu

 will work. Actually, this address will not work, because the domain for M.I.T.'s computer science department is 

lcs.mit.edu

. However, as a service to people who do not know this, M.I.T. could create a 

CNAME

 entry to point people and programs in the right direction. An entry like this one might do the job: 

447




cs.mit.edu  86400  IN  CNAME  lcs.mit.edu  

Like 

CNAME

, 

PTR

 points to another name. However, unlike 

CNAME

, which is really just a macro definition, 

PTR

 is a regular DNS datatype whose interpretation depends on the context in which it is found. In practice, it is nearly always used to associate a name with an IP address to allow lookups of the IP address and return the name of the corresponding machine. These are called 

reverse lookups

. 

HINFO

 records allow people to find out what kind of machine and operating system a domain corresponds to. Finally, 

TXT

 records allow domains to identify themselves in arbitrary ways. Both of these record types are for user convenience. Neither is required, so programs cannot count on getting them (and probably cannot deal with them if they do get them). 

Finally, we have the 

Value

 field. This field can be a number, a domain name, or an ASCII string. The semantics depend on the record type. A short description of the 

Value

 fields for each of the principal record types is given in 

Fig. 7-2

. 

For an example of the kind of information one might find in the DNS database of a domain, see 

Fig. 7-3

. This figure depicts part of a (semihypothetical) database for the 

cs.vu.nl

 domain shown in 

Fig. 7-1

. The database contains seven types of resource records. 

Figure 7-3. A portion of a possible DNS database for 

cs.vu.nl

 

The first noncomment line of 

Fig. 7-3

 gives some basic information about the domain, which will not concern us further. The next two lines give textual information about where the domain is located. Then come two entries giving the first and second places to try to deliver e-mail sent to 

person@cs.vu.nl

. The 

zephyr

 (a specific machine) should be tried first. If that fails, the 

top

 should be tried as the next choice. 

After the blank line, added for readability, come lines telling that the 

flits

 is a Sun workstation running UNIX and giving both of its IP addresses. Then three choices are given for handling e-

448




mail sent to 

flits.cs.vu.nl

. First choice is naturally the 

flits

 itself, but if it is down, the 

zephyr

 and 

top

 are the second and third choices. Next comes an alias, 

www.cs.vu.nl

, so that this address can be used without designating a specific machine. Creating this alias allows 

cs.vu.nl

 to change its World Wide Web server without invalidating the address people use to get to it. A similar argument holds for 

ftp.cs.vu.nl

. 

The next four lines contain a typical entry for a workstation, in this case, 

rowboat.cs.vu.nl

. The information provided contains the IP address, the primary and secondary mail drops, and information about the machine. Then comes an entry for a non-UNIX system that is not capable of receiving mail itself, followed by an entry for a laser printer that is connected to the Internet. 

What are not shown (and are not in this file) are the IP addresses used to look up the top-level domains. These are needed to look up distant hosts, but since they are not part of the 

cs.vu.nl

 domain, they are not in this file. They are supplied by the root servers, whose IP addresses are present in a system configuration file and loaded into the DNS cache when the DNS server is booted. There are about a dozen root servers spread around the world, and each one knows the IP addresses of all the top-level domain servers. Thus, if a machine knows the IP address of at least one root server, it can look up any DNS name. 

7.1.3 Name Servers 

In theory at least, a single name server could contain the entire DNS database and respond to all queries about it. In practice, this server would be so overloaded as to be useless. Furthermore, if it ever went down, the entire Internet would be crippled. 

To avoid the problems associated with having only a single source of information, the DNS name space is divided into nonoverlapping 

zones

. One possible way to divide the name space of 

Fig. 7-1

 is shown in 

Fig. 7-4

. Each zone contains some part of the tree and also contains name servers holding the information about that zone. Normally, a zone will have one primary name server, which gets its information from a file on its disk, and one or more secondary name servers, which get their information from the primary name server. To improve reliability, some servers for a zone can be located outside the zone. 

Figure 7-4. Part of the DNS name space showing the division into zones. 

 

Where the zone boundaries are placed within a zone is up to that zone's administrator. This decision is made in large part based on how many name servers are desired, and where. For example, in 

Fig. 7-4

, Yale has a server for 

yale.edu

 that handles 

eng.yale.edu

 but not 

cs.yale.edu

, which is a separate zone with its own name servers. Such a decision might be made when a department such as English does not wish to run its own name server, but a 

449




department such as computer science does. Consequently, 

cs.yale.edu

 is a separate zone but 

eng.yale.edu

 is not. 

When a resolver has a query about a domain name, it passes the query to one of the local name servers. If the domain being sought falls under the jurisdiction of the name server, such as 

ai.cs.yale.edu

 falling under 

cs.yale.edu

, it returns the authoritative resource records. An 

authoritative record

 is one that comes from the authority that manages the record and is thus always correct. Authoritative records are in contrast to cached records, which may be out of date. 

If, however, the domain is remote and no information about the requested domain is available locally, the name server sends a query message to the top-level name server for the domain requested. To make this process clearer, consider the example of 

Fig. 7-5

. Here, a resolver on 

flits.cs.vu.nl

 wants to know the IP address of the host 

linda.cs.yale.edu

. In step 1, it sends a query to the local name server, 

cs.vu.nl

. This query contains the domain name sought, the type (

A

) and the class (

IN

). 

Figure 7-5. How a resolver looks up a remote name in eight steps. 

 

Let us suppose the local name server has never had a query for this domain before and knows nothing about it. It may ask a few other nearby name servers, but if none of them know, it sends a UDP packet to the server for 

edu

 given in its database (see 

Fig. 7-5

), 

edu-server.net

. It is unlikely that this server knows the address of 

linda.cs.yale.edu

, and probably does not know 

cs.yale.edu

 either, but it must know all of its own children, so it forwards the request to the name server for 

yale.edu

 (step 3). In turn, this one forwards the request to 

cs.yale.edu

 (step 4), which must have the authoritative resource records. Since each request is from a client to a server, the resource record requested works its way back in steps 5 through 8. 

Once these records get back to the 

cs.vu.nl

 name server, they will be entered into a cache there, in case they are needed later. However, this information is not authoritative, since changes made at 

cs.yale.edu

 will not be propagated to all the caches in the world that may know about it. For this reason, cache entries should not live too long. This is the reason that the 

Time_to_live

 field is included in each resource record. It tells remote name servers how long to cache records. If a certain machine has had the same IP address for years, it may be safe to cache that information for 1 day. For more volatile information, it might be safer to purge the records after a few seconds or a minute. 

It is worth mentioning that the query method described here is known as a 

recursive query

, since each server that does not have the requested information goes and finds it somewhere, then reports back. An alternative form is also possible. In this form, when a query cannot be satisfied locally, the query fails, but the name of the next server along the line to try is returned. Some servers do not implement recursive queries and always return the name of the next server to try. 

It is also worth pointing out that when a DNS client fails to get a response before its timer goes off, it normally will try another server next time. The assumption here is that the server is probably down, rather than that the request or reply got lost. 

While DNS is extremely important to the correct functioning of the Internet, all it really does is map symbolic names for machines onto their IP addresses. It does not help locate people, resources, services, or objects in general. For locating these things, another directory service 

450




has been defined, called 

LDAP

 (

Lightweight Directory Access Protocol

). It is a simplified version of the OSI X.500 directory service and is described in RFC 2251. It organizes information as a tree and allows searches on different components. It can be regarded as a ''white pages'' telephone book. We will not discuss it further in this book, but for more information see (Weltman and Dahbura, 2000). 

7.2 Electronic Mail 

Electronic mail, or 

e-mail

, as it is known to its many fans, has been around for over two decades. Before 1990, it was mostly used in academia. During the 1990s, it became known to the public at large and grew exponentially to the point where the number of e-mails sent per day now is vastly more than the number of 

snail mail

 (i.e., paper) letters. 

E-mail, like most other forms of communication, has its own conventions and styles. In particular, it is very informal and has a low threshold of use. People who would never dream of calling up or even writing a letter to a Very Important Person do not hesitate for a second to send a sloppily-written e-mail. 

E-mail is full of jargon such as BTW (By The Way), ROTFL (Rolling On The Floor Laughing), and IMHO (In My Humble Opinion). Many people also use little ASCII symbols called 

smileys

 or 

emoticons

 in their e-mail. A few of the more interesting ones are reproduced in 

Fig. 7-6

. For most, rotating the book 90 degrees clockwise will make them clearer. For a minibook giving over 650 smileys, see (Sanderson and Dougherty, 1993). 

Figure 7-6. Some smileys. They will not be on the final exam :-) 

 

The first e-mail systems simply consisted of file transfer protocols, with the convention that the first line of each message (i.e., file) contained the recipient's address. As time went on, the limitations of this approach became more obvious. 

Some of the complaints were as follows: 

1. Sending a message to a group of people was inconvenient. Managers often need this facility to send memos to all their subordinates. 

2. Messages had no internal structure, making computer processing difficult. For example, if a forwarded message was included in the body of another message, extracting the forwarded part from the received message was difficult. 

3. The originator (sender) never knew if a message arrived or not. 

4. If someone was planning to be away on business for several weeks and wanted all incoming e-mail to be handled by his secretary, this was not easy to arrange. 

5. The user interface was poorly integrated with the transmission system requiring users first to edit a file, then leave the editor and invoke the file transfer program. 

6. It was not possible to create and send messages containing a mixture of text, drawings, facsimile, and voice. 

451




As experience was gained, more elaborate e-mail systems were proposed. In 1982, the ARPANET e-mail proposals were published as RFC 821 (transmission protocol) and RFC 822 (message format). Minor revisions, RFC 2821 and RFC 2822, have become Internet standards, but everyone still refers to Internet e-mail as RFC 822. 

In 1984, CCITT drafted its X.400 recommendation. After two decades of competition, e-mail systems based on RFC 822 are widely used, whereas those based on X.400 have disappeared. How a system hacked together by a handful of computer science graduate students beat an official international standard strongly backed by all the PTTs in the world, many governments, and a substantial part of the computer industry brings to mind the Biblical story of David and Goliath. 

The reason for RFC 822's success is not that it is so good, but that X.400 was so poorly designed and so complex that nobody could implement it well. Given a choice between a simple-minded, but working, RFC 822-based e-mail system and a supposedly truly wonderful, but nonworking, X.400 e-mail system, most organizations chose the former. Perhaps there is a lesson lurking in there somewhere. Consequently, our discussion of e-mail will focus on the Internet e-mail system. 

7.2.1 Architecture and Services 

In this section we will provide an overview of what e-mail systems can do and how they are organized. They normally consist of two subsystems: the 

user agents

, which allow people to read and send e-mail, and the 

message transfer agents

, which move the messages from the source to the destination. The user agents are local programs that provide a command-based, menu-based, or graphical method for interacting with the e-mail system. The message transfer agents are typically system 

daemons

, that is, processes that run in the background. Their job is to move e-mail through the system. 

Typically, e-mail systems support five basic functions. Let us take a look at them. 

Composition

 refers to the process of creating messages and answers. Although any text editor can be used for the body of the message, the system itself can provide assistance with addressing and the numerous header fields attached to each message. For example, when answering a message, the e-mail system can extract the originator's address from the incoming e-mail and automatically insert it into the proper place in the reply. 

Transfer

 refers to moving messages from the originator to the recipient. In large part, this requires establishing a connection to the destination or some intermediate machine, outputting the message, and releasing the connection. The e-mail system should do this automatically, without bothering the user. 

Reporting

 has to do with telling the originator what happened to the message. Was it delivered? Was it rejected? Was it lost? Numerous applications exist in which confirmation of delivery is important and may even have legal significance (''Well, Your Honor, my e-mail system is not very reliable, so I guess the electronic subpoena just got lost somewhere''). 

Displaying

 incoming messages is needed so people can read their e-mail. Sometimes conversion is required or a special viewer must be invoked, for example, if the message is a PostScript file or digitized voice. Simple conversions and formatting are sometimes attempted as well. 

Disposition

 is the final step and concerns what the recipient does with the message after receiving it. Possibilities include throwing it away before reading, throwing it away after reading, saving it, and so on. It should also be possible to retrieve and reread saved messages, forward them, or process them in other ways. 

452




In addition to these basic services, some e-mail systems, especially internal corporate ones, provide a variety of advanced features. Let us just briefly mention a few of these. When people move or when they are away for some period of time, they may want their e-mail forwarded, so the system should be able to do this automatically. 

Most systems allow users to create 

mailboxes

 to store incoming e-mail. Commands are needed to create and destroy mailboxes, inspect the contents of mailboxes, insert and delete messages from mailboxes, and so on. 

Corporate managers often need to send a message to each of their subordinates, customers, or suppliers. This gives rise to the idea of a 

mailing list

, which is a list of e-mail addresses. When a message is sent to the mailing list, identical copies are delivered to everyone on the list. 

Other advanced features are carbon copies, blind carbon copies, high-priority e-mail, secret (i.e., encrypted) e-mail, alternative recipients if the primary one is not currently available, and the ability for secretaries to read and answer their bosses' e-mail. 

E-mail is now widely used within industry for intracompany communication. It allows far-flung employees to cooperate on complex projects, even over many time zones. By eliminating most cues associated with rank, age, and gender, e-mail debates tend to focus on ideas, not on corporate status. With e-mail, a brilliant idea from a summer student can have more impact than a dumb one from an executive vice president. 

A key idea in e-mail systems is the distinction between the 

envelope

 and its contents. The envelope encapsulates the message. It contains all the information needed for transporting the message, such as the destination address, priority, and security level, all of which are distinct from the message itself. The message transport agents use the envelope for routing, just as the post office does. 

The message inside the envelope consists of two parts: the 

header

 and the 

body

. The header contains control information for the user agents. The body is entirely for the human recipient. Envelopes and messages are illustrated in 

Fig. 7-7

. 

Figure 7-7. Envelopes and messages. (a) Paper mail. (b) Electronic mail. 

453




 

7.2.2 The User Agent 

E-mail systems have two basic parts, as we have seen: the user agents and the message transfer agents. In this section we will look at the user agents. A user agent is normally a program (sometimes called a mail reader) that accepts a variety of commands for composing, receiving, and replying to messages, as well as for manipulating mailboxes. Some user agents have a fancy menu- or icon-driven interface that requires a mouse, whereas others expect 1-character commands from the keyboard. Functionally, these are the same. Some systems are menu- or icon-driven but also have keyboard shortcuts. 

Sending E-mail 

To send an e-mail message, a user must provide the message, the destination address, and possibly some other parameters. The message can be produced with a free-standing text editor, a word processing program, or possibly with a specialized text editor built into the user agent. The destination address must be in a format that the user agent can deal with. Many user agents expect addresses of the form 

user@dns-address

. Since we have studied DNS earlier in this chapter, we will not repeat that material here. 

However, it is worth noting that other forms of addressing exist. In particular, X.400 addresses look radically different from DNS addresses. They are composed of 

attribute = value

 pairs separated by slashes, for example, 

/C=US/ST=MASSACHUSETTS/L=CAMBRIDGE/PA=360 MEMORIAL DR./CN=KEN SMITH/  

This address specifies a country, state, locality, personal address and a common name (Ken Smith). Many other attributes are possible, so you can send e-mail to someone whose exact e-mail address you do not know, provided you know enough other attributes (e.g., company and job title). Although X.400 names are considerably less convenient than DNS names, most e-mail systems have aliases (sometimes called nicknames) that allow users to enter or select a person's name and get the correct e-mail address. Consequently, even with X.400 addresses, it is usually not necessary to actually type in these strange strings. 

454




Most e-mail systems support mailing lists, so that a user can send the same message to a list of people with a single command. If the mailing list is maintained locally, the user agent can just send a separate message to each intended recipient. However, if the list is maintained remotely, then messages will be expanded there. For example, if a group of bird watchers has a mailing list called 

birders

 installed on 

meadowlark.arizona.edu

, then any message sent to 

birders@meadowlark.arizona.edu

 will be routed to the University of Arizona and expanded there into individual messages to all the mailing list members, wherever in the world they may be. Users of this mailing list cannot tell that it is a mailing list. It could just as well be the personal mailbox of Prof. Gabriel O. Birders. 

Reading E-mail 

Typically, when a user agent is started up, it looks at the user's mailbox for incoming e-mail before displaying anything on the screen. Then it may announce the number of messages in the mailbox or display a one-line summary of each one and wait for a command. 

As an example of how a user agent works, let us take a look at a typical mail scenario. After starting up the user agent, the user asks for a summary of his e-mail. A display like that of 

Fig. 7-8

 then appears on the screen. Each line refers to one message. In this example, the mailbox contains eight messages. 

Figure 7-8. An example display of the contents of a mailbox. 

 

Each line of the display contains several fields extracted from the envelope or header of the corresponding message. In a simple e-mail system, the choice of fields displayed is built into the program. In a more sophisticated system, the user can specify which fields are to be displayed by providing a 

user profile

, a file describing the display format. In this basic example, the first field is the message number. The second field, 

Flags

, can contain a 

K

, meaning that the message is not new but was read previously and kept in the mailbox; an 

A

, meaning that the message has already been answered; and/or an 

F

, meaning that the message has been forwarded to someone else. Other flags are also possible. 

The third field tells how long the message is, and the fourth one tells who sent the message. Since this field is simply extracted from the message, this field may contain first names, full names, initials, login names, or whatever else the sender chooses to put there. Finally, the 

Subject

 field gives a brief summary of what the message is about. People who fail to include a 

Subject

 field often discover that responses to their e-mail tend not to get the highest priority. 

After the headers have been displayed, the user can perform any of several actions, such as displaying a message, deleting a message, and so on. The older systems were text based and typically used one-character commands for performing these tasks, such as T (type message), A (answer message), D (delete message), and F (forward message). An argument specified the message in question. More recent systems use graphical interfaces. Usually, the user selects a message with the mouse and then clicks on an icon to type, answer, delete, or forward it. 

455




E-mail has come a long way from the days when it was just file transfer. Sophisticated user agents make managing a large volume of e-mail possible. For people who receive and send thousands of messages a year, such tools are invaluable. 

7.2.3 Message Formats 

Let us now turn from the user interface to the format of the e-mail messages themselves. First we will look at basic ASCII e-mail using RFC 822. After that, we will look at multimedia extensions to RFC 822. 

RFC 822 

Messages consist of a primitive envelope (described in RFC 821), some number of header fields, a blank line, and then the message body. Each header field (logically) consists of a single line of ASCII text containing the field name, a colon, and, for most fields, a value. RFC 822 was designed decades ago and does not clearly distinguish the envelope fields from the header fields. Although it was revised in RFC 2822, completely redoing it was not possible due to its widespread usage. In normal usage, the user agent builds a message and passes it to the message transfer agent, which then uses some of the header fields to construct the actual envelope, a somewhat old-fashioned mixing of message and envelope. 

The principal header fields related to message transport are listed in 

Fig. 7-9

. The 

To:

 field gives the DNS address of the primary recipient. Having multiple recipients is also allowed. The 

Cc:

 field gives the addresses of any secondary recipients. In terms of delivery, there is no distinction between the primary and secondary recipients. It is entirely a psychological difference that may be important to the people involved but is not important to the mail system. The term 

Cc:

 (Carbon copy) is a bit dated, since computers do not use carbon paper, but it is well established. The 

Bcc:

 (Blind carbon copy) field is like the 

Cc:

 field, except that this line is deleted from all the copies sent to the primary and secondary recipients. This feature allows people to send copies to third parties without the primary and secondary recipients knowing this. 

Figure 7-9. RFC 822 header fields related to message transport. 

 

The next two fields, 

From:

 and 

Sender:

, tell who wrote and sent the message, respectively. These need not be the same. For example, a business executive may write a message, but her secretary may be the one who actually transmits it. In this case, the executive would be listed in the 

From:

 field and the secretary in the 

Sender:

 field. The 

From:

 field is required, but the 

Sender:

 field may be omitted if it is the same as the 

From:

 field. These fields are needed in case the message is undeliverable and must be returned to the sender. 

A line containing 

Received:

 is added by each message transfer agent along the way. The line contains the agent's identity, the date and time the message was received, and other information that can be used for finding bugs in the routing system. 

456




The 

Return-Path:

 field is added by the final message transfer agent and was intended to tell how to get back to the sender. In theory, this information can be gathered from all the 

Received:

 headers (except for the name of the sender's mailbox), but it is rarely filled in as such and typically just contains the sender's address. 

In addition to the fields of 

Fig. 7-9

, RFC 822 messages may also contain a variety of header fields used by the user agents or human recipients. The most common ones are listed in 

Fig. 

7-10

. Most of these are self-explanatory, so we will not go into all of them in detail. 

Figure 7-10. Some fields used in the RFC 822 message header. 

 

The 

Reply-To:

 field is sometimes used when neither the person composing the message nor the person sending the message wants to see the reply. For example, a marketing manager writes an e-mail message telling customers about a new product. The message is sent by a secretary, but the 

Reply-To:

 field lists the head of the sales department, who can answer questions and take orders. This field is also useful when the sender has two e-mail accounts and wants the reply to go to the other one. 

The RFC 822 document explicitly says that users are allowed to invent new headers for their own private use, provided that these headers start with the string 

X-

. It is guaranteed that no future headers will use names starting with 

X-

, to avoid conflicts between official and private headers. Sometimes wiseguy undergraduates make up fields like 

X-Fruit-of-the-Day:

 or 

X-Disease-of-the-Week:

, which are legal, although not always illuminating. 

After the headers comes the message body. Users can put whatever they want here. Some people terminate their messages with elaborate signatures, including simple ASCII cartoons, quotations from greater and lesser authorities, political statements, and disclaimers of all kinds (e.g., The XYZ Corporation is not responsible for my opinions; in fact, it cannot even comprehend them). 

MIME—The Multipurpose Internet Mail Extensions 

In the early days of the ARPANET, e-mail consisted exclusively of text messages written in English and expressed in ASCII. For this environment, RFC 822 did the job completely: it specified the headers but left the content entirely up to the users. Nowadays, on the worldwide Internet, this approach is no longer adequate. The problems include sending and receiving 

1. Messages in languages with accents (e.g., French and German). 

2. Messages in non-Latin alphabets (e.g., Hebrew and Russian). 

3. Messages in languages without alphabets (e.g., Chinese and Japanese). 

4. Messages not containing text at all (e.g., audio or images). 

A solution was proposed in RFC 1341 and updated in RFCs 2045–2049. This solution, called 

MIME

 (

Multipurpose Internet Mail Extensions

) is now widely used. We will now describe it. For additional information about MIME, see the RFCs. 

457




The basic idea of MIME is to continue to use the RFC 822 format, but to add structure to the message body and define encoding rules for non-ASCII messages. By not deviating from RFC 822, MIME messages can be sent using the existing mail programs and protocols. All that has to be changed are the sending and receiving programs, which users can do for themselves. 

MIME defines five new message headers, as shown in 

Fig. 7-11

. The first of these simply tells the user agent receiving the message that it is dealing with a MIME message, and which version of MIME it uses. Any message not containing a 

MIME-Version:

 header is assumed to be an English plaintext message and is processed as such. 

Figure 7-11. RFC 822 headers added by MIME. 

 

The 

Content-Description:

 header is an ASCII string telling what is in the message. This header is needed so the recipient will know whether it is worth decoding and reading the message. If the string says: ''Photo of Barbara's hamster'' and the person getting the message is not a big hamster fan, the message will probably be discarded rather than decoded into a high-resolution color photograph. 

The 

Content-Id:

 header identifies the content. It uses the same format as the standard 

Message-Id:

 header. 

The 

Content-Transfer-Encoding:

 tells how the body is wrapped for transmission through a network that may object to most characters other than letters, numbers, and punctuation marks. Five schemes (plus an escape to new schemes) are provided. The simplest scheme is just ASCII text. ASCII characters use 7 bits and can be carried directly by the e-mail protocol provided that no line exceeds 1000 characters. 

The next simplest scheme is the same thing, but using 8-bit characters, that is, all values from 0 up to and including 255. This encoding scheme violates the (original) Internet e-mail protocol but is used by some parts of the Internet that implement some extensions to the original protocol. While declaring the encoding does not make it legal, having it explicit may at least explain things when something goes wrong. Messages using the 8-bit encoding must still adhere to the standard maximum line length. 

Even worse are messages that use binary encoding. These are arbitrary binary files that not only use all 8 bits but also do not even respect the 1000-character line limit. Executable programs fall into this category. No guarantee is given that messages in binary will arrive correctly, but some people try anyway. 

The correct way to encode binary messages is to use 

base64 encoding

, sometimes called 

ASCII armor

. In this scheme, groups of 24 bits are broken up into four 6-bit units, with each unit being sent as a legal ASCII character. The coding is ''A'' for 0, ''B'' for 1, and so on, followed by the 26 lower-case letters, the ten digits, and finally + and / for 62 and 63, respectively. The == and = sequences indicate that the last group contained only 8 or 16 bits, respectively. Carriage returns and line feeds are ignored, so they can be inserted at will to keep the lines short enough. Arbitrary binary text can be sent safely using this scheme. 

458




For messages that are almost entirely ASCII but with a few non-ASCII characters, base64 encoding is somewhat inefficient. Instead, an encoding known as 

quoted-printable encoding

 is used. This is just 7-bit ASCII, with all the characters above 127 encoded as an equal sign followed by the character's value as two hexadecimal digits. 

In summary, binary data should be sent encoded in base64 or quoted-printable form. When there are valid reasons not to use one of these schemes, it is possible to specify a user-defined encoding in the 

Content-Transfer-Encoding:

 header. 

The last header shown in 

Fig. 7-11

 is really the most interesting one. It specifies the nature of the message body. Seven types are defined in RFC 2045, each of which has one or more subtypes. The type and subtype are separated by a slash, as in 

Content-Type: video/mpeg  

The subtype must be given explicitly in the header; no defaults are provided. The initial list of types and subtypes specified in RFC 2045 is given in 

Fig. 7-12

. Many new ones have been added since then, and additional entries are being added all the time as the need arises. 

Figure 7-12. The MIME types and subtypes defined in RFC 2045. 

 

Let us now go briefly through the list of types. The 

text

 type is for straight ASCII text. The 

text/plain

 combination is for ordinary messages that can be displayed as received, with no encoding and no further processing. This option allows ordinary messages to be transported in MIME with only a few extra headers. 

The 

text/enriched

 subtype allows a simple markup language to be included in the text. This language provides a system-independent way to express boldface, italics, smaller and larger point sizes, indentation, justification, sub- and superscripting, and simple page layout. The markup language is based on SGML, the Standard Generalized Markup Language also used as the basis for the World Wide Web's HTML. For example, the message 

The <bold> time </bold> has come the <italic> walrus </italic> said ...  

would be displayed as 

459




The 

time

 has come the 

walrus

 said ... 

It is up to the receiving system to choose the appropriate rendition. If boldface and italics are available, they can be used; otherwise, colors, blinking, underlining, reverse video, etc., can be used for emphasis. Different systems can, and do, make different choices. 

When the Web became popular, a new subtype 

text/html

 was added (in RFC 2854) to allow Web pages to be sent in RFC 822 e-mail. A subtype for the extensible markup language, 

text/xml

, is defined in RFC 3023. We will study HTML and XML later in this chapter. 

The next MIME type is 

image

, which is used to transmit still pictures. Many formats are widely used for storing and transmitting images nowadays, both with and without compression. Two of these, GIF and JPEG, are built into nearly all browsers, but many others exist as well and have been added to the original list. 

The 

audio

 and 

video

 types are for sound and moving pictures, respectively. Please note that 

video

 includes only the visual information, not the soundtrack. If a movie with sound is to be transmitted, the video and audio portions may have to be transmitted separately, depending on the encoding system used. The first video format defined was the one devised by the modestly-named Moving Picture Experts Group (MPEG), but others have been added since. In addition to 

audio/basic

, a new audio type, 

audio/mpeg

 was added in RFC 3003 to allow people to e-mail MP3 audio files. 

The 

application

 type is a catchall for formats that require external processing not covered by one of the other types. An 

octet-stream

 is just a sequence of uninterpreted bytes. Upon receiving such a stream, a user agent should probably display it by suggesting to the user that it be copied to a file and prompting for a file name. Subsequent processing is then up to the user. 

The other defined subtype is 

postscript

, which refers to the PostScript language defined by Adobe Systems and widely used for describing printed pages. Many printers have built-in PostScript interpreters. Although a user agent can just call an external PostScript interpreter to display incoming PostScript files, doing so is not without some danger. PostScript is a full-blown programming language. Given enough time, a sufficiently masochistic person could write a C compiler or a database management system in PostScript. Displaying an incoming PostScript message is done by executing the PostScript program contained in it. In addition to displaying some text, this program can read, modify, or delete the user's files, and have other nasty side effects. 

The 

message

 type allows one message to be fully encapsulated inside another. This scheme is useful for forwarding e-mail, for example. When a complete RFC 822 message is encapsulated inside an outer message, the 

rfc822

 subtype should be used. 

The 

partial

 subtype makes it possible to break an encapsulated message into pieces and send them separately (for example, if the encapsulated message is too long). Parameters make it possible to reassemble all the parts at the destination in the correct order. 

Finally, the 

external-body

 subtype can be used for very long messages (e.g., video films). Instead of including the MPEG file in the message, an FTP address is given and the receiver's user agent can fetch it over the network at the time it is needed. This facility is especially useful when sending a movie to a mailing list of people, only a few of whom are expected to view it (think about electronic junk mail containing advertising videos). 

The final type is 

multipart

, which allows a message to contain more than one part, with the beginning and end of each part being clearly delimited. The 

mixed

 subtype allows each part to be different, with no additional structure imposed. Many e-mail programs allow the user to 

460




provide one or more attachments to a text message. These attachments are sent using the 

multipart

 type. 

In contrast to 

multipart

, the 

alternative

 subtype, allows the same message to be included multiple times but expressed in two or more different media. For example, a message could be sent in plain ASCII, in enriched text, and in PostScript. A properly-designed user agent getting such a message would display it in PostScript if possible. Second choice would be enriched text. If neither of these were possible, the flat ASCII text would be displayed. The parts should be ordered from simplest to most complex to help recipients with pre-MIME user agents make some sense of the message (e.g., even a pre-MIME user can read flat ASCII text). 

The 

alternative

 subtype can also be used for multiple languages. In this context, the Rosetta Stone can be thought of as an early 

multipart/alternative

 message. 

A multimedia example is shown in 

Fig. 7-13

. Here a birthday greeting is transmitted both as text and as a song. If the receiver has an audio capability, the user agent there will fetch the sound file, 

birthday.snd

, and play it. If not, the lyrics are displayed on the screen in stony silence. The parts are delimited by two hyphens followed by a (software-generated) string specified in the 

boundary

 parameter. 

Figure 7-13. A multipart message containing enriched and audio alternatives. 

 

Note that the 

Content-Type

 header occurs in three positions within this example. At the top level, it indicates that the message has multiple parts. Within each part, it gives the type and subtype of that part. Finally, within the body of the second part, it is required to tell the user agent what kind of an external file it is to fetch. To indicate this slight difference in usage, we have used lower case letters here, although all headers are case insensitive. The 

content-transfer-encoding

 is similarly required for any external body that is not encoded as 7-bit ASCII. 

461




Getting back to the subtypes for multipart messages, two more possibilities exist. The 

parallel

 subtype is used when all parts must be ''viewed'' simultaneously. For example, movies often have an audio channel and a video channel. Movies are more effective if these two channels are played back in parallel, instead of consecutively. 

Finally, the 

digest

 subtype is used when many messages are packed together into a composite message. For example, some discussion groups on the Internet collect messages from subscribers and then send them out to the group as a single 

multipart/digest

 message. 

7.2.4 Message Transfer 

The message transfer system is concerned with relaying messages from the originator to the recipient. The simplest way to do this is to establish a transport connection from the source machine to the destination machine and then just transfer the message. After examining how this is normally done, we will examine some situations in which this does not work and what can be done about them. 

SMTP—The Simple Mail Transfer Protocol 

Within the Internet, e-mail is delivered by having the source machine establish a TCP connection to port 25 of the destination machine. Listening to this port is an e-mail daemon that speaks 

SMTP

 (

Simple Mail Transfer Protocol

). This daemon accepts incoming connections and copies messages from them into the appropriate mailboxes. If a message cannot be delivered, an error report containing the first part of the undeliverable message is returned to the sender. 

SMTP is a simple ASCII protocol. After establishing the TCP connection to port 25, the sending machine, operating as the client, waits for the receiving machine, operating as the server, to talk first. The server starts by sending a line of text giving its identity and telling whether it is prepared to receive mail. If it is not, the client releases the connection and tries again later. 

If the server is willing to accept e-mail, the client announces whom the e-mail is coming from and whom it is going to. If such a recipient exists at the destination, the server gives the client the go-ahead to send the message. Then the client sends the message and the server acknowledges it. No checksums are needed because TCP provides a reliable byte stream. If there is more e-mail, that is now sent. When all the e-mail has been exchanged in both directions, the connection is released. A sample dialog for sending the message of 

Fig. 7-13

, including the numerical codes used by SMTP, is shown in 

Fig. 7-14

. The lines sent by the client are marked 

C:

. Those sent by the server are marked 

S:

. 

Figure 7-14. Transferring a message from 

elinor@abcd.com

 to 

carolyn@xyz.com

. 

462




 

A few comments about 

Fig. 7-14

 may be helpful. The first command from the client is indeed 

HELO

. Of the various four-character abbreviations for 

HELLO

, this one has numerous advantages over its biggest competitor. Why all the commands had to be four characters has been lost in the mists of time. 

In 

Fig. 7-14

, the message is sent to only one recipient, so only one 

RCPT

 command is used. Such commands are allowed to send a single message to multiple receivers. Each one is individually acknowledged or rejected. Even if some recipients are rejected (because they do not exist at the destination), the message can be sent to the other ones. 

Finally, although the syntax of the four-character commands from the client is rigidly specified, the syntax of the replies is less rigid. Only the numerical code really counts. Each implementation can put whatever string it wants after the code. 

To get a better feel for how SMTP and some of the other protocols described in this chapter work, try them out. In all cases, first go to a machine connected to the Internet. On a UNIX system, in a shell, type 

463




telnet mail.isp.com 25  

substituting the DNS name of your ISP's mail server for 

mail.isp.com

. On a Windows system, click on Start, then Run, and type the command in the dialog box. This command will establish a telnet (i.e., TCP) connection to port 25 on that machine. Port 25 is the SMTP port (see 

Fig. 6-

27

 for some common ports). You will probably get a response something like this: 

Trying 192.30.200.66...  

Connected to mail.isp.com  

Escape character is '^]'.  

220 mail.isp.com Smail #74 ready at Thu, 25 Sept 2002 13:26 +0200  

The first three lines are from telnet telling you what it is doing. The last line is from the SMTP server on the remote machine announcing its willingness to talk to you and accept e-mail. To find out what commands it accepts, type 

HELP  

From this point on, a command sequence such as the one in 

Fig. 7-14

 is possible, starting with the client's 

HELO

 command. 

It is worth noting that the use of lines of ASCII text for commands is not an accident. Most Internet protocols work this way. Using ASCII text makes the protocols easy to test and debug. They can be tested by sending commands manually, as we saw above, and dumps of the messages are easy to read. 

Even though the SMTP protocol is completely well defined, a few problems can still arise. One problem relates to message length. Some older implementations cannot handle messages exceeding 64 KB. Another problem relates to timeouts. If the client and server have different timeouts, one of them may give up while the other is still busy, unexpectedly terminating the connection. Finally, in rare situations, infinite mailstorms can be triggered. For example, if host 1 holds mailing list 

A

 and host 2 holds mailing list 

B

 and each list contains an entry for the other one, then a message sent to either list could generate a never-ending amount of e-mail traffic unless somebody checks for it. 

To get around some of these problems, extended SMTP (

ESMTP

) has been defined in RFC 2821. Clients wanting to use it should send an 

EHLO

 message instead of 

HELO

 initially. If this is rejected, then the server is a regular SMTP server, and the client should proceed in the usual way. If the 

EHLO

 is accepted, then new commands and parameters are allowed. 

7.2.5 Final Delivery 

Up until now, we have assumed that all users work on machines that are capable of sending and receiving e-mail. As we saw, e-mail is delivered by having the sender establish a TCP connection to the receiver and then ship the e-mail over it. This model worked fine for decades when all ARPANET (and later Internet) hosts were, in fact, on-line all the time to accept TCP connections. 

However, with the advent of people who access the Internet by calling their ISP over a modem, it breaks down. The problem is this: what happens when Elinor wants to send Carolyn e-mail and Carolyn is not currently on-line? Elinor cannot establish a TCP connection to Carolyn and thus cannot run the SMTP protocol. 

One solution is to have a message transfer agent on an ISP machine accept e-mail for its customers and store it in their mailboxes on an ISP machine. Since this agent can be on-line all the time, e-mail can be sent to it 24 hours a day. 

464




POP3 

Unfortunately, this solution creates another problem: how does the user get the e-mail from the ISP's message transfer agent? The solution to this problem is to create another protocol that allows user transfer agents (on client PCs) to contact the message transfer agent (on the ISP's machine) and allow e-mail to be copied from the ISP to the user. One such protocol is 

POP3

 (

Post Office Protocol Version 3

), which is described in RFC 1939. 

The situation that used to hold (both sender and receiver having a permanent connection to the Internet) is illustrated in 

Fig. 7-15(a)

. A situation in which the sender is (currently) on-line but the receiver is not is illustrated in 

Fig. 7-15(b)

. 

Figure 7-15. (a) Sending and reading mail when the receiver has a permanent Internet connection and the user agent runs on the same machine as the message transfer agent. (b) Reading e-mail when the receiver has a dial-up connection to an ISP. 

 

POP3 begins when the user starts the mail reader. The mail reader calls up the ISP (unless there is already a connection) and establishes a TCP connection with the message transfer agent at port 110. Once the connection has been established, the POP3 protocol goes through three states in sequence: 

1. Authorization. 

2. Transactions. 

3. Update. 

The authorization state deals with having the user log in. The transaction state deals with the user collecting the e-mails and marking them for deletion from the mailbox. The update state actually causes the e-mails to be deleted. 

This behavior can be observed by typing something like: 

telnet mail.isp.com 110  

where 

mail.isp.com

 represents the DNS name of your ISP's mail server. Telnet establishes a TCP connection to port 110, on which the POP3 server listens. Upon accepting the TCP connection, the server sends an ASCII message announcing that it is present. Usually, it begins with 

+OK

 followed by a comment. An example scenario is shown in 

Fig. 7-16

 starting after the TCP connection has been established. As before, the lines marked 

C:

 are from the 

465




client (user) and those marked 

S:

 are from the server (message transfer agent on the ISP's machine). 

Figure 7-16. Using POP3 to fetch three messages. 

 

During the authorization state, the client sends over its user name and then its password. After a successful login, the client can then send over the 

LIST

 com 

mand, which causes the server to list the contents of the mailbox, one message per line, giving the length of that message. The list is terminated by a period. 

Then the client can retrieve messages using the 

RETR

 command and mark them for deletion with 

DELE

. When all messages have been retrieved (and possibly marked for deletion), the client gives the 

QUIT

 command to terminate the transaction state and enter the update state. When the server has deleted all the messages, it sends a reply and breaks the TCP connection. 

While it is true that the POP3 protocol supports the ability to download a specific message or set of messages and leave them on the server, most e-mail programs just download everything and empty the mailbox. This behavior means that in practice, the only copy is on the user's hard disk. If that crashes, all e-mail may be lost permanently. 

Let us now briefly summarize how e-mail works for ISP customers. Elinor creates a message for Carolyn using some e-mail program (i.e., user agent) and clicks on an icon to send it. The e-mail program hands the message over to the message transfer agent on Elinor's host. The message transfer agent sees that it is directed to 

carolyn@xyz.com

 so it uses DNS to look up the 

MX

 record for 

xyz.com

 (where 

xyz.com

 is Carolyn's ISP). This query returns the DNS name of 

xyz.com

's mail server. The message transfer agent now looks up the IP address of this machine using DNS again, for example, using 

gethostbyname

. It then establishes a TCP connection to the SMTP server on port 25 of this machine. Using an SMTP command sequence analogous to that of 

Fig. 7-14

, it transfers the message to Carolyn's mailbox and breaks the TCP connection. 

In due course of time, Carolyn boots up her PC, connects to her ISP, and starts her e-mail program. The e-mail program establishes a TCP connection to the POP3 server at port 110 of the ISP's mail server machine. The DNS name or IP address of this machine is typically configured when the e-mail program is installed or the subscription to the ISP is made. After 

466




the TCP connection has been established, Carolyn's e-mail program runs the POP3 protocol to fetch the contents of the mailbox to her hard disk using commands similar to those of 

Fig. 7-

16

. Once all the e-mail has been transferred, the TCP connection is released. In fact, the connection to the ISP can also be broken now, since all the e-mail is on Carolyn's hard disk. Of course, to send a reply, the connection to the ISP will be needed again, so it is not generally broken right after fetching the e-mail. 

IMAP 

For a user with one e-mail account at one ISP that is always accessed from one PC, POP3 works fine and is widely used due to its simplicity and robustness. However, it is a computer-industry truism that as soon as something works well, somebody will start demanding more features (and getting more bugs). That happened with e-mail, too. For example, many people have a single e-mail account at work or school and want to access it from work, from their home PC, from their laptop when on business trips, and from cybercafes when on so-called vacation. While POP3 allows this, since it normally downloads all stored messages at each contact, the result is that the user's e-mail quickly gets spread over multiple machines, more or less at random, some of them not even the user's. 

This disadvantage gave rise to an alternative final delivery protocol, 

IMAP

 (

Internet Message Access Protocol

), which is defined in RFC 2060. Unlike POP3, which basically assumes that the user will clear out the mailbox on every contact and work off-line after that, IMAP assumes that all the e-mail will remain on the server indefinitely in multiple mailboxes. IMAP provides extensive mechanisms for reading messages or even parts of messages, a feature useful when using a slow modem to read the text part of a multipart message with large audio and video attachments. Since the working assumption is that messages will not be transferred to the user's computer for permanent storage, IMAP provides mechanisms for creating, destroying, and manipulating multiple mailboxes on the server. In this way a user can maintain a mailbox for each correspondent and move messages there from the inbox after they have been read. 

IMAP has many features, such as the ability to address mail not by arrival number as is done in 

Fig. 7-8

, but by using attributes (e.g., give me the first message from Bobbie). Unlike POP3, IMAP can also accept outgoing e-mail for shipment to the destination as well as deliver incoming e-mail. 

The general style of the IMAP protocol is similar to that of POP3 as shown in 

Fig. 7-16

, except that are there dozens of commands. The IMAP server listens to port 143. A comparison of POP3 and IMAP is given in 

Fig. 7-17

. It should be noted, however, that not every ISP supports both protocols and not every e-mail program supports both protocols. Thus, when choosing an e-mail program, it is important to find out which protocol(s) it supports and make sure the ISP supports at least one of them. 

Figure 7-17. A comparison of POP3 and IMAP. 

467




 

Delivery Features 

Independently of whether POP3 or IMAP is used, many systems provide hooks for additional processing of incoming e-mail. An especially valuable feature for many e-mail users is the ability to set up 

filters

. These are rules that are checked when e-mail comes in or when the user agent is started. Each rule specifies a condition and an action. For example, a rule could say that any message received from the boss goes to mailbox number 1, any message from a select group of friends goes to mailbox number 2, and any message containing certain objectionable words in the Subject line is discarded without comment. 

Some ISPs provide a filter that automatically categorizes incoming e-mail as either important or spam (junk e-mail) and stores each message in the corresponding mailbox. Such filters typically work by first checking to see if the source is a known spammer. Then they usually examine the subject line. If hundreds of users have just received a message with the same subject line, it is probably spam. Other techniques are also used for spam detection. 

Another delivery feature often provided is the ability to (temporarily) forward incoming e-mail to a different address. This address can even be a computer operated by a commercial paging service, which then pages the user by radio or satellite, displaying the 

Subject:

 line on his pager. 

Still another common feature of final delivery is the ability to install a 

vacation daemon

. This is a program that examines each incoming message and sends the sender an insipid reply such as 

Hi. I'm on vacation. I'll be back on the 24th of August. Have a nice summer.  

Such replies can also specify how to handle urgent matters in the interim, other people to contact for specific problems, etc. Most vacation daemons keep track of whom they have sent canned replies to and refrain from sending the same person a second reply. The good ones also check to see if the incoming message was sent to a mailing list, and if so, do not send a canned reply at all. (People who send messages to large mailing lists during the summer probably do not want to get hundreds of replies detailing everyone's vacation plans.) 

The author once ran into an extreme form of delivery processing when he sent an e-mail message to a person who claims to get 600 messages a day. His identity will not be disclosed here, lest half the readers of this book also send him e-mail. Let us call him John. 

468




John has installed an e-mail robot that checks every incoming message to see if it is from a new correspondent. If so, it sends back a canned reply explaining that John can no longer personally read all his e-mail. Instead, he has produced a personal FAQ (Frequently Asked Questions) document that answers many questions he is commonly asked. Normally, newsgroups have FAQs, not people. 

John's FAQ gives his address, fax, and telephone numbers and tells how to contact his company. It explains how to get him as a speaker and describes where to get his papers and other documents. It also provides pointers to software he has written, a conference he is running, a standard he is the editor of, and so on. Perhaps this approach is necessary, but maybe a personal FAQ is the ultimate status symbol. 

Webmail 

One final topic worth mentioning is Webmail. Some Web sites, for example, Hotmail and Yahoo, provide e-mail service to anyone who wants it. They work as follows. They have normal message transfer agents listening to port 25 for incoming SMTP connections. To contact, say, Hotmail, you have to acquire their DNS 

MX

 record, for example, by typing 

host –a –v hotmail.com  

on a UNIX system. Suppose that the mail server is called 

mx10.hotmail.com

, then by typing 

telnet mx10.hotmail.com 25  

you can establish a TCP connection over which SMTP commands can be sent in the usual way. So far, nothing unusual, except that these big servers are often busy, so it may take several attempts to get a TCP connection accepted. 

The interesting part is how e-mail is delivered. Basically, when the user goes to the e-mail Web page, a form is presented in which the user is asked for a login name and password. When the user clicks on Sign In, the login name and password are sent to the server, which then validates them. If the login is successful, the server finds the user's mailbox and builds a listing similar to that of 

Fig. 

7-8

, only formatted as a Web page in HTML. The Web page is then sent to the browser for display. Many of the items on the page are clickable, so messages can be read, deleted, and so on. 

 

7.3 The World Wide Web 

The World Wide Web is an architectural framework for accessing linked documents spread out over millions of machines all over the Internet. In 10 years, it went from being a way to distribute high-energy physics data to the application that millions of people think of as being ''The Internet.'' Its enormous popularity stems from the fact that it has a colorful graphical interface that is easy for beginners to use, and it provides an enormous wealth of information on almost every conceivable subject, from aardvarks to Zulus. 

The Web (also known as 

WWW

) began in 1989 at CERN, the European center for nuclear research. CERN has several accelerators at which large teams of scientists from the participating European countries carry out research in particle physics. These teams often have members from half a dozen or more countries. Most experiments are highly complex and require years of advance planning and equipment construction. The Web grew out of the need to have these large teams of internationally dispersed researchers collaborate using a constantly changing collection of reports, blueprints, drawings, photos, and other documents. 

469




The initial proposal for a web of linked documents came from CERN physicist Tim Berners-Lee in March 1989. The first (text-based) prototype was operational 18 months later. In December 1991, a public demonstration was given at the Hypertext '91 conference in San Antonio, Texas. 

This demonstration and its attendant publicity caught the attention of other researchers, which led Marc Andreessen at the University of Illinois to start developing the first graphical browser, Mosaic. It was released in February 1993. Mosaic was so popular that a year later, Andreessen left to form a company, Netscape Communications Corp., whose goal was to develop clients, servers, and other Web software. When Netscape went public in 1995, investors, apparently thinking this was the next Microsoft, paid $1.5 billion for the stock. This record was all the more surprising because the company had only one product, was operating deeply in the red, and had announced in its prospectus that it did not expect to make a profit for the foreseeable future. For the next three years, Netscape Navigator and Microsoft's Internet Explorer engaged in a ''browser war,'' each one trying frantically to add more features (and thus more bugs) than the other one. In 1998, America Online bought Netscape Communications Corp. for $4.2 billion, thus ending Netscape's brief life as an independent company. 

In 1994, CERN and M.I.T. signed an agreement setting up the 

World Wide Web Consortium

 (sometimes abbreviated as 

W3C

), an organization devoted to further developing the Web, standardizing protocols, and encouraging interoperability between sites. Berners-Lee became the director. Since then, several hundred universities and companies have joined the consortium. Although there are now more books about the Web than you can shake a stick at, the best place to get up-to-date information about the Web is (naturally) on the Web itself. The consortium's home page is at 

www.w3.org

. Interested readers are referred there for links to pages covering all of the consortium's numerous documents and activities. 

7.3.1 Architectural Overview 

From the users' point of view, the Web consists of a vast, worldwide collection of documents or 

Web pages

, often just called 

pages

 for short. Each page may contain links to other pages anywhere in the world. Users can follow a link by clicking on it, which then takes them to the page pointed to. This process can be repeated indefinitely. The idea of having one page point to another, now called 

hypertext

, was invented by a visionary M.I.T. professor of electrical engineering, Vannevar Bush, in 1945, long before the Internet was invented. 

Pages are viewed with a program called a 

browser

, of which Internet Explorer and Netscape Navigator are two popular ones. The browser fetches the page requested, interprets the text and formatting commands on it, and displays the page, properly formatted, on the screen. An example is given in 

Fig. 7-18(a)

. Like many Web pages, this one starts with a title, contains some information, and ends with the e-mail address of the page's maintainer. Strings of text that are links to other pages, called 

hyperlinks

, are often highlighted, by underlining, displaying them in a special color, or both. To follow a link, the user places the mouse cursor on the highlighted area, which causes the cursor to change, and clicks on it. Although nongraphical browsers, such as Lynx, exist, they are not as popular as graphical browsers, so we will concentrate on the latter. Voice-based browsers are also being developed. 

Figure 7-18. (a) A Web page. (b) The page reached by clicking on 

Department of Animal Psychology.

470




 

Users who are curious about the Department of Animal Psychology can learn more about it by clicking on its (underlined) name. The browser then fetches the page to which the name is linked and displays it, as shown in 

Fig. 7-18(b)

. The underlined items here can also be clicked on to fetch other pages, and so on. The new page can be on the same machine as the first one or on a machine halfway around the globe. The user cannot tell. Page fetching is done by the browser, without any help from the user. If the user ever returns to the main page, the links that have already been followed may be shown with a dotted underline (and possibly a different color) to distinguish them from links that have not been followed. Note that clicking on the 

Campus Information

 line in the main page does nothing. It is not underlined, which means that it is just text and is not linked to another page. 

The basic model of how the Web works is shown in 

Fig. 7-19

. Here the browser is displaying a Web page on the client machine. When the user clicks on a line of text that is linked to a page on the 

abcd.com

 server, the browser follows the hyperlink by sending a message to the 

abcd.com

 server asking it for the page. When the page arrives, it is displayed. If this page contains a hyperlink to a page on the 

xyz.com

 server that is clicked on, the browser then sends a request to that machine for the page, and so on indefinitely. 

Figure 7-19. The parts of the Web model. 

471




 

The Client Side 

Let us now examine the client side of 

Fig. 7-19

 in more detail. In essence, a browser is a program that can display a Web page and catch mouse clicks to items on the displayed page. When an item is selected, the browser follows the hyperlink and fetches the page selected. Therefore, the embedded hyperlink needs a way to name any other page on the Web. Pages are named using 

URLs

 (

Uniform Resource Locators

). A typical URL is 

http://www.abcd.com/products.html

We will explain URLs later in this chapter. For the moment, it is sufficient to know that a URL has three parts: the name of the protocol (

http

), the DNS name of the machine where the page is located (

www.abcd.com

), and (usually) the name of the file containing the page (

products.html

). 

When a user clicks on a hyperlink, the browser carries out a series of steps in order to fetch the page pointed to. Suppose that a user is browsing the Web and finds a link on Internet telephony that points to ITU's home page, which is 

http://www.itu.org/home/index.html

. Let us trace the steps that occur when this link is selected. 

1. The browser determines the URL (by seeing what was selected). 

2. The browser asks DNS for the IP address of 

www.itu.org

. 

3. DNS replies with 156.106.192.32. 

4. The browser makes a TCP connection to port 80 on 156.106.192.32. 

5. It then sends over a request asking for file 

/home/index.html

. 

6. The 

www.itu.org

 server sends the file 

/home/index.html

. 

7. The TCP connection is released. 

8. The browser displays all the text in 

/home/index.html

. 

9. The browser fetches and displays all images in this file. 

Many browsers display which step they are currently executing in a status line at the bottom of the screen. In this way, when the performance is poor, the user can see if it is due to DNS not responding, the server not responding, or simply network congestion during page transmission. 

To be able to display the new page (or any page), the browser has to understand its format. To allow all browsers to understand all Web pages, Web pages are written in a standardized 

472




language called HTML, which describes Web pages. We will discuss it in detail later in this chapter. 

Although a browser is basically an HTML interpreter, most browsers have numerous buttons and features to make it easier to navigate the Web. Most have a button for going back to the previous page, a button for going forward to the next page (only operative after the user has gone back from it), and a button for going straight to the user's own start page. Most browsers have a button or menu item to set a bookmark on a given page and another one to display the list of bookmarks, making it possible to revisit any of them with only a few mouse clicks. Pages can also be saved to disk or printed. Numerous options are generally available for controlling the screen layout and setting various user preferences. 

In addition to having ordinary text (not underlined) and hypertext (underlined), Web pages can also contain icons, line drawings, maps, and photographs. Each of these can (optionally) be linked to another page. Clicking on one of these elements causes the browser to fetch the linked page and display it on the screen, the same as clicking on text. With images such as photos and maps, which page is fetched next may depend on what part of the image was clicked on. 

Not all pages contain HTML. A page may consist of a formatted document in PDF format, an icon in GIF format, a photograph in JPEG format, a song in MP3 format, a video in MPEG format, or any one of hundreds of other file types. Since standard HTML pages may link to any of these, the browser has a problem when it encounters a page it cannot interpret. 

Rather than making the browsers larger and larger by building in interpreters for a rapidly growing collection of file types, most browsers have chosen a more general solution. When a server returns a page, it also returns some additional information about the page. This information includes the MIME type of the page (see 

Fig. 7-12

). Pages of type 

text/html

 are just displayed directly, as are pages in a few other built-in types. If the MIME type is not one of the built-in ones, the browser consults its table of MIME types to tell it how to display the page. This table associates a MIME type with a viewer. 

There are two possibilities: plug-ins and helper applications. A 

plug-in

 is a code module that the browser fetches from a special directory on the disk and installs as an extension to itself, as illustrated in 

Fig. 7-20(a)

. Because plug-ins run inside the browser, they have access to the current page and can modify its appearance. After the plug-in has done its job (usually after the user has moved to a different Web page), the plug-in is removed from the browser's memory. 

Figure 7-20. (a) A browser plug-in. (b) A helper application. 

 

Each browser has a set of procedures that all plug-ins must implement so the browser can call the plug-in. For example, there is typically a procedure the browser's base code calls to supply the plug-in with data to display. This set of procedures is the plug-in's interface and is browser specific. 

473




In addition, the browser makes a set of its own procedures available to the plug-in, to provide services to plug-ins. Typical procedures in the browser interface are for allocating and freeing memory, displaying a message on the browser's status line, and querying the browser about parameters. 

Before a plug-in can be used, it must be installed. The usual installation procedure is for the user to go to the plug-in's Web site and download an installation file. On Windows, this is typically a self-extracting zip file with extension 

.exe

. When the zip file is double clicked, a little program attached to the front of the zip file is executed. This program unzips the plug-in and copies it to the browser's plug-in directory. Then it makes the appropriate calls to register the plug-in's MIME type and to associate the plug-in with it. On UNIX, the installer is often a shell script that handles the copying and registration. 

The other way to extend a browser is to use a 

helper application

. This is a complete program, running as a separate process. It is illustrated in 

Fig. 7-20(b)

. Since the helper is a separate program, it offers no interface to the browser and makes no use of browser services. Instead, it usually just accepts the name of a scratch file where the content file has been stored, opens the file, and displays the contents. Typically, helpers are large programs that exist independently of the browser, such as Adobe's Acrobat Reader for displaying PDF files or Microsoft Word. Some programs (such as Acrobat) have a plug-in that invokes the helper itself. 

Many helper applications use the MIME type 

application

. A considerable number of subtypes have been defined, for example, 

application/pdf

 for PDF files and 

application/msword

 for Word files. In this way, a URL can point directly to a PDF or Word file, and when the user clicks on it, Acrobat or Word is automatically started and handed the name of a scratch file containing the content to be displayed. Consequently, browsers can be configured to handle a virtually unlimited number of document types with no changes to the browser. Modern Web servers are often configured with hundreds of type/subtype combinations and new ones are often added every time a new program is installed. 

Helper applications are not restricted to using the 

application

 MIME type. Adobe Photoshop uses 

image/x-photoshop

 and RealOne Player is capable of handling 

audio/mp3

, for example. 

On Windows, when a program is installed on the computer, it registers the MIME types it wants to handle. This mechanism leads to conflict when multiple viewers are available for some subtype, such as 

video/mpg

. What happens is that the last program to register overwrites existing (MIME type, helper application) associations, capturing the type for itself. As a consequence, installing a new program may change the way a browser handles existing types. 

On UNIX, this registration process is generally not automatic. The user must manually update certain configuration files. This approach leads to more work but fewer surprises. 

Browsers can also open local files, rather than fetching them from remote Web servers. Since local files do not have MIME types, the browser needs some way to determine which plug-in or helper to use for types other than its built-in types such as 

text/html

 and 

image/jpeg

. To handle local files, helpers can be associated with a file extension as well as with a MIME type. With the standard configuration, opening 

foo.pdf

 will open it in Acrobat and opening 

bar.doc

 will open it in Word. Some browsers use the MIME type, the file extension, and even information taken from the file itself to guess the MIME type. In particular, Internet Explorer relies more heavily on the file extension than on the MIME type when it can. 

Here, too, conflicts can arise since many programs are willing, in fact, eager, to handle, say, 

.mpg

. During installation, programs intended for professionals often display checkboxes for the MIME types and extensions they are prepared to handle to allow the user to select the appropriate ones and thus not overwrite existing associations by accident. Programs aimed at the consumer market assume that the user does not have a clue what a MIME type is and 

474




simply grab everything they can without regard to what previously installed programs have done. 

The ability to extend the browser with a large number of new types is convenient but can also lead to trouble. When Internet Explorer fetches a file with extension 

exe

, it realizes that this file is an executable program and therefore has no helper. The obvious action is to run the program. However, this could be an enormous security hole. All a malicious Web site has to do is produce a Web page with pictures of, say, movie stars or sports heroes, all of which are linked to a virus. A single click on a picture then causes an unknown and potentially hostile executable program to be fetched and run on the user's machine. To prevent unwanted guests like this, Internet Explorer can be configured to be selective about running unknown programs automatically, but not all users understand how to manage the configuration. 

On UNIX an analogous problem can exist with shell scripts, but that requires the user to consciously install the shell as a helper. Fortunately, this installation is sufficiently complicated that nobody could possibly do it by accident (and few people can even do it intentionally). 

The Server Side 

So much for the client side. Now let us take a look at the server side. As we saw above, when the user types in a URL or clicks on a line of hypertext, the browser parses the URL and interprets the part between 

http://

 and the next slash as a DNS name to look up. Armed with the IP address of the server, the browser establishes a TCP connection to port 80 on that server. Then it sends over a command containing the rest of the URL, which is the name of a file on that server. The server then returns the file for the browser to display. 

To a first approximation, a Web server is similar to the server of 

Fig. 6-6

. That server, like a real Web server, is given the name of a file to look up and return. In both cases, the steps that the server performs in its main loop are: 

1. Accept a TCP connection from a client (a browser). 

2. Get the name of the file requested. 

3. Get the file (from disk). 

4. Return the file to the client. 

5. Release the TCP connection. 

Modern Web servers have more features, but in essence, this is what a Web server does. 

A problem with this design is that every request requires making a disk access to get the file. The result is that the Web server cannot serve more requests per second than it can make disk accesses. A high-end SCSI disk has an average access time of around 5 msec, which limits the server to at most 200 requests/sec, less if large files have to be read often. For a major Web site, this figure is too low. 

One obvious improvement (used by all Web servers) is to maintain a cache in memory of the 

n

 most recently used files. Before going to disk to get a file, the server checks the cache. If the file is there, it can be served directly from memory, thus eliminating the disk access. Although effective caching requires a large amount of main memory and some extra processing time to check the cache and manage its contents, the savings in time are nearly always worth the overhead and expense. 

The next step for building a faster server is to make the server multithreaded. In one design, the server consists of a front-end module that accepts all incoming requests and 

k

 processing modules, as shown in 

Fig. 7-21

. The 

k

 + 1 threads all belong to the same process so the processing modules all have access to the cache within the process' address space. When a request comes in, the front end accepts it and builds a short record describing it. It then hands 

475




the record to one of the processing modules. In another possible design, the front end is eliminated and each processing module tries to acquire its own requests, but a locking protocol is then required to prevent conflicts. 

Figure 7-21. A multithreaded Web server with a front end and processing modules. 

 

The processing module first checks the cache to see if the file needed is there. If so, it updates the record to include a pointer to the file in the record. If it is not there, the processing module starts a disk operation to read it into the cache (possibly discarding some other cached files to make room for it). When the file comes in from the disk, it is put in the cache and also sent back to the client. 

The advantage of this scheme is that while one or more processing modules are blocked waiting for a disk operation to complete (and thus consuming no CPU time), other modules can be actively working on other requests. Of course, to get any real improvement over the single-threaded model, it is necessary to have multiple disks, so more than one disk can be busy at the same time. With 

k

 processing modules and 

k

 disks, the throughput can be as much as 

k

 times higher than with a single-threaded server and one disk. 

In theory, a single-threaded server and 

k

 disks could also gain a factor of 

k

, but the code and administration are far more complicated since normal blocking 

READ

 system calls cannot be used to access the disk. With a multithreaded server, they can be used since then a 

READ

 blocks only the thread that made the call, not the entire process. 

Modern Web servers do more than just accept file names and return files. In fact, the actual processing of each request can get quite complicated. For this reason, in many servers each processing module performs a series of steps. The front end passes each incoming request to the first available module, which then carries it out using some subset of the following steps, depending on which ones are needed for that particular request. 

1. Resolve the name of the Web page requested. 

2. Authenticate the client. 

3. Perform access control on the client. 

4. Perform access control on the Web page. 

5. Check the cache. 

6. Fetch the requested page from disk. 

7. Determine the MIME type to include in the response. 

8. Take care of miscellaneous odds and ends. 

9. Return the reply to the client. 

10. Make an entry in the server log. 

476




Step 1 is needed because the incoming request may not contain the actual name of the file as a literal string. For example, consider the URL 

http://www.cs.vu.nl

, which has an empty file name. It has to be expanded to some default file name. Also, modern browsers can specify the user's default language (e.g., Italian or English), which makes it possible for the server to select a Web page in that language, if available. In general, name expansion is not quite so trivial as it might at first appear, due to a variety of conventions about file naming. 

Step 2 consists of verifying the client's identity. This step is needed for pages that are not available to the general public. We will discuss one way of doing this later in this chapter. 

Step 3 checks to see if there are restrictions on whether the request may be satisfied given the client's identity and location. Step 4 checks to see if there are any access restrictions associated with the page itself. If a certain file (e.g., 

.htaccess

) is present in the directory where the desired page is located, it may restrict access to the file to particular domains, for example, only users from inside the company. 

Steps 5 and 6 involve getting the page. Step 6 needs to be able to handle multiple disk reads at the same time. 

Step 7 is about determining the MIME type from the file extension, first few words of the file, a configuration file, and possibly other sources. Step 8 is for a variety of miscellaneous tasks, such as building a user profile or gathering certain statistics. 

Step 9 is where the result is sent back and step 10 makes an entry in the system log for administrative purposes. Such logs can later be mined for valuable information about user behavior, for example, the order in which people access the pages. 

If too many requests come in each second, the CPU will not be able to handle the processing load, no matter how many disks are used in parallel. The solution is to add more nodes (computers), possibly with replicated disks to avoid having the disks become the next bottleneck. This leads to the 

server farm

 model of 

Fig. 7-22

. A front end still accepts incoming requests but sprays them over multiple CPUs rather than multiple threads to reduce the load on each computer. The individual machines may themselves be multithreaded and pipelined as before. 

Figure 7-22. A server farm. 

 

One problem with server farms is that there is no longer a shared cache because each processing node has its own memory—unless an expensive shared-memory multiprocessor is used. One way to counter this performance loss is to have a front end keep track of where it sends each request and send subsequent requests for the same page to the same node. Doing this makes each node a specialist in certain pages so that cache space is not wasted by having every file in every cache. 

Another problem with server farms is that the client's TCP connection terminates at the front end, so the reply must go through the front end. This situation is depicted in 

Fig. 7-23(a)

, where the incoming request (1) and outgoing reply (4) both pass through the front end. 

477




Sometimes a trick, called 

TCP handoff

, is used to get around this problem. With this trick, the TCP end point is passed to the processing node so it can reply directly to the client, shown as (3) in 

Fig. 7-23(b)

. This handoff is done in a way that is transparent to the client. 

Figure 7-23. (a) Normal request-reply message sequence. (b) Sequence when TCP handoff is used. 

 

URLs—Uniform Resource Locators 

We have repeatedly said that Web pages may contain pointers to other Web pages. Now it is time to see in a bit more detail how these pointers are implemented. When the Web was first created, it was immediately apparent that having one page point to another Web page required mechanisms for naming and locating pages. In particular, three questions had to be answered before a selected page could be displayed: 

1. What is the page called? 

2. Where is the page located? 

3. How can the page be accessed? 

If every page were somehow assigned a unique name, there would not be any ambiguity in identifying pages. Nevertheless, the problem would not be solved. Consider a parallel between people and pages. In the United States, almost everyone has a social security number, which is a unique identifier, as no two people are supposed to have the same one. Nevertheless, if you are armed only with a social security number, there is no way to find the owner's address, and certainly no way to tell whether you should write to the person in English, Spanish, or Chinese. The Web has basically the same problems. 

The solution chosen identifies pages in a way that solves all three problems at once. Each page is assigned a 

URL

 (

Uniform Resource Locator

) that effectively serves as the page's worldwide name. URLs have three parts: the protocol (also known as the 

scheme

), the DNS name of the machine on which the page is located, and a local name uniquely indicating the specific page (usually just a file name on the machine where it resides). As an example example, the Web site for the author's department contains several videos about the university and the city of Amsterdam. The URL for the video page is 

http://www.cs.vu.nl/video/index-en.html

This URL consists of three parts: the protocol (

http

), the DNS name of the host (

www.cs.vu.nl

), and the file name (

video/index-en.html

), with certain punctuation separating the pieces. The file name is a path relative to the default Web directory at 

cs.vu.nl

. 

Many sites have built-in shortcuts for file names. At many sites, a null file name defaults to the organization's main home page. Typically, when the file named is a directory, this implies a file 

478




named 

index.html

. Finally, 

~user/

 might be mapped onto 

user

's WWW directory, and then onto the file 

index.html

 in that directory. Thus, the author's home page can be reached at 

http://www.cs.vu.nl/~ast/

even though the actual file name is 

index.html

 in a certain default directory. 

Now we can see how hypertext works. To make a piece of text clickable, the page writer must provide two items of information: the clickable text to be displayed and the URL of the page to go to if the text is selected. We will explain the command syntax later in this chapter. 

When the text is selected, the browser looks up the host name using DNS. Once it knows the host's IP address, the browser establishes a TCP connection to the host. Over that connection, it sends the file name using the specified protocol. Bingo. Back comes the page. 

This URL scheme is open-ended in the sense that it is straightforward to have browsers use multiple protocols to get at different kinds of resources. In fact, URLs for various other common protocols have been defined. Slightly simplified forms of the more common ones are listed in 

Fig. 7-24

. 

Figure 7-24. Some common URLs. 

 

Let us briefly go over the list. The 

http

 protocol is the Web's native language, the one spoken by Web servers. 

HTTP

 stands for 

HyperText Transfer Protocol

. We will examine it in more detail later in this chapter. 

The 

ftp

 protocol is used to access files by FTP, the Internet's file transfer protocol. FTP has been around more than two decades and is well entrenched. Numerous FTP servers all over the world allow people anywhere on the Internet to log in and download whatever files have been placed on the FTP server. The Web does not change this; it just makes obtaining files by FTP easier, as FTP has a somewhat arcane interface (but it is more powerful than HTTP, for example, it allows a user on machine 

A

 to transfer a file from machine 

B

 to machine 

C

). 

It is possible to access a local file as a Web page, either by using the 

file

 protocol, or more simply, by just naming it. This approach is similar to using FTP but does not require having a server. Of course, it works only for local files, not remote ones. 

Long before there was an Internet, there was the USENET news system. It consists of about 30,000 newsgroups in which millions of people discuss a wide variety of topics by posting and reading articles related to the topic of the newsgroup. The 

news

 protocol can be used to call up a news article as though it were a Web page. This means that a Web browser is simultaneously a news reader. In fact, many browsers have buttons or menu items to make reading USENET news even easier than using standard news readers. 

479




Two formats are supported for the 

news

 protocol. The first format specifies a newsgroup and can be used to get a list of articles from a preconfigured news site. The second one requires the identifier of a specific news article to be given, in this case 

AA0134223112@cs.utah.edu

. The browser then fetches the given article from its preconfigured news site using the 

NNTP

 (

Network News Transfer Protocol

). We will not study NNTP in this book, but it is loosely based on SMTP and has a similar style. 

The 

gopher

 protocol was used by the Gopher system, which was designed at the University of Minnesota and named after the school's athletic teams, the Golden Gophers (as well as being a slang expression meaning ''go for'', i.e., go fetch). Gopher predates the Web by several years. It was an information retrieval scheme, conceptually similar to the Web itself, but supporting only text and no images. It is essentially obsolete now and rarely used any more. 

The last two protocols do not really have the flavor of fetching Web pages, but are useful anyway. The 

mailto

 protocol allows users to send e-mail from a Web browser. The way to do this is to click on the OPEN button and specify a URL consisting of 

mailto:

 followed by the recipient's e-mail address. Most browsers will respond by starting an e-mail program with the address and some of the header fields already filled in. 

The telnet protocol is used to establish an on-line connection to a remote machine. It is used the same way as the telnet program, which is not surprising, since most browsers just call the telnet program as a helper application. 

In short, the URLs have been designed to not only allow users to navigate the Web, but to deal with FTP, news, Gopher, e-mail, and telnet as well, making all the specialized user interface programs for those other services unnecessary and thus integrating nearly all Internet access into a single program, the Web browser. If it were not for the fact that this idea was thought of by a physics researcher, it could easily pass for the output of some software company's advertising department. 

Despite all these nice properties, the growing use of the Web has turned up an inherent weakness in the URL scheme. A URL points to one specific host. For pages that are heavily referenced, it is desirable to have multiple copies far apart, to reduce the network traffic. The trouble is that URLs do not provide any way to reference a page without simultaneously telling where it is. There is no way to say: I want page xyz, but I do not care where you get it. To solve this problem and make it possible to replicate pages, IETF is working on a system of 

URN

s (

Universal Resource Names

). A URN can be thought of as a generalized URL. This topic is still the subject of research, although a proposed syntax is given in RFC 2141. 

Statelessness and Cookies 

As we have seen repeatedly, the Web is basically stateless. There is no concept of a login session. The browser sends a request to a server and gets back a file. Then the server forgets that it has ever seen that particular client. 

At first, when the Web was just used for retrieving publicly available documents, this model was perfectly adequate. But as the Web started to acquire other functions, it caused problems. For example, some Web sites require clients to register (and possibly pay money) to use them. This raises the question of how servers can distinguish between requests from registered users and everyone else. A second example is from e-commerce. If a user wanders around an electronic store, tossing items into her shopping cart from time to time, how does the server keep track of the contents of the cart? A third example is customized Web portals such as Yahoo. Users can set up a detailed initial page with only the information they want (e.g., their stocks and their favorite sports teams), but how can the server display the correct page if it does not know who the user is? 

480




At first glance, one might think that servers could track users by observing their IP addresses. However, this idea does not work. First of all, many users work on shared computers, especially at companies, and the IP address merely identifies the computer, not the user. Second, and even worse, many ISPs use NAT, so all outgoing packets from all users bear the same IP address. From the server's point of view, all the ISP's thousands of customers use the same IP address. 

To solve this problem, Netscape devised a much-criticized technique called 

cookies

. The name derives from ancient programmer slang in which a program calls a procedure and gets something back that it may need to present later to get some work done. In this sense, a UNIX file descriptor or a Windows object handle can be considered as a cookie. Cookies were later formalized in RFC 2109. 

When a client requests a Web page, the server can supply additional information along with the requested page. This information may include a cookie, which is a small (at most 4 KB) file (or string). Browsers store offered cookies in a cookie directory on the client's hard disk unless the user has disabled cookies. Cookies are just files or strings, not executable programs. In principle, a cookie could contain a virus, but since cookies are treated as data, there is no official way for the virus to actually run and do damage. However, it is always possible for some hacker to exploit a browser bug to cause activation. 

A cookie may contain up to five fields, as shown in 

Fig. 7-25

. The 

Domain

 tells where the cookie came from. Browsers are supposed to check that servers are not lying about their domain. Each domain may store no more than 20 cookies per client. The 

Path

 is a path in the server's directory structure that identifies which parts of the server's file tree may use the cookie. It is often /, which means the whole tree. 

Figure 7-25. Some examples of cookies. 

 

The 

Content

 field takes the form 

name = value

. Both 

name

 and 

value

 can be anything the server wants. This field is where the cookie's content is stored. 

The 

Expires

 field specifies when the cookie expires. If this field is absent, the browser discards the cookie when it exits. Such a cookie is called a 

nonpersistent cookie

. If a time and date are supplied, the cookie is said to be 

persistent

 and is kept until it expires. Expiration times are given in Greenwich Mean Time. To remove a cookie from a client's hard disk, a server just sends it again, but with an expiration time in the past. 

Finally, the 

Secure

 field can be set to indicate that the browser may only return the cookie to a secure server. This feature is used for e-commerce, banking, and other secure applications. 

We have now seen how cookies are acquired, but how are they used? Just before a browser sends a request for a page to some Web site, it checks its cookie directory to see if any cookies there were placed by the domain the request is going to. If so, all the cookies placed by that domain are included in the request message. When the server gets them, it can interpret them any way it wants to. 

Let us examine some possible uses for cookies. In 

Fig. 7-25

, the first cookie was set by 

toms-casino.com

 and is used to identify the customer. When the client logs in next week to throw away some more money, the browser sends over the cookie so the server knows who it is. 

481




Armed with the customer ID, the server can look up the customer's record in a database and use this information to build an appropriate Web page to display. Depending on the customer's known gambling habits, this page might consist of a poker hand, a listing of today's horse races, or a slot machine. 

The second cookie came from 

joes-store.com

. The scenario here is that the client is wandering around the store, looking for good things to buy. When she finds a bargain and clicks on it, the server builds a cookie containing the number of items and the product code and sends it back to the client. As the client continues to wander around the store, the cookie is returned on every new page request. As more purchases accumulate, the server adds them to the cookie. In the figure, the cart contains three items, the last of which is desired in duplicate. Finally, when the client clicks on 

PROCEED TO CHECKOUT

, the cookie, now containing the full list of purchases, is sent along with the request. In this way the server knows exactly what has been purchased. 

The third cookie is for a Web portal. When the customer clicks on a link to the portal, the browser sends over the cookie. This tells the portal to build a page containing the stock prices for Sun Microsystems and Oracle, and the New York Jets football results. Since a cookie can be up to 4 KB, there is plenty of room for more detailed preferences concerning newspaper headlines, local weather, special offers, etc. 

Cookies can also be used for the server's own benefit. For example, suppose a server wants to keep track of how many unique visitors it has had and how many pages each one looked at before leaving the site. When the first request comes in, there will be no accompanying cookie, so the server sends back a cookie containing 

Counter = 1

. Subsequent clicks on that site will send the cookie back to the server. Each time the counter is incremented and sent back to the client. By keeping track of the counters, the server can see how many people give up after seeing the first page, how many look at two pages, and so on. 

Cookies have also been misused. In theory, cookies are only supposed to go back to the originating site, but hackers have exploited numerous bugs in the browsers to capture cookies not intended for them. Since some e-commerce sites put credit card numbers in cookies, the potential for abuse is clear. 

A controversial use of cookies is to secretly collect information about users' Web browsing habits. It works like this. An advertising agency, say, Sneaky Ads, contacts major Web sites and places banner ads for its corporate clients' products on their pages, for which it pays the site owners a fee. Instead of giving the site a GIF or JPEG file to place on each page, it gives them a URL to add to each page. Each URL it hands out contains a unique number in the file part, such as 

http://www.sneaky.com/382674902342.gif

When a user first visits a page, 

P

, containing such an ad, the browser fetches the HTML file. Then the browser inspects the HTML file and sees the link to the image file at 

www.sneaky.com

, so it sends a request there for the image. A GIF file containing an ad is returned, along with a cookie containing a unique user ID, 3627239101 in 

Fig. 7-25

. Sneaky records the fact that the user with this ID visited page 

P

. This is easy to do since the file requested (

382674902342.gif

) is referenced only on page 

P

. Of course, the actual ad may appear on thousands of pages, but each time with a different file name. Sneaky probably collects a couple of pennies from the product manufacturer each time it ships out the ad. 

Later, when the user visits another Web page containing any of Sneaky's ads, after the browser has fetched the HTML file from the server, it sees the link to, say, 

http://www.sneaky.com/493654919923.gif

 and requests that file. Since it already has a cookie from the domain 

sneaky.com

, the browser includes Sneaky's cookie containing the user ID. Sneaky now knows a second page the user has visited. 

482




In due course of time, Sneaky can build up a complete profile of the user's browsing habits, even though the user has never clicked on any of the ads. Of course, it does not yet have the user's name (although it does have his IP address, which may be enough to deduce the name from other databases). However, if the user ever supplies his name to any site cooperating with Sneaky, a complete profile along with a name is now available for sale to anyone who wants to buy it. The sale of this information may be profitable enough for Sneaky to place more ads on more Web sites and thus collect more information. The most insidious part of this whole business is that most users are completely unaware of this information collection and may even think they are safe because they do not click on any of the ads. 

And if Sneaky wants to be supersneaky, the ad need not be a classical banner ad. An ''ad'' consisting of a single pixel in the background color (and thus invisible), has exactly the same effect as a banner ad: it requires the browser to go fetch the 1 x 1-pixel gif image and send it all cookies originating at the pixel's domain. 

To maintain some semblance of privacy, some users configure their browsers to reject all cookies. However, this can give problems with legitimate Web sites that use cookies. To solve this problem, users sometimes install cookie-eating software. These are special programs that inspect each incoming cookie upon arrival and accept or discard it depending on choices the user has given it (e.g., about which Web sites can be trusted). This gives the user fine-grained control over which cookies are accepted and which are rejected. Modern browsers, such as Mozilla (

www.mozilla.org

), have elaborate user-controls over cookies built in. 

7.3.2 Static Web Documents 

The basis of the Web is transferring Web pages from server to client. In the simplest form, Web pages are static, that is, are just files sitting on some server waiting to be retrieved. In this context, even a video is a static Web page because it is just a file. In this section we will look at static Web pages in detail. In the next one, we will examine dynamic content. 

HTML—The HyperText Markup Language 

Web pages are currently written in a language called 

HTML

 (

HyperText Markup Language

). HTML allows users to produce Web pages that include text, graphics, and pointers to other Web pages. HTML is a markup language, a language for describing how documents are to be formatted. The term ''markup'' comes from the old days when copyeditors actually marked up documents to tell the printer—in those days, a human being—which fonts to use, and so on. Markup languages thus contain explicit commands for formatting. For example, in HTML, 

<b>

 

means start boldface mode, and

 

</b>

 means leave boldface mode. The advantage of a markup language over one with no explicit markup is that writing a browser for it is straightforward: the browser simply has to understand the markup commands. TeX and troff are other well-known examples of markup languages. 

By embedding all the markup commands within each HTML file and standardizing them, it becomes possible for any Web browser to read and reformat any Web page. Being able to reformat Web pages after receiving them is crucial because a page may have been produced in a 1600 x 1200 window with 24-bit color but may have to be displayed in a 640 x 320 window configured for 8-bit color. 

Below we will give a brief introduction to HTML, just to give an idea of what it is like. While it is certainly possible to write HTML documents with any standard editor, and many people do, it is also possible to use special HTML editors or word processors that do most of the work (but correspondingly give the user less control over all the details of the final result). 

A Web page consists of a head and a body, each enclosed by 

<html>

 and 

</html>

 

tags

 (formatting commands), although most browsers do not complain if these tags are missing. As 

483




can be seen from 

Fig. 7-26(a)

, the head is bracketed by the 

<head>

 and 

</head>

 tags and the body is bracketed by the 

<body>

 and 

</body>

 tags. The strings inside the tags are called 

directives

. Most HTML tags have this format, that is they use, 

<something>

 to mark the beginning of something and 

</something>

 to mark its end. Most browsers have a menu item VIEW SOURCE or something like that. Selecting this item displays the current page's HTML source, instead of its formatted output. 

Figure 7-26. (a) The HTML for a sample Web page. (b) The formatted page. 

 

484




Tags can be in either lower case or upper case. Thus, 

<head>

 and 

<HEAD>

 mean the same thing, but newer versions of the standard require lower case only. Actual layout of the HTML document is irrelevant. HTML parsers ignore extra spaces and carriage returns since they have to reformat the text to make it fit the current display area. Consequently, white space can be added at will to make HTML documents more readable, something most of them are badly in need of. As another consequence, blank lines cannot be used to separate paragraphs, as they are simply ignored. An explicit tag is required. 

Some tags have (named) parameters, called 

attributes

. For example, 

<img src="abc" alt="foobar">  

is a tag, 

<img>

, with parameter 

src

 set equal to 

abc

 and parameter 

alt

 set equal to 

foobar

. For each tag, the HTML standard gives a list of what the permitted parameters, if any, are, and what they mean. Because each parameter is named, the order in which the parameters are given is not significant. 

Technically, HTML documents are written in the ISO 8859-1 Latin-1 character set, but for users whose keyboards support only ASCII, escape sequences are present for the special characters, such as è. The list of special characters is given in the standard. All of them begin with an ampersand and end with a semicolon. For example, 

&nbsp;

 produces a space, 

&egrave;

 produces è and 

&eacute;

 produces é. Since <, >, and & have special meanings, they can be expressed only with their escape sequences, &lt;, &gt;, and &amp;, respectively. 

The main item in the head is the title, delimited by 

<title>

 and 

</title>

, but certain kinds of meta-information may also be present. The title itself is not displayed on the page. Some browsers use it to label the page's window. 

Let us now take a look at some of the other features illustrated in 

Fig. 7-26

. All of the tags used in 

Fig. 7-26

 and some others are shown in 

Fig. 7-27

. Headings are generated by an 

<h

n

>

 tag, where 

n

 is a digit in the range 1 to 6. Thus 

<h1>

 is the most important heading; 

<h6>

 is the least important one. It is up to the browser to render these appropriately on the screen. Typically the lower numbered headings will be displayed in a larger and heavier font. The browser may also choose to use different colors for each level of heading. Typically 

<h1>

 headings are large and boldface with at least one blank line above and below. In contrast, 

<h2>

 headings are in a smaller font with less space above and below. 

Figure 7-27. A selection of common HTML tags. Some can have additional parameters. 

485




 

The tags 

<b>

 and 

<i>

 are used to enter boldface and italics mode, respectively. If the browser is not capable of displaying boldface and italics, it must use some other method of rendering them, for example, using a different color for each or perhaps reverse video. 

HTML provides various mechanisms for making lists, including nested lists. Lists are started with 

<ul>

 or 

<ol>

, with 

<li>

 used to mark the start of the items in both cases. The 

<ul>

 tag starts an unordered list. The individual items, which are marked with the 

<li>

 tag in the source, appear with bullets (•) in front of them. A variant of this mechanism is 

<ol>

, which is for ordered lists. When this tag is used, the 

<li>

 items are numbered by the browser. Other than the use of different starting and ending tags, 

<ul>

 and 

<ol>

 have the same syntax and similar results. 

The 

<br>

, 

<p>

, and 

<hr>

 tags all indicate a boundary between sections of text. The precise format can be determined by the style sheet (see below) associated with the page. The 

<br>

 tag just forces a line break. Typically, browsers do not insert a blank line after 

<br>

. In contrast, 

<p>

 starts a paragraph, which might, for example, insert a blank line and possibly some indentation. (Theoretically, 

</p>

 exists to mark the end of a paragraph, but it is rarely used; most HTML authors do not even know it exists.) Finally, 

<hr>

 forces a break and draws a horizontal line across the screen. 

HTML allows images to be included in-line on a Web page. The 

<img>

 tag specifies that an image is to be displayed at the current position in the page. It can have several parameters. The 

src

 parameter gives the URL of the image. The HTML standard does not specify which graphic formats are permitted. In practice, all browsers support GIF amd JPEG files. Browsers are free to support other formats, but this extension is a two-edged sword. If a user is accustomed to a browser that supports, say, BMP files, he may include these in his Web pages and later be surprised when other browsers just ignore all of his wonderful art. 

Other parameters of 

<img>

 are 

align

, which controls the alignment of the image with respect to the text baseline (

top

, 

middle

, 

bottom

), 

alt

, which provides text to use instead of the image when the user has disabled images, and 

ismap

,a flag indicating that the image is an active map (i.e., clickable picture). 

Finally, we come to hyperlinks, which use the 

<a>

 (anchor) and 

</a>

 tags. Like 

<img>

, 

<a>

 has various parameters, including 

href

 (the URL) and 

name

 (the hyperlink's name). The text 

486




between the 

<a>

 and 

</a>

 is displayed. If it is selected, the hyperlink is followed to a new page. It is also permitted to put an 

<img>

 image there, in which case clicking on the image also activates the hyperlink. 

As an example, consider the following HTML fragment: 

<a href="http://www.nasa.gov"> NASA's home page </a>  

When a page with this fragment is displayed, what appears on the screen is 

NASA's home page

  

If the user subsequently clicks on this text, the browser immediately fetches the page whose URL is 

http://www.nasa.gov

 and displays it. 

As a second example, now consider 

<a href="http://www.nasa.gov"> <img src="shuttle.gif" alt="NASA"> </a>  

When displayed, this page shows a picture (e.g., of the space shuttle). Clicking on the picture switches to NASA's home page, just as clicking on the underlined text did in the previous example. If the user has disabled automatic image display, the text NASA will be displayed where the picture belongs. 

The 

<a>

 tag can take a parameter 

name

 to plant a hyperlink, to allow a hyperlink to point to the middle of a page. For example, some Web pages start out with a clickable table of contents. By clicking on an item in the table of contents, the user jumps to the corresponding section of the page. 

HTML keeps evolving. HTML 1.0 and HTML 2.0 did not have tables, but they were added in HTML 3.0. An HTML table consists of one or more rows, each consisting of one or more 

cells

. Cells can contain a wide range of material, including text, figures, icons, photographs, and even other tables. Cells can be merged, so, for example, a heading can span multiple columns. Page authors have limited control over the layout, including alignment, border styles, and cell margins, but the browsers have the final say in rendering tables. 

An HTML table definition is listed in 

Fig. 7-28(a)

 and a possible rendition is shown in 

Fig. 7-

28(b)

. This example just shows a few of the basic features of HTML tables. Tables are started by the 

<table>

 tag. Additional information can be provided to describe general properties of the table. 

Figure 7-28. (a) An HTML table. (b) A possible rendition of this table. 

487




 

The 

<caption>

 tag can be used to provide a figure caption. Each row begins with a 

<tr>

 (Table Row) tag. The individual cells are marked as 

<th>

 (Table Header) or 

<td>

 (Table Data). The distinction is made to allow browsers to use different renditions for them, as we have done in the example. 

Numerous attributes are also allowed in tables. They include ways to specify horizontal and vertical cell alignments, justification within a cell, borders, grouping of cells, units, and more. 

In HTML 4.0, more new features were added. These include accessibility features for handicapped users, object embedding (a generalization of the 

<img>

 tag so other objects can also be embedded in pages), support for scripting languages (to allow dynamic content), and more. 

When a Web site is complex, consisting of many pages produced by multiple authors working for the same company, it is often desirable to have a way to prevent different pages from 

488




having a different appearance. This problem can be solved using 

style sheets

. When these are used, individual pages no longer use physical styles, such as boldface and italics. Instead, page authors use logical styles such as 

<dn>

 (define), 

<em>

 (weak emphasis), 

<strong>

 (strong emphasis), and 

<var>

 (program variables). The logical styles are defined in the style sheet, which is referred to at the start of each page. In this way all pages have the same style, and if the Webmaster decides to change 

<strong>

 from 14-point italics in blue to 18-point boldface in shocking pink, all it requires is changing one definition to convert the entire Web site. A style sheet can be compared to an 

#include

 file in a C program: changing one macro definition there changes it in all the program files that include the header. 

Forms 

HTML 1.0 was basically one-way. Users could call up pages from information providers, but it was difficult to send information back the other way. As more and more commercial organizations began using the Web, there was a large demand for two-way traffic. For example, many companies wanted to be able to take orders for products via their Web pages, software vendors wanted to distribute software via the Web and have customers fill out their registration cards electronically, and companies offering Web searching wanted to have their customers be able to type in search keywords. 

These demands led to the inclusion of 

forms

 starting in HTML 2.0. Forms contain boxes or buttons that allow users to fill in information or make choices and then send the information back to the page's owner. They use the <input> tag for this purpose. It has a variety of parameters for determining the size, nature, and usage of the box displayed. The most common forms are blank fields for accepting user text, boxes that can be checked, active maps, and 

submit

 buttons. The example of 

Fig. 7-29

 illustrates some of these choices. 

Figure 7-29. (a) The HTML for an order form. (b) The formatted page. 

489




 

Let us start our discussion of forms by going over this example. Like all forms, this one is enclosed between the 

<form>

 and 

</form>

 tags. Text not enclosed in a tag is just displayed. All the usual tags (e.g., 

<b>

) are allowed in a form. Three kinds of input boxes are used in this form. 

The first kind of input box follows the text ''Name''. The box is 46 characters wide and expects the user to type in a string, which is then stored in the variable 

customer

 for later processing. The 

<p>

 tag instructs the browser to display subsequent text and boxes on the next line, even if there is room on the current line. By using 

<p>

 and other layout tags, the author of the page can control the look of the form on the screen. 

The next line of the form asks for the user's street address, 40 columns wide, also on a line by itself. Then comes a line asking for the city, state, and country. No 

<p>

 tags are used between the fields here, so the browser displays them all on one line if they will fit. As far as the browser is concerned, this paragraph just contains six items: three strings alternating with three boxes. It displays them linearly from left to right, going over to a new line whenever the current line cannot hold the next item. Thus, it is conceivable that on a 1600 x 1200 screen all 

490




three strings and their corresponding boxes will appear on the same line, but on a 1024 x 768 screen they might be split over two lines. In the worst scenario, the word ''Country'' is at the end of one line and its box is at the beginning of the next line. 

The next line asks for the credit card number and expiration date. Transmitting credit card numbers over the Internet should only be done when adequate security measures have been taken. We will discuss some of these in 

Chap. 8

. 

Following the expiration date we encounter a new feature: radio buttons. These are used when a choice must be made among two or more alternatives. The intellectual model here is a car radio with half a dozen buttons for choosing stations. The browser displays these boxes in a form that allows the user to select and deselect them by clicking on them (or using the keyboard). Clicking on one of them turns off all the other ones in the same group. The visual presentation is up to the browser. Widget size also uses two radio buttons. The two groups are distinguished by their 

name

 field, not by static scoping using something like 

<radiobutton>

 ... 

</radiobutton>

. 

The 

value

 parameters are used to indicate which radio button was pushed. Depending on which of the credit card options the user has chosen, the variable 

cc

 will be set to either the string ''mastercard'' or the string ''visacard''. 

After the two sets of radio buttons, we come to the shipping option, represented by a box of type 

checkbox

. It can be either on or off. Unlike radio buttons, where exactly one out of the set must be chosen, each box of type 

checkbox

 can be on or off, independently of all the others. For example, when ordering a pizza via Electropizza's Web page, the user can choose sardines 

and

 onions 

and

 pineapple (if she can stand it), but she cannot choose small 

and

 medium 

and

 large for the same pizza. The pizza toppings would be represented by three separate boxes of type 

checkbox

, whereas the pizza size would be a set of radio buttons. 

As an aside, for very long lists from which a choice must be made, radio buttons are somewhat inconvenient. Therefore, the 

<select>

 and 

</select>

 tags are provided to bracket a list of alternatives, but with the semantics of radio buttons (unless the 

multiple

 parameter is given, in which case the semantics are those of checkable boxes). Some browsers render the items located between 

<select>

 and 

</select>

 as a drop-down menu. 

We have now seen two of the built-in types for the <input> tag: 

radio

 and 

checkbox

. In fact, we have already seen a third one as well: 

text

. Because this type is the default, we did not bother to include the parameter 

type = text

, but we could have. Two other types are 

password

 and 

textarea

. A 

password

 box is the same as a 

text

 box, except that the characters are not displayed as they are typed. A 

textarea

 box is also the same as a 

text

 box, except that it can contain multiple lines. 

Getting back to the example of 

Fig. 7-29

, we now come across an example of a 

submit

 button. When this is clicked, the user information on the form is sent back to the machine that provided the form. Like all the other types, 

submit

 is a reserved word that the browser understands. The 

value

 string here is the label on the button and is displayed. All boxes can have values; only here we needed that feature. For 

text

 boxes, the contents of the 

value

 field are displayed along with the form, but the user can edit or erase it. 

checkbox

 and 

radio

 boxes can also be initialized, but with a field called 

checked

 (because 

value

 just gives the text, but does not indicate a preferred choice). 

When the user clicks the 

submit

 button, the browser packages the collected information into a single long line and sends it back to the server for processing. The & is used to separate fields and + is used to represent space. For our example form, the line might look like the contents of 

Fig. 7-30

 (broken into three lines here because the page is not wide enough): 

491




Figure 7-30. A possible response from the browser to the server with information 

 

The string would be sent back to the server as one line, not three. If a 

checkbox

 is not selected, it is omitted from the string. It is up to the server to make sense of this string. We will discuss how this could be done later in this chapter. 

XML and XSL 

HTML, with or without forms, does not provide any structure to Web pages. It also mixes the content with the formatting. As e-commerce and other applications become more common, there is an increasing need for structuring Web pages and separating the content from the formatting. For example, a program that searches the Web for the best price for some book or CD needs to analyze many Web pages looking for the item's title and price. With Web pages in HTML, it is very difficult for a program to figure out where the title is and where the price is. 

For this reason, the W3C has developed an enhancement to HTML to allow Web pages to be structured for automated processing. Two new languages have been developed for this purpose. First, 

XML

 (

eXtensible Markup Language

) describes Web content in a structured way and second, 

XSL

 (

eXtensible Style Language

) describes the formatting independently of the content. Both of these are large and complicated topics, so our brief introduction below just scratches the surface, but it should give an idea of how they work. 

Consider the example XML document of 

Fig. 7-31

. It defines a structure called 

book

_

list

, which is a list of 

book

s. Each book has three fields, the title, author, and year of publication. These structures are extremely simple. It is permitted to have structures with repeated fields (e.g., multiple authors), optional fields (e.g., title of included CD-ROM), and alternative fields (e.g., URL of a bookstore if it is in print or URL of an auction site if it is out of print). 

Figure 7-31. A simple Web page in XML. 

 

492




In this example, each of the three fields is an indivisible entity, but it is also permitted to further subdivide the fields. For example, the author field could have been done as follows to give a finer-grained control over searching and formatting: 

<author>  

  <first_name> Andrew </first_name>  

  <last_name> Tanenbaum </last_name>  

</author>  

Each field can be subdivided into subfields and subsubfields arbitrarily deep. 

All the file of 

Fig. 7-31

 does is define a book list containing three books. It says nothing about how to display the Web page on the screen. To provide the formatting information, we need a second file, 

book

_

list.xsl

, containing the XSL definition. This file is a style sheet that tells how to display the page. (There are alternatives to style sheets, such as a way to convert XML into HTML, but these alternatives are beyond the scope of this book.) 

A sample XSL file for formatting 

Fig. 7-31

 is given in 

Fig. 7-32

. After some necessary declarations, including the URL of the XSL standard, the file contains tags starting with 

<html>

 and 

<body>

. These define the start of the Web page, as 

Figure 7-32. A style sheet in XSL. 

 

usual. Then comes a table definition, including the headings for the three columns. Note that in addition to the 

<th>

 tags there are 

</th>

 tags as well, something we did not bother with so far. The XML and XSL specifications are much stricter than HTML specification. They state that rejecting syntactically incorrect files is mandatory, even if the browser can determine what the Web designer meant. A browser that accepts a syntactically incorrect XML or XSL file and repairs the errors itself is not conformant and will be rejected in a conformance test. Browsers are allowed to pinpoint the error, however. This somewhat draconian measure is needed to deal with the immense number of sloppy Web pages currently out there. 

493




The statement 

<xsl:for-each select="book_list/book">  

is analogous to a 

for

 statement in C. It causes the browser to iterate the loop body (ended by 

<xsl:for-each>

) one iteration for each 

book

. Each iteration outputs five lines: 

<tr>

, the title, author, and year, and 

</tr>

. After the loop, the closing tags 

</body>

 and 

</html>

 are output. The result of the browser's interpreting this style sheet is the same as if the Web page contained the table in-line. However, in this 

format, programs can analyze the XML file and easily find books published after 2000, for example. It is worth emphasizing that even though our XSL file contained a kind of a loop, Web pages in XML and XSL are still static since they simply contain instructions to the browser about how to display the page, just as HTML pages do. Of course, to use XML and XSL, the browser has to be able to interpret XML and XSL, but most of them already have this capability. It is not yet clear whether XSL will take over from traditional style sheets. 

We have not shown how to do this, but XML allows the Web site designer to make up definition files in which the structures are defined in advance. These definition files can be included, making it possible to use them to build complex Web pages. For additional information on this and the many other features of XML and XSL, see one of the many books on the subject. Two examples are (Livingston, 2002; and Williamson, 2001). 

Before ending our discussion of XML and XSL, it is worth commenting on a ideological battle going on within the WWW consortium and the Web designer community. The original goal of HTML was to specify the 

structure

 of the document, not its 

appearance

. For example, 

<h1> Deborah's Photos </h1>  

instructs the browser to emphasize the heading, but does not say anything about the typeface, point size, or color. That was left up to the browser, which knows the properties of the display (e.g., how many pixels it has). However, many Web page designers wanted absolute control over how their pages appeared, so new tags were added to HTML to control appearance, such as 

<font face="helvetica" size="24" color="red"> Deborah's Photos </font>  

Also, ways were added to control positioning on the screen accurately. The trouble with this approach is that it is not portable. Although a page may render perfectly with the browser it is developed on, with another browser or another release of the same browser or a different screen resolution, it may be a complete mess. XML was in part an attempt to go back to the original goal of specifying just the structure, not the appearance of a document. However, XSL is also provided to manage the appearance. Both formats can be misused, however. You can count on it. 

XML can be used for purposes other than describing Web pages. One growing use of it is as a language for communication between application programs. In particular, 

SOAP

 (

Simple Object Access Protocol

) is a way for performing RPCs between applications in a language- and system-independent way. The client constructs the request as an XML message and sends it to the server, using the HTTP protocol (described below). The server sends back a reply as an XML formatted message. In this way, applications on heterogeneous platforms can communicate. 

XHTML—The eXtended HyperText Markup Language 

494




HTML keeps evolving to meet new demands. Many people in the industry feel that in the future, the majority of Web-enabled devices will not be PCs, but wireless, handheld PDA-type devices. These devices have limited memory for large browsers full of heuristics that try to somehow deal with syntactically incorrect Web pages. Thus, the next step after HTML 4 is a language that is Very Picky. It is called 

XHTML

 (

eXtended HyperText Markup Language

) rather than HTML 5 because it is essentially HTML 4 reformulated in XML. By this we mean that tags such as 

<h1>

 have no intrinsic meaning. To get the HTML 4 effect, a definition is needed in the XSL file. XHTML is the new Web standard and should be used for all new Web pages to achieve maximum portability across platforms and browsers. 

There are six major differences and a variety of minor differences between XHTML and HTML 4, Let us now go over the major differences. First, XHTML pages and browsers must strictly conform to the standard. No more shoddy Web pages. This property was inherited from XML. 

Second, all tags and attributes must be in lower case. Tags like 

<HTML>

 are not valid in XHTML. The use of tags like 

<html>

 is now mandatory. Similarly, 

<img SRC="pic001.jpg">

 is also forbidden because it contains an upper-case attribute. 

Third, closing tags are required, even for 

</p>

. For tags that have no natural closing tag, such as 

<br>

, 

<hr>

, and 

<img>

, a slash must precede the closing ''>,'' for example 

<img src="pic001.jpg" />  

Fourth, attributes must be contained within quotation marks. For example, 

<img SRC="pic001.jpg" height=500 />  

is no longer allowed. The 500 has to be enclosed in quotation marks, just like the name of the JPEG file, even though 500 is just a number. 

Fifth, tags must nest properly. In the past, proper nesting was not required as long as the final state achieved was correct. For example, 

<center> <b> Vacation Pictures </center> </b>  

used to be legal. In XHTML it is not. Tags must be closed in the inverse order that they were opened. 

Sixth, every document must specify its document type. We saw this in 

Fig. 7-32

, for example. For a discussion of all the changes, major and minor, see 

www.w3.org

. 

7.3.3 Dynamic Web Documents 

So far, the model we have used is that of 

Fig. 6-6

: the client sends a file name to the server, which then returns the file. In the early days of the Web, all content was, in fact, static like this (just files). However, in recent years, more and more content has become dynamic, that is, generated on demand, rather than stored on disk. Content generation can take place either on the server side or on the client side. Let us now examine each of these cases in turn. 

Server-Side Dynamic Web Page Generation 

To see why server-side content generation is needed, consider the use of forms, as described earlier. When a user fills in a form and clicks on the 

submit

 button, a message is sent to the server indicating that it contains the contents of a form, along with the fields the user filled in. This message is not the name of a file to return. What is needed is that the message is given 

495




to a program or script to process. Usually, the processing involves using the user-supplied information to look up a record in a database on the server's disk and generate a custom HTML page to send back to the client. For example, in an e-commerce application, after the user clicks on 

PROCEED TO CHECKOUT

, the browser returns the cookie containing the contents of the shopping cart, but some program or script on the server has to be invoked to process the cookie and generate an HTML page in response. The HTML page might display a form containing the list of items in the cart and the user's last-known shipping address along with a request to verify the information and to specify the method of payment. The steps required to process the information from an HTML form are illustrated in 

Fig. 7-33

. 

Figure 7-33. Steps in processing the information from an HTML form. 

 

The traditional way to handle forms and other interactive Web pages is a system called the 

CGI

 (

Common Gateway Interface

). It is a standardized interface to allow Web servers to talk to back-end programs and scripts that can accept input (e.g., from forms) and generate HTML pages in response. Usually, these back-ends are scripts written in the Perl scripting language because Perl scripts are easier and faster to write than programs (at least, if you know how to program in Perl). By convention, they live in a directory called 

cgi-bin

, which is visible in the URL. Sometimes another scripting language, Python, is used instead of Perl. 

As an example of how CGI often works, consider the case of a product from the Truly Great Products Company that comes without a warranty registration card. Instead, the customer is told to go to 

www.tgpc.com

 to register on-line. On that page, there is a hyperlink that says 

Click here to register your product  

This link points to a Perl script, say, 

www.tgpc.com/cgi-bin/reg.perl

. When this script is invoked with no parameters, it sends back an HTML page containing the registration form. When the user fills in the form and clicks on 

submit

, a message is sent back to this script containing the values filled in using the style of 

Fig. 7-30

. The Perl script then parses the parameters, makes an entry in the database for the new customer, and sends back an HTML page providing a registration number and a telephone number for the help desk. This is not the only way to handle forms, but it is a common way. There are many books about making CGI scripts and programming in Perl. A few examples are (Hanegan, 2001; Lash, 2002; and Meltzer and Michalski, 2001). 

CGI scripts are not the only way to generate dynamic content on the server side. Another common way is to embed little scripts inside HTML pages and have them be executed by the server itself to generate the page. A popular language for writing these scripts is 

PHP

 (

PHP: Hypertext Preprocessor

). To use it, the server has to understand PHP (just as a browser has to understand XML to interpret Web pages written in XML). Usually, servers expect Web pages containing PHP to have file extension 

php

 rather than 

html

 or 

htm

. 

A tiny PHP script is illustrated in 

Fig. 7-34

; it should work with any server that has PHP installed. It contains normal HTML, except for the PHP script inside the 

<?php ... ?>

 tag. What it does is generate a Web page telling what it knows about the browser invoking it. Browsers normally send over some information along with their request (and any applicable cookies) and this information is put in the variable 

HTTP_USER_AGENT

. When this listing is put 

496




in a file 

test.php

 in the WWW directory at the ABCD company, then typing the URL 

www.abcd.com/test.php

 will produce a Web page telling the user what browser, language, and operating system he is using. 

Figure 7-34. A sample HTML page with embedded PHP. 

 

PHP is especially good at handling forms and is simpler than using a CGI script. As an example of how it works with forms, consider the example of 

Fig. 7-35(a)

. This figure contains a normal HTML page with a form in it. The only unusual thing about it is the first line, which specifies that the file 

action.php

 is to be invoked to handle the parameters after the user has filled in and submitted the form. The page displays two text boxes, one with a request for a name and one with a request for an age. After the two boxes have been filled in and the form submitted, the server parses the 

Fig. 7-30

-type string sent back, putting the name in the 

name

 variable and the age in the 

age

 variable. It then starts to process the 

action.php

 file, shown in 

Fig. 7-

35(b)

 as a reply. During the processing of this file, the PHP commands are executed. If the user filled in ''Barbara'' and ''24'' in the boxes, the HTML file sent back will be the one given in 

Fig. 7-35(c)

. Thus, handling forms becomes extremely simple using PHP. 

Figure 7-35. (a) A Web page containing a form. (b) A PHP script for handling the output of the form. (c) Output from the PHP script when the inputs are ''Barbara'' and 24, respectively. 

497




 

Although PHP is easy to use, it is actually a powerful programming language oriented toward interfacing between the Web and a server database. It has variables, strings, arrays, and most of the control structures found in C, but much more powerful I/O than just 

printf

. PHP is open source code and freely available. It was designed specifically to work well with Apache, which is also open source and is the world's most widely used Web server. For more information about PHP, see (Valade, 2002). 

We have now seen two different ways to generate dynamic HTML pages: CGI scripts and embedded PHP. There is also a third technique, called 

JSP

 (

JavaServer Pages

), which is similar to PHP, except that the dynamic part is written in the Java programming language instead of in PHP. Pages using this technique have the file extension 

jsp

. A fourth technique, 

ASP

 (

Active Server Pages

), is Microsoft's version of PHP and JavaServer Pages. It uses Microsoft's proprietary scripting language, Visual Basic Script, for generating the dynamic content. Pages using this technique have extension 

asp

. The choice among 

PHP

, 

JSP

, and 

ASP

 usually has more to do with politics (open source vs. Sun vs. Microsoft) than with technology, since the three languages are roughly comparable. 

The collection of technologies for generating content on the fly is sometimes called 

dynamic HTML

. 

Client-Side Dynamic Web Page Generation 

CGI, PHP, JSP, and ASP scripts solve the problem of handling forms and interactions with databases on the server. They can all accept incoming information from forms, look up information in one or more databases, and generate HTML pages with the results. What none of them can do is respond to mouse movements or interact with users directly. For this purpose, it is necessary to have scripts embedded in HTML pages that are executed on the client machine rather than the server machine. Starting with HTML 4.0, such scripts are 

498




permitted using the tag 

<script>

. The most popular scripting language for the client side is 

JavaScript

, so we will now take a quick look at it. 

JavaScript is a scripting language, 

very

 loosely inspired by some ideas from the Java programming language. It is definitely not Java. Like other scripting languages, it is a very high level language. For example, in a single line of JavaScript it is possible to pop up a dialog box, wait for text input, and store the resulting string in a variable. High-level features like this make JavaScript ideal for designing interactive Web pages. On the other hand, the fact that it is not standardized and is mutating faster than a fruit fly trapped in an X-ray machine makes it extremely difficult to write JavaScript programs that work on all platforms, but maybe some day it will stabilize. 

As an example of a program in JavaScript, consider that of 

Fig. 7-36

. Like that of 

Fig. 7-35(a)

, it displays a form asking for a name and age, and then predicts how old the person will be next year. The body is almost the same as the PHP example, the main difference being the declaration of the submit button and the assignment statement in it. This assignment statement tells the browser to invoke the 

response

 script on a button click and pass it the form as a parameter. 

Figure 7-36. Use of JavaScript for processing a form. 

 

What is completely new here is the declaration of the JavaScript function 

response

 in the head of the HTML file, an area normally reserved for titles, background colors, and so on. This function extracts the value of the 

name

 field from the form and stores it in the variable 

person

 as a string. It also extracts the value of the 

age

 field, converts it to an integer by using the 

eval

 function, adds 1 to it, and stores the result in 

years

. Then it opens a document for output, does four writes to it using the 

writeln

 method, and closes the document. The document is an HTML file, as can be seen from the various HTML tags in it. The browser then displays the document on the screen. 

499




It is very important to understand that while 

Fig. 7-35

 and 

Fig. 7-36

 look similar, they are processed totally differently. In 

Fig. 7-35

, after the user has clicked on the 

submit

 button, the browser collects the information into a long string of the 

style of 

Fig. 7-30

 and sends it off to the server that sent the page. The server sees the name of the PHP file and executes it. The PHP script produces a new HTML page and that page is sent back to the browser for display. With 

Fig. 7-36

, when the 

submit

 button is clicked the browser interprets a JavaScript function contained on the page. All the work is done locally, inside the browser. There is no contact with the server. As a consequence, the result is displayed virtually instantaneously, whereas with PHP, there can be a delay of several seconds before the resulting HTML arrives at the client. The difference between server-side scripting and client-side scripting is illustrated in 

Fig. 7-37

, including the steps involved. In both cases, the numbered steps start after the form has been displayed. Step 1 consists of accepting the user input. Then comes the processing of the input, which differs in the two cases. 

Figure 7-37. (a) Server-side scripting with PHP. (b) Client-side scripting with JavaScript. 

 

This difference does not mean that JavaScript is better than PHP. Their uses are completely different. PHP (and, by implication, JSP and ASP) are used when interaction with a remote database is needed. JavaScript is used when the interaction is with the user at the client computer. It is certainly possible (and common) to have HTML pages that use both PHP and JavaScript, although they cannot do the same work or own the same button, of course. 

JavaScript is a full-blown programming language, with all the power of C or Java. It has variables, strings, arrays, objects, functions, and all the usual control structures. It also has a large number of facilities specific for Web pages, including the ability to manage windows and frames, set and get cookies, deal with forms, and handle hyperlinks. An example of a JavaScript program that uses a recursive function is given in 

Fig. 7-38

. 

Figure 7-38. A JavaScript program for computing and printing factorials. 

500




 

JavaScript can also track mouse motion over objects on the screen. Many JavaScript Web pages have the property that when the mouse cursor is moved over some text or image, something happens. Often the image changes or a menu suddenly appears. This kind of behavior is easy to program in JavaScript and leads to lively Web pages. An example is given in 

Fig. 7-39

. 

Figure 7-39. An interactive Web page that responds to mouse movement. 

 

JavaScript is not the only way to make Web pages highly interactive. Another popular method is through the use of 

applets

. These are small Java programs that have been compiled into machine instructions for a virtual computer called the 

JVM

 (

Java Virtual Machine

). Applets can be embedded in HTML pages (between 

<applet>

 and 

</applet>

) and interpreted by JVM-capable browsers. Because Java applets are interpreted rather than directly executed, the Java interpreter can prevent them from doing Bad Things. At least in theory. In practice, applet writers have found a nearly endless stream of bugs in the Java I/O libraries to exploit. 

501




Microsoft's answer to Sun's Java applets was allowing Web pages to hold 

ActiveX controls

, which are programs compiled to Pentium machine language and executed on the bare hardware. This feature makes them vastly faster and more flexible than interpreted Java applets because they can do anything a program can do. When Internet Explorer sees an ActiveX control in a Web page, it downloads it, verifies its identity, and executes it. However, downloading and running foreign programs raises security issues, which we will address in 

Chap. 8

. 

Since nearly all browsers can interpret both Java programs and JavaScript, a designer who wants to make a highly-interactive Web page has a choice of at least two techniques, and if portability to multiple platforms is not an issue, ActiveX in addition. As a general rule, JavaScript programs are easier to write, Java applets execute faster, and ActiveX controls run fastest of all. Also, since all browers implement exactly the same JVM but no two browsers implement the same version of JavaScript, Java applets are more portable than JavaScript programs. For more information about JavaScript, there are many books, each with many (often > 1000) pages. A few examples are (Easttom, 2001; Harris, 2001; and McFedries, 2001). 

Before leaving the subject of dynamic Web content, let us briefly summarize what we have covered so far. Complete Web pages can be generated on-the-fly by various scripts on the server machine. Once they are received by the browser, they are treated as normal HTML pages and just displayed. The scripts can be written in Perl, PHP, JSP, or ASP, as shown in 

Fig. 

7-40

. 

Figure 7-40. The various ways to generate and display content. 

 

Dynamic content generation is also possible on the client side. Web pages can be written in XML and then converted to HTML according to an XSL file. JavaScript programs can perform arbitrary computations. Finally, plug-ins and helper applications can be used to display content in a variety of formats. 

7.3.4 HTTP—The HyperText Transfer Protocol 

The transfer protocol used throughout the World Wide Web is 

HTTP

 (

HyperText Transfer Protocol

). It specifies what messages clients may send to servers and what responses they get back in return. Each interaction consists of one ASCII request, followed by one RFC 822 MIME-like response. All clients and all servers must obey this protocol. It is defined in RFC 2616. In this section we will look at some of its more important properties. 

Connections 

The usual way for a browser to contact a server is to establish a TCP connection to port 80 on the server's machine, although this procedure is not formally required. The value of using TCP is that neither browsers nor servers have to worry about lost messages, duplicate messages, 

502




long messages, or acknowledgements. All of these matters are handled by the TCP implementation. 

In HTTP 1.0, after the connection was established, a single request was sent over and a single response was sent back. Then the TCP connection was released. In a world in which the typical Web page consisted entirely of HTML text, this method was adequate. Within a few years, the average Web page contained large numbers of icons, images, and other eye candy, so establishing a TCP connection to transport a single icon became a very expensive way to operate. 

This observation led to HTTP 1.1, which supports 

persistent connections

. With them, it is possible to establish a TCP connection, send a request and get a response, and then send additional requests and get additional responses. By amortizing the TCP setup and release over multiple requests, the relative overhead due to TCP is much less per request. It is also possible to pipeline requests, that is, send request 2 before the response to request 1 has arrived. 

Methods 

Although HTTP was designed for use in the Web, it has been intentionally made more general than necessary with an eye to future object-oriented applications. For this reason, operations, called 

methods

, other than just requesting a Web page are supported. This generality is what permitted SOAP to come into existence. Each request consists of one or more lines of ASCII text, with the first word on the first line being the name of the method requested. The built-in methods are listed in 

Fig. 7-41

. For accessing general objects, additional object-specific methods may also be available. The names are case sensitive, so 

GET

 is a legal method but 

get

 is not. 

Figure 7-41. The built-in HTTP request methods. 

 

The 

GET

 method requests the server to send the page (by which we mean object, in the most general case, but in practice normally just a file). The page is suitably encoded in MIME. The vast majority of requests to Web servers are 

GET

s. The usual form of 

GET

 is 

GET filename HTTP/1.1  

where 

filename

 names the resource (file) to be fetched and 1.1 is the protocol version being used. 

The 

HEAD

 method just asks for the message header, without the actual page. This method can be used to get a page's time of last modification, to collect information for indexing purposes, or just to test a URL for validity. 

The 

PUT

 method is the reverse of 

GET

: instead of reading the page, it writes the page. This method makes it possible to build a collection of Web pages on a remote server. The body of 

503




the request contains the page. It may be encoded using MIME, in which case the lines following the 

PUT

 might include 

Content-Type

 and authentication headers, to prove that the caller indeed has permission to perform the requested operation. 

Somewhat similar to 

PUT

 is the 

POST

 method. It, too, bears a URL, but instead of replacing the existing data, the new data is ''appended'' to it in some generalized sense. Posting a message to a newsgroup or adding a file to a bulletin board system are examples of appending in this context. In practice, neither 

PUT

 nor 

POST

 is used very much. 

DELETE

 does what you might expect: it removes the page. As with 

PUT

, authentication and permission play a major role here. There is no guarantee that 

DELETE

 succeeds, since even if the remote HTTP server is willing to delete the page, the underlying file may have a mode that forbids the HTTP server from modifying or removing it. 

The 

TRACE

 method is for debugging. It instructs the server to send back the request. This method is useful when requests are not being processed correctly and the client wants to know what request the server actually got. 

The 

CONNECT

 method is not currently used. It is reserved for future use. 

The 

OPTIONS

 method provides a way for the client to query the server about its properties or those of a specific file. 

Every request gets a response consisting of a status line, and possibly additional information (e.g., all or part of a Web page). The status line contains a three-digit status code telling whether the request was satisfied, and if not, why not. The first digit is used to divide the responses into five major groups, as shown in 

Fig. 7-42

. The 1xx codes are rarely used in practice. The 2xx codes mean that the request was handled successfully and the content (if any) is being returned. The 3xx codes tell the client to look elsewhere, either using a different URL or in its own cache (discussed later). The 4xx codes mean the request failed due to a client error such an invalid request or a nonexistent page. Finally, the 5xx errors mean the server itself has a problem, either due to an error in its code or to a temporary overload. 

Figure 7-42. The status code response groups. 

 

Message Headers 

The request line (e.g., the line with the 

GET

 method) may be followed by additional lines with more information. They are called 

request headers

. This information can be compared to the parameters of a procedure call. Responses may also have 

response headers

. Some headers can be used in either direction. A selection of the most important ones is given in 

Fig. 7-43

. 

Figure 7-43. Some HTTP message headers. 

504




 

The 

User-Agent

 header allows the client to inform the server about its browser, operating system, and other properties. In 

Fig. 7-34

 we saw that the server magically had this information and could produce it on demand in a PHP script. This header is used by the client to provide the server with the information. 

The four 

Accept

 headers tell the server what the client is willing to accept in the event that it has a limited repertoire of what is acceptable. The first header specifies the MIME types that are welcome (e.g., text/html). The second gives the character set (e.g., ISO-8859-5 or Unicode-1-1). The third deals with compression methods (e.g., gzip). The fourth indicates a natural language (e.g., Spanish) If the server has a choice of pages, it can use this information to supply the one the client is looking for. If it is unable to satisfy the request, an error code is returned and the request fails. 

The 

Host

 header names the server. It is taken from the URL. This header is mandatory. It is used because some IP addresses may serve multiple DNS names and the server needs some way to tell which host to hand the request to. 

The 

Authorization

 header is needed for pages that are protected. In this case, the client may have to prove it has a right to see the page requested. This header is used for that case. 

Although cookies are dealt with in RFC 2109 rather than RFC 2616, they also have two headers. The 

Cookie

 header is used by clients to return to the server a cookie that was previously sent by some machine in the server's domain. 

The 

Date

 header can be used in both directions and contains the time and date the message was sent. The 

Upgrade

 header is used to make it easier to make the transition to a future (possibly incompatible) version of the HTTP protocol. It allows the client to announce what it can support and the server to assert what it is using. 

Now we come to the headers used exclusively by the server in response to requests. The first one, 

Server

, allows the server to tell who it is and some of its properties if it wishes. 

505




The next four headers, all starting with 

Content-

, allow the server to describe properties of the page it is sending. 

The 

Last-Modified

 header tells when the page was last modified. This header plays an important role in page caching. 

The 

Location

 header is used by the server to inform the client that it should try a different URL. This can be used if the page has moved or to allow multiple URLs to refer to the same page (possibly on different servers). It is also used for companies that have a main Web page in the 

com

 domain, but which redirect clients to a national or regional page based on their IP address or preferred language. 

If a page is very large, a small client may not want it all at once. Some servers will accept requests for byte ranges, so the page can be fetched in multiple small units. The 

Accept-Ranges

 header announces the server's willingness to handle this type of partial page request. 

The second cookie header, 

Set-Cookie

, is how servers send cookies to clients. The client is expected to save the cookie and return it on subsequent requests to the server. 

Example HTTP Usage 

Because HTTP is an ASCII protocol, it is quite easy for a person at a terminal (as opposed to a browser) to directly talk to Web servers. All that is needed is a TCP connection to port 80 on the server. Readers are encouraged to try this scenario personally (preferably from a UNIX system, because some other systems do not return the connection status). The following command sequence will do it: 

telnet www.ietf.org 80 >log  

GET /rfc.html HTTP/1.1  

Host: www.ietf.org  

 

close  

This sequence of commands starts up a telnet (i.e., TCP) connection to port 80 on IETF's Web server, 

www.ietf.org

. The result of the session is redirected to the file 

log

 for later inspection. Then comes the 

GET

 command naming the file and the protocol. The next line is the mandatory 

Host

 header. The blank line is also required. It signals the server that there are no more request headers. The 

close

 command instructs the telnet program to break the connection. 

The 

log

 can be inspected using any editor. It should start out similarly to the listing in 

Fig. 7-

44

, unless IETF has changed it recently. 

Figure 7-44. The start of the output of 

www.ietf.org/rfc.html

. 

506




 

The first three lines are output from the telnet program, not from the remote site. The line beginning HTTP/1.1 is IETF's response saying that it is willing to talk HTTP/1.1 with you. Then come a number of headers and then the content. We have seen all the headers already except for 

ETag

 which is a unique page identifier related to caching, and 

X-Pad

 which is nonstandard and probably a workaround for some buggy browser. 

7.3.5 Performance Enhancements 

The popularity of the Web has almost been its undoing. Servers, routers, and lines are frequently overloaded. Many people have begun calling the WWW the World Wide Wait. As a consequence of these endless delays, researchers have developed various techniques for improving performance. We will now examine three of them: caching, server replication, and content delivery networks. 

Caching 

A fairly simple way to improve performance is to save pages that have been requested in case they are used again. This technique is especially effective with pages that are visited a great deal, such as 

www.yahoo.com

 and 

www.cnn.com

. Squirreling away pages for subsequent use is called 

caching

. The usual procedure is for some process, called a 

proxy

, to maintain the cache. To use caching, a browser can be configured to make all page requests to a proxy instead of to the page's real server. If the proxy has the page, it returns the page immediately. If not, it fetches the page from||the server, adds it to the cache for future use, and returns it to the client that requested it. 

Two important questions related to caching are as follows: 

1. Who should do the caching? 

507




2. How long should pages be cached? 

There are several answers to the first question. Individual PCs often run proxies so they can quickly look up pages previously visited. On a company LAN, the proxy is often a machine shared by all the machines on the LAN, so if one user looks at a certain page and then another one on the same LAN wants the same page, it can be fetched from the proxy's cache. Many ISPs also run proxies, in order to speed up access for all their customers. Often all of these caches operate at the same time, so requests first go to the local proxy. If that fails, the local proxy queries the LAN proxy. If that fails, the LAN proxy tries the ISP proxy. The latter must succeed, either from its cache, a higher-level cache, or from the server itself. A scheme involving multiple caches tried in sequence is called 

hierarchical caching

. A possible implementation is illustrated in 

Fig. 7-45

. 

Figure 7-45. Hierarchical caching with three proxies. 

 

How long should pages be cached is a bit trickier. Some pages should not be cached at all. For example, a page containing the prices of the 50 most active stocks changes every second. If it were to be cached, a user getting a copy from the cache would get 

stale

 (i.e., obsolete) data. On the other hand, once the stock exchange has closed for the day, that page will remain valid for hours or days, until the next trading session starts. Thus, the cacheability of a page may vary wildly over time. 

The key issue with determining when to evict a page from the cache is how much staleness users are willing to put up with (since cached pages are kept on disk, the amount of storage consumed is rarely an issue). If a proxy throws out pages quickly, it will rarely return a stale page but it will also not be very effective (i.e., have a low hit rate). If it keeps pages too long, it may have a high hit rate but at the expense of often returning stale pages. 

There are two approaches to dealing with this problem. The first one uses a heuristic to guess how long to keep each page. A common one is to base the holding time on the 

Last-Modified

 header (see 

Fig. 7-43

). If a page was modified an hour ago, it is held in the cache for an hour. If it was modified a year ago, it is obviously a very stable page (say, a list of the gods from Greek and Roman mythology), so it can be cached for a year with a reasonable expectation of it not changing during the year. While this heuristic often works well in practice, it does return stale pages from time to time. 

The other approach is more expensive but eliminates the possibility of stale pages by using special features of RFC 2616 that deal with cache management. One of the most useful of these features is the 

If-Modified-Since

 request header, which a proxy can send to a server. It specifies the page the proxy wants and the time the cached page was last modified (from the 

Last-Modified

 header). If the page has not been modified since then, the server sends back a short 

Not Modified

 message (status code 304 in 

Fig. 7-42

), which instructs the proxy to use the cached page. If the page has been modified since then, the new page is returned. While this approach always requires a request message and a reply message, the reply message will be very short when the cache entry is still valid. 

These two approaches can easily be combined. For the first ?

T

 after fetching the page, the proxy just returns it to clients asking for it. After the page has been around for a while, the 

508




proxy uses 

If-Modified-Since

 messages to check on its freshness. Choosing ?

T

 invariably involves some kind of heuristic, depending on how long ago the page was last modified. 

Web pages containing dynamic content (e.g., generated by a PHP script) should never be cached since the parameters may be different next time. To handle this and other cases, there is a general mechanism for a server to instruct all proxies along the path back to the client not to use the current page again without verifying its freshness. This mechanism can also be used for any page expected to change quickly. A variety of other cache control mechanisms are also defined in RFC 2616. 

Yet another approach to improving performance is proactive caching. When a proxy fetches a page from a server, it can inspect the page to see if there are any hyperlinks on it. If so, it can issue requests to the relevant servers to preload the cache with the pages pointed to, just in case they are needed. This technique may reduce access time on subsequent requests, but it may also flood the communication lines with pages that are never needed. 

Clearly, Web caching is far from trivial. A lot more can be said about it. In fact, entire books have been written about it, for example (Rabinovich and Spatscheck, 2002; and Wessels, 2001); But it is time for us to move on to the next topic. 

Server Replication 

Caching is a client-side technique for improving performance, but server-side techniques also exist. The most common approach that servers take to improve performance is to replicate their contents at multiple, widely-separated locations. This technique is sometimes called 

mirroring

. 

A typical use of mirroring is for a company's main Web page to contain a few images along with links for, say, the company's Eastern, Western, Northern, and Southern regional Web sites. The user then clicks on the nearest one to get to that server. From then on, all requests go to the server selected. 

Mirrored sites are generally completely static. The company decides where it wants to place the mirrors, arranges for a server in each region, and puts more or less the full content at each location (possibly omitting the snow blowers from the Miami site and the beach blankets from the Anchorage site). The choice of sites generally remains stable for months or years. 

Unfortunately, the Web has a phenomenon known as 

flash crowds

 in which a Web site that was previously an unknown, unvisited, backwater all of a sudden becomes the center of the known universe. For example, until Nov. 6, 2000, the Florida Secretary of State's Web site, 

www.dos.state.fl.us

, was quietly providing minutes of the meetings of the Florida State cabinet and instructions on how to become a notary in Florida. But on Nov. 7, 2000, when the U.S. Presidency suddenly hinged on a few thousand disputed votes in a handful of Florida counties, it became one of the top five Web sites in the world. Needless to say, it could not handle the load and nearly died trying. 

What is needed is a way for a Web site that suddenly notices a massive increase in traffic to automatically clone itself at as many locations as needed and keep those sites operational until the storm passes, at which time it shuts many or all of them down. To have this ability, a site needs an agreement in advance with some company that owns many hosting sites, saying that it can create replicas on demand and pay for the capacity it actually uses. 

An even more flexible strategy is to create dynamic replicas on a per-page basis depending on where the traffic is coming from. Some research on this topic is reported in (Pierre et al., 2001; and Pierre et al., 2002). 

509




Content Delivery Networks 

The brilliance of capitalism is that somebody has figured out how to make money from the World Wide Wait. It works like this. Companies called 

CDN

s (

Content Delivery Networks

) talk to content providers (music sites, newspapers, and others that want their content easily and rapidly available) and offer to deliver their content to end users efficiently for a fee. After the contract is signed, the content owner gives the CDN the contents of its Web site for preprocessing (discussed shortly) and then distribution. 

Then the CDN talks to large numbers of ISPs and offers to pay them well for permission to place a remotely-managed server bulging with valuable content on their LANs. Not only is this a source of income, but it also provides the ISP's customers with excellent response time for getting at the CDN's content, thereby giving the ISP a competitive advantage over other ISPs that have not taken the free money from the CDN. Under these conditions, signing up with a CDN is kind of a no-brainer for the ISP. As a consequence, the largest CDNs have more than 10,000 servers deployed all over the world. 

With the content replicated at thousands of sites worldwide, there is clearly great potential for improving performance. However, to make this work, there has to be a way to redirect the client's request to the nearest CDN server, preferably one colocated at the client's ISP. Also, this redirection must be done without modifying DNS or any other part of the Internet's standard infrastructure. A slightly simplified description of how Akamai, the largest CDN, does it follows. 

The whole process starts when the content provider hands the CDN its Web site. The CDN then runs each page through a preprocessor that replaces all the URLs with modified ones. The working model behind this strategy is that the content provider's Web site consists of many pages that are tiny (just HTML text), but that these pages often link to large files, such as images, audio, and video. The modified HTML pages are stored on the content provider's server and are fetched in the usual way; it is the images, audio, and video that go on the CDN's servers. 

To see how this scheme actually works, consider Furry Video's Web page of 

Fig. 7-46(a)

. After preprocessing, it is transformed to 

Fig. 7-46(b)

 and placed on Furry Video's server as 

www.furryvideo.com/index.html

. 

Figure 7-46. (a) Original Web page. (b) Same page after transformation. 

510




 

When a user types in the URL 

www.furryvideo.com

, DNS returns the IP address of Furry Video's own Web site, allowing the main (HTML) page to be fetched in the normal way. When any of the hyperlinks is clicked on, the browser asks DNS to look up 

cdn-server.com

, which it does. The browser then sends an HTTP request to this IP address, expecting to get back an MPEG file. 

That does not happen because 

cdn-server.com

 does not host any content. Instead, it is CDN's fake HTTP server. It examines the file name and server name to find out which page at which content provider is needed. It also examines the IP address of the incoming request and looks it up in its database to determine where the user is likely to be. Armed with this information, it determines which of CDN's content servers can give the user the best service. This decision is difficult because the closest one geographically may not be the closest one in terms of network topology, and the closest one in terms in network topology may be very busy at the moment. After making a choice, 

cdn-server.com

 sends back a response with status code 301 and a 

Location

 header giving the file's URL on the CDN content server nearest to the client. For this example, let us assume that URL is 

www.CDN-0420.com/furryvideo/bears.mpg

. The browser then processes this URL in the usual way to get the actual MPEG file. 

The steps involved are illustrated in 

Fig. 7-47

. The first step is looking up 

www.furryvideo.com

 to get its IP address. After that, the HTML page can be fetched and displayed in the usual way. The page contains three hyperlinks to 

cdn-server

 [see 

Fig. 7-46(b)

]. When, say, the first one is selected, its DNS address is looked up (step 5) and returned (step 6). When a request for 

bears.mpg

 is sent to 

cdn-server

 (step 7), the client is told to go to 

CDN-0420.com

 instead (step 8). When it does as instructed (step 9), it is given the file from the proxy's cache (step 10). The property that makes this whole mechanism work is step 8, the fake HTTP server redirecting the client to a CDN proxy close to the client. 

Figure 7-47. Steps in looking up a URL when a CDN is used. 

511




 

The CDN server to which the client is redirected is typically a proxy with a large cache preloaded with the most important content. If, however, someone asks for a file not in the cache, it is fetched from the true server and placed in the cache for subsequent use. By making the content server a proxy rather than a complete replica, the CDN has the ability to trade off disk size, preload time, and the various performance parameters. 

For more on content delivery networks see (Hull, 2002; and Rabinovich and Spatscheck, 2002). 

7.3.6 The Wireless Web 

There is considerable interest in small portable devices capable of accessing the Web via a wireless link. In fact, the first tentative steps in that direction have already been taken. No doubt there will be a great deal of change in this area in the coming years, but it is still worth examining some of the current ideas relating to the wireless Web to see where we are now and where we might be heading. We will focus on the first two wide area wireless Web systems to hit the market: WAP and i-mode. 

WAP—The Wireless Application Protocol 

Once the Internet and mobile phones had become commonplace, it did not take long before somebody got the idea to combine them into a mobile phone with a built-in screen for wireless access to e-mail and the Web. The ''somebody'' in this case was a consortium initially led by Nokia, Ericsson, Motorola, and phone.com (formerly Unwired Planet) and now boasting hundreds of members. The system is called 

WAP

 (

Wireless Application Protocol

). 

A WAP device may be an enhanced mobile phone, PDA, or notebook computer without any voice capability. The specification allows all of them and more. The basic idea is to use the existing digital wireless infrastructure. Users can literally call up a WAP gateway over the wireless link and send Web page requests to it. The gateway then checks its cache for the page requested. If present, it sends it; if absent, it fetches it over the wired Internet. In essence, this means that WAP 1.0 is a circuit-switched system with a fairly high per-minute connect charge. To make a long story short, people did not like accessing the Internet on a tiny screen and paying by the minute, so WAP was something of a flop (although there were other problems as well). However, WAP and its competitor, i-mode (discussed below), appear to be converging on a similar technology, so WAP 2.0 may yet be a big success. Since WAP 1.0 was the first attempt at wireless Internet, it is worth describing it at least briefly. 

WAP is essentially a protocol stack for accessing the Web, but optimized for low-bandwidth connections using wireless devices having a slow CPU, little memory, and a small screen. These requirements are obviously different from those of the standard desktop PC scenario, which leads to some protocol differences. The layers are shown in 

Fig. 7-48

. 

512




Figure 7-48. The WAP protocol stack. 

 

The lowest layer supports all the existing mobile phone systems, including GSM, D-AMPS, and CDMA. The WAP 1.0 data rate is 9600 bps. On top of this is the datagram protocol, 

WDP

 (

Wireless Datagram Protocol

), which is essentially UDP. Then comes a layer for security, obviously needed in a wireless system. WTLS is a subset of Netscape's SSL, which we will look at in 

Chap. 8

. Above this is a transaction layer, which manages requests and responses, either reliably or unreliably. This layer replaces TCP, which is not used over the air link for efficiency reasons. Then comes a session layer, which is similar to HTTP/1.1 but with some restrictions and extensions for optimization purposes. At the top is a microbrowser (WAE). 

Besides cost, the other aspect that no doubt hurt WAP's acceptance is the fact that it does not use HTML. Instead, the WAE layer uses a markup language called 

WML

 (

Wireless Markup Language

), which is an application of XML. As a consequence, in principle, a WAP device can only access those pages that have been converted to WML. However, since this greatly restricts the value of WAP, the architecture calls for an on-the-fly filter from HTML to WML to increase the set of pages available. This architecture is illustrated in 

Fig. 7-49

. 

Figure 7-49. The WAP architecture. 

 

In all fairness, WAP was probably a little ahead of its time. When WAP was first started, XML was hardly known outside W3C and so the press reported its launch as 

WAP DOES NOT USE HTML.

 A more accurate headline would have been: 

WAP ALREADY USES THE NEW HTML STANDARD.

 But once the damage was done, it was hard to repair and WAP 1.0 never caught on. We will revisit WAP after first looking at its major competitor. 

I-Mode 

While a multi-industry consortium of telecom vendors and computer companies was busy hammering out an open standard using the most advanced version of HTML available, other developments were going on in Japan. There, a Japanese woman, Mari Matsunaga, invented a 

513




different approach to the wireless Web called 

i-mode

 (

information-mode

). She convinced the wireless subsidiary of the former Japanese telephone monopoly that her approach was right, and in Feb. 1999 NTT DoCoMo (literally: Japanese Telephone and Telegraph Company everywhere you go) launched the service in Japan. Within 3 years it had over 35 million Japanese subscribers, who could access over 40,000 special i-mode Web sites. It also had most of the world's telecom companies drooling over its financial success, especially in light of the fact that WAP appeared to be going nowhere. Let us now take a look at what i-mode is and how it works. 

The i-mode system has three major components: a new transmission system, a new handset, and a new language for Web page design. The transmission system consists of two separate networks: the existing circuit-switched mobile phone network (somewhat comparable to D-AMPS), and a new packet-switched network constructed specifically for i-mode service. Voice mode uses the circuit switched network and is billed per minute of connection time. I-mode uses the packet-switched network and is always on (like ADSL or cable), so there is no billing for connect time. Instead, there is a charge for each packet sent. It is not currently possible to use both networks at once. 

The handsets look like mobile phones, with the addition of a small screen. NTT DoCoMo heavily advertises i-mode devices as better mobile phones rather than wireless Web terminals, even though that is precisely what they are. In fact, probably most customers are not even aware they are on the Internet. They think of their i-mode devices as mobile phones with enhanced services. In keeping with this model of i-mode being a service, the handsets are not user programmable, although they contain the equivalent of a 1995 PC and could probably run Windows 95 or UNIX. 

When the i-mode handset is switched on, the user is presented with a list of categories of the officially-approved services. There are well over 1000 services divided into about 20 categories. Each service, which is actually a small i-mode Web site, is run by an independent company. The major categories on the official menu include e-mail, news, weather, sports, games, shopping, maps, horoscopes, entertainment, travel, regional guides, ringing tones, recipes, gambling, home banking, and stock prices. The service is somewhat targeted at teenagers and people in their 20s, who tend to love electronic gadgets, especially if they come in fashionable colors. The mere fact that over 40 companies are selling ringing tones says something. The most popular application is e-mail, which allows up to 500-byte messages, and thus is seen as a big improvement over SMS (Short Message Service) with its 160-byte messages. Games are also popular. 

There are also over 40,000 i-mode Web sites, but they have to be accessed by typing in their URL, rather than selecting them from a menu. In a sense, the official list is like an Internet portal that allows other Web sites to be accessed by clicking rather than by typing a URL. 

NTT DoCoMo tightly controls the official services. To be allowed on the list, a service must meet a variety of published criteria. For example, a service must not have a bad influence on society, Japanese-English dictionaries must have enough words, services with ringing tones must add new tones frequently, and no site may inflame faddish behavior or reflect badly on NTT DoCoMo (Frengle, 2002). The 40,000 Internet sites can do whatever they want. 

The i-mode business model is so different from that of the conventional Internet that it is worth explaining. The basic i-mode subscription fee is a few dollars per month. Since there is a charge for each packet received, the basic subscription includes a small number of packets. Alternatively the customer can choose a subscription with more free packets, with the per-packet charge dropping sharply as you go from 1 MB per month to 10 MB per month. If the free packets are used up halfway through the month, additional packets can be purchased on-line. 

514




To use a service, you have to subscribe to it, something accomplished by just clicking on it and entering your PIN code. Most official services cost around $1–$2 per month. NTT DoCoMo adds the charge to the phone bill and passes 91% of it onto the service provider, keeping 9% itself. If an unofficial service has 1 million customers, it has to send out 1 million bills for (about) $1 each every month. If that service becomes official, NTT DoCoMo handles the billing and just transfers $910,000 to the service's bank account every month. Not having to handle billing is a huge incentive for a service provider to become official, which generates more revenue for NTT DoCoMo. Also, being official gets you on the initial menu, which makes your site much easier to find. The user's phone bill includes phone calls, i-mode subscription charges, service subscription charges, and extra packets. 

Despite its massive success in Japan, it is far from clear whether it will catch on in the U.S. and Europe. In some ways, the Japanese circumstances are different from those in the West. First, most potential customers in the West (e.g., teenagers, college students, and businesspersons) already have a large-screen PC at home, almost assuredly with an Internet connection at a speed of at least 56 kbps, often much more. In Japan, few people have an Internet-connected PC at home, in part due to lack of space, but also due to NTT's exorbitant charges for local telephone services (something like $700 for installing a line and $1.50 per hour for local calls). For most users, i-mode is their only Internet connection. 

Second, people in the West are not used to paying $1 a month to access CNN's Web site, $1 a month to access Yahoo's Web site, $1 a month to access Google's Web site, and so on, not to mention a few dollars per MB downloaded. Most Internet providers in the West now charge a fixed monthly fee independent of actual usage, largely in response to customer demand. 

Third, for many Japanese people, prime i-mode time is while they are commuting to or from work or school on the train or subway. In Europe, fewer people commute by train than in Japan, and in the U.S. hardly anyone does. Using i-mode at home next to your computer with a 17-inch monitor, a 1-Mbps ADSL connection, and all the free megabytes you want does not make a lot of sense. Nevertheless, nobody predicted the immense popularity of mobile phones at all, so i-mode may yet find a niche in the West. 

As we mentioned above, i-mode handsets use the existing circuit-switched network for voice and a new packet-switched network for data. The data network is based on CDMA and transmits 128-byte packets at 9600 bps. A diagram of the network is given in 

Fig. 7-50

. Handsets talk 

LTP

 (

Lightweight Transport Protocol

) over the air link to a protocol conversion gateway. The gateway has a wideband fiber-optic connection to the i-mode server, which is connected to all the services. When the user selects a service from the official menu, the request is sent to the i-mode server, which caches most of the pages to improve performance. Requests to sites not on the official menu bypass the i-mode server and go directly through the Internet. 

Figure 7-50. Structure of the i-mode data network showing the transport protocols. 

515




 

Current handsets have CPUs that run at about 100 MHz, several megabytes of flash ROM, perhaps 1 MB of RAM, and a small built-in screen. I-mode requires the screen to be at least 72 x 94 pixels, but some high-end devices have as many as 120 x 160 pixels. Screens usually have 8-bit color, which allows 256 colors. This is not enough for photographs but is adequate for line drawings and simple cartoons. Since there is no mouse, on-screen navigation is done with the arrow keys. 

The software structure is as shown in 

Fig. 7-51

. The bottom layer consists of a simple real-time operating system for controlling the hardware. Then comes a module for doing network communication, using NTT DoCoMo's proprietary LTP protocol. Above that is a simple window manager that handles text and simple graphics (GIF files). With screens having only about 120 x 160 pixels at best, there is not much to manage. 

Figure 7-51. Structure of the i-mode software. 

 

The fourth layer contains the Web page interpreter (i.e., the browser). I-mode does not use full HTML, but a subset of it, called 

cHTML

 (

compact HTML

), based loosely on HTML 1.0. This layer also allows helper applications and plug-ins, just as PC browsers do. One standard helper application is an interpreter for a slightly modified version of JVM. 

At the top is a user interaction module, which manages communication with the user. 

Let us now take a closer look at cHTML. As mentioned, it is approximately HTML 1.0, with a few omissions and some extensions for use on a mobile handsets. It was submitted to W3C for standardization, but W3C showed little interest in it, so it is likely to remain a proprietary product. 

Most of the basic HTML tags are allowed, including 

<html>

, 

<head>

, 

<title>

, 

<body>

, 

<hn

 

>,

 

<center>,

 

<ul>,

 

<ol>,

 

<menu>,

 

<li>,

 

<br>,

 

<p>,

 

<hr>,

 

<img>,

 

<form>,

 and 

<input>

. The 

<b>

 and 

<i>

 tags are not permitted. 

516




The 

<a>

 tag is allowed for linking to other pages, but with the additional scheme 

tel

 for dialing telephone numbers. In a sense 

tel

 is analogous to 

mailto

. When a hyperlink using the 

mailto

 scheme is selected, the browser pops up a form to send e-mail to the destination named in the link. When a hyperlink using the 

tel

 scheme is selected, the browser dials the telephone number. For example, an address book could have simple pictures of various people. When selecting one of them, the handset would call him or her. RFC 2806 discusses telephone URLs. 

The cHTML browser is limited in other ways. It does not support JavaScript, frames, style sheets, background colors, or background images. It also does not support JPEG images, because they take too much time to decompress. Java applets are allowed, but are (currently) limited to 10 KB due to the slow transmission speed over the air link. 

Although NTT DoCoMo removed some HTML tags, it also added some new ones. The 

<blink>

 tag makes text turn on and off. While it may seem inconsistent to forbid 

<b>

 (on the grounds that Web sites should not handle the appearance) and then add 

<blink>

 which relates only to the appearance, this is how they did it. Another new tag is 

<marquee>

, which scrolls its contents on the screen in the manner of a stock ticker. 

One new feature is the 

align

 attribute for the 

<br>

 tag. It is needed because with a screen of typically 6 rows of 16 characters, there is a great danger of words being broken in the middle, as shown in 

Fig. 7-52(a)

. 

Align

 helps reduce this problem to make it possible to get something more like 

Fig. 7-52(b)

. It is interesting to note that Japanese does not suffer from words being broken over lines. For kanji text, the screen is broken up into a rectangular grid of cells of size 9 x 10 pixels or 12 x 12 pixels, depending on the font supported. Each cell holds exactly one kanji character, which is the equivalent of a word in English. Line breaks between words are always allowed in Japanese. 

Figure 7-52. Lewis Carroll meets a 16 x 6 screen. 

 

Although the Japanese language has tens of thousands of kanji, NTT DoCoMo invented 166 brand new ones, called 

emoji

, with a higher cuteness factor— essentially pictograms like the smileys of 

Fig. 7-6

. They include symbols for the astrological signs, beer, hamburger, amusement park, birthday, mobile phone, dog, cat, Christmas, broken heart, kiss, mood, sleepy, and, of course, one meaning cute. 

Another new attribute is the ability for allowing users to select hyperlinks using the keyboard, clearly an important property on a mouseless computer. An example of how this attribute is used is shown in the cHTML file of 

Fig. 7-53

. 

Figure 7-53. An example cHTML file. 

517




 

Although the client side is somewhat limited, the i-mode server is a full-blown computer, with all the usual bells and whistles. It supports CGI, Perl, PHP, JSP, ASP, and everything else Web servers normally support. 

A brief comparison of the WAP and i-mode as actually implemented in the first-generation systems is given in 

Fig. 7-54

. While some of the differences may seem small, often they are important. For example, 15-year-olds do not have credit cards, so being able to buy things via e-commerce and have them charged to the phone bill makes a big difference in their interest in the system. 

Figure 7-54. A comparison of first-generation WAP and i-mode. 

 

For additional information about i-mode, see (Frengle, 2002; and Vacca, 2002). 

Second-Generation Wireless Web 

WAP 1.0, based on recognized international standards, was supposed to be a serious tool for people in business on the move. It failed. I-mode was an electronic toy for Japanese teenagers using proprietary everything. It was a huge success. What happens next? Each side learned something from the first generation of wireless Web. The WAP consortium learned that content matters. Not having a large number of Web sites that speak your markup language is fatal. NTT DoCoMo learned that a closed, proprietary system closely tied to tiny handsets and Japanese culture is not a good export product. The conclusion that both sides drew is that to convince a large number of Web sites to put their content in your format, it is necessary to have an open, stable, markup language that is universally accepted. Format wars are not good for business. 

Both services are about to enter the second generation of wireless Web technology. WAP 2.0 came out first, so we will use that as our example. WAP 1.0 got some things right, and they 

518




have been continued. For one thing, WAP can be carried on a variety of different networks. The first generation used circuit-switched networks, but packet-switched networks were always an option and still are. Second-generation systems are likely to use packet switching, for example, GPRS. For another, WAP initially was aimed at supporting a wide variety of devices, from mobile phones to powerful notebook computers, and still is. 

WAP 2.0 also has some new features. The most significant ones are: 

1. Push model as well as pull model. 

2. Support for integrating telephony into applications. 

3. Multimedia messaging. 

4. Inclusion of 264 pictograms. 

5. Interface to a storage device. 

6. Support for plug-ins in the browser. 

The pull model is well known: the client asks for a page and gets it. The push model supports data arriving without being asked for, such as a continuous feed of stock prices or traffic alerts. 

Voice and data are starting to merge, and WAP 2.0 supports them in a variety of ways. We saw one example of this earlier with i-mode's ability to hyperlink an icon or text on the screen to a telephone number to be called. Along with e-mail and telephony, multimedia messaging is supported. 

The huge popularity of i-mode's emoji stimulated the WAP consortium to invent 264 of its own emoji. The categories include animals, appliances, dress, emotion, food, human body, gender, maps, music, plants, sports, time, tools, vehicles, weapons, and weather. Interesting enough, the standard just names each pictogram; it does not give the actual bit map, probably out of fear that some culture's representation of ''sleepy'' or ''hug'' might be insulting to another culture. I-mode did not have that problem since it was intended for a single country. 

Providing for a storage interface does not mean that every WAP 2.0 phone will come with a large hard disk. Flash ROM is also a storage device. A WAP-enabled wireless camera could use the flash ROM for temporary image storage before beaming the best pictures to the Internet. 

Finally, plug-ins can extend the browser's capabilities. A scripting language is also provided. 

Various technical differences are also present in WAP 2.0. The two biggest ones concern the protocol stack and the markup language. WAP 2.0 continues to support the old protocol stack of 

Fig. 7-48

, but it also supports the standard Internet stack with TCP and HTTP/1.1 as well. However, four minor (but compatible) changes to TCP were made (to simplify the code): (1) Use of a fixed 64-KB window, (2) no slow start, (3) a maximum MTU of 1500 bytes, and (4) a slightly different retransmission algorithm. TLS is the transport-layer security protocol standardized by IETF; we will examine it in 

Chap. 8

. Many initial devices will probably contain both stacks, as shown in 

Fig. 7-55

. 

Figure 7-55. WAP 2.0 supports two protocol stacks. 

519




 

The other technical difference with WAP 1.0 is the markup language. WAP 2.0 supports XHTML Basic, which is intended for small wireless devices. Since NTT DoCoMo has also agreed to support this subset, Web site designers can use this format and know that their pages will work on the fixed Internet and on all wireless devices. These decisions will end the markup language format wars that were impeding growth of the wireless Web industry. 

A few words about XHTML Basic are perhaps in order. It is intended for mobile phones, televisions, PDAs, vending machines, pagers, cars, game machines, and even watches. For this reason, it does not support style sheets, scripts, or frames, but most of the standard tags are there. They are grouped into 11 modules. Some are required; some are optional. All are defined in XML. The modules and some example tags are listed in 

Fig. 7-56

. We have not gone over all the example tags, but more information can be found at 

www.w3.org

. 

Figure 7-56. The XHTML Basic modules and tags. 

 

Despite the agreement on the use of XHTML Basic, a threat to WAP and i-mode is lurking in the air: 802.11. The second-generation wireless Web is supposed to run at 384 kbps, far better than the 9600 bps of the first generation, but far worse than the 11 Mbps or 54 Mbps offered by 802.11. Of course, 802.11 is not everywhere, but as more restaurants, hotels, stores, companies, airports, bus stations, museums, universities, hospitals, and other organizations decide to install base stations for their employees and customers, there may be enough coverage in urban areas that people are willing to walk a few blocks to sit down in an 802.11-enabled fast food restaurant for a cup of coffee and an e-mail. Businesses may routinely put 802.11 logos next to the logos that show which credit cards they accept, and for the same reason: to attract customers. City maps (downloadable, naturally) may show covered areas in green and silent areas in red, so people can wander from base station to base station, like nomads trekking from oasis to oasis in the desert. 

Although fast food restaurants may be quick to install 802.11 base stations, farmers will probably not, so coverage will be spotty and limited to the downtown areas of cities, due to the 

520




limited range of 802.11 (a few hundred meters at best). This may lead to dual-mode wireless devices that use 802.11 if they can pick up a signal and fall back to WAP if they cannot. 

7.4 Multimedia 

The wireless Web is an exciting new development, but it is not the only one. For many people, multimedia is the holy grail of networking. When the word is mentioned, both the propeller heads and the suits begin salivating as if on cue. The former see immense technical challenges in providing (interactive) video on demand to every home. The latter see equally immense profits in it. Since multimedia requires high bandwidth, getting it to work over fixed connections is hard enough. Even VHS-quality video over wireless is a few years away, so our treatment will focus on wired systems. 

Literally, multimedia is just two or more media. If the publisher of this book wanted to join the current hype about multimedia, it could advertise the book as using multimedia technology. After all, it contains two media: text and graphics (the figures). Nevertheless, when most people refer to multimedia, they generally mean the combination of two or more 

continuous media

, that is, media that have to be played during some well-defined time interval, usually with some user interaction. In practice, the two media are normally audio and video, that is, sound plus moving pictures. 

However, many people often refer to pure audio, such as Internet telephony or Internet radio as multimedia as well, which it is clearly not. Actually, a better term is 

streaming media

, but we will follow the herd and consider real-time audio to be multimedia as well. In the following sections we will examine how computers process audio and video, how they are compressed, and some network applications of these technologies. For a comprehensive (three volume) treatment on networked multimedia, see (Steinmetz and Nahrstedt, 2002; Steinmetz and Nahrstedt, 2003a; and Steinmetz and Nahrstedt, 2003b). 

7.4.1 Introduction to Digital Audio 

An audio (sound) wave is a one-dimensional acoustic (pressure) wave. When an acoustic wave enters the ear, the eardrum vibrates, causing the tiny bones of the inner ear to vibrate along with it, sending nerve pulses to the brain. These pulses are perceived as sound by the listener. In a similar way, when an acoustic wave strikes a microphone, the microphone generates an electrical signal, representing the sound amplitude as a function of time. The representation, processing, storage, and transmission of such audio signals are a major part of the study of multimedia systems. 

The frequency range of the human ear runs from 20 Hz to 20,000 Hz. Some animals, notably dogs, can hear higher frequencies. The ear hears logarithmically, so the ratio of two sounds with power 

A

 and 

B

 is conventionally expressed in 

dB

 (

decibels

) according to the formula 

 

 

If we define the lower limit of audibility (a pressure of about 0.0003 dyne/cm

2

) for a 1-kHz sine wave as 0 dB, an ordinary conversation is about 50 dB and the pain threshold is about 120 dB, a dynamic range of a factor of 1 million. 

The ear is surprisingly sensitive to sound variations lasting only a few milliseconds. The eye, in contrast, does not notice changes in light level that last only a few milliseconds. The result of this observation is that jitter of only a few milliseconds during a multimedia transmission affects the perceived sound quality more than it affects the perceived image quality. 

521




Audio waves can be converted to digital form by an 

ADC

 (

Analog Digital Converter

). An ADC takes an electrical voltage as input and generates a binary number as output. In 

Fig. 7-

57(a)

 we see an example of a sine wave. To represent this signal digitally, we can sample it every ?

T

 seconds, as shown by the bar heights in 

Fig. 7-57(b)

. If a sound wave is not a pure sine wave but a linear superposition of sine waves where the highest frequency component present is 

f

, then the Nyquist theorem (see 

Chap. 2

) states that it is sufficient to make samples at a frequency 2

f

. Sampling more often is of no value since the higher frequencies that such sampling could detect are not present. 

Figure 7-57. (a) A sine wave. (b) Sampling the sine wave. (c) Quantizing the samples to 4 bits. 

 

Digital samples are never exact. The samples of 

Fig. 7-57(c)

 allow only nine values, from -1.00 to +1.00 in steps of 0.25. An 8-bit sample would allow 256 distinct values. A 16-bit sample would allow 65,536 distinct values. The error introduced by the finite number of bits per sample is called the 

quantization noise

. If it is too large, the ear detects it. 

Two well-known examples where sampled sound is used are the telephone and audio compact discs. Pulse code modulation, as used within the telephone system, uses 8-bit samples made 8000 times per second. In North America and Japan, 7 bits are for data and 1 is for control; in Europe all 8 bits are for data. This system gives a data rate of 56,000 bps or 64,000 bps. With only 8000 samples/sec, frequencies above 4 kHz are lost. 

Audio CDs are digital with a sampling rate of 44,100 samples/sec, enough to capture frequencies up to 22,050 Hz, which is good enough for people, but bad for canine music lovers. The samples are 16 bits each and are linear over the range of amplitudes. Note that 16-bit samples allow only 65,536 distinct values, even though the dynamic range of the ear is about 1 million when measured in steps of the smallest audible sound. Thus, using only 16 bits per sample introduces some quantization noise (although the full dynamic range is not covered—CDs are not supposed to hurt). With 44,100 samples/sec of 16 bits each, an audio CD needs a bandwidth of 705.6 kbps for monaural and 1.411 Mbps for stereo. While this is lower than what video needs (see below), it still takes almost a full T1 channel to transmit uncompressed CD quality stereo sound in real time. 

Digitized sound can be easily processed by computers in software. Dozens of programs exist for personal computers to allow users to record, display, edit, mix, and store sound waves from multiple sources. Virtually all professional sound recording and editing are digital nowadays. 

Music, of course, is just a special case of general audio, but an important one. Another important special case is speech. Human speech tends to be in the 600-Hz to 6000-Hz range. Speech is made up of vowels and consonants, which have different properties. Vowels are produced when the vocal tract is unobstructed, producing resonances whose fundamental frequency depends on the size and shape of the vocal system and the position of the speaker's tongue and jaw. These sounds are almost periodic for intervals of about 30 msec. Consonants 

522




are produced when the vocal tract is partially blocked. These sounds are less regular than vowels. 

Some speech generation and transmission systems make use of models of the vocal system to reduce speech to a few parameters (e.g., the sizes and shapes of various cavities), rather than just sampling the speech waveform. How these vocoders work is beyond the scope of this book, however. 

7.4.2 Audio Compression 

CD-quality audio requires a transmission bandwidth of 1.411 Mbps, as we just saw. Clearly, substantial compression is needed to make transmission over the Internet practical. For this reason, various audio compression algorithms have been developed. Probably the most popular one is MPEG audio, which has three layers (variants), of which 

MP3

 (

MPEG audio layer 3

) is the most powerful and best known. Large amounts of music in MP3 format are available on the Internet, not all of it legal, which has resulted in numerous lawsuits from the artists and copyright owners. MP3 belongs to the audio portion of the MPEG video compression standard. We will discuss video compression later in this chapter; let us look at audio compression now. 

Audio compression can be done in one of two ways. In 

waveform coding

 the signal is transformed mathematically by a Fourier transform into its frequency components. 

Figure 2-

1(a)

 shows an example function of time and its Fourier amplitudes. The amplitude of each component is then encoded in a minimal way. The goal is to reproduce the waveform accurately at the other end in as few bits as possible. 

The other way, 

perceptual coding

, exploits certain flaws in the human auditory system to encode a signal in such a way that it sounds the same to a human listener, even if it looks quite different on an oscilloscope. Perceptual coding is based on the science of 

psychoacoustics

—how people perceive sound. MP3 is based on perceptual coding. 

The key property of perceptual coding is that some sounds can 

mask

 other sounds. Imagine you are broadcasting a live flute concert on a warm summer day. Then all of a sudden, a crew of workmen nearby turn on their jackhammers and start tearing up the street. No one can hear the flute any more. Its sounds have been masked by the jackhammers. For transmission purposes, it is now sufficient to encode just the frequency band used by the jackhammers because the listeners cannot hear the flute anyway. This is called 

frequency masking

—the ability of a loud sound in one frequency band to hide a softer sound in another frequency band that would have been audible in the absence of the loud sound. In fact, even after the jackhammers stop, the flute will be inaudible for a short period of time because the ear turns down its gain when they start and it takes a finite time to turn it up again. This effect is called 

temporal masking

. 

To make these effects more quantitative, imagine experiment 1. A person in a quiet room puts on headphones connected to a computer's sound card. The computer generates a pure sine wave at 100 Hz at low, but gradually increasing power. The person is instructed to strike a key when she hears the tone. The computer records the current power level and then repeats the experiment at 200 Hz, 300 Hz, and all the other frequencies up to the limit of human hearing. When averaged over many people, a log-log graph of how much power it takes for a tone to be audible looks like that of 

Fig. 7-58(a)

. A direct consequence of this curve is that it is never necessary to encode any frequencies whose power falls below the threshold of audibility. For example, if the power at 100 Hz were 20 dB in 

Fig. 7-58(a)

, it could be omitted from the output with no perceptible loss of quality because 20 dB at 100 Hz falls below the level of audibility. 

523




Figure 7-58. (a) The threshold of audibility as a function of frequency. (b) The masking effect. 

 

Now consider Experiment 2. The computer runs experiment 1 again, but this time with a constant-amplitude sine wave at, say, 150 Hz, superimposed on the test frequency. What we discover is that the threshold of audibility for frequencies near 150 Hz is raised, as shown in 

Fig. 7-58(b)

. 

The consequence of this new observation is that by keeping track of which signals are being masked by more powerful signals in nearby frequency bands, we can omit more and more frequencies in the encoded signal, saving bits. In 

Fig. 7-58

, the 125-Hz signal can be completely omitted from the output and no one will be able to hear the difference. Even after a powerful signal stops in some frequency band, knowledge of its temporal masking properties allow us to continue to omit the masked frequencies for some time interval as the ear recovers. The essence of MP3 is to Fourier-transform the sound to get the power at each frequency and then transmit only the unmasked frequencies, encoding these in as few bits as possible. 

With this information as background, we can now see how the encoding is done. The audio compression is done by sampling the waveform at 32 kHz, 44.1 kHz, or 48 kHz. Sampling can be done on one or two channels, in any of four configurations: 

1. Monophonic (a single input stream). 

2. Dual monophonic (e.g., an English and a Japanese soundtrack). 

3. Disjoint stereo (each channel compressed separately). 

4. Joint stereo (interchannel redundancy fully exploited). 

First, the output bit rate is chosen. MP3 can compress a stereo rock 'n roll CD down to 96 kbps with little perceptible loss in quality, even for rock 'n roll fans with no hearing loss. For a piano concert, at least 128 kbps are needed. These differ because the signal-to-noise ratio for rock 'n roll is much higher than for a piano concert (in an engineering sense, anyway). It is also possible to choose lower output rates and accept some loss in quality. 

Then the samples are processed in groups of 1152 (about 26 msec worth). Each group is first passed through 32 digital filters to get 32 frequency bands. At the same time, the input is fed into a psychoacoustic model in order to determine the masked frequencies. Next, each of the 32 frequency bands is further transformed to provide a finer spectral resolution. 

In the next phase the available bit budget is divided among the bands, with more bits allocated to the bands with the most unmasked spectral power, fewer bits allocated to unmasked bands with less spectral power, and no bits allocated to masked bands. Finally, the bits are encoded using Huffman encoding, which assigns short codes to numbers that appear frequently and long codes to those that occur infrequently. 

524




There is actually more to the story. Various techniques are also used for noise reduction, antialiasing, and exploiting the interchannel redundancy, if possible, but these are beyond the scope of this book. A more formal mathematical introduction to the process is given in (Pan, 1995). 

7.4.3 Streaming Audio 

Let us now move from the technology of digital audio to three of its network applications. Our first one is streaming audio, that is, listening to sound over the Internet. This is also called music on demand. In the next two, we will look at Internet radio and voice over IP, respectively. 

The Internet is full of music Web sites, many of which list song titles that users can click on to play the songs. Some of these are free sites (e.g., new bands looking for publicity); others require payment in return for music, although these often offer some free samples as well (e.g., the first 15 seconds of a song). The most straightforward way to make the music play is illustrated in 

Fig. 7-59

. 

Figure 7-59. A straightforward way to implement clickable music on a Web page. 

 

The process starts when the user clicks on a song. Then the browser goes into action. Step 1 is for it to establish a TCP connection to the Web server to which the song is hyperlinked. Step 2 is to send over a 

GET

 request in HTTP to request the song. Next (steps 3 and 4), the server fetches the song (which is just a file in MP3 or some other format) from the disk and sends it back to the browser. If the file is larger than the server's memory, it may fetch and send the music a block at a time. 

Using the MIME type, for example, 

audio/mp3

, (or the file extension), the browser looks up how it is supposed to display the file. Normally, there will be a helper application such as RealOne Player, Windows Media Player, or Winamp, associated with this type of file. Since the usual way for the browser to communicate with a helper is to write the content to a scratch file, it will save the entire music file as a scratch file on the disk (step 5) first. Then it will start the media player and pass it the name of the scratch file. In step 6, the media player starts fetching and playing the music, block by block. 

In principle, this approach is completely correct and will play the music. The only trouble is that the entire song must be transmitted over the network before the music starts. If the song is 4 MB (a typical size for an MP3 song) and the modem is 56 kbps, the user will be greeted by almost 10 minutes of silence while the song is being downloaded. Not all music lovers like this idea. Especially since the next song will also start with 10 minutes of download time, and the one after that as well. 

To get around this problem without changing how the browser works, music sites have come up with the following scheme. The file linked to the song title is not the actual music file. Instead, it is what is called a 

metafile

, a very short file just naming the music. A typical metafile might be only one line of ASCII text and look like this: 

525




rtsp://joes-audio-server/song-0025.mp3  

When the browser gets the 1-line file, it writes it to disk on a scratch file, starts the media player as a helper, and hands it the name of the scratch file, as usual. The media player then reads the file and sees that it contains a URL. Then it contacts 

joes-audio-server

 and asks for the song. Note that the browser is not in the loop any more. 

In most cases, the server named in the metafile is not the same as the Web server. In fact, it is generally not even an HTTP server, but a specialized media server. In this example, the media server uses 

RTSP

 (

Real Time Streaming Protocol

), as indicated by the scheme name 

rtsp

. It is described in RFC 2326. 

The media player has four major jobs to do: 

1. Manage the user interface. 

2. Handle transmission errors. 

3. Decompress the music. 

4. Eliminate jitter. 

Most media players nowadays have a glitzy user interface, sometimes simulating a stereo unit, with buttons, knobs, sliders, and visual displays. Often there are interchangeable front panels, called 

skins

, that the user can drop onto the player. The media player has to manage all this and interact with the user. 

Its second job is dealing with errors. Real-time music transmission rarely uses TCP because an error and retransmission might introduce an unacceptably long gap in the music. Instead, the actual transmission is usually done with a protocol like RTP, which we studied in 

Chap. 6

. Like most real-time protocols, RTP is layered on top of UDP, so packets may be lost. It is up to the player to deal with this. 

In some cases, the music is interleaved to make error handling easier to do. For example, a packet might contain 220 stereo samples, each containing a pair of 16-bit numbers, normally good for 5 msec of music. But the protocol might send all the odd samples for a 10-msec interval in one packet and all the even samples in the next one. A lost packet then does not represent a 5 msec gap in the music, but loss of every other sample for 10 msec. This loss can be handled easily by having the media player interpolate using the previous and succeeding samples. estimate the missing value. 

The use of interleaving to achieve error recovery is illustrated in 

Fig. 7-60

. Here each packet holds the alternate time samples for an interval of 10 msec. Consequently, losing packet 3, as shown, does not create a gap in the music, but only lowers the temporal resolution for some interval. The missing values can be interpolated to provide continuous music. This particular scheme only works with uncompressed sampling, but shows how clever coding can convert a lost packet into lower quality rather than a time gap. However, RFC 3119 gives a scheme that works with compressed audio. 

Figure 7-60. When packets carry alternate samples, the loss of a packet reduces the temporal resolution rather than creating a gap in time. 

526




 

The media player's third job is decompressing the music. Although this task is computationally intensive, it is fairly straightforward. 

The fourth job is to eliminate jitter, the bane of all real-time systems. All streaming audio systems start by buffering about 10–15 sec worth of music before starting to play, as shown in 

Fig. 7-61

. Ideally, the server will continue to fill the buffer at the exact rate it is being drained by the media player, but in reality this may not happen, so feedback in the loop may be helpful. 

Figure 7-61. The media player buffers input from the media server and plays from the buffer rather than directly from the network. 

 

Two approaches can be used to keep the buffer filled. With a 

pull server

,as long as there is room in the buffer for another block, the media player just keeps sending requests for an additional block to the server. Its goal is to keep the buffer as full as possible. 

The disadvantage of a pull server is all the unnecessary data requests. The server knows it has sent the whole file, so why have the player keep asking? For this reason, it is rarely used. 

With a 

push server

, the media player sends a 

PLAY

 request and the server just keeps pushing data at it. There are two possibilities here: the media server runs at normal playback speed or it runs faster. In both cases, some data is buffered before playback begins. If the server runs at normal playback speed, data arriving from it are appended to the end of the buffer and the player removes data from the front of the buffer for playing. As long as everything works perfectly, the amount of data in the buffer remains constant in time. This scheme is simple because no control messages are required in either direction. 

The other push scheme is to have the server pump out data faster than it is needed. The advantage here is that if the server cannot be guaranteed to run at a regular rate, it has the opportunity to catch up if it ever gets behind. A problem here, however, is potential buffer overruns if the server can pump out data faster than it is consumed (and it has to be able to do this to avoid gaps). 

The solution is for the media player to define a 

low-water mark

 and a 

high-water mark

 in the buffer. Basically, the server just pumps out data until the buffer is filled to the high-water 

527




mark. Then the media player tells it to pause. Since data will continue to pour in until the server has gotten the pause request, the distance between the high-water mark and the end of the buffer has to be greater than the bandwidth-delay product of the network. After the server has stopped, the buffer will begin to empty. When it hits the low-water mark, the media player tells the media server to start again. The low-water mark has to be positioned so that buffer underrun does not occur. 

To operate a push server, the media player needs a remote control for it. This is what RTSP provides. It is defined in RFC 2326 and provides the mechanism for the player to control the server. It does not provide for the data stream, which is usually RTP. The main commands provided for by RTSP are listed in 

Fig. 7-62

. 

Figure 7-62. RTSP commands from the player to the server. 

 

7.4.4 Internet Radio 

Once it became possible to stream audio over the Internet, commercial radio stations got the idea of broadcasting their content over the Internet as well as over the air. Not so long after that, college radio stations started putting their signal out over the Internet. Then college 

students

 started their own radio stations. With current technology, virtually anyone can start a radio station. The whole area of Internet radio is very new and in a state of flux, but it is worth saying a little bit about. 

There are two general approaches to Internet radio. In the first one, the programs are prerecorded and stored on disk. Listeners can connect to the radio station's archives and pull up any program and download it for listening. In fact, this is exactly the same as the streaming audio we just discussed. It is also possible to store each program just after it is broadcast live, so the archive is only running, say, half an hour, or less behind the live feed. The advantages of this approach are that it is easy to do, all the techniques we have discussed work here too, and listeners can pick and choose among all the programs in the archive. 

The other approach is to broadcast live over the Internet. Some stations broadcast over the air and over the Internet simultaneously, but there are increasingly many radio stations that are Internet only. Some of the techniques that are applicable to streaming audio are also applicable to live Internet radio, but there are also some key differences. 

One point that is the same is the need for buffering on the user side to smooth out jitter. By collecting 10 or 15 seconds worth of radio before starting the playback, the audio can be kept going smoothly even in the face of substantial jitter over the network. As long as all the packets arrive before they are needed, it does not matter when they arrived. 

One key difference is that streaming audio can be pushed out at a rate greater than the playback rate since the receiver can stop it when the high-water mark is hit. Potentially, this gives it the time to retransmit lost packets, although this strategy is not commonly used. In contrast, live radio is always broadcast at exactly the rate it is generated and played back. 

528




Another difference is that a live radio station usually has hundreds or thousands of simultaneous listeners whereas streaming audio is point to point. Under these circumstances, Internet radio should use multicasting with the RTP/RTSP protocols. This is clearly the most efficient way to operate. 

In current practice, Internet radio does not work like this. What actually happens is that the user establishes a TCP connection to the station and the feed is sent over the TCP connection. Of course, this creates various problems, such as the flow stopping when the window is full, lost packets timing out and being retransmitted, and so on. 

The reason TCP unicasting is used instead of RTP multicasting is threefold. First, few ISPs support multicasting, so that is not a practical option. Second, RTP is less well known than TCP and radio stations are often small and have little computer expertise, so it is just easier to use a protocol that is widely understood and supported by all software packages. Third, many people listen to Internet radio at work, which in practice, often means behind a firewall. Most system administrators configure their firewall to protect their LAN from unwelcome visitors. They usually allow TCP connections from remote port 25 (SMTP for e-mail), UDP packets from remote port 53 (DNS), and TCP connections from remote port 80 (HTTP for the Web). Almost everything else may be blocked, including RTP. Thus, the only way to get the radio signal through the firewall is for the Web site to pretend it is an HTTP server, at least to the firewall, and use HTTP servers, which speak TCP. These severe measures, while providing only minimal security. often force multimedia applications into drastically less efficient modes of operation. 

Since Internet radio is a new medium, format wars are in full bloom. RealAudio, Windows Media Audio, and MP3 are aggressively competing in this market to become the dominant format for Internet radio. A newcomer is Vorbis, which is technically similar to MP3 but open source and different enough that it does not use the patents MP3 is based on. 

A typical Internet radio station has a Web page listing its schedule, information about its DJs and announcers, and many ads. There are also one or more icons listing the audio formats it supports (or just LISTEN NOW if only one format is supported). These icons or LISTEN NOW are linked metafiles of the type we discussed above. 

When a user clicks on one of the icons, the short metafile is sent over. The browser uses its MIME type or file extension to determine the appropriate helper (i.e., media player) for the metafile. Then it writes the metafile to a scratch file on disk, starts the media player, and hands it the name of the scratch file. The media player reads the scratch file, sees the URL contained in it (usually with scheme 

http

 rather than 

rtsp

 to get around the firewall problem and because some popular multimedia applications work that way), contacts the server, and starts acting like a radio. As an aside, audio has only one stream, so 

http

 works, but for video, which has at least two streams, 

http

 fails and something like 

rtsp

 is really needed. 

Another interesting development in the area of Internet radio is an arrangement in which anybody, even a student, can set up and operate a radio station. The main components are illustrated in 

Fig. 7-63

. The basis of the station is an ordinary PC with a sound card and a microphone. The software consists of a media player, such as Winamp or Freeamp, with a plug-in for audio capture and a codec for the selected output format, for example, MP3 or Vorbis. 

Figure 7-63. A student radio station. 

529




 

The audio stream generated by the station is then fed over the Internet to a large server, which handles distributing it to large numbers of TCP connections. The server typically supports many small stations. It also maintains a directory of what stations it has and what is currently on the air on each one. Potential listeners go to the server, select a station, and get a TCP feed. There are commercial software packages for managing all the pieces, as well as open source packages such as icecast. There are also servers that are willing to handle the distribution for a fee. 

7.4.5 Voice over IP 

Once upon a time, the public switched telephone system was primarily used for voice traffic with a little bit of data traffic here and there. But the data traffic grew and grew, and by 1999, the number of data bits moved equaled the number of voice bits (since voice is in PCM on the trunks, it can be measured in bits/sec). By 2002, the volume of data traffic was an order of magnitude more than the volume of voice traffic and still growing exponentially, with voice traffic being almost flat (5% growth per year). 

As a consequence of these numbers, many packet-switching network operators suddenly became interested in carrying voice over their data networks. The amount of additional bandwidth required for voice is minuscule since the packet networks are dimensioned for the data traffic. However, the average person's phone bill is probably larger than his Internet bill, so the data network operators saw Internet telephony as a way to earn a large amount of additional money without having to put any new fiber in the ground. Thus 

Internet telephony

 (also known as 

voice over IP

), was born. 

H.323 

One thing that was clear to everyone from the start was that if each vendor designed its own protocol stack, the system would never work. To avoid this problem, a number of interested parties got together under ITU auspices to work out standards. In 1996 ITU issued recommendation 

H.323

 entitled ''Visual Telephone Systems and Equipment for Local Area Networks Which Provide a Non-Guaranteed Quality of Service.'' Only the telephone industry would think of such a name. The recommendation was revised in 1998, and this revised H.323 was the basis for the first widespread Internet telephony systems. 

H.323 is more of an architectural overview of Internet telephony than a specific protocol. It references a large number of specific protocols for speech coding, call setup, signaling, data transport, and other areas rather than specifying these things itself. The general model is depicted in 

Fig. 7-64

. At the center is a 

gateway

 that connects the Internet to the telephone network. It speaks the H.323 protocols on the Internet side and the PSTN protocols on the telephone side. The communicating devices are called 

terminals

. A LAN may have a 

gatekeeper

, which controls the end points under its jurisdiction, called a 

zone

. 

Figure 7-64. The H.323 architectural model for Internet telephony. 

530




 

A telephone network needs a number of protocols. To start with, there is a protocol for encoding and decoding speech. The PCM system we studied in 

Chap. 2

 is defined in ITU recommendation 

G.711

. It encodes a single voice channel by sampling 8000 times per second with an 8-bit sample to give uncompressed speech at 64 kbps. All H.323 systems must support G.711. However, other speech compression protocols are also permitted (but not required). They use different compression algorithms and make different trade-offs between quality and bandwidth. For example, 

G.723.1

 takes a block of 240 samples (30 msec of speech) and uses predictive coding to reduce it to either 24 bytes or 20 bytes. This algorithm gives an output rate of either 6.4 kbps or 5.3 kbps (compression factors of 10 and 12), respectively, with little loss in perceived quality. Other codecs are also allowed. 

Since multiple compression algorithms are permitted, a protocol is needed to allow the terminals to negotiate which one they are going to use. This protocol is called 

H.245

. It also negotiates other aspects of the connection such as the bit rate. RTCP is need for the control of the RTP channels. Also required is a protocol for establishing and releasing connections, providing dial tones, making ringing sounds, and the rest of the standard telephony. ITU 

Q.931

 is used here. The terminals need a protocol for talking to the gatekeeper (if present). For this purpose, 

H.225

 is used. The PC-to-gatekeeper channel it manages is called the 

RAS

 (

Registration/Admission/Status

 ) channel. This channel allows terminals to join and leave the zone, request and return bandwidth, and provide status updates, among other things. Finally, a protocol is needed for the actual data transmission. RTP is used for this purpose. It is managed by RTCP, as usual. The positioning of all these protocols is shown in 

Fig. 7-65

. 

Figure 7-65. The H.323 protocol stack. 

 

To see how these protocols fit together, consider the case of a PC terminal on a LAN (with a gatekeeper) calling a remote telephone. The PC first has to discover the gatekeeper, so it broadcasts a UDP gatekeeper discovery packet to port 1718. When the gatekeeper responds, the PC learns the gatekeeper's IP address. Now the PC registers with the gatekeeper by sending it a RAS message in a UDP packet. After it has been accepted, the PC sends the gatekeeper a RAS admission message requesting bandwidth. Only after bandwidth has been granted may call setup begin. The idea of requesting bandwidth in advance is to allow the gatekeeper to limit the number of calls to avoid oversubscribing the outgoing line in order to help provide the necessary quality of service. 

531




The PC now establishes a TCP connection to the gatekeeper to begin call setup. Call setup uses existing telephone network protocols, which are connection oriented, so TCP is needed. In contrast, the telephone system has nothing like RAS to allow telephones to announce their presence, so the H.323 designers were free to use either UDP or TCP for RAS, and they chose the lower-overhead UDP. 

Now that it has bandwidth allocated, the PC can send a Q.931 

SETUP

 message over the TCP connection. This message specifies the number of the telephone being called (or the IP address and port, if a computer is being called). The gatekeeper responds with a Q.931 

CALL PROCEEDING

 message to acknowledge correct receipt of the request. The gatekeeper then forwards the 

SETUP

 message to the gateway. 

The gateway, which is half computer, half telephone switch, then makes an ordinary telephone call to the desired (ordinary) telephone. The end office to which the telephone is attached rings the called telephone and also sends back a Q.931 

ALERT

 message to tell the calling PC that ringing has begun. When the person at the other end picks up the telephone, the end office sends back a Q.931 

CONNECT

 message to signal the PC that it has a connection. 

Once the connection has been established, the gatekeeper is no longer in the loop, although the gateway is, of course. Subsequent packets bypass the gatekeeper and go directly to the gateway's IP address. At this point, we just have a bare tube running between the two parties. This is just a physical layer connection for moving bits, no more. Neither side knows anything about the other one. 

The H.245 protocol is now used to negotiate the parameters of the call. It uses the H.245 control channel, which is always open. Each side starts out by announcing its capabilities, for example, whether it can handle video (H.323 can handle video) or conference calls, which codecs it supports, etc. Once each side knows what the other one can handle, two unidirectional data channels are set up and a codec and other parameters assigned to each one. Since each side may have different equipment, it is entirely possible that the codecs on the forward and reverse channels are different. After all negotiations are complete, data flow can begin using RTP. It is managed using RTCP, which plays a role in congestion control. If video is present, RTCP handles the audio/video synchronization. The various channels are shown in 

Fig. 7-66

. When either party hangs up, the Q.931 call signaling channel is used to tear down the connection. 

Figure 7-66. Logical channels between the caller and callee during a call. 

 

When the call is terminated, the calling PC contacts the gatekeeper again with a RAS message to release the bandwidth it has been assigned. Alternatively, it can make another call. 

We have not said anything about quality of service, even though this is essential to making voice over IP a success. The reason is that QoS falls outside the scope of H.323. If the underlying network is capable of producing a stable, jitterfree connection from the calling PC 

532




(e.g., using the techniques we discussed in 

Chap. 5

) to the gateway, then the QoS on the call will be good; otherwise it will not be. The telephone part uses PCM and is always jitter free. 

SIP—The Session Initiation Protocol 

H.323 was designed by ITU. Many people in the Internet community saw it as a typical telco product: large, complex, and inflexible. Consequently, IETF set up a committee to design a simpler and more modular way to do voice over IP. The major result to date is the 

SIP

 (

Session Initiation Protocol

), which is described in RFC 3261. This protocol describes how to set up Internet telephone calls, video conferences, and other multimedia connections. Unlike H.323, which is a complete protocol suite, SIP is a single module, but it has been designed to interwork well with existing Internet applications. For example, it defines telephone numbers as URLs, so that Web pages can contain them, allowing a click on a link to initiate a telephone call (the same way the 

mailto

 scheme allows a click on a link to bring up a program to send an e-mail message). 

SIP can establish two-party sessions (ordinary telephone calls), multiparty sessions (where everyone can hear and speak), and multicast sessions (one sender, many receivers). The sessions may contain audio, video, or data, the latter being useful for multiplayer real-time games, for example. SIP just handles setup, management, and termination of sessions. Other protocols, such as RTP/RTCP, are used for data transport. SIP is an application-layer protocol and can run over UDP or TCP. 

SIP supports a variety of services, including locating the callee (who may not be at his home machine) and determining the callee's capabilities, as well as handling the mechanics of call setup and termination. In the simplest case, SIP sets up a session from the caller's computer to the callee's computer, so we will examine that case first. 

Telephone numbers in SIP are represented as URLs using the 

sip

 scheme, for example, 

sip:ilse@cs.university.edu

 for a user named Ilse at the host specified by the DNS name 

cs.university.edu

. SIP URLs may also contain IPv4 addresses, IPv6 address, or actual telephone numbers. 

The SIP protocol is a text-based protocol modeled on HTTP. One party sends a message in ASCII text consisting of a method name on the first line, followed by additional lines containing headers for passing parameters. Many of the headers are taken from MIME to allow SIP to interwork with existing Internet applications. The six methods defined by the core specification are listed in 

Fig. 7-67

. 

Figure 7-67. The SIP methods defined in the core specification. 

 

To establish a session, the caller either creates a TCP connection with the callee and sends an 

INVITE

 message over it or sends the 

INVITE

 message in a UDP packet. In both cases, the headers on the second and subsequent lines describe the structure of the message body, which contains the caller's capabilities, media types, and formats. If the callee accepts the call, it responds with an HTTP-type reply code (a three-digit number using the groups of 

Fig. 7-42

, 

533




200 for acceptance). Following the reply-code line, the callee also may supply information about its capabilities, media types, and formats. 

Connection is done using a three-way handshake, so the caller responds with an 

ACK

 message to finish the protocol and confirm receipt of the 200 message. 

Either party may request termination of a session by sending a message containing the 

BYE

 method. When the other side acknowledges it, the session is terminated. 

The 

OPTIONS

 method is used to query a machine about its own capabilities. It is typically used before a session is initiated to find out if that machine is even capable of voice over IP or whatever type of session is being contemplated. 

The 

REGISTER

 method relates to SIP's ability to track down and connect to a user who is away from home. This message is sent to a SIP location server that keeps track of who is where. That server can later be queried to find the user's current location. The operation of redirection is illustrated in 

Fig. 7-68

. Here the caller sends the 

INVITE

 message to a proxy server to hide the possible redirection. The proxy then looks up where the user is and sends the 

INVITE

 message there. It then acts as a relay for the subsequent messages in the three-way handshake. The 

LOOKUP

 and 

REPLY

 messages are not part of SIP; any convenient protocol can be used, depending on what kind of location server is used. 

Figure 7-68. Use a proxy and redirection servers with SIP. 

 

SIP has a variety of other features that we will not describe here, including call waiting, call screening, encryption, and authentication. It also has the ability to place calls from a computer to an ordinary telephone, if a suitable gateway between the Internet and telephone system is available. 

Comparison of H.323 and SIP 

H.323 and SIP have many similarities but also some differences. Both allow two-party and multiparty calls using both computers and telephones as end points. Both support parameter negotiation, encryption, and the RTP/RTCP protocols. A summary of the similarities and differences is given in 

Fig. 7-69

. 

Figure 7-69. Comparison of H.323 and SIP 

534




 

Although the feature sets are similar, the two protocols differ widely in philosophy. H.323 is a typical, heavyweight, telephone-industry standard, specifying the complete protocol stack and defining precisely what is allowed and what is forbidden. This approach leads to very well defined protocols in each layer, easing the task of interoperability. The price paid is a large, complex, and rigid standard that is difficult to adapt to future applications. 

In contrast, SIP is a typical Internet protocol that works by exchanging short lines of ASCII text. It is a lightweight module that interworks well with other Internet protocols but less well with existing telephone system signaling protocols. Because the IETF model of voice over IP is highly modular, it is flexible and can be adapted to new applications easily. The downside is potential interoperability problems, although these are addressed by frequent meetings where different implementers get together to test their systems. 

Voice over IP is an up-and-coming topic. Consequently, there are several books on the subject already. A few examples are (Collins, 2001; Davidson and Peters, 2000; Kumar et al., 2001; and Wright, 2001). The May/June 2002 issue of 

Internet Computing

 has several articles on this topic. 

7.4.6 Introduction to Video 

We have discussed the ear at length now; time to move on to the eye (no, this section is not followed by one on the nose). The human eye has the property that when an image appears on the retina, the image is retained for some number of milliseconds before decaying. If a sequence of images is drawn line by line at 50 images/sec, the eye does not notice that it is looking at discrete images. All video (i.e., television) systems exploit this principle to produce moving pictures. 

Analog Systems 

To understand video, it is best to start with simple, old-fashioned black-and-white television. To represent the two-dimensional image in front of it as a one-dimensional voltage as a function of time, the camera scans an electron beam rapidly across the image and slowly down it, recording the light intensity as it goes. At the end of the scan, called a 

frame

, the beam 

535




retraces. This intensity as a function of time is broadcast, and receivers repeat the scanning process to reconstruct the image. The scanning pattern used by both the camera and the receiver is shown in 

Fig. 7-70

. (As an aside, CCD cameras integrate rather than scan, but some cameras and all monitors do scan.) 

Figure 7-70. The scanning pattern used for NTSC video and television. 

 

The exact scanning parameters vary from country to country. The system used in North and South America and Japan has 525 scan lines, a horizontal-to-vertical aspect ratio of 4:3, and 30 frames/sec. The European system has 625 scan lines, the same aspect ratio of 4:3, and 25 frames/sec. In both systems, the top few and bottom few lines are not displayed (to approximate a rectangular image on the original round CRTs). Only 483 of the 525 NTSC scan lines (and 576 of the 625 PAL/SECAM scan lines) are displayed. The beam is turned off during the vertical retrace, so many stations (especially in Europe) use this time to broadcast TeleText (text pages containing news, weather, sports, stock prices, etc.). 

While 25 frames/sec is enough to capture smooth motion, at that frame rate many people, especially older ones, will perceive the image to flicker (because the old image has faded off the retina before the new one appears). Rather than increase the frame rate, which would require using more scarce bandwidth, a different approach is taken. Instead of the scan lines being displayed in order, first all the odd scan lines are displayed, then the even ones are displayed. Each of these half frames is called a 

field

. Experiments have shown that although people notice flicker at 25 frames/sec, they do not notice it at 50 fields/sec. This technique is called 

interlacing

. Noninterlaced television or video is called 

progressive

. Note that movies run at 24 fps, but each frame is fully visible for 1/24 sec. 

Color video uses the same scanning pattern as monochrome (black and white), except that instead of displaying the image with one moving beam, it uses three beams moving in unison. One beam is used for each of the three additive primary colors: red, green, and blue (RGB). This technique works because any color can be constructed from a linear superposition of red, green, and blue with the appropriate intensities. However, for transmission on a single channel, the three color signals must be combined into a single 

composite

 signal. 

When color television was invented, various methods for displaying color were technically possible, and different countries made different choices, leading to systems that are still incompatible. (Note that these choices have nothing to do with VHS versus Betamax versus P2000, which are recording methods.) In all countries, a political requirement was that programs transmitted in color had to be receivable on existing black-and-white television sets. 

536




Consequently, the simplest scheme, just encoding the RGB signals separately, was not acceptable. RGB is also not the most efficient scheme. 

The first color system was standardized in the United States by the 

National Television Standards Committee

, which lent its acronym to the standard: 

NTSC

. Color television was introduced in Europe several years later, by which time the technology had improved substantially, leading to systems with greater noise immunity and better colors. These systems are called 

SECAM

 (

SEquentiel Couleur Avec Memoire

), which is used in France and Eastern Europe, and 

PAL

 (

Phase Alternating Line

) used in the rest of Europe. The difference in color quality between the NTSC and PAL/SECAM has led to an industry joke that NTSC really stands for Never Twice the Same Color. 

To allow color transmissions to be viewed on black-and-white receivers, all three systems linearly combine the RGB signals into a 

luminance

 (brightness) signal and two 

chrominance

 (color) signals, although they all use different coefficients for constructing these signals from the RGB signals. Oddly enough, the eye is much more sensitive to the luminance signal than to the chrominance signals, so the latter need not be transmitted as accurately. Consequently, the luminance signal can be broadcast at the same frequency as the old black-and-white signal, so it can be received on black-and-white television sets. The two chrominance signals are broadcast in narrow bands at higher frequencies. Some television sets have controls labeled brightness, hue, and saturation (or brightness, tint, and color) for controlling these three signals separately. Understanding luminance and chrominance is necessary for understanding how video compression works. 

In the past few years, there has been considerable interest in 

HDTV

 (

High Definition TeleVision

), which produces sharper images by roughly doubling the number of scan lines. The United States, Europe, and Japan have all developed HDTV systems, all different and all mutually incompatible. Did you expect otherwise? The basic principles of HDTV in terms of scanning, luminance, chrominance, and so on, are similar to the existing systems. However, all three formats have a common aspect ratio of 16:9 instead of 4:3 to match them better to the format used for movies (which are recorded on 35 mm film, which has an aspect ratio of 3:2). 

Digital Systems 

The simplest representation of digital video is a sequence of frames, each consisting of a rectangular grid of picture elements, or 

pixels

. Each pixel can be a single bit, to represent either black or white. The quality of such a system is similar to what you get by sending a color photograph by fax—awful. (Try it if you can; otherwise photocopy a color photograph on a copying machine that does not rasterize.) 

The next step up is to use 8 bits per pixel to represent 256 gray levels. This scheme gives high-quality black-and-white video. For color video, good systems use 8 bits for each of the RGB colors, although nearly all systems mix these into composite video for transmission. While using 24 bits per pixel limits the number of colors to about 16 million, the human eye cannot even distinguish this many colors, let alone more. Digital color images are produced using three scanning beams, one per color. The geometry is the same as for the analog system of 

Fig. 7-70

 except that the continuous scan lines are now replaced by neat rows of discrete pixels. 

To produce smooth motion, digital video, like analog video, must display at least 25 frames/sec. However, since good-quality computer monitors often rescan the screen from images stored in memory at 75 times per second or more, interlacing is not needed and consequently is not normally used. Just repainting (i.e., redrawing) the same frame three times in a row is enough to eliminate flicker. 

In other words, smoothness of motion is determined by the number of 

different

 images per second, whereas flicker is determined by the number of times the screen is painted per 

537




second. These two parameters are different. A still image painted at 20 frames/sec will not show jerky motion, but it will flicker because one frame will decay from the retina before the next one appears. A movie with 20 different frames per second, each of which is painted four times in a row, will not flicker, but the motion will appear jerky. 

The significance of these two parameters becomes clear when we consider the bandwidth required for transmitting digital video over a network. Current computer monitors most use the 4:3 aspect ratio so they can use inexpensive, mass-produced picture tubes designed for the consumer television market. Common configurations are 1024 x 768, 1280 x 960, and 1600 x 1200. Even the smallest of these with 24 bits per pixel and 25 frames/sec needs to be fed at 472 Mbps. It would take a SONET OC-12 carrier to manage this, and running an OC-12 SONET carrier into everyone's house is not exactly on the agenda. Doubling this rate to avoid flicker is even less attractive. A better solution is to transmit 25 frames/sec and have the computer store each one and paint it twice. Broadcast television does not use this strategy because television sets do not have memory. And even if they did have memory, analog signals cannot be stored in RAM without conversion to digital form first, which requires extra hardware. As a consequence, interlacing is needed for broadcast television but not for digital video. 

7.4.7 Video Compression 

It should be obvious by now that transmitting uncompressed video is completely out of the question. The only hope is that massive compression is possible. Fortunately, a large body of research over the past few decades has led to many compression techniques and algorithms that make video transmission feasible. In this section we will study how video compression is accomplished. 

All compression systems require two algorithms: one for compressing the data at the source, and another for decompressing it at the destination. In the literature, these algorithms are referred to as the 

encoding

 and 

decoding

 algorithms, respectively. We will use this terminology here, too. 

These algorithms exhibit certain asymmetries that are important to understand. First, for many applications, a multimedia document, say, a movie will only be encoded once (when it is stored on the multimedia server) but will be decoded thousands of times (when it is viewed by customers). This asymmetry means that it is acceptable for the encoding algorithm to be slow and require expensive hardware provided that the decoding algorithm is fast and does not require expensive hardware. After all, the operator of a multimedia server might be quite willing to rent a parallel supercomputer for a few weeks to encode its entire video library, but requiring consumers to rent a supercomputer for 2 hours to view a video is not likely to be a big success. Many practical compression systems go to great lengths to make decoding fast and simple, even at the price of making encoding slow and complicated. 

On the other hand, for real-time multimedia, such as video conferencing, slow encoding is unacceptable. Encoding must happen on-the-fly, in real time. Consequently, real-time multimedia uses different algorithms or parameters than storing videos on disk, often with appreciably less compression. 

A second asymmetry is that the encode/decode process need not be invertible. That is, when compressing a file, transmitting it, and then decompressing it, the user expects to get the original back, accurate down to the last bit. With multimedia, this requirement does not exist. It is usually acceptable to have the video signal after encoding and then decoding be slightly different from the original. When the decoded output is not exactly equal to the original input, the system is said to be 

lossy

. If the input and output are identical, the system is 

lossless

. Lossy systems are important because accepting a small amount of information loss can give a huge payoff in terms of the compression ratio possible. 

538




The JPEG Standard 

A video is just a sequence of images (plus sound). If we could find a good algorithm for encoding a single image, this algorithm could be applied to each image in succession to achieve video compression. Good still image compression algorithms exist, so let us start our study of video compression there. The 

JPEG

 (

Joint Photographic Experts Group

) standard for compressing continuous-tone still pictures (e.g., photographs) was developed by photographic experts working under the joint auspices of ITU, ISO, and IEC, another standards body. It is important for multimedia because, to a first approximation, the multimedia standard for moving pictures, MPEG, is just the JPEG encoding of each frame separately, plus some extra features for interframe compression and motion detection. JPEG is defined in International Standard 10918. 

JPEG has four modes and many options. It is more like a shopping list than a single algorithm. For our purposes, though, only the lossy sequential mode is relevant, and that one is illustrated in 

Fig. 7-71

. Furthermore, we will concentrate on the way JPEG is normally used to encode 24-bit RGB video images and will leave out some of the minor details for the sake of simplicity. 

Figure 7-71. The operation of JPEG in lossy sequential mode. 

 

Step 1 of encoding an image with JPEG is block preparation. For the sake of specificity, let us assume that the JPEG input is a 640 x 480 RGB image with 24 bits/pixel, as shown in 

Fig. 7-

72(a)

. Since using luminance and chrominance gives better compression, we first compute the luminance, 

Y

, and the two chrominances, 

I

 and 

Q

 (for NTSC), according to the following formulas: 

Figure 7-72. (a) RGB input data. (b) After block preparation. 

 

 

 

For PAL, the chrominances are called 

U

 and 

V

 and the coefficients are different, but the idea is the same. SECAM is different from both NTSC and PAL. 

539




Separate matrices are constructed for 

Y

, 

I

, and 

Q

, each with elements in the range 0 to 255. Next, square blocks of four pixels are averaged in the 

I

 and 

Q

 matrices to reduce them to 320 x 240. This reduction is lossy, but the eye barely notices it since the eye responds to luminance more than to chrominance. Nevertheless, it compresses the total amount of data by a factor of two. Now 128 is subtracted from each element of all three matrices to put 0 in the middle of the range. Finally, each matrix is divided up into 8 x 8 blocks. The 

Y

 matrix has 4800 blocks; the other two have 1200 blocks each, as shown in 

Fig. 7-72(b)

. 

Step 2 of JPEG is to apply a 

DCT

 (

Discrete Cosine Transformation

) to each of the 7200 blocks separately. The output of each DCT is an 8 x 8 matrix of DCT coefficients. DCT element (0, 0) is the average value of the block. The other elements tell how much spectral power is present at each spatial frequency. In theory, a DCT is lossless, but in practice, using floating-point numbers and transcendental functions always introduces some roundoff error that results in a little information loss. Normally, these elements decay rapidly with distance from the origin, (0, 0), as suggested by 

Fig. 7-73

. 

Figure 7-73. (a) One block of the 

Y

 matrix. (b) The DCT coefficients. 

 

Once the DCT is complete, JPEG moves on to step 3, called 

quantization

,in which the less important DCT coefficients are wiped out. This (lossy) transformation is done by dividing each of the coefficients in the 8 x 8 DCT matrix by a weight taken from a table. If all the weights are 1, the transformation does nothing. However, if the weights increase sharply from the origin, higher spatial frequencies are dropped quickly. 

An example of this step is given in 

Fig. 7-74

. Here we see the initial DCT matrix, the quantization table, and the result obtained by dividing each DCT element by the corresponding quantization table element. The values in the quantization table are not part of the JPEG standard. Each application must supply its own, allowing it to control the loss-compression trade-off. 

Figure 7-74. Computation of the quantized DCT coefficients. 

 

Step 4 reduces the (0, 0) value of each block (the one in the upper-left corner) by replacing it with the amount it differs from the corresponding element in the previous block. Since these 

540




elements are the averages of their respective blocks, they should change slowly, so taking the differential values should reduce most of them to small values. No differentials are computed from the other values. The (0, 0) values are referred to as the DC components; the other values are the AC components. 

Step 5 linearizes the 64 elements and applies run-length encoding to the list. Scanning the block from left to right and then top to bottom will not concentrate the zeros together, so a zigzag scanning pattern is used, as shown in 

Fig. 7-75

. In this example, the zig zag pattern produces 38 consecutive 0s at the end of the matrix. This string can be reduced to a single count saying there are 38 zeros, a technique known as 

run-length encoding

. 

Figure 7-75. The order in which the quantized values are transmitted. 

 

Now we have a list of numbers that represent the image (in transform space). Step 6 Huffman-encodes the numbers for storage or transmission, assigning common numbers shorter codes that uncommon ones. 

JPEG may seem complicated, but that is because it 

is

 complicated. Still, since it often produces a 20:1 compression or better, it is widely used. Decoding a JPEG image requires running the algorithm backward. JPEG is roughly symmetric: decoding takes as long as encoding. This property is not true of all compression algorithms, as we shall now see. 

The MPEG Standard 

Finally, we come to the heart of the matter: the 

MPEG

 (

Motion Picture Experts Group

) standards. These are the main algorithms used to compress videos and have been international standards since 1993. Because movies contain both images and sound, MPEG can compress both audio and video. We have already examined audio compression and still image compression, so let us now examine video compression. 

The first standard to be finalized was MPEG-1 (International Standard 11172). Its goal was to produce video-recorder-quality output (352 x 240 for NTSC) using a bit rate of 1.2 Mbps. A 352 x 240 image with 24 bits/pixel and 25 frames/sec requires 50.7 Mbps, so getting it down to 1.2 Mbps is not entirely trivial. A factor of 40 compression is needed. MPEG-1 can be transmitted over twisted pair transmission lines for modest distances. MPEG-1 is also used for storing movies on CD-ROM. 

The next standard in the MPEG family was MPEG-2 (International Standard 13818), which was originally designed for compressing broadcast-quality video into 4 to 6 Mbps, so it could fit in a NTSC or PAL broadcast channel. Later, MPEG-2 was expanded to support higher resolutions, including HDTV. It is very common now, as it forms the basis for DVD and digital satellite television. 

541




The basic principles of MPEG-1 and MPEG-2 are similar, but the details are different. To a first approximation, MPEG-2 is a superset of MPEG-1, with additional features, frame formats, and encoding options. We will first discuss MPEG-1, then MPEG-2. 

MPEG-1 has three parts: audio, video, and system, which integrates the other two, as shown in 

Fig. 7-76

. The audio and video encoders work independently, which raises the issue of how the two streams get synchronized at the receiver. This problem is solved by having a 90-kHz system clock that outputs the current time value to both encoders. These values are 33 bits, to allow films to run for 24 hours without wrapping around. These timestamps are included in the encoded output and propagated all the way to the receiver, which can use them to synchronize the audio and video streams. 

Figure 7-76. Synchronization of the audio and video streams in MPEG-1. 

 

Now let us consider MPEG-1 video compression. Two kinds of redundancies exist in movies: spatial and temporal. MPEG-1 uses both. Spatial redundancy can be utilized by simply coding each frame separately with JPEG. This approach is occasionally used, especially when random access to each frame is needed, as in editing video productions. In this mode, a compressed bandwidth in the 8- to 10-Mbps range is achievable. 

Additional compression can be achieved by taking advantage of the fact that consecutive frames are often almost identical. This effect is smaller than it might first appear since many moviemakers cut between scenes every 3 or 4 seconds (time a movie and count the scenes). Nevertheless, even a run of 75 highly similar frames offers the potential of a major reduction over simply encoding each frame separately with JPEG. 

For scenes in which the camera and background are stationary and one or two actors are moving around slowly, nearly all the pixels will be identical from frame to frame. Here, just subtracting each frame from the previous one and running JPEG on the difference would do fine. However, for scenes where the camera is panning or zooming, this technique fails badly. What is needed is some way to compensate for this motion. This is precisely what MPEG does; it is the main difference between MPEG and JPEG. 

MPEG-1 output consists of four kinds of frames: 

1. I (Intracoded) frames: Self-contained JPEG-encoded still pictures. 

2. P (Predictive) frames: Block-by-block difference with the last frame. 

3. B (Bidirectional) frames: Differences between the last and next frame. 

4. D (DC-coded) frames: Block averages used for fast forward. 

I-frames are just still pictures coded using a variant of JPEG, also using full-resolution luminance and half-resolution chrominance along each axis. It is necessary to have I-frames appear in the output stream periodically for three reasons. First, MPEG-1 can be used for a multicast transmission, with viewers tuning it at will. If all frames depended on their predecessors going back to the first frame, anybody who missed the first frame could never decode any subsequent frames. Second, if any frame were received in error, no further 

542




decoding would be possible. Third, without I-frames, while doing a fast forward or rewind, the decoder would have to calculate every frame passed over so it would know the full value of the one it stopped on. For these reasons, I-frames are inserted into the output once or twice per second. 

P-frames, in contrast, code interframe differences. They are based on the idea of 

macroblocks

, which cover 16 x 16 pixels in luminance space and 8 x 8 pixels in chrominance space. A macroblock is encoded by searching the previous frame for it or something only slightly different from it. 

An example of where P-frames would be useful is given in 

Fig. 7-77

. Here we see three consecutive frames that have the same background, but differ in the position of one person. The macroblocks containing the background scene will match exactly, but the macroblocks containing the person will be offset in position by some unknown amount and will have to be tracked down. 

Figure 7-77. Three consecutive frames. 

 

The MPEG-1 standard does not specify how to search, how far to search, or how good a match has to be to count. This is up to each implementation. For example, an implementation might search for a macroblock at the current position in the previous frame, and all other positions offset ±?

x

 in the 

x

 direction and ±?

y

 in the 

y

 direction. For each position, the number of matches in the luminance matrix could be computed. The position with the highest score would be declared the winner, provided it was above some predefined threshold. Otherwise, the macroblock would be said to be missing. Much more sophisticated algorithms are also possible, of course. 

If a macroblock is found, it is encoded by taking the difference with its value in the previous frame (for luminance and both chrominances). These difference matrices are then subject to the discrete cosine transformation, quantization, run-length encoding, and Huffman encoding, just as with JPEG. The value for the macroblock in the output stream is then the motion vector (how far the macro-block moved from its previous position in each direction), followed by the Huffman-encoded list of numbers. If the macroblock is not located in the previous frame, the current value is encoded with JPEG, just as in an I-frame. 

Clearly, this algorithm is highly asymmetric. An implementation is free to try every plausible position in the previous frame if it wants to, in a desperate attempt to locate every last macroblock, no matter where it moved to. This approach will minimize the encoded MPEG-1 stream at the expense of very slow encoding. This approach might be fine for a one-time encoding of a film library but would be terrible for real-time videoconferencing. 

Similarly, each implementation is free to decide what constitutes a ''found'' macroblock. This freedom allows implementers to compete on the quality and speed of their algorithms, but always produce compliant MPEG-1. No matter what search algorithm is used, the final output is either the JPEG encoding of the current macroblock or the JPEG encoding of the difference between the current macroblock and one in the previous frame at a specified offset from the current one. 

543




So far, decoding MPEG-1 is straightforward. Decoding I-frames is the same as decoding JPEG images. Decoding P-frames requires the decoder to buffer the previous frame and then build up the new one in a second buffer based on fully encoded macroblocks and macroblocks containing differences from the previous frame. The new frame is assembled macroblock by macroblock. 

B-frames are similar to P-frames, except that they allow the reference macro-block to be in either a previous frame or in a succeeding frame. This additional freedom allows improved motion compensation and is also useful when objects pass in front of, or behind, other objects. To do B-frame encoding, the encoder needs to hold three decoded frames in memory at once: the past one, the current one, and the future one. Although B-frames give the best compression, not all implementations support them. 

D-frames are only used to make it possible to display a low-resolution image when doing a rewind or fast forward. Doing the normal MPEG-1 decoding in real time is difficult enough. Expecting the decoder to do it when slewing through the video at ten times normal speed is asking a bit much. Instead, the D-frames are used to produce low-resolution images. Each D-frame entry is just the average value of one block, with no further encoding, making it easy to display in real time. This facility is important to allow people to scan through a video at high speed in search of a particular scene. The D-frames are generally placed just before the corresponding I-frames so if fast forwarding is stopped, it will be possible to start viewing at normal speed. 

Having finished our treatment of MPEG-1, let us now move on to MPEG-2. MPEG-2 encoding is fundamentally similar to MPEG-1 encoding, with I-frames, P-frames, and B-frames. D-frames are not supported, however. Also, the discrete cosine transformation uses a 10 x 10 block instead of a 8 x 8 block, to give 50 percent more coefficients, hence better quality. Since MPEG-2 is targeted at broadcast television as well as DVD, it supports both progressive and interlaced images, in contrast to MPEG-1, which supports only progressive images. Other minor details also differ between the two standards. 

Instead of supporting only one resolution level, MPEG-2 supports four: low (352 x 240), main (720 x 480), high-1440 (1440 x 1152), and high (1920 x 1080). Low resolution is for VCRs and backward compatibility with MPEG-1. Main is the normal one for NTSC broadcasting. The other two are for HDTV. For high-quality output, MPEG-2 usually runs at 4–8 Mbps. 

7.4.8 Video on Demand 

Video on demand is sometimes compared to an electronic video rental store. The user (customer) selects any one of a large number of available videos and takes it home to view. Only with video on demand, the selection is made at home using the television set's remote control, and the video starts immediately. No trip to the store is needed. Needless to say, implementing video on demand is a wee bit more complicated than describing it. In this section, we will give an overview of the basic ideas and their implementation. 

Is video on demand really like renting a video, or is it more like picking a movie to watch from a 500-channel cable system? The answer has important technical implications. In particular, video rental users are used to the idea of being able to stop a video, make a quick trip to the kitchen or bathroom, and then resume from where the video stopped. Television viewers do not expect to put programs on pause. 

If video on demand is going to compete successfully with video rental stores, it may be necessary to allow users to stop, start, and rewind videos at will. Giving users this ability virtually forces the video provider to transmit a separate copy to each one. 

On the other hand, if video on demand is seen more as advanced television, then it may be sufficient to have the video provider start each popular video, say, every 10 minutes, and run 

544




these nonstop. A user wanting to see a popular video may have to wait up to 10 minutes for it to start. Although pause/resume is not possible here, a viewer returning to the living room after a short break can switch to another channel showing the same video but 10 minutes behind. Some material will be repeated, but nothing will be missed. This scheme is called 

near video on demand

. It offers the potential for much lower cost, because the same feed from the video server can go to many users at once. The difference between video on demand and near video on demand is similar to the difference between driving your own car and taking the bus. 

Watching movies on (near) demand is but one of a vast array of potential new services possible once wideband networking is available. The general model that many people use is illustrated in 

Fig. 7-78

. Here we see a high-bandwidth (national or international) wide area backbone network at the center of the system. Connected to it are thousands of local distribution networks, such as cable TV or telephone company distribution systems. The local distribution systems reach into people's houses, where they terminate in 

set-top boxes

, which are, in fact, powerful, specialized personal computers. 

Figure 7-78. Overview of a video-on-demand system. 

 

Attached to the backbone by high-bandwidth optical fibers are numerous information providers. Some of these will offer pay-per-view video or pay-per-hear audio CDs. Others will offer specialized services, such as home shopping (letting viewers rotate a can of soup and zoom in on the list of ingredients or view a video clip on how to drive a gasoline-powered lawn mower). Sports, news, reruns of ''I Love Lucy,'' WWW access, and innumerable other possibilities will no doubt quickly become available. 

Also included in the system are local spooling servers that allow videos to be placed closer to the users (in advance), to save bandwidth during peak hours. How these pieces will fit together and who will own what are matters of vigorous debate within the industry. Below we will examine the design of the main pieces of the system: the video servers and the distribution network. 

Video Servers 

To have (near) video on demand, we need 

video servers

 capable of storing and outputting a large number of movies simultaneously. The total number of movies ever made is estimated at 

545




65,000 (Minoli, 1995). When compressed in MPEG-2, a normal movie occupies roughly 4 GB of storage, so 65,000 of them would require something like 260 terabytes. Add to this all the old television programs ever made, sports films, newsreels, talking shopping catalogs, etc., and it is clear that we have an industrial-strength storage problem on our hands. 

The cheapest way to store large volumes of information is on magnetic tape. This has always been the case and probably always will be. An Ultrium tape can store 200 GB (50 movies) at a cost of about $1–$2 per movie. Large mechanical tape servers that hold thousands of tapes and have a robot arm for fetching any tape and inserting it into a tape drive are commercially available now. The problem with these systems is the access time (especially for the 50th movie on a tape), the transfer rate, and the limited number of tape drives (to serve 

n

 movies at once, the unit would need 

n

 drives). 

Fortunately, experience with video rental stores, public libraries, and other such organizations shows that not all items are equally popular. Experimentally, when 

N

 movies are available, the fraction of all requests being for the 

k

th most popular one is approximately 

C/k

. Here 

C

 is computed to normalize the sum to 1, namely, 

 

 

Thus, the most popular movie is seven times as popular as the number seven movie. This result is known as 

Zipf's law

 (Zipf, 1949). 

The fact that some movies are much more popular than others suggests a possible solution in the form of a storage hierarchy, as shown in 

Fig. 7-79

. Here, the performance increases as one moves up the hierarchy. 

Figure 7-79. A video server storage hierarchy. 

 

An alternative to tape is optical storage. Current DVDs hold 4.7 GB, good for one movie, but the next generation will hold two movies. Although seek times are slow compared to magnetic disks (50 msec versus 5 msec), their low cost and high reliability make optical juke boxes containing thousands of DVDs a good alternative to tape for the more heavily used movies. 

Next come magnetic disks. These have short access times (5 msec), high transfer rates (320 MB/sec for SCSI 320), and substantial capacities (> 100 GB), which makes them well suited to holding movies that are actually being transmitted (as opposed to just being stored in case somebody ever wants them). Their main drawback is the high cost for storing movies that are rarely accessed. 

At the top of the pyramid of 

Fig. 7-79

 is RAM. RAM is the fastest storage medium, but also the most expensive. When RAM prices drop to $50/GB, a 4-GB movie will occupy $200 dollars worth of RAM, so having 100 movies in RAM will cost $20,000 for the 200 GB of memory. Still, for a video server feeding out 100 movies, just keeping all the movies in RAM is beginning to look feasible. And if the video server has 100 customers but they are collectively watching only 20 different movies, it begins to look not only feasible, but a good design. 

546




Since a video server is really just a massive real-time I/O device, it needs a different hardware and software architecture than a PC or a UNIX workstation. The hardware architecture of a typical video server is illustrated in 

Fig. 7-80

. The server has one or more high-performance CPUs, each with some local memory, a shared main memory, a massive RAM cache for popular movies, a variety of storage devices for holding the movies, and some networking hardware, normally an optical interface to a SONET or ATM backbone at OC-12 or higher. These subsystems are connected by an extremely high speed bus (at least 1 GB/sec). 

Figure 7-80. The hardware architecture of a typical video server. 

 

Now let us take a brief look at video server software. The CPUs are used for accepting user requests, locating movies, moving data between devices, customer billing, and many other functions. Some of these are not time critical, but many others are, so some, if not all, the CPUs will have to run a real-time operating system, such as a real-time microkernel. These systems normally break work up into small tasks, each with a known deadline. The scheduler can then run an algorithm such as nearest deadline next or the rate monotonic algorithm (Liu and Layland, 1973). 

The CPU software also defines the nature of the interface that the server presents to the clients (spooling servers and set-top boxes). Two designs are popular. The first one is a traditional file system, in which the clients can open, read, write, and close files. Other than the complications introduced by the storage hierarchy and real-time considerations, such a server can have a file system modeled after that of UNIX. 

The second kind of interface is based on the video recorder model. The commands to the server request it to open, play, pause, fast forward, and rewind files. The difference with the UNIX model is that once a 

PLAY

 command is given, the server just keeps pumping out data at a constant rate, with no new commands required. 

The heart of the video server software is the disk management software. It has two main jobs: placing movies on the magnetic disk when they have to be pulled up from optical or tape storage, and handling disk requests for the many output streams. Movie placement is important because it can greatly affect performance. 

Two possible ways of organizing disk storage are the disk farm and the disk array. With the 

disk farm

, each drive holds some number of entire movies. For performance and reliability reasons, each movie should be present on at least two drives, maybe more. The other storage organization is the 

disk array

 or 

RAID

 (

Redundant Array of Inexpensive Disks

), in which each movie is spread out over multiple drives, for example, block 0 on drive 0, block 1 on 

547




drive 1, and so on, with block 

n

 - 1 on drive 

n

 - 1. After that, the cycle repeats, with block 

n

 on drive 0, and so forth. This organizing is called 

striping

. 

A striped disk array has several advantages over a disk farm. First, all 

n

 drives can be running in parallel, increasing the performance by a factor of 

n

. Second, it can be made redundant by adding an extra drive to each group of 

n

, where the redundant drive contains the block-by-block exclusive OR of the other drives, to allow full data recovery in the event one drive fails. Finally, the problem of load balancing is solved (manual placement is not needed to avoid having all the popular movies on the same drive). On the other hand, the disk array organization is more complicated than the disk farm and highly sensitive to multiple failures. It is also ill-suited to video recorder operations such as rewinding or fast forwarding a movie. 

The other job of the disk software is to service all the real-time output streams and meet their timing constraints. Only a few years ago, this required complex disk scheduling algorithms, but with memory prices so low now, a much simpler approach is beginning to be possible. For each stream being served, a buffer of, say, 10 sec worth of video (5 MB) is kept in RAM. It is filled by a disk process and emptied by a network process. With 500 MB of RAM, 100 streams can be fed directly from RAM. Of course, the disk subsystem must have a sustained data rate of 50 MB/sec to keep the buffers full, but a RAID built from high-end SCSI disks can handle this requirement easily. 

The Distribution Network 

The distribution network is the set of switches and lines between the source and destination. As we saw in 

Fig. 7-78

, it consists of a backbone, connected to a local distribution network. Usually, the backbone is switched and the local distribution network is not. 

The main requirement imposed on the backbone is high bandwidth. It used to be that low jitter was also a requirement, but with even the smallest PC now able to buffer 10 sec of high-quality MPEG-2 video, low jitter is not a requirement anymore. 

Local distribution is highly chaotic, with different companies trying out different networks in different regions. Telephone companies, cable TV companies, and new entrants, such as power companies, are all convinced that whoever gets there first will be the big winner. Consequently, we are now seeing a proliferation of technologies being installed. In Japan, some sewer companies are in the Internet business, arguing that they have the biggest pipe of all into everyone's house (they run an optical fiber through it, but have to be very careful about precisely where it emerges). The four main local distribution schemes for video on demand go by the acronyms ADSL, FTTC, FTTH, and HFC. We will now explain each of these in turn. 

ADSL

 is the first telephone industry's entrant in the local distribution sweepstakes. We studied ADSL in 

Chap. 2

 and will not repeat that material here. The idea is that virtually every house in the United States, Europe, and Japan already has a copper twisted pair going into it (for analog telephone service). If these wires could be used for video on demand, the telephone companies could clean up. 

The problem, of course, is that these wires cannot support even MPEG-1 over their typical 10-km length, let alone MPEG-2. High-resolution, full-color, full motion video needs 4–8 Mbps, depending on the quality desired. ADSL is not really fast enough except for very short local loops. 

The second telephone company design is 

FTTC

 (

Fiber To The Curb

). In FTTC, the telephone company runs optical fiber from the end office into each residential neighborhood, terminating in a device called an 

ONU

 (

Optical Network Unit

). On the order of 16 copper local loops can terminate in an ONU. These loops are now so short that it is possible to run full-duplex T1 or T2 over them, allowing MPEG-1 and MPEG-2 movies, respectively. In addition, 

548




videoconferencing for home workers and small businesses is now possible because FTTC is symmetric. 

The third telephone company solution is to run fiber into everyone's house. It is called 

FTTH

 (

Fiber To The Home

). In this scheme, everyone can have an OC-1, OC-3, or even higher carrier if that is required. FTTH is very expensive and will not happen for years but clearly will open a vast range of new possibilities when it finally happens. In 

Fig. 7-63

 we saw how everybody could operate his or her own radio station. What do you think about each member of the family operating his or her own personal television station? ADSL, FTTC, and FTTH are all point-to-point local distribution networks, which is not surprising given how the current telephone system is organized. 

A completely different approach is 

HFC

 (

Hybrid Fiber Coax

), which is the preferred solution currently being installed by cable TV providers. It is illustrated in 

Fig. 2-47(a)

. The story goes something like this. The current 300- to 450-MHz coax cables are being replaced by 750-MHz coax cables, upgrading the capacity from 50 to 75 6-MHz channels to 125 6-MHz channels. Seventy-five of the 125 channels will be used for transmitting analog television. 

The 50 new channels will each be modulated using QAM-256, which provides about 40 Mbps per channel, giving a total of 2 Gbps of new bandwidth. The headends will be moved deeper into the neighborhoods so that each cable runs past only 500 houses. Simple division shows that each house can then be allocated a dedicated 4-Mbps channel, which can handle an MPEG-2 movie. 

While this sounds wonderful, it does require the cable providers to replace all the existing cables with 750-MHz coax, install new headends, and remove all the one-way amplifiers—in short, replace the entire cable TV system. Consequently, the amount of new infrastructure here is comparable to what the telephone companies need for FTTC. In both cases the local network provider has to run fiber into residential neighborhoods. Again, in both cases, the fiber terminates at an optoelectrical converter. In FTTC, the final segment is a point-to-point local loop using twisted pairs. In HFC, the final segment is a shared coaxial cable. Technically, these two systems are not really as different as their respective proponents often make out. 

Nevertheless, there is one real difference that is worth pointing out. HFC uses a shared medium without switching and routing. Any information put onto the cable can be removed by any subscriber without further ado. FTTC, which is fully switched, does not have this property. As a result, HFC operators want video servers to send out encrypted streams so customers who have not paid for a movie cannot see it. FTTC operators do not especially want encryption because it adds complexity, lowers performance, and provides no additional security in their system. From the point of view of the company running a video server, is it a good idea to encrypt or not? A server operated by a telephone company or one of its subsidiaries or partners might intentionally decide not to encrypt its videos, claiming efficiency as the reason but really to cause economic losses to its HFC competitors. 

For all these local distribution networks, it is possible that each neighborhood will be outfitted with one or more spooling servers. These are, in fact, just smaller versions of the video servers we discussed above. The big advantage of these local servers is that they move some load off the backbone. 

They can be preloaded with movies by reservation. If people tell the provider which movies they want well in advance, they can be downloaded to the local server during off-peak hours. This observation is likely to lead the network operators to lure away airline executives to do their pricing. One can envision tariffs in which movies ordered 24 to 72 hours in advance for viewing on a Tuesday or Thursday evening before 6 P.M, or after 11 P.M. get a 27 percent discount. Movies ordered on the first Sunday of the month before 8 A.M. for viewing on a Wednesday afternoon on a day whose date is a prime number get a 43 percent discount, and so on. 

549




7.4.9 The MBone—The Multicast Backbone 

While all these industries are making great—and highly publicized—plans for future (inter)national digital video on demand, the Internet community has been quietly implementing its own digital multimedia system, 

MBone

 (

Multicast Backbone

). In this section we will give a brief overview of what it is and how it works. 

MBone can be thought of as Internet television. Unlike video on demand, where the emphasis is on calling up and viewing precompressed movies stored on a server, MBone is used for broadcasting live video in digital form all over the world via the Internet. It has been operational since early 1992. Many scientific conferences, especially IETF meetings, have been broadcast, as well as newsworthy scientific events, such as space shuttle launches. A Rolling Stones concert was once broadcast over MBone as were portions of the Cannes Film Festival. Whether this qualifies as a newsworthy scientific event is arguable. 

Technically, MBone is a virtual overlay network on top of the Internet. It consists of multicast-capable islands connected by tunnels, as shown in 

Fig. 7-81

. In this figure, MBone consists of six islands, 

A

 through 

F

, connected by seven tunnels. Each island (typically a LAN or group of interconnected LANs) supports hardware multicast to its hosts. The tunnels propagate MBone packets between the islands. Some day in the future, when all the routers are capable of handling multicast traffic directly, this superstructure will no longer be needed, but for the moment, it does the job. 

Figure 7-81. MBone consists of multicast islands connected by tunnels. 

 

Each island contains one or more special routers called 

mrouters

 (

multicast routers

). Some of these are actually normal routers, but most are just UNIX workstations running special user-level software (but as the root). The mrouters are logically connected by tunnels. MBone packets are encapsulated within IP packets and sent as regular unicast packets to the destination mrouter's IP address. 

Tunnels are configured manually. Usually, a tunnel runs above a path for which a physical connection exists, but this is not a requirement. If, by accident, the physical path underlying a tunnel goes down, the mrouters using the tunnel will not even notice it, since the Internet will automatically reroute all the IP traffic between them via other lines. 

When a new island appears and wishes to join MBone, such as 

G

 in 

Fig. 7-81

, its administrator sends a message announcing its existence to the MBone mailing list. The administrators of nearby sites then contact him to arrange to set up tunnels. Sometimes existing tunnels are reshuffled to take advantage of the new island to optimize the topology. After all, tunnels have no physical existence. They are defined by tables in the mrouters and can be added, deleted, 

550




or moved simply by changing these tables. Typically, each country on MBone has a backbone, with regional islands attached to it. Normally, MBone is configured with one or two tunnels crossing the Atlantic and Pacific oceans, making MBone global in scale. 

Thus, at any instant, MBone consists of a specific topology consisting of islands and tunnels, independent of the number of multicast addresses currently in use and who is listening to them or watching them. This situation is very similar to a normal (physical) subnet, so the normal routing algorithms apply to it. Consequently, MBone initially used a routing algorithm, 

DVMRP

 (

Distance Vector Multicast Routing Protocol

) based on the Bellman-Ford distance vector algorithm. For example, in 

Fig. 7-81

, island 

C

 can route to 

A

 either via 

B

 or via 

E

 (or conceivably via 

D

). It makes its choice by taking the values those nodes give it about their respective distances to 

A

 and then adding its distance to them. In this way, every island determines the best route to every other island. The routes are not actually used in this way, however, as we will see shortly. 

Now let us consider how multicasting actually happens. To multicast an audio or video program, a source must first acquire a class D multicast address, which acts like a station frequency or channel number. Class D addresses are reserved by a program that looks in a database for free multicast addresses. Many multicasts may be going on at once, and a host can ''tune'' to the one it is interested in by listening to the appropriate multicast address. 

Periodically, each mrouter sends out an IGMP broadcast packet limited to its island asking who is interested in which channel. Hosts wishing to (continue to) receive one or more channels send another IGMP packet back in response. These responses are staggered in time, to avoid overloading the local LAN. Each mrouter keeps a table of which channels it must put out onto its LAN, to avoid wasting bandwidth by multicasting channels that nobody wants. 

Multicasts propagate through MBone as follows. When an audio or video source generates a new packet, it multicasts it to its local island, using the hardware multicast facility. This packet is picked up by the local mrouter, which then copies it into all the tunnels to which it is connected. 

Each mrouter getting such a packet via a tunnel then checks to see if the packet came along the best route, that is, the route that its table says to use to reach the source (as if it were a destination). If the packet came along the best route, the mrouter copies the packet to all its other tunnels. If the packet arrived via a suboptimal route, it is discarded. Thus, for example, in 

Fig. 7-81

, if 

C

's tables tell it to use 

B

 to get to 

A

, then when a multicast packet from 

A

 reaches 

C

 via 

B

, the packet is copied to 

D

 and 

E

. However, when a multicast packet from 

A

 reaches 

C

 via 

E

 (not the best path), it is simply discarded. This algorithm is just the reverse path forwarding algorithm that we saw in 

Chap. 5

. While not perfect, it is fairly good and very simple to implement. 

In addition to using reverse path forwarding to prevent flooding the Internet, the IP 

Time to live

 field is also used to limit the scope of multicasting. Each packet starts out with some value (determined by the source). Each tunnel is assigned a weight. A packet is passed through a tunnel only if it has enough weight. Otherwise it is discarded. For example, transoceanic tunnels are normally configured with a weight of 128, so packets can be limited to the continent of origin by being given an initial 

Time to live

 of 127 or less. After passing through a tunnel, the 

Time to live

 field is decremented by the tunnel's weight. 

While the MBone routing algorithm works, much research has been devoted to improving it. One proposal keeps the idea of distance vector routing, but makes the algorithm hierarchical by grouping MBone sites into regions and first routing to them (Thyagarajan and Deering, 1995). 

Another proposal is to use a modified form of link state routing instead of distance vector routing. In particular, an IETF working group modified OSPF to make it suitable for 

551




multicasting within a single autonomous system. The resulting multicast OSPF is called 

MOSPF

 (Moy, 1994). What the modifications do is have the full map built by MOSPF keep track of multicast islands and tunnels, in addition to the usual routing information. Armed with the complete topology, it is straightforward to compute the best path from every island to every other island using the tunnels. Dijkstra's algorithm can be used, for example. 

A second area of research is inter-AS routing. Here an algorithm called 

PIM

 (

Protocol Independent Multicast

) was developed by another IETF working group. PIM comes in two versions, depending on whether the islands are dense (almost everyone wants to watch) or sparse (almost nobody wants to watch). Both versions use the standard unicast routing tables, instead of creating an overlay topology as DVMRP and MOSPF do. 

In PIM-DM (dense mode), the idea is to prune useless paths. Pruning works as follows. When a multicast packet arrives via the ''wrong'' tunnel, a prune packet is sent back through the tunnel telling the sender to stop sending it packets from the source in question. When a packet arrives via the ''right'' tunnel, it is copied to all the other tunnels that have not previously pruned themselves. If all the other tunnels have pruned themselves and there is no interest in the channel within the local island, the mrouter sends a prune message back through the ''right'' channel. In this way, the multicast adapts automatically and only goes where it is wanted. 

PIM-SM (spare mode), described in RFC 2362, works differently. The idea here is to prevent saturating the Internet because three people in Berkeley want to hold a conference call over a class D address. PIM-SM works by setting up rendezvous points. Each of the sources in a PIM-SM multicast group send their packets to the rendezvous points. Any site interested in joining up asks one of the rendezvous points to set up a tunnel to it. In this way, all PIM-SM traffic is transported by unicast instead of by multicast. PIM-SM is becoming more popular, and the MBone is migrating toward its use. As PIM-SM becomes more widely used, MOSPF is gradually disappearing. On the other hand, the MBone itself seems to be somewhat stagnant and will probably never catch on in a big way. 

Nevertheless, networked multimedia is still an exciting and rapidly moving field, even if the MBone does not become a huge success. New technologies and applications are announced daily. Increasingly, multicasting and quality of service are coming together, as discussed in (Striegel and Manimaran, 2002). Another hot topic is wireless multicast (Gossain et al., 2002). The whole area of multicasting and everything related to it are likely to remain important for years to come. 

7.5 Summary 

Naming in the Internet uses a hierarchical scheme called the domain name system (DNS). At the top level are the well-known generic domains, including 

com

 and 

edu

 as well as about 200 country domains. DNS is implemented as a distributed database system with servers all over the world. DNS holds records with IP addresses, mail exchanges, and other information. By querying a DNS server, a process can map an Internet domain name onto the IP address used to communicate with that domain. 

E-mail is one of the two killer apps for the Internet. Everyone from small children to grandparents now use it. Most e-mail systems in the world use the mail system now defined in RFCs 2821 and 2822. Messages sent in this system use system ASCII headers to define message properties. Many kinds of content can be sent using MIME. Messages are sent using SMTP, which works by making a TCP connection from the source host to the destination host and directly delivering the e-mail over the TCP connection. 

The other killer app for the Internet is the World Wide Web. The Web is a system for linking hypertext documents. Originally, each document was a page written in HTML with hyperlinks 

552




to other documents. Nowadays, XML is gradually starting to take over from HTML. Also, a large amount of content is dynamically generated, using server-side scripts (PHP, JSP, and ASP), as well as clientside scripts (notably JavaScript). A browser can display a document by establishing a TCP connection to its server, asking for the document, and then closing the connection. These request messages contain a variety of headers for providing additional information. Caching, replication, and content delivery networks are widely used to enhance Web performance. 

The wireless Web is just getting started. The first systems are WAP and i-mode, each with small screens and limited bandwidth, but the next generation will be more powerful. 

Multimedia is also a rising star in the networking firmament. It allows audio and video to be digitized and transported electronically for display. Audio requires less bandwidth, so it is further along. Streaming audio, Internet radio, and voice over IP are a reality now, with new applications coming along all the time. Video on demand is an up-and-coming area in which there is great interest. Finally, the MBone is an experimental, worldwide digital live television service sent over the Internet. 

Problems 

1. Many business computers have three distinct and worldwide unique identifiers. What are they? 

2. According to the information given in 

Fig. 7-3

, is 

little-sister.cs.vu.nl

 on a class A, B, or C network? 

3. In 

Fig. 7-3

, there is no period after 

rowboat

? Why not? 

4. Make a guess about what the smiley 

:-X

 (sometimes written as 

:-#

) might mean. 

5. DNS uses UDP instead of TCP. If a DNS packet is lost, there is no automatic recovery. Does this cause a problem, and if so, how is it solved? 

6. In addition to being subject to loss, UDP packets have a maximum length, potentially as low as 576 bytes. What happens when a DNS name to be looked up exceeds this length? Can it be sent in two packets? 

7. Can a machine with a single DNS name have multiple IP addresses? How could this occur? 

8. Can a computer have two DNS names that fall in different top-level domains? If so, give a plausible example. If not, explain why not. 

9. The number of companies with a Web site has grown explosively in recent years. As a result, thousands of companies are registered in the 

com

 domain, causing a heavy load on the top-level server for this domain. Suggest a way to alleviate this problem without changing the naming scheme (i.e., without introducing new top-level domain names). It is permitted that your solution requires changes to the client code. 

10. Some e-mail systems support a header field 

Content Return:

. It specifies whether the body of a message is to be returned in the event of nondelivery. Does this field belong to the envelope or to the header? 

11. Electronic mail systems need directories so people's e-mail addresses can be looked up. To build such directories, names should be broken up into standard components (e.g., first name, last name) to make searching possible. Discuss some problems that must be solved for a worldwide standard to be acceptable. 

12. A person's e-mail address is his or her login name @ the name of a DNS domain with an 

MX

 record. Login names can be first names, last names, initials, and all kinds of other names. Suppose that a large company decided too much e-mail was getting lost because people did not know the login name of the recipient. Is there a way for them to fix this problem without changing DNS? If so, give a proposal and explain how it works. If not, explain why it is impossible. 

13. A binary file is 3072 bytes long. How long will it be if encoded using base64 encoding, with a CR+LF pair inserted after every 80 bytes sent and at the end? 

14. Consider the quoted-printable MIME encoding scheme. Mention a problem not discussed in the text and propose a solution. 

553




15. Name five MIME types not listed in the book. You can check your browser or the Internet for information. 

16. Suppose that you want to send an MP3 file to a friend, but your friend's ISP limits the amount of incoming mail to 1 MB and the MP3 file is 4 MB. Is there a way to handle this situation by using RFC 822 and MIME? 

17. Suppose that someone sets up a vacation daemon and then sends a message just before logging out. Unfortunately, the recipient has been on vacation for a week and also has a vacation daemon in place. What happens next? Will canned replies go back and forth until somebody returns? 

18. In any standard, such as RFC 822, a precise grammar of what is allowed is needed so that different implementations can interwork. Even simple items have to be defined carefully. The SMTP headers allow white space between the tokens. Give 

two

 plausible alternative definitions of white space between tokens. 

19. Is the vacation daemon part of the user agent or the message transfer agent? Of course, it is set up using the user agent, but does the user agent actually send the replies? Explain your answer. 

20. POP3 allows users to fetch and download e-mail from a remote mailbox. Does this mean that the internal format of mailboxes has to be standardized so any POP3 program on the client side can read the mailbox on any mail server? Discuss your answer. 

21. From an ISP's point of view, POP3 and IMAP differ in an important way. POP3 users generally empty their mailboxes every day. IMAP users keep their mail on the server indefinitely. Imagine that you were called in to advise an ISP on which protocol it should support. What considerations would you bring up? 

22. Does Webmail use POP3, IMAP, or neither? If one of these, why was that one chosen? If neither, which one is it closer to in spirit? 

23. When Web pages are sent out, they are prefixed by MIME headers. Why? 

24. When are external viewers needed? How does a browser know which one to use? 

25. Is it possible that when a user clicks on a link with Netscape, a particular helper is started, but clicking on the same link in Internet Explorer causes a completely different helper to be started, even though the MIME type returned in both cases is identical? Explain your answer. 

26. A multithreaded Web server is organized as shown in 

Fig. 7-21

. It takes 500 µsec to accept a request and check the cache. Half the time the file is found in the cache and returned immediately. The other half of the time the module has to block for 9 msec while its disk request is queued and processed. How many modules should the server have to keep the CPU busy all the time (assuming the disk is not a bottleneck)? 

27. The standard 

http

 URL assumes that the Web server is listening on port 80. However, it is possible for a Web server to listen to some other port. Devise a reasonable syntax for a URL accessing a file on a nonstandard port. 

28. Although it was not mentioned in the text, an alternative form for a URL is to use the IP address instead of its DNS name. An example of using an IP address is 

http://192.31.231.66/index.html

. How does the browser know whether the name following the scheme is a DNS name or an IP address? 

29. Imagine that someone in the CS Department at Stanford has just written a new program that he wants to distribute by FTP. He puts the program in the FTP directory 

ftp/pub/freebies/newprog.c

. What is the URL for this program likely to be? 

30. In 

Fig. 7-25

, 

www.aportal.com

 keeps track of user preferences in a cookie. A disadvantage of this scheme is that cookies are limited to 4 KB, so if the preferences are extensive, for example, many stocks, sports teams, types of news stories, weather for multiple cities, specials in numerous product categories, and more, the 4-KB limit may be reached. Design an alternative way to keep track of preferences that does not have this problem. 

31. Sloth Bank wants to make on-line banking easy for its lazy customers, so after a customer signs up and is authenticated by a password, the bank returns a cookie containing a customer ID number. In this way, the customer does not have to identify himself or type a password on future visits to the on-line bank. What do you think of this idea? Will it work? Is it a good idea? 

554




32. In 

Fig. 7-26

, the 

ALT

 parameter is set in the <img> tag. Under what conditions does the browser use it, and how? 

33. How do you make an image clickable in HTML? Give an example. 

34. Show the <a> tag that is needed to make the string ''ACM'' be a hyperlink to 

http://www.acm.org

. 

35. Design a form for a new company, Interburger, that allows hamburgers to be ordered via the Internet. The form should include the customer's name, address, and city, as well as a choice of size (either gigantic or immense) and a cheese option. The burgers are to be paid for in cash upon delivery, so no credit card information is needed. 

36. Design a form that requests the user to type in two numbers. When the user clicks on the submit button, the server returns their sum. Write the server side as a PHP script. 

37. For each of the following applications, tell whether it would be (1) possible and (2) better to use a PHP script or JavaScript and why. 

a. (a) Displaying a calendar for any requested month since September 1752. 

b. (b) Displaying the schedule of flights from Amsterdam to New York. 

c. (c) Graphing a polynomial from user-supplied coefficients 

38. Write a program in JavaScript that accepts an integer greater than 2 and tells whether it is a prime number. Note that JavaScript has 

if

 and 

while

 statements with the same syntax as C and Java. The modulo operator is %. If you need the square root of 

x

, use 

Math.sqrt

 (

x

). 

39. An HTML page is as follows:  

40. <html> <body>  

41. <a href="www.info-source.com/welcome.html"> Click here for info </a>  

</body> </html>  

If the user clicks on the hyperlink, a TCP connection is opened and a series of lines is sent to the server. List all the lines sent. 

42. The 

If-Modified-Since

 header can be used to check whether a cached page is still valid. Requests can be made for pages containing images, sound, video, and so on, as well as HTML. Do you think the effectiveness of this technique is better or worse for JPEG images as compared to HTML? Think carefully about what ''effectiveness'' means and explain your answer. 

43. On the day of a major sporting event, such as the championship game in some popular sport, many people go to the official Web site. Is this a flash crowd in the same sense as the Florida election in 2000? Why or why not? 

44. Does it make sense for a single ISP to function as a CDN? If so, how would that work? If not, what is wrong with the idea? 

45. Under what conditions is using a CDN a bad idea? 

46. Wireless Web terminals have low bandwidth, which makes efficient coding important. Devise a scheme for transmitting English text efficiently over a wireless link to a WAP device. You may assume that the terminal has several megabytes of ROM and a moderately powerful CPU. 

Hint

: think about how you transmit Japanese, in which each symbol is a word. 

47. A compact disc holds 650 MB of data. Is compression used for audio CDs? Explain your reasoning. 

48. In 

Fig. 7-57(c)

 quantization noise occurs due to the use of 4-bit samples to represent nine signal values. The first sample, at 0, is exact, but the next few are not. What is the percent error for the samples at 1/32, 2/32, and 3/32 of the period? 

49. Could a psychoacoustic model be used to reduce the bandwidth needed for Internet telephony? If so, what conditions, if any, would have to be met to make it work? If not, why not? 

50. An audio streaming server has a one-way distance of 50 msec with a media player. It outputs at 1 Mbps. If the media player has a 1-MB buffer, what can you say about the position of the low-water mark and the high-water mark? 

51. The interleaving algorithm of 

Fig. 7-60

 has the advantage of being able to survive an occasional lost packet without introducing a gap in the playback. However, when used for Internet telephony, it also has a small disadvantage. What is it? 

555




52. Does voice over IP have the same problems with firewalls that streaming audio does? Discuss your answer. 

53. What is the bit rate for transmitting uncompressed 800 x 600 pixel color frames with 8 bits/pixel at 40 frames/sec? 

54. Can a 1-bit error in an MPEG frame affect more than the frame in which the error occurs? Explain your answer. 

55. Consider a 100,000-customer video server, where each customer watches two movies per month. Half the movies are served at 8 P.M. How many movies does the server have to transmit at once during this time period? If each movie requires 4 Mbps, how many OC-12 connections does the server need to the network? 

56. Suppose that Zipf's law holds for accesses to a 10,000-movie video server. If the server holds the most popular 1000 movies on magnetic disk and the remaining 9000 on optical disk, give an expression for the fraction of all references that will be to magnetic disk. Write a little program to evaluate this expression numerically. 

57. Some cybersquatters have registered domain names that are misspellings of common corporate sites, for example, 

www.microsfot.com

. Make a list of at least five such domains. 

58. Numerous people have registered DNS names that consist of a 

www.word.com

 where 

word

 is a common word. For each of the following categories, list five Web sites and briefly summarize what it is (e.g., 

www.stomach.com

 is a gastroenterologist on Long Island). Here is the list of categories: animals, foods, household objects, and body parts. For the last category, please stick to body parts above the waist. 

59. Design some emoji of your own using a 12 x 12 bit map. Include boyfriend, girlfriend, professor, and politician. 

60. Write a POP3 server that accepts the following commands: 

USER

, 

PASS

, 

LIST

, 

RETR

, 

DELE

, and 

QUIT

. 

61. Rewrite the server of 

Fig. 6-6

 as a true Web server using the 

GET

 command for HTTP 1.1. It should also accept the 

Host

 message. The server should maintain a cache of files recently fetched from the disk and serve requests from the cache when possible. 

 

556




Chapter 8. Network Security 

For the first few decades of their existence, computer networks were primarily used by university researchers for sending e-mail and by corporate employees for sharing printers. Under these conditions, security did not get a lot of attention. But now, as millions of ordinary citizens are using networks for banking, shopping, and filing their tax returns, network security is looming on the horizon as a potentially massive problem. In this chapter, we will study network security from several angles, point out numerous pitfalls, and discuss many algorithms and protocols for making networks more secure. 

Security is a broad topic and covers a multitude of sins. In its simplest form, it is concerned with making sure that nosy people cannot read, or worse yet, secretly modify messages intended for other recipients. It is concerned with people trying to access remote services that they are not authorized to use. It also deals with ways to tell whether that message purportedly from the IRS saying: Pay by Friday or else is really from the IRS and not from the Mafia. Security also deals with the problems of legitimate messages being captured and replayed, and with people trying to deny that they sent certain messages. 

Most security problems are intentionally caused by malicious people trying to gain some benefit, get attention, or to harm someone. A few of the most common perpetrators are listed in 

Fig. 8-1

. It should be clear from this list that making a network secure involves a lot more than just keeping it free of programming errors. It involves outsmarting often intelligent, dedicated, and sometimes well-funded adversaries. It should also be clear that measures that will thwart casual adversaries will have little impact on the serious ones. Police records show that most attacks are not perpetrated by outsiders tapping a phone line but by insiders with a grudge. Consequently, security systems should be designed with this fact in mind. 

Figure 8-1. Some people who cause security problems and why. 

 

Network security problems can be divided roughly into four closely intertwined areas: secrecy, authentication, nonrepudiation, and integrity control. Secrecy, also called confidentiality, has to do with keeping information out of the hands of unauthorized users. This is what usually comes to mind when people think about network security. Authentication deals with determining whom you are talking to before revealing sensitive information or entering into a business deal. Nonrepudiation deals with signatures: How do you prove that your customer really placed an electronic order for ten million left-handed doohickeys at 89 cents each when he later claims the price was 69 cents? Or maybe he claims he never placed any order. Finally, how can you be sure that a message you received was really the one sent and not something that a malicious adversary modified in transit or concocted? 

557




All these issues (secrecy, authentication, nonrepudiation, and integrity control) occur in traditional systems, too, but with some significant differences. Integrity and secrecy are achieved by using registered mail and locking documents up. Robbing the mail train is harder now than it was in Jesse James' day. 

Also, people can usually tell the difference between an original paper document and a photocopy, and it often matters to them. As a test, make a photocopy of a valid check. Try cashing the original check at your bank on Monday. Now try cashing the photocopy of the check on Tuesday. Observe the difference in the bank's behavior. With electronic checks, the original and the copy are indistinguishable. It may take a while for banks to learn how to handle this. 

People authenticate other people by recognizing their faces, voices, and handwriting. Proof of signing is handled by signatures on letterhead paper, raised seals, and so on. Tampering can usually be detected by handwriting, ink, and paper experts. None of these options are available electronically. Clearly, other solutions are needed. 

Before getting into the solutions themselves, it is worth spending a few moments considering where in the protocol stack network security belongs. There is probably no one single place. Every layer has something to contribute. In the physical layer, wiretapping can be foiled by enclosing transmission lines in sealed tubes containing gas at high pressure. Any attempt to drill into a tube will release some gas, reducing the pressure and triggering an alarm. Some military systems use this technique. 

In the data link layer, packets on a point-to-point line can be encrypted as they leave one machine and decrypted as they enter another. All the details can be handled in the data link layer, with higher layers oblivious to what is going on. This solution breaks down when packets have to traverse multiple routers, however, because packets have to be decrypted at each router, leaving them vulnerable to attacks from within the router. Also, it does not allow some sessions to be protected (e.g., those involving on-line purchases by credit card) and others not. Nevertheless, 

link encryption

, as this method is called, can be added to any network easily and is often useful. 

In the network layer, firewalls can be installed to keep good packets and bad packets out. IP security also functions in this layer. 

In the transport layer, entire connections can be encrypted, end to end, that is, process to process. For maximum security, end-to-end security is required. 

Finally, issues such as user authentication and nonrepudiation can only be handled in the application layer. 

Since security does not fit neatly into any layer, it does not fit into any chapter of this book. For this reason, it rates its own chapter. 

While this chapter is long, technical, and essential, and it is also quasi-irrelevant for the moment. It is well documented that most security failures at banks, for example, are due to incompetent employees, lax security procedures, or insider fraud, rather than clever criminals tapping phone lines and then decoding encrypted messages. If a person can walk into a random branch of a bank with an ATM slip he found on the street claiming to have forgotten his PIN and get a new one on the spot (in the name of good customer relations), all the cryptography in the world will not prevent abuse. In this respect, Ross Anderson's book is a real eye-opener, as it documents hundreds of examples of security failures in numerous industries, nearly all of them due to what might politely be called sloppy business practices or inattention to security (Anderson, 2001). Nevertheless, we are optimistic that as e-commerce becomes more widespread, companies will eventually debug their operational procedures, eliminating this loophole and bringing the technical aspects of security to center stage again. 

558




Except for physical layer security, nearly all security is based on cryptographic principles. For this reason, we will begin our study of security by examining cryptography in some detail. In 

Sec. 8.1

, we will look at some of the basic principles. In 

Sec. 8-2

 through 

Sec. 8-5

, we will examine some of the fundamental algorithms and data structures used in cryptography. Then we will examine in detail how these concepts can be used to achieve security in networks. We will conclude with some brief thoughts about technology and society. 

Before starting, one last thought is in order: what is not covered. We have tried to focus on networking issues, rather than operating system and application issues, although the line is often hard to draw. For example, there is nothing here about user authentication using biometrics, password security, buffer overflow attacks, Trojan horses, login spoofing, logic bombs, viruses, worms, and the like. All of these topics are covered at great length in 

Chap. 9

 of 

Modern Operating Systems

 (Tanenbaum, 2001). The interested reader is referred to that book for the systems aspects of security. Now let us begin our journey. 

 

8.1 Cryptography 

Cryptography

 comes from the Greek words for ''secret writing.'' It has a long and colorful history going back thousands of years. In this section we will just sketch some of the highlights, as background information for what follows. For a complete history of cryptography, Kahn's (1995) book is recommended reading. For a comprehensive treatment of the current state-of-the-art in security and cryptographic algorithms, protocols, and applications, see (Kaufman et al., 2002). For a more mathematical approach, see (Stinson, 2002). For a less mathematical approach, see (Burnett and Paine, 2001). 

Professionals make a distinction between ciphers and codes. A 

cipher

 is a character-for-character or bit-for-bit transformation, without regard to the linguistic structure of the message. In contrast, a 

code

 replaces one word with another word or symbol. Codes are not used any more, although they have a glorious history. The most successful code ever devised was used by the U.S. armed forces during World War II in the Pacific. They simply had Navajo Indians talking to each other using specific Navajo words for military terms, for example 

chay-dagahi-nail-tsaidi

 (literally: tortoise killer) for antitank weapon. The Navajo language is highly tonal, exceedingly complex, and has no written form. And not a single person in Japan knew anything about it. 

In September 1945, the 

San Diego Union

 described the code by saying ''For three years, wherever the Marines landed, the Japanese got an earful of strange gurgling noises interspersed with other sounds resembling the call of a Tibetan monk and the sound of a hot water bottle being emptied.'' The Japanese never broke the code and many Navajo code talkers were awarded high military honors for extraordinary service and bravery. The fact that the U.S. broke the Japanese code but the Japanese never broke the Navajo code played a crucial role in the American victories in the Pacific. 

8.1.1 Introduction to Cryptography 

Historically, four groups of people have used and contributed to the art of cryptography: the military, the diplomatic corps, diarists, and lovers. Of these, the military has had the most important role and has shaped the field over the centuries. Within military organizations, the messages to be encrypted have traditionally been given to poorly-paid, low-level code clerks for encryption and transmission. The sheer volume of messages prevented this work from being done by a few elite specialists. 

Until the advent of computers, one of the main constraints on cryptography had been the ability of the code clerk to perform the necessary transformations, often on a battlefield with 

559




little equipment. An additional constraint has been the difficulty in switching over quickly from one cryptographic method to another one, since this entails retraining a large number of people. However, the danger of a code clerk being captured by the enemy has made it essential to be able to change the cryptographic method instantly if need be. These conflicting requirements have given rise to the model of 

Fig. 8-2

. 

Figure 8-2. The encryption model (for a symmetric-key cipher). 

 

The messages to be encrypted, known as the 

plaintext

, are transformed by a function that is parameterized by a 

key

. The output of the encryption process, known as the 

ciphertext

, is then transmitted, often by messenger or radio. We assume that the enemy, or 

intruder

, hears and accurately copies down the complete ciphertext. However, unlike the intended recipient, he does not know what the decryption key is and so cannot decrypt the ciphertext easily. Sometimes the intruder can not only listen to the communication channel (passive intruder) but can also record messages and play them back later, inject his own messages, or modify legitimate messages before they get to the receiver (active intruder). The art of breaking ciphers, called 

cryptanalysis

, and the art devising them (cryptography) is collectively known as 

cryptology

. 

It will often be useful to have a notation for relating plaintext, ciphertext, and keys. We will use 

C

 = 

E

K

(

P

) to mean that the encryption of the plaintext 

P

 using key 

K

 gives the ciphertext 

C

. Similarly, 

P

 = 

D

K

(

C

) represents the decryption of 

C

 to get the plaintext again. It then follows that 

 

 

This notation suggests that 

E

 and 

D

 are just mathematical functions, which they are. The only tricky part is that both are functions of two parameters, and we have written one of the parameters (the key) as a subscript, rather than as an argument, to distinguish it from the message. 

A fundamental rule of cryptography is that one must assume that the cryptanalyst knows the methods used for encryption and decryption. In other words, the cryptanalyst knows how the encryption method, 

E

, and decryption, 

D

,of 

Fig. 8-2

 work in detail. The amount of effort necessary to invent, test, and install a new algorithm every time the old method is compromised (or thought to be compromised) has always made it impractical to keep the encryption algorithm secret. Thinking it is secret when it is not does more harm than good. 

This is where the key enters. The key consists of a (relatively) short string that selects one of many potential encryptions. In contrast to the general method, which may only be changed 

560




every few years, the key can be changed as often as required. Thus, our basic model is a stable and publicly-known general method parameterized by a secret and easily changed key. The idea that the cryptanalyst knows the algorithms and that the secrecy lies exclusively in the keys is called 

Kerckhoff's principle

, named after the Flemish military cryptographer Auguste Kerckhoff who first stated it in 1883 (Kerckhoff, 1883). Thus, we have: 

Kerckhoff's principle: All algorithms must be public; only the keys are secret

The nonsecrecy of the algorithm cannot be emphasized enough. Trying to keep the algorithm secret, known in the trade as 

security by obscurity

, never works. Also, by publicizing the algorithm, the cryptographer gets free consulting from a large number of academic cryptologists eager to break the system so they can publish papers demonstrating how smart they are. If many experts have tried to break the algorithm for 5 years after its publication and no one has succeeded, it is probably pretty solid. 

Since the real secrecy is in the key, its length is a major design issue. Consider a simple combination lock. The general principle is that you enter digits in sequence. Everyone knows this, but the key is secret. A key length of two digits means that there are 100 possibilities. A key length of three digits means 1000 possibilities, and a key length of six digits means a million. The longer the key, the higher the 

work factor

 the cryptanalyst has to deal with. The work factor for breaking the system by exhaustive search of the key space is exponential in the key length. Secrecy comes from having a strong (but public) algorithm and a long key. To prevent your kid brother from reading your e-mail, 64-bit keys will do. For routine commercial use, at least 128 bits should be used. To keep major governments at bay, keys of at least 256 bits, preferably more, are needed. 

From the cryptanalyst's point of view, the cryptanalysis problem has three principal variations. When he has a quantity of ciphertext and no plaintext, he is confronted with the 

ciphertext-only

 problem. The cryptograms that appear in the puzzle section of newspapers pose this kind of problem. When the cryptanalyst has some matched ciphertext and plaintext, the problem is called the 

known plaintext

 problem. Finally, when the cryptanalyst has the ability to encrypt pieces of plaintext of his own choosing, we have the 

chosen plaintext

 problem. Newspaper cryptograms could be broken trivially if the cryptanalyst were allowed to ask such questions as: What is the encryption of ABCDEFGHIJKL? 

Novices in the cryptography business often assume that if a cipher can withstand a ciphertext-only attack, it is secure. This assumption is very naive. In many cases the cryptanalyst can make a good guess at parts of the plaintext. For example, the first thing many computers say when you call them up is login: . Equipped with some matched plaintext-ciphertext pairs, the cryptanalyst's job becomes much easier. To achieve security, the cryptographer should be conservative and make sure that the system is unbreakable even if his opponent can encrypt arbitrary amounts of chosen plaintext. 

Encryption methods have historically been divided into two categories: substitution ciphers and transposition ciphers. We will now deal with each of these briefly as background information for modern cryptography. 

8.1.2 Substitution Ciphers 

In a 

substitution cipher

 each letter or group of letters is replaced by another letter or group of letters to disguise it. One of the oldest known ciphers is the 

Caesar cipher

, attributed to Julius Caesar. In this method, 

a

 becomes 

D

, 

b

 becomes 

E

, 

c

 becomes 

F

, 

...

 , and 

z

 becomes 

C

. For example, 

attack

 becomes 

DWWDFN

. In examples, plaintext will be given in lower case letters, and ciphertext in upper case letters. 

A slight generalization of the Caesar cipher allows the ciphertext alphabet to be shifted by 

k

 letters, instead of always 3. In this case 

k

 becomes a key to the general method of circularly 

561




shifted alphabets. The Caesar cipher may have fooled Pompey, but it has not fooled anyone since. 

The next improvement is to have each of the symbols in the plaintext, say, the 26 letters for simplicity, map onto some other letter. For example, 

plaintext: a b c d e f g h i j k l m n o p q r s t u v w x y z 

ciphertext: Q W E R T Y U I O P A S D F G H J K L Z X C V B N M 

The general system of symbol-for-symbol substitution is called a 

monoalphabetic substitution

, with the key being the 26-letter string corresponding to the full alphabet. For the key above, the plaintext 

attack

 would be transformed into the ciphertext 

QZZQEA

. 

At first glance this might appear to be a safe system because although the cryptanalyst knows the general system (letter-for-letter substitution), he does not know which of the 26! 

4 x 10

26

 possible keys is in use. In contrast with the Caesar cipher, trying all of them is not a promising approach. Even at 1 nsec per solution, a computer would take 10

10

 years to try all the keys. 

Nevertheless, given a surprisingly small amount of ciphertext, the cipher can be broken easily. The basic attack takes advantage of the statistical properties of natural languages. In English, for example, 

e

 is the most common letter, followed by 

t

, 

o

, 

a

, 

n

, 

i

, etc. The most common two-letter combinations, or 

digrams

, are 

th

, 

in

, 

er

, 

re

, and 

an

. The most common three-letter combinations, or 

trigrams

, are 

the

, 

ing

, 

and

, and 

ion

. 

A cryptanalyst trying to break a monoalphabetic cipher would start out by counting the relative frequencies of all letters in the ciphertext. Then he might tentatively assign the most common one to 

e

 and the next most common one to 

t

. He would then look at trigrams to find a common one of the form 

tXe

, which strongly suggests that 

X

 is 

h

. Similarly, if the pattern 

thYt

 occurs frequently, the 

Y

 probably stands for 

a

. With this information, he can look for a frequently occurring trigram of the form 

aZW

, which is most likely 

and

. By making guesses at common letters, digrams, and trigrams and knowing about likely patterns of vowels and consonants, the cryptanalyst builds up a tentative plaintext, letter by letter. 

Another approach is to guess a probable word or phrase. For example, consider the following ciphertext from an accounting firm (blocked into groups of five characters): 

CTBMN BYCTC BTJDS QXBNS GSTJC BTSWX CTQTZ CQVUJ  

QJSGS TJQZZ MNQJS VLNSX VSZJU JDSTS JQUUS JUBXJ  

DSKSU JSNTK BGAQJ ZBGYQ TLCTZ BNYBN QJSW  

A likely word in a message from an accounting firm is 

financial

. Using our knowledge that 

financial

 has a repeated letter (

i

), with four other letters between their occurrences, we look for repeated letters in the ciphertext at this spacing. We find 12 hits, at positions 6, 15, 27, 31, 42, 48, 56, 66, 70, 71, 76, and 82. However, only two of these, 31 and 42, have the next letter (corresponding to 

n

 in the plaintext) repeated in the proper place. Of these two, only 31 also has the 

a

 correctly positioned, so we know that 

financial

 begins at position 30. From this point on, deducing the key is easy by using the frequency statistics for English text. 

8.1.3 Transposition Ciphers 

Substitution ciphers preserve the order of the plaintext symbols but disguise them. 

Transposition ciphers

, in contrast, reorder the letters but do not disguise them. 

Figure 8-3

 depicts a common transposition cipher, the columnar transposition. The cipher is keyed by a word or phrase not containing any repeated letters. In this example, MEGABUCK is the key. 

562




The purpose of the key is to number the columns, column 1 being under the key letter closest to the start of the alphabet, and so on. The plaintext is written horizontally, in rows, padded to fill the matrix if need be. The ciphertext is read out by columns, starting with the column whose key letter is the lowest. 

Figure 8-3. A transposition cipher. 

 

To break a transposition cipher, the cryptanalyst must first be aware that he is dealing with a transposition cipher. By looking at the frequency of 

E

, 

T

, 

A

, 

O

, 

I

, 

N

, etc., it is easy to see if they fit the normal pattern for plaintext. If so, the cipher is clearly a transposition cipher, because in such a cipher every letter represents itself, keeping the frequency distribution intact. 

The next step is to make a guess at the number of columns. In many cases a probable word or phrase may be guessed at from the context. For example, suppose that our cryptanalyst suspects that the plaintext phrase 

milliondollars

 occurs somewhere in the message. Observe that digrams 

MO

, 

IL

, 

LL

, 

LA

, 

IR

 and 

OS

 occur in the ciphertext as a result of this phrase wrapping around. The ciphertext letter 

O

 follows the ciphertext letter 

M

 (i.e., they are vertically adjacent in column 4) because they are separated in the probable phrase by a distance equal to the key length. If a key of length seven had been used, the digrams 

MD

, 

IO

, 

LL

, 

LL

, 

IA

, 

OR

, and 

NS

 would have occurred instead. In fact, for each key length, a different set of digrams is produced in the ciphertext. By hunting for the various possibilities, the cryptanalyst can often easily determine the key length. 

The remaining step is to order the columns. When the number of columns, 

k

, is small, each of the 

k

(

k

 - 1) column pairs can be examined to see if its digram frequencies match those for English plaintext. The pair with the best match is assumed to be correctly positioned. Now each remaining column is tentatively tried as the successor to this pair. The column whose digram and trigram frequencies give the best match is tentatively assumed to be correct. The predecessor column is found in the same way. The entire process is continued until a potential ordering is found. Chances are that the plaintext will be recognizable at this point (e.g., if 

milloin

 occurs, it is clear what the error is). 

Some transposition ciphers accept a fixed-length block of input and produce a fixed-length block of output. These ciphers can be completely described by giving a list telling the order in which the characters are to be output. For example, the cipher of 

Fig. 8-3

 can be seen as a 64 character block cipher. Its output is 4, 12, 20, 28, 36, 44, 52, 60, 5, 13 , ... , 62. In other words, the fourth input character, 

a

, is the first to be output, followed by the twelfth, 

f

, and so on. 

8.1.4 One-Time Pads 

Constructing an unbreakable cipher is actually quite easy; the technique has been known for decades. First choose a random bit string as the key. Then convert the plaintext into a bit string, for example by using its ASCII representation. Finally, compute the XOR (eXclusive OR) 

563




of these two strings, bit by bit. The resulting ciphertext cannot be broken, because in a sufficiently large sample of ciphertext, each letter will occur equally often, as will every digram, every trigram, and so on. This method, known as the 

one-time pad

, is immune to all present and future attacks no matter how much computational power the intruder has. The reason derives from information theory: there is simply no information in the message because all possible plaintexts of the given length are equally likely. 

An example of how one-time pads are used is given in 

Fig. 8-4

. First, message 1, ''I love you.'' is converted to 7-bit ASCII. Then a one-time pad, pad 1, is chosen and XORed with the message to get the ciphertext. A cryptanalyst could try all possible one-time pads to see what plaintext came out for each one. For example, the one-time pad listed as pad 2 in the figure could be tried, resulting in plaintext 2, ''Elvis lives'', which may or may not be plausible (a subject beyond the scope of this book). In fact, for every 11-character ASCII plaintext, there is a one-time pad that generates it. That is what we mean by saying there is no information in the ciphertext: you can get any message of the correct length out of it. 

Figure 8-4. The use of a one-time pad for encryption and the possibility of getting any possible plaintext from the ciphertext by the use of some other pad. 

 

One-time pads are great in theory but have a number of disadvantages in practice. To start with, the key cannot be memorized, so both sender and receiver must carry a written copy with them. If either one is subject to capture, written keys are clearly undesirable. Additionally, the total amount of data that can be transmitted is limited by the amount of key available. If the spy strikes it rich and discovers a wealth of data, he may find himself unable to transmit it back to headquarters because the key has been used up. Another problem is the sensitivity of the method to lost or inserted characters. If the sender and receiver get out of synchronization, all data from then on will appear garbled. 

With the advent of computers, the one-time pad might potentially become practical for some applications. The source of the key could be a special DVD that contains several gigabytes of information and if transported in a DVD movie box and prefixed by a few minutes of video, would not even be suspicious. Of course, at gigabit network speeds, having to insert a new DVD every 30 sec could become tedious. And the DVDs must be personally carried from the sender to the receiver before any messages can be sent, which greatly reduces their practical utility. 

Quantum Cryptography 

Interestingly, there may be a solution to the problem of how to transmit the one-time pad over the network, and it comes from a very unlikely source: quantum mechanics. This area is still experimental, but initial tests are promising. If it can be perfected and be made efficient, virtually all cryptography will eventually be done using one-time pads since they are provably secure. Below we will briefly explain how this method, 

quantum cryptography

, works. In particular, we will describe a protocol called 

BB84

 after its authors and publication year (Bennet and Brassard, 1984). 

A user, Alice, wants to establish a one-time pad with a second user, Bob. Alice and Bob are called 

principals

, the main characters in our story. For example, Bob is a banker with whom Alice would like to do business. The names ''Alice'' and ''Bob'' have been used for the principals 

564




in virtually every paper and book on cryptography in the past decade. Cryptographers love tradition. If we were to use ''Andy'' and ''Barbara'' as the principals, no one would believe anything in this chapter. So be it. 

If Alice and Bob could establish a one-time pad, they could use it to communicate securely. The question is: How can they establish it without previously exchanging DVDs? We can assume that Alice and Bob are at opposite ends of an optical fiber over which they can send and receive light pulses. However, an intrepid intruder, Trudy, can cut the fiber to splice in an active tap. Trudy can read all the bits in both directions. She can also send false messages in both directions. The situation might seem hopeless for Alice and Bob, but quantum cryptography can shed some new light on the subject. 

Quantum cryptography is based on the fact that light comes in little packets called 

photons

, which have some peculiar properties. Furthermore, light can be polarized by being passed through a polarizing filter, a fact well known to both sunglasses wearers and photographers. If a beam of light (i.e., a stream of photons) is passed through a polarizing filter, all the photons emerging from it will be polarized in the direction of the filter's axis (e.g., vertical). If the beam is now passed through a second polarizing filter, the intensity of the light emerging from the second filter is proportional to the square of the cosine of the angle between the axes. If the two axes are perpendicular, no photons get through. The absolute orientation of the two filters does not matter; only the angle between their axes counts. 

To generate a one-time pad, Alice needs two sets of polarizing filters. Set one consists of a vertical filter and a horizontal filter. This choice is called a 

rectilinear basis

. A basis (plural: bases) is just a coordinate system. The second set of filters is the same, except rotated 45 degrees, so one filter runs from the lower left to the upper right and the other filter runs from the upper left to the lower right. This choice is called a 

diagonal basis

. Thus, Alice has two bases, which she can rapidly insert into her beam at will. In reality, Alice does not have four separate filters, but a crystal whose polarization can be switched electrically to any of the four allowed directions at great speed. Bob has the same equipment as Alice. The fact that Alice and Bob each have two bases available is essential to quantum cryptography. 

For each basis, Alice now assigns one direction as 0 and the other as 1. In the example presented below, we assume she chooses vertical to be 0 and horizontal to be 1. Independently, she also chooses lower left to upper right as 0 and upper left to lower right as 1. She sends these choices to Bob as plaintext. 

Now Alice picks a one-time pad, for example based on a random number generator (a complex subject all by itself). She transfers it bit by bit to Bob, choosing one of her two bases at random for each bit. To send a bit, her photon gun emits one photon polarized appropriately for the basis she is using for that bit. For example, she might choose bases of diagonal, rectilinear, rectilinear, diagonal, rectilinear, etc. To send her one-time pad of 1001110010100110 with these bases, she would send the photons shown in 

Fig. 8-5(a)

. Given the one-time pad and the sequence of bases, the polarization to use for each bit is uniquely determined. Bits sent one photon at a time are called 

qubits

. 

Figure 8-5. An example of quantum cryptography. 

565




 

Bob does not know which bases to use, so he picks one at random for each arriving photon and just uses it, as shown in 

Fig. 8-5(b)

. If he picks the correct basis, he gets the correct bit. If he picks the incorrect basis, he gets a random bit because if a photon hits a filter polarized at 45 degrees to its own polarization, it randomly jumps to the polarization of the filter or to a polarization perpendicular to the filter with equal probability. This property of photons is fundamental to quantum mechanics. Thus, some of the bits are correct and some are random, but Bob does not know which are which. Bob's results are depicted in 

Fig. 8-5(c)

. 

How does Bob find out which bases he got right and which he got wrong? He simply tells Alice which basis he used for each bit in plaintext and she tells him which are right and which are wrong in plaintext, as shown in 

Fig. 8-5(d)

. From this information both of them can build a bit string from the correct guesses, as shown in 

Fig. 8-5(e)

. On the average, this bit string will be half the length of the original bit string, but since both parties know it, they can use it as a one-time pad. All Alice has to do is transmit a bit string slightly more than twice the desired length and she and Bob have a one-time pad of the desired length. Problem solved. 

But wait a minute. We forgot Trudy. Suppose that she is curious about what Alice has to say and cuts the fiber, inserting her own detector and transmitter. Unfortunately for her, she does not know which basis to use for each photon either. The best she can do is pick one at random for each photon, just as Bob does. An example of her choices is shown in 

Fig. 8-5(f)

. When Bob later reports (in plaintext) which bases he used and Alice tells him (in plaintext) which ones are correct, Trudy now knows when she got it right and when she got it wrong. In 

Fig. 8-

5

 she got it right for bits 0, 1, 2, 3, 4, 6, 8, 12, and 13. But she knows from Alice's reply in 

Fig. 

8-5(d)

 that only bits 1, 3, 7, 8, 10, 11, 12, and 14 are part of the one-time pad. For four of these bits (1, 3, 8, and 12), she guessed right and captured the correct bit. For the other four (7, 10, 11, and 14) she guessed wrong and does not know the bit transmitted. Thus, Bob knows the one-time pad starts with 01011001, from 

Fig. 8-5(e)

 but all Trudy has is 01?1??0?, from 

Fig. 8-5(g)

. 

Of course, Alice and Bob are aware that Trudy may have captured part of their one-time pad, so they would like to reduce the information Trudy has. They can do this by performing a transformation on it. For example, they could divide the one-time pad into blocks of 1024 bits and square each one to form a 2048-bit number and use the concatenation of these 2048-bit numbers as the one-time pad. With her partial knowledge of the bit string transmitted, Trudy has no way to generate its square and so has nothing. The transformation from the original one-time pad to a different one that reduces Trudy's knowledge is called 

privacy 

566




amplification

. In practice, complex transformations in which every output bit depends on every input bit are used instead of squaring. 

Poor Trudy. Not only does she have no idea what the one-time pad is, but her presence is not a secret either. After all, she must relay each received bit to Bob to trick him into thinking he is talking to Alice. The trouble is, the best she can do is transmit the qubit she received, using the polarization she used to receive it, and about half the time she will be wrong, causing many errors in Bob's one-time pad. 

When Alice finally starts sending data, she encodes it using a heavy forward-error-correcting code. From Bob's point of view, a 1-bit error in the one-time pad is the same as a 1-bit transmission error. Either way, he gets the wrong bit. If there is enough forward error correction, he can recover the original message despite all the errors, but he can easily count how many errors were corrected. If this number is far more than the expected error rate of the equipment, he knows that Trudy has tapped the line and can act accordingly (e.g., tell Alice to switch to a radio channel, call the police, etc.). If Trudy had a way to clone a photon so she had one photon to inspect and an identical photon to send to Bob, she could avoid detection, but at present no way to clone a photon perfectly is known. But even if Trudy could clone photons, the value of quantum cryptography to establish one-time pads would not be reduced. 

Although quantum cryptography has been shown to operate over distances of 60 km of fiber, the equipment is complex and expensive. Still, the idea has promise. For more information about quantum cryptography, see (Mullins, 2002). 

8.1.5 Two Fundamental Cryptographic Principles 

Although we will study many different cryptographic systems in the pages ahead, two principles underlying all of them are important to understand. 

Redundancy 

The first principle is that all encrypted messages must contain some redundancy, that is, information not needed to understand the message. An example may make it clear why this is needed. Consider a mail-order company, The Couch Potato (TCP), with 60,000 products. Thinking they are being very efficient, TCP's programmers decide that ordering messages should consist of a 16-byte customer name followed by a 3-byte data field (1 byte for the quantity and 2 bytes for the product number). The last 3 bytes are to be encrypted using a very long key known only by the customer and TCP. 

At first this might seem secure, and in a sense it is because passive intruders cannot decrypt the messages. Unfortunately, it also has a fatal flaw that renders it useless. Suppose that a recently-fired employee wants to punish TCP for firing her. Just before leaving, she takes the customer list with her. She works through the night writing a program to generate fictitious orders using real customer names. Since she does not have the list of keys, she just puts random numbers in the last 3 bytes, and sends hundreds of orders off to TCP. 

When these messages arrive, TCP's computer uses the customer's name to locate the key and decrypt the message. Unfortunately for TCP, almost every 3-byte message is valid, so the computer begins printing out shipping instructions. While it might seem odd for a customer to order 837 sets of children's swings or 540 sandboxes, for all the computer knows, the customer might be planning to open a chain of franchised playgrounds. In this way an active intruder (the ex-employee) can cause a massive amount of trouble, even though she cannot understand the messages her computer is generating. 

This problem can be solved by the addition of redundancy to all messages. For example, if order messages are extended to 12 bytes, the first 9 of which must be zeros, then this attack 

567




no longer works because the ex-employee can no longer generate a large stream of valid messages. The moral of the story is that all messages must contain considerable redundancy so that active intruders cannot send random junk and have it be interpreted as a valid message. 

However, adding redundancy also makes it easier for cryptanalysts to break messages. Suppose that the mail order business is highly competitive, and The Couch Potato's main competitor, The Sofa Tuber, would dearly love to know how many sandboxes TCP is selling. Consequently, they have tapped TCP's telephone line. In the original scheme with 3-byte messages, cryptanalysis was nearly impossible, because after guessing a key, the cryptanalyst had no way of telling whether the guess was right. After all, almost every message is technically legal. With the new 12-byte scheme, it is easy for the cryptanalyst to tell a valid message from an invalid one. Thus, we have 

Cryptographic principle 1: Messages must contain some redundancy

In other words, upon decrypting a message, the recipient must be able to tell whether it is valid by simply inspecting it and perhaps performing a simple computation. This redundancy is needed to prevent active intruders from sending garbage and tricking the receiver into decrypting the garbage and acting on the ''plaintext.'' However, this same redundancy makes it much easier for passive intruders to break the system, so there is some tension here. Furthermore, the redundancy should never be in the form of 

n

 zeros at the start or end of a message, since running such messages through some cryptographic algorithms gives more predictable results, making the cryptanalysts' job easier. A CRC polynomial is much better than a run of 0s since the receiver can easily verify it, but it generates more work for the cryptanalyst. Even better is to use a cryptographic hash, a concept we will explore later. 

Getting back to quantum cryptography for a moment, we can also see how redundancy plays a role there. Due to Trudy's interception of the photons, some bits in Bob's one-time pad will be wrong. Bob needs some redundancy in the incoming messages to determine that errors are present. One very crude form of redundancy is repeating the message two times. If the two copies are not identical, Bob knows that either the fiber is very noisy or someone is tampering with the transmission. Of course, sending everything twice is overkill; a Hamming or Reed-Solomon code is a more efficient way to do error detection and correction. But it should be clear that some redundancy is needed to distinguish a valid message from an invalid message, especially in the face of an active intruder. 

Freshness 

The second cryptographic principle is that some measures must be taken to ensure that each message received can be verified as being fresh, that is, sent very recently. This measure is needed to prevent active intruders from playing back old messages. If no such measures were taken, our ex-employee could tap TCP's phone line and just keep repeating previously sent valid messages. Restating this idea we get: 

Cryptographic principle 2: Some method is needed to foil replay attacks

One such measure is including in every message a timestamp valid only for, say, 10 seconds. The receiver can then just keep messages around for 10 seconds, to compare newly arrived messages to previous ones to filter out duplicates. Messages older than 10 seconds can be thrown out, since any replays sent more than 10 seconds later will be rejected as too old. Measures other than timestamps will be discussed later. 

568




8.2 Symmetric-Key Algorithms 

Modern cryptography uses the same basic ideas as traditional cryptography (transposition and substitution) but its emphasis is different. Traditionally, cryptographers have used simple algorithms. Nowadays the reverse is true: the object is to make the encryption algorithm so complex and involuted that even if the cryptanalyst acquires vast mounds of enciphered text ofhis own choosing, he will not be able to make any sense of it at all without the key. 

The first class of encryption algorithms we will study in this chapter are called 

symmetric-key

 

algorithms

 because they used the same key for encryption and decryption. 

Fig. 8-2

 illustratesthe use of a symmetric-key algorithm. In particular, we will focus on 

block ciphers

, which take an 

n

-bit block of plaintext as input and transform it using the key into 

n

-bit block of ciphertext. 

Cryptographic algorithms can be implemented in either hardware (for speed) or in software (for flexibility). Although most of our treatment concerns the algorithms and protocols, which are independent of the actual implementation, a few words about building cryptographic hardware may be of interest. Transpositions and substitutions can be implemented with simple electrical circuits. 

Figure 8-6(a)

 shows a device, known as a 

P-box

 (P stands for permutation), used to effect a transposition on an 8-bit input. If the 8 bits are designated from top to bottom as 01234567, the output of this particular P-box is 36071245. By appropriate internal wiring, a

 

P-box can be made to perform any transposition and do it at practically the speed of light sinceno computation is involved, just signal propagation. This design follows Kerckhoff's principle: the attacker knows that the general method is permuting the bits. What he does not know is which bit goes where, which is the key. 

Figure 8-6. Basic elements of product ciphers. (a) P-box. (b) S-box. (c)Product. 

 

Substitutions are performed by 

S-boxes

, as shown in 

Fig. 8-6(b)

. In this example a 3-bit plaintext is entered and a 3-bit ciphertext is output. The 3-bit input selects one of the eight lines exiting from the first stage and sets it to 1; all the other lines are 0. The second stage is a P-box. The third stage encodes the selected input line in binary again. With the wiring shown, if the eight octal numbers 01234567 were input one after another, the output sequence would be 24506713. In other words, 0 has been replaced by 2, 1 has been replaced by 4, etc. Again, by appropriate wiring of the P-box inside the S-box, any substitution can be accomplished. Furthermore, such a device can be built in hardware and can achieve great speed since encoders and decoders have only one or two (subnanosecond) gate delays and thepropagation time across the P-box may well be less than 1 picosecond. 

The real power of these basic elements only becomes apparent when we cascade a whole series of boxes to form a 

product cipher

, as shown in 

Fig. 8-6(c)

. In this example, 12 input lines are transposed (i.e., permuted) by the first stage (

P

1

). Theoretically, it would be possible to have the second stage be an S-box that mapped a 12-bit number onto another 12-bit number. However, such a device would need 2

12

 = 4096 crossed wires in its middle stage. Instead, the input is broken up into four groups of 3 bits, each of which is substituted 

569




independently of the others. Although this method is less general, it is still powerful. By inclusion of a sufficiently large number of stages in the product cipher, the output can be made

 

to be an exceedingly complicated function of the input. 

Product ciphers that operate on 

k

-bit inputs to produce 

k

-bit outputs are very common. Typically, 

k

 is 64 to 256. A hardware implementation usually has at least 18 physical stages, instead of just seven as in 

Fig. 8-6(c)

. A software implementation is programmed as a loop with at least 8 iterations, each one performing S-box-type substitutions on subblocks of the 64- to 256-bit data block, followed by a permutation that mixes the outputs of the S-boxes. Often there is a special initial permutation and one at the end as well. In the literature, the iterations are called 

rounds

. 

8.2.1 DES—The Data Encryption Standard 

In January 1977, the U.S. Government adopted a product cipher developed by IBM as its official standard for unclassified information. This cipher, 

DES

 (

Data Encryption Standard

), was widely adopted by the industry for use in security products. It is no longer secure in its original form, but in a modified form it is still useful. We will now explain how DES works. 

An outline of DES is shown in 

Fig. 8-7(a)

. Plaintext is encrypted in blocks of 64 bits, yielding 64 bits of ciphertext. The algorithm, which is parameterized by a 56-bit key, has 19 distinct stages. The first stage is a key-independent transposition on the 64-bit plaintext. The last stage is the exact inverse of this transposition. The stage prior to the last one exchanges the leftmost 32 bits with the rightmost 32 bits. The remaining 16 stages are functionally identical but are parameterized by different functions of the key. The algorithm has been designed to allow decryption to be done with the same key as encryption, a property needed in any symmetric-key algorithm. The steps are just run in the reverse order. 

Figure 8-7. The data encryption standard. (a) General outline. (b) Detail of one iteration. The circled + means exclusive OR. 

 

The operation of one of these intermediate stages is illustrated in 

Fig. 8-7(b)

. Each stage takes

 

two 32-bit inputs and produces two 32-bit outputs. The left output is simply a copy of the right

 

570




input. The right output is the bitwise XOR of the left input and a function of the right input and the key for this stage, 

K.

i

 All the complexity lies in this function. 

The function consists of four steps, carried out in sequence. First, a 48-bit number, 

E

, is constructed by expanding the 32-bit 

R

i

 

- 1

 according to a fixed transposition and duplication rule. Second, 

E

 and 

K

i

 are XORed together. This output is then partitioned into eight groups of 6 bits each, each of which is fed into a different S-box. Each of the 64 possible inputs to an S-box is mapped onto a 4-bit output. Finally, these 8 x 4 bits are passed through a P-box. 

In each of the 16 iterations, a different key is used. Before the algorithm starts, a 56-bit transposition is applied to the key. Just before each iteration, the key is partitioned into two 28-bit units, each of which is rotated left by a number of bits dependent on the iteration number. 

K

i

 is derived from this rotated key by applying yet another 56-bit transposition to it. A

 

different 48-bit subset of the 56 bits is extracted and permuted on each round. 

A technique that is sometimes used to make DES stronger is called 

whitening

. It consists of XORing a random 64-bit key with each plaintext block before feeding it into DES and then XORing a second 64-bit key with the resulting ciphertext before transmitting it. Whitening can easily be removed by running the reverse operations (if the receiver has the two whitening keys). Since this technique effectively adds more bits to the key length, it makes exhaustive search of the key space much more time consuming. Note that the same whitening key is used

 

for each block (i.e., there is only one whitening key). 

DES has been enveloped in controversy since the day it was launched. It was based on a cipher developed and patented by IBM, called Lucifer, except that IBM's cipher used a 128-bit key instead of a 56-bit key. When the U.S. Federal Government wanted to standardize on one cipher for unclassified use, it ''invited'' IBM to ''discuss'' the matter with NSA, the U.S. Government's code-breaking arm, which is the world's largest employer of mathematicians andcryptologists. NSA is so secret that an industry joke goes: 

 

Q1:

 

What does NSA stand for? 

A1: 

 

No Such Agency. 

 

Actually, NSA stands for National Security Agency. 

After these discussions took place, IBM reduced the key from 128 bits to 56 bits and decided to keep secret the process by which DES was designed. Many people suspected that the key length was reduced to make sure that NSA could just break DES, but no organization with a smaller budget could. The point of the secret design was supposedly to hide a back door that could make it even easier for NSA to break DES. When an NSA employee discreetly told IEEE to cancel a planned conference on cryptography, that did not make people any more comfortable. NSA denied everything. 

In 1977, two Stanford cryptography researchers, Diffie and Hellman (1977), designed a machine to break DES and estimated that it could be built for 20 million dollars. Given a small piece of plaintext and matched ciphertext, this machine could find the key by exhaustive search of the 2

56

-entry key space in under 1 day. Nowadays, such a machine would cost well under 1 million dollars. 

Triple DES 

571




As early as 1979, IBM realized that the DES key length was too short and devised a way to effectively increase it, using triple encryption (Tuchman, 1979). The method chosen, which has since been incorporated in International Standard 8732, is illustrated in 

Fig. 8-8

. Here two keys and three stages are used. In the first stage, the plaintext is encrypted using DES in the usual way with 

K

1

. In the second stage, DES is run in decryption mode, using 

K

2

 as the key. Finally, another DES encryption is done with 

K

1

. 

Figure 8-8. (a) Triple encryption using DES. (b) Decryption. 

 

This design immediately gives rise to two questions. First, why are only two keys used, instead of three? Second, why is 

EDE

 (

Encrypt Decrypt Encrypt

) used, instead of 

EEE

 (

Encrypt Encrypt Encrypt

)? The reason that two keys are used is that even the most paranoid cryptographers believe that 112 bits is adequate for routine commercial applications for the time being. (And among cryptographers, paranoia is considered a feature, not a bug.) Going to 168 bits would just add the unnecessary overhead of managing and transporting another key for little real gain. 

The reason for encrypting, decrypting, and then encrypting again is backward compatibility with existing single-key DES systems. Both the encryption and decryption functions are mappings between sets of 64-bit numbers. From a cryptographic point of view, the two mappings are equally strong. By using EDE, however, instead of EEE, a computer using triple encryption can speak to one using single encryption by just setting 

K

1

 = 

K

2

. This property allows triple encryption to be phased in gradually, something of no concern to academic cryptographers, but of considerable importance to IBM and its customers. 

8.2.2 AES—The Advanced Encryption Standard 

As DES began approaching the end of its useful life, even with triple DES, 

NIST

 (

National Institute of Standards and Technology

), the agency of the U.S. Dept. of Commerce charged with approving standards for the U.S. Federal Government, decided that the government needed a new cryptographic standard for unclassified use. NIST was keenly aware of all the controversy surrounding DES and well knew that if it just announced a new standard, everyone knowing anything about cryptography would automatically assume that NSA had built a back door into it so NSA could read everything encrypted with it. Under these conditions, probably no one would use the standard and it would most likely die a quiet death. 

So NIST took a surprisingly different approach for a government bureaucracy: it sponsored a cryptographic bake-off (contest). In January 1997, researchers from all over the world were invited to submit proposals for a new standard, to be called 

AES

 (

Advanced Encryption Standard

). The bake-off rules were: 

1. The algorithm must be a symmetric block cipher. 

2. The full design must be public. 

3. Key lengths of 128, 192, and 256 bits must be supported. 

4. Both software and hardware implementations must be possible. 

5. The algorithm must be public or licensed on nondiscriminatory terms. 

Fifteen serious proposals were made, and public conferences were organized in which they were presented and attendees were actively encouraged to find flaws in all of them. In August 1998, NIST selected five finalists primarily on the basis of their security, efficiency, simplicity, flexibility, and memory requirements (important for embedded systems). More conferences 

572




were held and more pot-shots taken. A nonbinding vote was taken at the last conference. The finalists and their scores were as follows: 

1. Rijndael (from Joan Daemen and Vincent Rijmen, 86 votes). 

2. Serpent (from Ross Anderson, Eli Biham, and Lars Knudsen, 59 votes). 

3. Twofish (from a team headed by Bruce Schneier, 31 votes). 

4. RC6 (from RSA Laboratories, 23 votes). 

5. MARS (from IBM, 13 votes). 

In October 2000, NIST announced that it, too, voted for Rijndael, and in November 2001 Rijndael became a U.S. Government standard published as Federal Information Processing Standard FIPS 197. Due to the extraordinary openness of the competition, the technical properties of Rijndael, and the fact that the winning team consisted of two young Belgian cryptographers (who are unlikely to have built in a back door just to please NSA), it is expected that Rijndael will become the world's dominant cryptographic standard for at least a decade. The name Rijndael, pronounced Rhine-doll (more or less), is derived from the last names of the authors: Rijmen + Daemen. 

Rijndael supports key lengths and block sizes from 128 bits to 256 bits in steps of 32 bits. The key length and block length may be chosen independently. However, AES specifies that the block size must be 128 bits and the key length must be 128, 192, or 256 bits. It is doubtful that anyone will ever use 192-bit keys, so de facto, AES has two variants: a 128-bit block with 128-bit key and a 128-bit block with a 256-bit key. 

In our treatment of the algorithm below, we will examine only the 128/128 case because this is likely to become the commercial norm. A 128-bit key gives a key space of 2

128

 

3 x 103

8

 keys. Even if NSA manages to build a machine with 1 billion parallel processors, each being able to evaluate one key per picosecond, it would take such a machine about 10

10

 years to search the key space. By then the sun will have burned out, so the folks then present will have to read the results by candlelight. 

Rijndael 

From a mathematical perspective, Rijndael is based on Galois field theory, which gives it some provable security properties. However, it can also be viewed as C code, without getting into the mathematics. 

Like DES, Rijndael uses substitution and permutations, and it also uses multiple rounds. The number of rounds depends on the key size and block size, being 10 for 128-bit keys with 128-bit blocks and moving up to 14 for the largest key or the largest block. However, unlike DES, all operations involve entire bytes, to allow for efficient implementations in both hardware and software. An outline of the code is given in 

Fig. 8-9

. 

Figure 8-9. An outline of Rijndael. 

573




 

The function 

rijndael

 has three parameters. They are: 

plaintext

, an array of 16 bytes containing the input data, 

ciphertext

, an array of 16 bytes where the enciphered output will be returned, and 

key

, the 16-byte key. During the calculation, the current state of the data is maintained in a byte array, 

state

, whose size is 

NROWS

 x 

NCOLS

. For 128-bit blocks, this array is 4 x 4 bytes. With 16 bytes, the full 128-bit data block can be stored. 

The 

state

 array is initialized to the plaintext and modified by every step in the computation. In some steps, byte-for-byte substitution is performed. In others, the bytes are permuted within the array. Other transformations are also used. At the end, the contents of the 

state

 are returned as the ciphertext. 

The code starts out by expanding the key into 11 arrays of the same size as the state. They are stored in 

rk

, which is an array of structs, each containing a state array. One of these will be used at the start of the calculation and the other 10 will be used during the 10 rounds, one per round. The calculation of the round keys from the encryption key is too complicated for us to get into here. Suffice it to say that the round keys are produced by repeated rotation and XORing of various groups of key bits. For all the details, see (Daemen and Rijmen, 2002). 

The next step is to copy the plaintext into the 

state

 array so it can be processed during the rounds. It is copied in column order, with the first four bytes going into column 0, the next four bytes going into column 1, and so on. Both the columns and the rows are numbered starting at 0, although the rounds are numbered starting at 1. This initial setup of the 12 byte arrays of size 4 x 4 is illustrated in 

Fig. 8-10

. 

Figure 8-10. Creating of the 

state

 and 

rk

 arrays. 

574




 

There is one more step before the main computation begins: 

rk

[0] is XORed into 

state

 byte for byte. In other words each of the 16 bytes in 

state

 is replaced by the XOR of itself and the corresponding byte in 

rk

[0]. 

Now it is time for the main attraction. The loop executes 10 iterations, one per round, transforming 

state

 on each iteration. The contents of each round consist of four steps. Step 1 does a byte-for-byte substitution on 

state

. Each byte in turn is used as an index into an S-box to replace its value by the contents of that S-box entry. This step is a straight monoalphabetic substitution cipher. Unlike DES, which has multiple S-boxes, Rijndael has only one S-box. 

Step 2 rotates each of the four rows to the left. Row 0 is rotated 0 bytes (i.e., not changed), row 1 is rotated 1 byte, row 2 is rotated 2 bytes, and row 3 is rotated 3 bytes. This step diffuses the contents of the current data around the block, analogous to the permutations of 

Fig. 8-6

. 

Step 3 mixes up each column independently of the other ones. The mixing is done using matrix multiplication in which the new column is the product of the old column and a constant matrix, with the multiplication done using the finite Galois field, 

GF

(2

8

). Although this may sound complicated, an algorithm exists that allows each element of the new column to be computed using two table lookups and three XORs (Daemen and Rijmen, 2002, Appendix E). 

Finally, step 4 XORs the key for this round into the 

state

 array. 

Since every step is reversible, decryption can be done just by running the algorithm backward. However, there is also a trick available in which decryption can be done by running the encryption algorithm, using different tables. 

The algorithm has been designed not only for great security, but also for great speed. A good software implementation on a 2-GHz machine should be able to achieve an encryption rate of 700 Mbps, which is fast enough to encrypt over 100 MPEG-2 videos in real time. Hardware implementations are faster still. 

8.2.3 Cipher Modes 

Despite all this complexity, AES (or DES or any block cipher for that matter) is basically a monoalphabetic substitution cipher using big characters (128-bit characters for AES and 64-bit characters for DES). Whenever the same plaintext block goes in the front end, the same ciphertext block comes out the back end. If you encrypt the plaintext 

abcdefgh

 100 times with the same DES key, you get the same ciphertext 100 times. An intruder can exploit this property to help subvert the cipher. 

Electronic Code Book Mode 

To see how this monoalphabetic substitution cipher property can be used to partially defeat the cipher, we will use (triple) DES because it is easier to depict 64-bit blocks than 128-bit blocks, 

575




but AES has exactly the same problem. The straightforward way to use DES to encrypt a long piece of plaintext is to break it up into consecutive 8-byte (64-bit) blocks and encrypt them one after another with the same key. The last piece of plaintext is padded out to 64 bits, if need be. This technique is known as 

ECB mode

 (

Electronic Code Book mode

) in analogy with old-fashioned code books where each plaintext word was listed, followed by its ciphertext (usually a five-digit decimal number). 

In 

Fig. 8-11

 we have the start of a computer file listing the annual bonuses a company has decided to award to its employees. This file consists of consecutive 32-byte records, one per employee, in the format shown: 16 bytes for the name, 8 bytes for the position, and 8 bytes for the bonus. Each of the sixteen 8-byte blocks (numbered from 0 to 15) is encrypted by (triple) DES. 

Figure 8-11. The plaintext of a file encrypted as 16 DES blocks. 

 

Leslie just had a fight with the boss and is not expecting much of a bonus. Kim, in contrast, is the boss' favorite, and everyone knows this. Leslie can get access to the file after it is encrypted but before it is sent to the bank. Can Leslie rectify this unfair situation, given only the encrypted file? 

No problem at all. All Leslie has to do is make a copy of the 12th ciphertext block (which contains Kim's bonus) and use it to replace the 4th ciphertext block (which contains Leslie's bonus). Even without knowing what the 12th block says, Leslie can expect to have a much merrier Christmas this year. (Copying the 8th ciphertext block is also a possibility, but is more likely to be detected; besides, Leslie is not a greedy person.) 

Cipher Block Chaining Mode 

To thwart this type of attack, all block ciphers can be chained in various ways so that replacing a block the way Leslie did will cause the plaintext decrypted starting at the replaced block to be garbage. One way of chaining is 

cipher block chaining

. In this method, shown in 

Fig. 8-

12

, each plaintext block is XORed with the previous ciphertext block before being encrypted. Consequently, the same plaintext block no longer maps onto the same ciphertext block, and the encryption is no longer a big monoalphabetic substitution cipher. The first block is XORed with a randomly chosen 

IV

 (

Initialization Vector

), which is transmitted (in plaintext) along with the ciphertext. 

Figure 8-12. Cipher block chaining. (a) Encryption. (b) Decryption. 

576




 

We can see how cipher block chaining mode works by examining the example of 

Fig. 8-12

. We start out by computing 

C

0

 = 

E

(

P

0

 XOR 

IV

). Then we compute 

C

1

 = 

E

(

P

1

 XOR 

C

0

), and so on. Decryption also uses XOR to reverse the process, with 

P

0

 = 

IV

 XOR 

D

(

C

0

), and so on. Note that the encryption of block 

i

 is a function of all the plaintext in blocks 0 through 

i

 - 1, so the same plaintext generates different ciphertext depending on where it occurs. A transformation of the type Leslie made will result in nonsense for two blocks starting at Leslie's bonus field. To an astute security officer, this peculiarity might suggest where to start the ensuing investigation. 

Cipher block chaining also has the advantage that the same plaintext block will not result in the same ciphertext block, making cryptanalysis more difficult. In fact, this is the main reason it is used. 

Cipher Feedback Mode 

However, cipher block chaining has the disadvantage of requiring an entire 64-bit block to arrive before decryption can begin. For use with interactive terminals, where people can type lines shorter than eight characters and then stop, waiting for a response, this mode is unsuitable. For byte-by-byte encryption, 

cipher feedback mode

, using (triple) DES is used, as shown in 

Fig. 8-13

. For AES the idea is exactly the same, only a 128-bit shift register is used. In this figure, the state of the encryption machine is shown after bytes 0 through 9 have been encrypted and sent. When plaintext byte 10 arrives, as illustrated in 

Fig. 8-13(a)

, the DES algorithm operates on the 64-bit shift register to generate a 64-bit ciphertext. The leftmost byte of that ciphertext is extracted and XORed with 

P

10

. That byte is transmitted on the transmission line. In addition, the shift register is shifted left 8 bits, causing 

C

2

 to fall off the left end, and 

C

10

 is inserted in the position just vacated at the right end by 

C

9

. Note that the contents of the shift register depend on the entire previous history of the plaintext, so a pattern that repeats multiple times in the plaintext will be encrypted differently each time in the ciphertext. As with cipher block chaining, an initialization vector is needed to start the ball rolling. 

Figure 8-13. Cipher feedback mode. (a) Encryption. (b) Decryption. 

577




 

Decryption with cipher feedback mode just does the same thing as encryption. In particular, the content of the shift register is 

encrypted

, not 

decrypted

, so the selected byte that is XORed with 

C

10

 to get 

P

10

 is the same one that was XORed with 

P

10

 to generate 

C

10

 in the first place. As long as the two shift registers remain identical, decryption works correctly. It is illustrated in 

Fig. 8-13(b)

. 

A problem with cipher feedback mode is that if one bit of the ciphertext is accidentally inverted during transmission, the 8 bytes that are decrypted while the bad byte is in the shift register will be corrupted. Once the bad byte is pushed out of the shift register, correct plaintext will once again be generated. Thus, the effects of a single inverted bit are relatively localized and do not ruin the rest of the message, but they do ruin as many bits as the shift register is wide. 

Stream Cipher Mode 

Nevertheless, applications exist in which having a 1-bit transmission error mess up 64 bits of plaintext is too large an effect. For these applications, a fourth option, 

stream cipher mode

, exists. It works by encrypting an initialization vector, using a key to get an output block. The output block is then encrypted, using the key to get a second output block. This block is then encrypted to get a third block, and so on. The (arbitrarily large) sequence of output blocks, called the 

keystream

, is treated like a one-time pad and XORed with the plaintext to get the ciphertext, as shown in 

Fig. 8-14(a)

. Note that the IV is used only on the first step. After that, the output is encrypted. Also note that the keystream is independent of the data, so it can be computed in advance, if need be, and is completely insensitive to transmission errors. Decryption is shown in 

Fig. 8-14(b)

. 

Figure 8-14. A stream cipher. (a) Encryption. (b) Decryption. 

 

Decryption occurs by generating the same keystream at the receiving side. Since the keystream depends only on the IV and the key, it is not affected by transmission errors in the ciphertext. Thus, a 1-bit error in the transmitted ciphertext generates only a 1-bit error in the decrypted plaintext. 

578




It is essential never to use the same (key, IV) pair twice with a stream cipher because doing so will generate the same keystream each time. Using the same keystream twice exposes the ciphertext to a 

keystream reuse attack

. Imagine that the plaintext block, 

P

0

, is encrypted with the keystream to get 

P

0

 XOR 

K

0

. Later, a second plaintext block, 

Q

0

, is encrypted with the same keystream to get 

Q

0

 XOR 

K

0

. An intruder who captures both of these ciphertext blocks can simply XOR them together to get 

P

0

 XOR 

Q

0

, which eliminates the key. The intruder now has the XOR of the two plaintext blocks. If one of them is known or can be guessed, the other can also be found. In any event, the XOR of two plaintext streams can be attacked by using statistical properties of the message. For example, for English text, the most common character in the stream will probably be the XOR of two spaces, followed by the XOR of space and the letter ''e'', etc. In short, equipped with the XOR of two plaintexts, the cryptanalyst has an excellent chance of deducing both of them. 

Counter Mode 

One problem that all the modes except electronic code book mode have is that random access to encrypted data is impossible. For example, suppose a file is transmitted over a network and then stored on disk in encrypted form. This might be a reasonable way to operate if the receiving computer is a notebook computer that might be stolen. Storing all critical files in encrypted form greatly reduces the damage due to secret information leaking out in the event that the computer falls into the wrong hands. 

However, disk files are often accessed in nonsequential order, especially files in databases. With a file encrypted using cipher block chaining, accessing a random block requires first decrypting all the blocks ahead of it, an expensive proposition. For this reason, yet another mode has been invented, 

counter mode

,as illustrated in 

Fig. 8-15

. Here the plaintext is not encrypted directly. Instead, the initialization vector plus a constant is encrypted, and the resulting ciphertext XORed with the plaintext. By stepping the initialization vector by 1 for each new block, it is easy to decrypt a block anywhere in the file without first having to decrypt all of its predecessors. 

Figure 8-15. Encryption using counter mode. 

 

Although counter mode is useful, it has a weakness that is worth pointing out. Suppose that the same key, 

K

, is used again in the future (with a different plaintext but the same IV) and an attacker acquires all the ciphertext from both runs. The keystreams are the same in both cases, exposing the cipher to a keystream reuse attack of the same kind we saw with stream ciphers. All the cryptanalyst has to do is to XOR the two ciphertexts together to eliminate all the cryptographic protection and just get the XOR of the plaintexts. This weakness does not mean counter mode is a bad idea. It just means that both keys and initialization vectors should be chosen independently and at random. Even if the same key is accidentally used twice, if the IV is different each time, the plaintext is safe. 

579




8.2.4 Other Ciphers 

DES and Rijndael are the best-known symmetric-key, cryptographic algorithms. However, it is worth mentioning that numerous other symmetric-key ciphers have been devised. Some of these are embedded inside various products. A few of the more common ones are listed in 

Fig. 

8-16

. 

Figure 8-16. Some common symmetric-key cryptographic algorithms. 

 

8.2.5 Cryptanalysis 

Before leaving the subject of symmetric-key cryptography, it is worth at least mentioning four developments in cryptanalysis. The first development is 

differential cryptanalysis

 (Biham and Shamir, 1993). This technique can be used to attack any block cipher. It works by beginning with a pair of plaintext blocks that differ in only a small number of bits and watching carefully what happens on each internal iteration as the encryption proceeds. In many cases, some bit patterns are much more common than other patterns, and this observation leads to a probabilistic attack. 

The second development worth noting is 

linear cryptanalysis

 (Matsui, 1994). It can break DES with only 2

43

 known plaintexts. It works by XORing certain bits in the plaintext and ciphertext together and examining the result for patterns. When this is done repeatedly, half the bits should be 0s and half should be 1s. Often, however, ciphers introduce a bias in one direction or the other, and this bias, however small, can be exploited to reduce the work factor. For the details, see Matsui's paper. 

The third development is using analysis of the electrical power consumption to find secret keys. Computers typically use 3 volts to represent a 1 bit and 0 volts to represent a 0 bit. Thus, processing a 1 takes more electrical energy than processing a 0. If a cryptographic algorithm consists of a loop in which the key bits are processed in order, an attacker who replaces the main 

n

-GHz clock with a slow (e.g., 100-Hz) clock and puts alligator clips on the CPU's power and ground pins, can precisely monitor the power consumed by each machine instruction. From this data, deducing the key is surprisingly easy. This kind of cryptanalysis can be defeated only by carefully coding the algorithm in assembly language to make sure power consumption is independent of the key and also independent of all the individual round keys. 

The fourth development is timing analysis. Cryptographic algorithms are full of 

if

 statements that test bits in the round keys. If the 

then

 and 

else

 parts take different amounts of time, by slowing down the clock and seeing how long various steps take, it may also be possible to deduce the round keys. Once all the round keys are known, the original key can usually be computed. Power and timing analysis can also be employed simultaneously to make the job 

580




easier. While power and timing analysis may seem exotic, in reality they are powerful techniques that can break any cipher not specifically designed to resist them. 

8.3 Public-Key Algorithms 

Historically, distributing the keys has always been the weakest link in most cryptosystems. No matter how strong a cryptosystem was, if an intruder could steal the key, the system was worthless. Cryptologists always took for granted that the encryption key and decryption key were the same (or easily derived from one another). But the key had to be distributed to all users of the system. Thus, it seemed as if there was an inherent built-in problem. Keys had to be protected from theft, but they also had to be distributed, so they could not just be locked up in a bank vault. 

In 1976, two researchers at Stanford University, Diffie and Hellman (1976), proposed a radically new kind of cryptosystem, one in which the encryption and decryption keys were different, and the decryption key could not feasibly be derived from the encryption key. In their proposal, the (keyed) encryption algorithm, 

E

, and the (keyed) decryption algorithm, 

D

, had to meet three requirements. These requirements can be stated simply as follows: 

1. 

D

(

E

(

P

)) = 

P.

 

2. It is exceedingly difficult to deduce 

D

 from 

E

. 

3. 

E

 cannot be broken by a chosen plaintext attack. 

The first requirement says that if we apply 

D

 to an encrypted message, 

E

(

P

), we get the original plaintext message, 

P

, back. Without this property, the legitimate receiver could not decrypt the ciphertext. The second requirement speaks for itself. The third requirement is needed because, as we shall see in a moment, intruders may experiment with the algorithm to their hearts' content. Under these conditions, there is no reason that the encryption key cannot be made public. 

The method works like this. A person, say, Alice, wanting to receive secret messages, first devises two algorithms meeting the above requirements. The encryption algorithm and Alice's key are then made public, hence the name 

public-key cryptography

. Alice might put her public key on her home page on the Web, for example. We will use the notation 

E

A

 to mean the encryption algorithm parameterized by Alice's public key. Similarly, the (secret) decryption algorithm parameterized by Alice's private key is 

D

A

. Bob does the same thing, publicizing 

E

B

 but keeping 

D

B

 secret. 

Now let us see if we can solve the problem of establishing a secure channel between Alice and Bob, who have never had any previous contact. Both Alice's encryption key, 

E

A

, and Bob's encryption key, 

E

B

, are assumed to be in publicly readable files. Now Alice takes her first message, 

P

, computes 

E

B

(

P

), and sends it to Bob. Bob then decrypts it by applying his secret key 

D

B

 [i.e., he computes 

D

B

(

E

B

(

P

)) = 

P

]. No one else can read the encrypted message, 

E

B

(

P

), because the encryption system is assumed strong and because it is too difficult to derive 

D

B

 from the publicly known 

E.

B

 To send a reply, 

R

, Bob transmits 

E

A

(

R

). Alice and Bob can now communicate securely. 

A note on terminology is perhaps useful here. Public-key cryptography requires each user to have two keys: a public key, used by the entire world for encrypting messages to be sent to that user, and a private key, which the user needs for decrypting messages. We will consistently refer to these keys as the 

public

 and 

private

 keys, respectively, and distinguish them from the 

secret

 keys used for conventional symmetric-key cryptography. 

581




8.3.1 RSA 

The only catch is that we need to find algorithms that indeed satisfy all three requirements. Due to the potential advantages of public-key cryptography, many researchers are hard at work, and some algorithms have already been published. One good method was discovered by a group at M.I.T. (Rivest et al., 1978). It is known by the initials of the three discoverers (Rivest, Shamir, Adleman): 

RSA

. It has survived all attempts to break it for more than a quarter of a century and is considered very strong. Much practical security is based on it. Its major disadvantage is that it requires keys of at least 1024 bits for good security (versus 128 bits for symmetric-key algorithms), which makes it quite slow. 

The RSA method is based on some principles from number theory. We will now summarize how to use the method; for details, consult the paper. 

1. Choose two large primes, 

p

 and 

q

 (typically 1024 bits). 

2. Compute 

n

 = 

p

 x 

q

 and 

z

 = (

p

 - 1) x (

q

 - 1)

.

 

3. Choose a number relatively prime to 

z

 and call it 

d

. 

4. Find 

e

 such that 

e

 x 

d

 = 1 

mod z.

 

With these parameters computed in advance, we are ready to begin encryption. Divide the plaintext (regarded as a bit string) into blocks, so that each plaintext message, 

P

, falls in the interval 0 

P < n.

 Do that by grouping the plaintext into blocks of 

k

 bits, where 

k

 is the largest integer for which 2

k

 

< n

is true. 

To encrypt a message, 

P

, compute 

C

 = 

P

e

 (mod 

n

). To decrypt 

C

, compute 

P

 = 

C

d

 (mod 

n

). It can be proven that for all 

P

 in the specified range, the encryption and decryption functions are inverses. To perform the encryption, you need 

e

 and 

n

. To perform the decryption, you need 

d

 and 

n

. Therefore, the public key consists of the pair (

e

, 

n

), and the private key consists of (

d

, 

n

)

.

The security of the method is based on the difficulty of factoring large numbers. If the cryptanalyst could factor the (publicly known) 

n

, he could then find 

p

 and 

q

, and from these 

z

. Equipped with knowledge of 

z

 and 

e

, 

d

 can be found using Euclid's algorithm. Fortunately, mathematicians have been trying to factor large numbers for at least 300 years, and the accumulated evidence suggests that it is an exceedingly difficult problem. 

According to Rivest and colleagues, factoring a 500-digit number requires 10

25

 years using brute force. In both cases, they assume the best known algorithm and a computer with a 1-µsec instruction time. Even if computers continue to get faster by an order of magnitude per decade, it will be centuries before factoring a 500-digit number becomes feasible, at which time our descendants can simply choose 

p

 and 

q

 still larger. 

A trivial pedagogical example of how the RSA algorithm works is given in 

Fig. 8-17

. For this example we have chosen 

p

 = 3 and 

q

 = 11, giving 

n

 = 33 and 

z

 = 20

.

 A suitable value for 

d

 is 

d

 = 7, since 7 and 20 have no common factors. With these choices, 

e

 can be found by solving the equation 7

e

 = 1 (mod 20), which yields 

e

 = 3

.

 The ciphertext, 

C

, for a plaintext message, 

P

, is given by 

C

 = 

P

3

 (mod 33)

.

 The ciphertext is decrypted by the receiver by making use of the rule 

P

 = 

C

7

 (mod 33)

.

 The figure shows the encryption of the plaintext ''SUZANNE'' as an example. 

Figure 8-17. An example of the RSA algorithm. 

582




 

Because the primes chosen for this example are so small, 

P

 must be less than 33, so each plaintext block can contain only a single character. The result is a monoalphabetic substitution cipher, not very impressive. If instead we had chosen 

p

 and 

q

 

2

512

, we would have 

n

 

2

1024

, so each block could be up to 1024 bits or 128 eight-bit characters, versus 8 characters for DES and 16 characters for AES. 

It should be pointed out that using RSA as we have described is similar to using a symmetric algorithm in ECB mode—the same input block gives the same output block. Therefore, some form of chaining is needed for data encryption. However, in practice, most RSA-based systems use public-key cryptography primarily for distributing one-time session keys for use with some symmetric-key algorithm such as AES or triple DES. RSA is too slow for actually encrypting large volumes of data but is widely used for key distribution. 

8.3.2 Other Public-Key Algorithms 

Although RSA is widely used, it is by no means the only public-key algorithm known. The first public-key algorithm was the knapsack algorithm (Merkle and Hellman, 1978). The idea here is that someone owns a large number of objects, each with a different weight. The owner encodes the message by secretly selecting a subset of the objects and placing them in the knapsack. The total weight of the objects in the knapsack is made public, as is the list of all possible objects. The list of objects in the knapsack is kept secret. With certain additional restrictions, the problem of figuring out a possible list of objects with the given weight was thought to be computationally infeasible and formed the basis of the public-key algorithm. 

The algorithm's inventor, Ralph Merkle, was quite sure that this algorithm could not be broken, so he offered a $100 reward to anyone who could break it. Adi Shamir (the ''S'' in RSA) promptly broke it and collected the reward. Undeterred, Merkle strengthened the algorithm and offered a $1000 reward to anyone who could break the new one. Ronald Rivest (the ''R'' in RSA) promptly broke the new one and collected the reward. Merkle did not dare offer $10,000 for the next version, so ''A'' (Leonard Adleman) was out of luck. Nevertheless, the knapsack algorithm is not considered secure and is not used in practice any more. 

Other public-key schemes are based on the difficulty of computing discrete logarithms. Algorithms that use this principle have been invented by El Gamal (1985) and Schnorr (1991). 

A few other schemes exist, such as those based on elliptic curves (Menezes and Vanstone, 1993), but the two major categories are those based on the difficulty of factoring large numbers and computing discrete logarithms modulo a large prime. These problems are thought to be genuinely difficult to solve—mathematicians have been working on them for many years without any great break-throughs. 

583




8.4 Digital Signatures 

The authenticity of many legal, financial, and other documents is determined by the presence or absence of an authorized handwritten signature. And photocopies do not count. For computerized message systems to replace the physical transport of paper and ink documents, a method must be found to allow documents to be signed in an unforgeable way. 

The problem of devising a replacement for handwritten signatures is a difficult one. Basically, what is needed is a system by which one party can send a signed message to another party in such a way that the following conditions hold: 

1. The receiver can verify the claimed identity of the sender. 

2. The sender cannot later repudiate the contents of the message. 

3. The receiver cannot possibly have concocted the message himself. 

The first requirement is needed, for example, in financial systems. When a customer's computer orders a bank's computer to buy a ton of gold, the bank's computer needs to be able to make sure that the computer giving the order really belongs to the company whose account is to be debited. In other words, the bank has to authenticate the customer (and the customer has to authenticate the bank). 

The second requirement is needed to protect the bank against fraud. Suppose that the bank buys the ton of gold, and immediately thereafter the price of gold drops sharply. A dishonest customer might sue the bank, claiming that he never issued any order to buy gold. When the bank produces the message in court, the customer denies having sent it. The property that no party to a contract can later deny having signed it is called 

nonrepudiation

. The digital signature schemes that we will now study help provide it. 

The third requirement is needed to protect the customer in the event that the price of gold shoots up and the bank tries to construct a signed message in which the customer asked for one bar of gold instead of one ton. In this fraud scenario, the bank just keeps the rest of the gold for itself. 

8.4.1 Symmetric-Key Signatures 

One approach to digital signatures is to have a central authority that knows everything and whom everyone trusts, say Big Brother (

BB

)

.

 Each user then chooses a secret key and carries it by hand to 

BB

's office. Thus, only Alice and 

BB

 know Alice's secret key, 

K

A

, and so on. 

When Alice wants to send a signed plaintext message, 

P

, to her banker, Bob, she generates 

K

A

(

B

, 

R

A

, 

t

, 

P

), where 

B

 is Bob's identity, 

R

A

 is a random number chosen by Alice, 

t

 is a timestamp to ensure freshness, and 

K

A

(

B

, 

R

A

, 

t

, 

P

) is the message encrypted with her key, 

K

A

. Then she sends it as depicted in 

Fig. 8-18

. 

BB

 sees that the message is from Alice, decrypts it, and sends a message to Bob as shown. The message to Bob contains the plaintext of Alice's message and also the signed message 

K

BB

 (

A

, 

t

, 

P

). Bob now carries out Alice's request. 

Figure 8-18. Digital signatures with Big Brother. 

 

584




What happens if Alice later denies sending the message? Step 1 is that everyone sues everyone (at least, in the United States). Finally, when the case comes to court and Alice vigorously denies sending Bob the disputed message, the judge will ask Bob how he can be sure that the disputed message came from Alice and not from Trudy. Bob first points out that 

BB

 will not accept a message from Alice unless it is encrypted with 

K

A

, so there is no possibility of Trudy sending 

BB

 a false message from Alice without BB detecting it immediately. 

Bob then dramatically produces Exhibit A: 

K

BB

 (

A

, 

t

, 

P

). Bob says that this is a message signed by 

BB

 which proves Alice sent 

P

 to Bob. The judge then asks 

BB

 (whom everyone trusts) to decrypt Exhibit A. When 

BB

 testifies that Bob is telling the truth, the judge decides in favor of Bob. Case dismissed. 

One potential problem with the signature protocol of 

Fig. 8-18

 is Trudy replaying either message. To minimize this problem, timestamps are used throughout. Furthermore, Bob can check all recent messages to see if 

R

A

 was used in any of them. If so, the message is discarded as a replay. Note that based on the timestamp, Bob will reject very old messages. To guard against instant replay attacks, Bob just checks the 

R

A

 of every incoming message to see if such a message has been received from Alice in the past hour. If not, Bob can safely assume this is a new request. 

8.4.2 Public-Key Signatures 

A structural problem with using symmetric-key cryptography for digital signatures is that everyone has to agree to trust Big Brother. Furthermore, Big Brother gets to read all signed messages. The most logical candidates for running the Big Brother server are the government, the banks, the accountants, and the lawyers. Unfortunately, none of these organizations inspire total confidence in all citizens. Hence, it would be nice if signing documents did not require a trusted authority. 

Fortunately, public-key cryptography can make an important contribution in this area. Let us assume that the public-key encryption and decryption algorithms have the property that 

E

(

D

(

P

)) = 

P

 in addition, of course, to the usual property that 

D

(

E

(

P

)) = 

P.

 (RSA has this property, so the assumption is not unreasonable.) Assuming that this is the case, Alice can send a signed plaintext message, 

P

, to Bob by transmitting 

E

B

(

D

A

(

P

))

.

 Note carefully that Alice knows her own (private) key, 

D

A

, as well as Bob's public key, 

E

B

, so constructing this message is something Alice can do. 

When Bob receives the message, he transforms it using his private key, as usual, yielding 

D

A

(

P

), as shown in 

Fig. 8-19

. He stores this text in a safe place and then applies 

E

A

 to get the original plaintext. 

Figure 8-19. Digital signatures using public-key cryptography. 

 

To see how the signature property works, suppose that Alice subsequently denies having sent the message 

P

 to Bob. When the case comes up in court, Bob can produce both 

P

 and 

D

A

(

P

)

.

 The judge can easily verify that Bob indeed has a valid message encrypted by 

D

A

 by simply applying 

E

A

 to it. Since Bob does not know what Alice's private key is, the only way Bob could 

585




have acquired a message encrypted by it is if Alice did indeed send it. While in jail for perjury and fraud, Alice will have plenty of time to devise interesting new public-key algorithms. 

Although using public-key cryptography for digital signatures is an elegant scheme, there are problems that are related to the environment in which they operate rather than with the basic algorithm. For one thing, Bob can prove that a message was sent by Alice only as long as 

D

A

 remains secret. If Alice discloses her secret key, the argument no longer holds, because anyone could have sent the message, including Bob himself. 

The problem might arise, for example, if Bob is Alice's stockbroker. Alice tells Bob to buy a certain stock or bond. Immediately thereafter, the price drops sharply. To repudiate her message to Bob, Alice runs to the police claiming that her home was burglarized and the PC holding her key was stolen. Depending on the laws in her state or country, she may or may not be legally liable, especially if she claims not to have discovered the break-in until getting home from work, several hours later. 

Another problem with the signature scheme is what happens if Alice decides to change her key. Doing so is clearly legal, and it is probably a good idea to do so periodically. If a court case later arises, as described above, the judge will apply the 

current E

A

 to 

D

A

(

P

) and discover that it does not produce 

P

. Bob will look pretty stupid at this point. 

In principle, any public-key algorithm can be used for digital signatures. The de facto industry standard is the RSA algorithm. Many security products use it. However, in 1991, NIST proposed using a variant of the El Gamal public-key algorithm for their new 

Digital Signature Standard

 (

DSS

). El Gamal gets its security from the difficulty of computing discrete logarithms, rather than from the difficulty of factoring large numbers. 

As usual when the government tries to dictate cryptographic standards, there was an uproar. DSS was criticized for being 

1. Too secret (NSA designed the protocol for using El Gamal). 

2. Too slow (10 to 40 times slower than RSA for checking signatures). 

3. Too new (El Gamal had not yet been thoroughly analyzed). 

4. Too insecure (fixed 512-bit key). 

In a subsequent revision, the fourth point was rendered moot when keys up to 1024 bits were allowed. Nevertheless, the first two points remain valid. 

8.4.3 Message Digests 

One criticism of signature methods is that they often couple two distinct functions: authentication and secrecy. Often, authentication is needed but secrecy is not. Also, getting an export license is often easier if the system in question provides only authentication but not secrecy. Below we will describe an authentication scheme that does not require encrypting the entire message. 

This scheme is based on the idea of a one-way hash function that takes an arbitrarily long piece of plaintext and from it computes a fixed-length bit string. This hash function, 

MD

, often called a 

message digest

, has four important properties: 

1. Given 

P

, it is easy to compute 

MD

(

P

). 

2. Given 

MD

(

P

), it is effectively impossible to find 

P

. 

3. Given 

P

 no one can find 

P

' such that 

MD

 (

P

') = 

MD

(

P

). 

4. A change to the input of even 1 bit produces a very different output. 

586




To meet criterion 3, the hash should be at least 128 bits long, preferably more. To meet criterion 4, the hash must mangle the bits very thoroughly, not unlike the symmetric-key encryption algorithms we have seen. 

Computing a message digest from a piece of plaintext is much faster than encrypting that plaintext with a public-key algorithm, so message digests can be used to speed up digital signature algorithms. To see how this works, consider the signature protocol of 

Fig. 8-18

 again. Instead of signing 

P

 with 

K

BB

 (

A

, 

t

, 

P

), 

BB

 now computes the message digest by applying 

MD

 to 

P

, yielding 

MD

(

P

). BB then encloses 

K

BB

 (

A

, 

t

, 

MD

(

P

)) as the fifth item in the list encrypted with 

K

B

 that is sent to Bob, instead of 

K

BB

 (

A

, 

t

, 

P

). 

If a dispute arises, Bob can produce both 

P

 and 

K

BB

 (

A

, 

t

, 

MD

(

P

)). After Big Brother has decrypted it for the judge, Bob has 

MD

(

P

), which is guaranteed to be genuine, and the alleged 

P

. However, since it is effectively impossible for Bob to find any other message that gives this hash, the judge will easily be convinced that Bob is telling the truth. Using message digests in this way saves both encryption time and message transport costs. 

Message digests work in public-key cryptosystems, too, as shown in 

Fig. 8-20

. Here, Alice first computes the message digest of her plaintext. She then signs the message digest and sends both the signed digest and the plaintext to Bob. If Trudy replaces 

P

 underway, Bob will see this when he computes 

MD

(

P

) himself. 

Figure 8-20. Digital signatures using message digests. 

 

MD5 

A variety of message digest functions have been proposed. The most widely used ones are MD5 (Rivest, 1992) and SHA-1 (NIST, 1993). 

MD5

 is the fifth in a series of message digests designed by Ronald Rivest. It operates by mangling bits in a sufficiently complicated way that every output bit is affected by every input bit. Very briefly, it starts out by padding the message to a length of 448 bits (modulo 512). Then the original length of the message is appended as a 64-bit integer to give a total input whose length is a multiple of 512 bits. The last precomputation step is initializing a 128-bit buffer to a fixed value. 

Now the computation starts. Each round takes a 512-bit block of input and mixes it thoroughly with the 128-bit buffer. For good measure, a table constructed from the sine function is also thrown in. The point of using a known function like the sine is not because it is more random than a random number generator, but to avoid any suspicion that the designer built in a clever back door through which only he can enter. Remember that IBM's refusal to disclose the principles behind the design of the S-boxes in DES led to a great deal of speculation about back doors. Rivest wanted to avoid this suspicion. Four rounds are performed per input block. This process continues until all the input blocks have been consumed. The contents of the 128-bit buffer form the message digest. 

MD5 has been around for over a decade now, and many people have attacked it. Some vulnerabilities have been found, but certain internal steps prevent it from being broken. However, if the remaining barriers within MD5 fall, it may eventually fail. Nevertheless, at the time of this writing, it was still standing. 

587




SHA-1 

The other major message digest function is 

SHA-1

 (

Secure Hash Algorithm 1

), developed by NSA and blessed by NIST in FIPS 180-1. Like MD5, SHA-1 processes input data in 512-bit blocks, only unlike MD5, it generates a 160-bit message digest. A typical way for Alice to send a nonsecret but signed message to Bob is illustrated in 

Fig. 8-21

. Here her plaintext message is fed into the SHA-1 algorithm to get a 160-bit SHA-1 hash. Alice then signs the hash with her RSA private key and sends both the plaintext message and the signed hash to Bob. 

Figure 8-21. Use of SHA-1 and RSA for signing nonsecret messages. 

 

After receving the message, Bob computes the SHA-1 hash himself and also applies Alice's public key to the signed hash to get the original hash, 

H

. If the two agree, the message is considered valid. Since there is no way for Trudy to modify the (plaintext) message while its is in transit and produce a new one that hashes to 

H

, Bob can easily detect any changes Trudy has made to the message. For messages whose integrity is important but whose contents are not secret, the scheme of 

Fig. 8-21

 is widely used. For a relatively small cost in computation, it guarantees that any modifications made to the plaintext message in transit can be detected with very high probability. 

Now let us briefly see how SHA-1 works. It starts out by padding the message by adding a 1 bit to the end, followed by as many 0 bits as are needed to make the length a multiple of 512 bits. Then a 64-bit number containing the message length before padding is ORed into the low-order 64 bits. In 

Fig. 8-22

, the message is shown with padding on the right because English text and figures go from left to right (i.e., the lower right is generally perceived as the end of the figure). With computers, this orientation corresponds to big-endian machines such as the SPARC, but SHA-1 always pads the end of the message, no matter which endian machine is used. 

Figure 8-22. (a) A message padded out to a multiple of 512 bits. (b) The output variables. (c) The word array. 

 

During the computation, SHA-1 maintains five 32-bit variables, 

H

0

 through 

H

4

, where the hash accumulates. These are shown in 

Fig. 8-22(b)

. They are initialized to constants specified in the standard. 

588




Each of the blocks 

M

0

 through 

M

n

 

-1

 is now processed in turn. For the current block, the 16 words are first copied into the start of an auxiliary 80-word array, 

W

, as shown in 

Fig. 8-22(c)

. Then the other 64 words in 

W

 are filled in using the formula 

 

 

where 

S

b

(

W

) represents the left circular rotation of the 32-bit word, 

W

, by 

b

 bits. Now five scratch variables, 

A

 through 

E

 are initialized from 

H

0

 through 

H

4

, respectively. 

The actual calculation can be expressed in pseudo-C as 

for (i = 0; i < 80; i++) {  

    temp = S

5

(A) + f

i

 (B, C, D) + E + W

i

 +K

i

;  

    E=D; D=C; C=S

30

(B); B = A; A = temp;  

}  

where the 

K

i

 constants are defined in the standard. The mixing functions 

f

i

 are defined as 

 

 

When all 80 iterations of the loop are completed, 

A

 through 

E

 are added to 

H

 

0

 through 

H

 

4

, respectively. 

Now that the first 512-bit block has been processed, the next one is started. The 

W

 array is reinitialized from the new block, but 

H

 is left as it was. When this block is finished, the next one is started, and so on, until all the 512-bit message blocks have been tossed into the soup. When the last block has been finished, the five 32-bit words in the 

H

 array are output as the 160-bit cryptographic hash. The complete C code for SHA-1 is given in RFC 3174. 

New versions of SHA-1 are under development for hashes of 256, 384, and 512 bits, respectively. 

8.4.4 The Birthday Attack 

In the world of crypto, nothing is ever what it seems to be. One might think that it would take on the order of 2

m

 operations to subvert an 

m

-bit message digest. In fact, 2

m/

2

 operations will often do using the 

birthday attack,

 an approach published by Yuval (1979) in his now-classic paper ''How to Swindle Rabin.'' 

The idea for this attack comes from a technique that math professors often use in their probability courses. The question is: How many students do you need in a class before the probability of having two people with the same birthday exceeds 1/2? Most students expect the answer to be way over 100. In fact, probability theory says it is just 23. Without giving a rigorous analysis, intuitively, with 23 people, we can form (23 x 22)

/

2 = 253 different pairs, each of which has a probability of 1/365 of being a hit. In this light, it is not really so surprising any more. 

589




More generally, if there is some mapping between inputs and outputs with 

n

 inputs (people, messages, etc.) and 

k

 possible outputs (birthdays, message digests, etc.), there are 

n

(

n

 - 1)

/

2 input pairs. If 

n

(

n

 - 1)

/

2 

>k

, the chance of having at least one match is pretty good. Thus, approximately, a match is likely for 

. This result means that a 64-bit message digest can probably be broken by generating about 2

32

 messages and looking for two with the same message digest. 

Let us look at a practical example. The Department of Computer Science at State University has one position for a tenured faculty member and two candidates, Tom and Dick. Tom was hired two years before Dick, so he goes up for review first. If he gets it, Dick is out of luck. Tom knows that the department chairperson, Marilyn, thinks highly of his work, so he asks her to write him a letter of recommendation to the Dean, who will decide on Tom's case. Once sent, all letters become confidential. 

Marilyn tells her secretary, Ellen, to write the Dean a letter, outlining what she wants in it. When it is ready, Marilyn will review it, compute and sign the 64-bit digest, and send it to the Dean. Ellen can send the letter later by e-mail. 

Unfortunately for Tom, Ellen is romantically involved with Dick and would like to do Tom in, so she writes the letter below with the 32 bracketed options. 

Dear Dean Smith, 

This [

letter | message

] is to give my [

honest | frank

] opinion of Prof. Tom Wilson, who is [

a candidate | up

] for tenure [

now | this year

]. I have [

known | worked with

] Prof. Wilson for [

about | almost

] six years. He is an [

outstanding | excellent

] researcher of great [

talent | ability

] known [

worldwide | internationally

] for his [

brilliant | creative

] insights into [

many | a wide variety of

] [

difficult | challenging

] problems. 

He is also a [

highly | greatly

] [

respected | admired

] [

teacher | educator

]. His students give his [

classes | courses

] [

rave | spectacular

] reviews. He is [

our | the Department's

] [

most popular | best-loved

] [

teacher | instructor

]. 

[

In addition | Additionally

] Prof. Wilson is a [

gifted | effective

] fund raiser. His [

grants | contracts

] have brought a [

large | substantial

] amount of money into [

the | our

] Department. [

This money has | These funds have

] [

enabled | permitted

] us to [

pursue | carry out

] many [

special | important

] programs, [

such as | for example

] your State 2000 program. Without these funds we would [

be unable | not be able

] to continue this program, which is so [

important | essential

] to both of us. I strongly urge you to grant him tenure. 

Unfortunately for Tom, as soon as Ellen finishes composing and typing in this letter, she also writes a second one: 

Dear Dean Smith, 

This [

letter

 | 

message

] is to give my [

honest

 | 

frank

] opinion of Prof. Tom Wilson, who is [

a candidate

 | 

up

] for tenure [

now

 | 

this year

]. I have [

known

 | 

worked with

] Tom for [

about

 | 

almost

] six years. He is a [

poor

 | 

weak

] researcher not well known in his [

field

 | 

area

]. His research [

hardly ever

 | 

rarely

] shows [

insight in

 | 

understanding of

] the [

key

 | 

major

] problems of [

the

 | 

our

] day. 

Furthermore, he is not a [

respected

 | 

admired

] [

teacher

 | 

educator

]. His students give his [

classes

 | 

courses

] [

poor

 | 

bad

 ] reviews. He is [

our

 | 

the Department's

] least popular [

teacher

 | 

instructor

], known [

mostly

 | 

primarily

] within [

the

 | 

our

] Department for his [

tendency

 | 

propensity

] to [

ridicule

 | 

embarrass

] students [

foolish

 | 

imprudent

] enough to ask questions in his classes. 

590




[

In addition

 | 

Additionally

] Tom is a [

poor

 | 

marginal

] fund raiser. His [

grants

 | 

contracts

] have brought only a [

meager

 | 

insignificant

] amount of money into [

the

 | 

our

] Department. Unless new [

money is

 | 

funds are

] quickly located, we may have to cancel some essential programs, such as your State 2000 program. Unfortunately, under these [

conditions

 | 

circumstances

] I cannot in good [

conscience

 | 

faith

] recommend him to you for [

tenure

 | 

a permanent position

]. 

Now Ellen programs her computer to compute the 2

32

 message digests of each letter overnight. Chances are, one digest of the first letter will match one digest of the second letter. If not, she can add a few more options and try again during the weekend. Suppose that she finds a match. Call the ''good'' letter 

A

 and the ''bad'' one 

B

. 

Ellen now e-mails letter 

A

 to Marilyn for her approval. Letter 

B

 she keeps completely secret, showing it to no one. Marilyn, of course, approves, computes her 64-bit message digest, signs the digest, and e-mails the signed digest off to Dean Smith. Independently, Ellen e-mails letter 

B

 to the Dean (not letter 

A

, as she is supposed to). 

After getting the letter and signed message digest, the Dean runs the message digest algorithm on letter 

B

, sees that it agrees with what Marilyn sent him, and fires Tom. The Dean does not realize that Ellen managed to generate two letters with the same message digest and sent her a different one than Marilyn saw and approved. (Optional ending: Ellen tells Dick what she did. Dick is appalled and breaks off with her. Ellen is furious and confesses to Marilyn. Marilyn calls the Dean. Tom gets tenure after all.) With MD5 the birthday attack is difficult because even at 1 billion digests per second, it would take over 500 years to compute all 2

64

 digests of two letters with 64 variants each, and even then a match is not guaranteed. Of course, with 5000 computers working in parallel, 500 years becomes 5 weeks. SHA-1 is better (because it is longer). 

8.5 Management of Public Keys 

Public-key cryptography makes it possible for people who do not share a common key to communicate securely. It also makes signing messages possible without the presence of a trusted third party. Finally, signed message digests make it possible to verify the integrity of received messages easily. 

However, there is one problem that we have glossed over a bit too quickly: if Alice and Bob do not know each other, how do they get each other's public keys to start the communication process? The obvious solution—put your public key on your Web site—does not work for the following reason. Suppose that Alice wants to look up Bob's public key on his Web site. How does she do it? She starts by typing in Bob's URL. Her browser then looks up the DNS address of Bob's home page and sends it a 

GET

 request, as shown in 

Fig. 8-23

. Unfortunately, Trudy intercepts the request and replies with a fake home page, probably a copy of Bob's home page except for the replacement of Bob's public key with Trudy's public key. When Alice now encrypts her first message with 

E

T

, Trudy decrypts it, reads it, reencrypts it with Bob's public key, and sends it to Bob, who is none the wiser that Trudy is reading his incoming messages. Worse yet, Trudy could modify the messages before reencrypting them for Bob. Clearly, some mechanism is needed to make sure that public keys can be exchanged securely. 

Figure 8-23. A way for Trudy to subvert public-key encryption. 

 

591




8.5.1 Certificates 

As a first attempt at distributing public keys securely, we could imagine a key distribution center available on-line 24 hours a day to provide public keys on demand. One of the many problems with this solution is that it is not scalable, and the key distribution center would rapidly become a bottleneck. Also, if it ever went down, Internet security would suddenly grind to a halt. 

For these reasons, people have developed a different solution, one that does not require the key distribution center to be on-line all the time. In fact, it does not have to be on-line at all. Instead, what it does is certify the public keys belonging to people, companies, and other organizations. An organization that certifies public keys is now called a 

CA

 (

Certification Authority

). 

As an example, suppose that Bob wants to allow Alice and other people to communicate with him securely. He can go to the CA with his public key along with his passport or driver's license and ask to be certified. The CA then issues a certificate similar to the one in 

Fig. 8-24

 and signs its SHA-1 hash with the CA's private key. Bob then pays the CA's fee and gets a floppy disk containing the certificate and its signed hash. 

Figure 8-24. A possible certificate and its signed hash. 

 

The fundamental job of a certificate is to bind a public key to the name of a principal (individual, company, etc.). Certificates themselves are not secret or protected. Bob might, for example, decide to put his new certificate on his Web site, with a link on the main page saying: Click here for my public-key certificate. The resulting click would return both the certificate and the signature block (the signed SHA-1 hash of the certificate). 

Now let us run through the scenario of 

Fig. 8-23

 again. When Trudy intercepts Alice's request for Bob's home page, what can she do? She can put her own certificate and signature block on the fake page, but when Alice reads the certificate she will immediately see that she is not talking to Bob because Bob's name is not in it. Trudy can modify Bob's home page on-the-fly, replacing Bob's public key with her own. However, when Alice runs the SHA-1 algorithm on the certificate, she will get a hash that does not agree with the one she gets when she applies the CA's well-known public key to the signature block. Since Trudy does not have the CA's private key, she has no way of generating a signature block that contains the hash of the modified Web page with her public key on it. In this way, Alice can be sure she has Bob's public key and not Trudy's or someone else's. And as we promised, this scheme does not require the CA to be on-line for verification, thus eliminating a potential bottleneck. 

While the standard function of a certificate is to bind a public key to a principal, a certificate can also be used to bind a public key to an 

attribute

. For example, a certificate could say: This public key belongs to someone over 18. It could be used to prove that the owner of the private key was not a minor and thus allowed to access material not suitable for children, and so on, but without disclosing the owner's identity. Typically, the person holding the certificate would send it to the Web site, principal, or process that cared about age. That site, principal, 

592




or process would then generate a random number and encrypt it with the public key in the certificate. If the owner were able to decrypt it and send it back, that would be proof that the owner indeed had the attribute stated in the certificate. Alternatively, the random number could be used to generate a session key for the ensuing conversation. 

Another example of where a certificate might contain an attribute is in an object-oriented distributed system. Each object normally has multiple methods. The owner of the object could provide each customer with a certificate giving a bit map of which methods the customer is allowed to invoke and binding the bit map to a public key using a signed certificate. Again here, if the certificate holder can prove possession of the corresponding private key, he will be allowed to perform the methods in the bit map. It has the property that the owner's identity need not be known, a property useful in situations where privacy is important. 

8.5.2 X.509 

If everybody who wanted something signed went to the CA with a different kind of certificate, managing all the different formats would soon become a problem. To solve this problem, a standard for certificates has been devised and approved by ITU. The standard is called 

X.509

 and is in widespread use on the Internet. It has gone through three versions since the initial standardization in 1988. We will discuss V3. 

X.509 has been heavily influenced by the OSI world, borrowing some of its worst features (e.g., naming and encoding). Surprisingly, IETF went along with X.509, even though in nearly every other area, from machine addresses to transport protocols to e-mail formats, IETF generally ignored OSI and tried to do it right. The IETF version of X.509 is described in RFC 3280. 

At its core, X.509 is a way to describe certificates. The primary fields in a certificate are listed in 

Fig. 8-25

. The descriptions given there should provide a general idea of what the fields do. For additional information, please consult the standard itself or RFC 2459. 

Figure 8-25. The basic fields of an X.509 certificate. 

 

For example, if Bob works in the loan department of the Money Bank, his X.500 address might be: 

/C=US/O=MoneyBank/OU=Loan/CN=Bob/  

where 

C

 is for country, 

O

 is for organization, 

OU

 is for organizational unit, and 

CN

 is for common name. CAs and other entities are named in a similar way. A substantial problem with X.500 names is that if Alice is trying to contact 

bob@moneybank.com

 and is given a certificate 

593




with an X.500 name, it may not be obvious to her that the certificate refers to the Bob she wants. Fortunately, starting with version 3, DNS names are now permitted instead of X.500 names, so this problem may eventually vanish. 

Certificates are encoded using the OSI 

ASN.1

 (

Abstract Syntax Notation 1

), which can be thought of as being like a struct in C, except with a very peculiar and verbose notation. More information about X.509 can be found in (Ford and Baum, 2000). 

8.5.3 Public Key Infrastructures 

Having a single CA to issue all the world's certificates obviously would not work. It would collapse under the load and be a central point of failure as well. A possible solution might be to have multiple CAs, all run by the same organization and all using the same private key to sign certificates. While this would solve the load and failure problems, it introduces a new problem: key leakage. If there were dozens of servers spread around the world, all holding the CA's private key, the chance of the private key being stolen or otherwise leaking out would be greatly increased. Since the compromise of this key would ruin the world's electronic security infrastructure, having a single central CA is very risky. 

In addition, which organization would operate the CA? It is hard to imagine any authority that would be accepted worldwide as legitimate and trustworthy. In some countries people would insist that it be a government, while in other countries they would insist that it not be a government. 

For these reasons, a different way for certifying public keys has evolved. It goes under the general name of 

PKI

 (

Public Key Infrastructure

). In this section we will summarize how it works in general, although there have been many proposals so the details will probably evolve in time. 

A PKI has multiple components, including users, CAs, certificates, and directories. What the PKI does is provide a way of structuring these components and define standards for the various documents and protocols. A particularly simple form of PKI is a hierarchy of CAs, as depicted in 

Fig. 8-26

. In this example we have shown three levels, but in practice there might be fewer or more. The top-level CA, the root, certifies second-level CAs, which we call 

RA

s (

Regional Authorities

) because they might cover some geographic region, such as a country or continent. This term is not standard, though; in fact, no term is really standard for the different levels of the tree. These in turn certify the real CAs, which issue the X.509 certificates to organizations and individuals. When the root authorizes a new RA, it generates an X.509 certificate stating that it has approved the RA, includes the new RA's public key in it, signs it, and hands it to the RA. Similarly, when an RA approves a new CA, it produces and signs a certificate stating its approval and containing the CA's public key. 

Figure 8-26. (a) A hierarchical PKI. (b) A chain of certificates. 

 

594




Our PKI works like this. Suppose that Alice needs Bob's public key in order to communicate with him, so she looks for and finds a certificate containing it, signed by CA 5. But Alice has never heard of CA 5. For all she knows, CA 5 might be Bob's 10-year-old daughter. She could go to CA 5 and say: Prove your legitimacy. CA 5 responds with the certificate it got from RA 2, which contains CA 5's public key. Now armed with CA 5's public key, she can verify that Bob's certificate was indeed signed by CA 5 and is thus legal. 

Unless RA 2 is Bob's 12-year-old son. So the next step is for her to ask RA 2 to prove it is legitimate. The response to her query is a certificate signed by the root and containing RA 2's public key. Now Alice is sure she has Bob's public key. 

But how does Alice find the root's public key? Magic. It is assumed that everyone knows the root's public key. For example, her browser might have been shipped with the root's public key built in. 

Bob is a friendly sort of guy and does not want to cause Alice a lot of work. He knows that she is going to have to check out CA 5 and RA 2, so to save her some trouble, he collects the two needed certificates and gives her the two certificates along with his. Now she can use her own knowledge of the root's public key to verify the top-level certificate and the public key contained therein to verify the second one. In this way, Alice does not need to contact anyone to do the verification. Because the certificates are all signed, she can easily detect any attempts to tamper with their contents. A chain of certificates going back to the root like this is sometimes called a 

chain of trust

 or a 

certification path

. The technique is widely used in practice. 

Of course, we still have the problem of who is going to run the root. The solution is not to have a single root, but to have many roots, each with its own RAs and CAs. In fact, modern browsers come preloaded with the public keys for over 100 roots, sometimes referred to as 

trust anchors

. In this way, having a single worldwide trusted authority can be avoided. 

But there is now the issue of how the browser vendor decides which purported trust anchors are reliable and which are sleazy. It all comes down to the user trusting the browser vendor to make wise choices and not simply approve all trust anchors willing to pay its inclusion fee. Most browsers allow users to inspect the root keys (usually in the form of certificates signed by the root) and delete any that seem shady. 

Directories 

Another issue for any PKI is where certificates (and their chains back to some known trust anchor) are stored. One possibility is to have each user store his or her own certificates. While doing this is safe (i.e., there is no way for users to tamper with signed certificates without detection), it is also inconvenient. One alternative that has been proposed is to use DNS as a certificate directory. Before contacting Bob, Alice probably has to look up his IP address using DNS, so why not have DNS return Bob's entire certificate chain along with his IP address? 

Some people think this is the way to go, but others would prefer dedicated directory servers whose only job is managing X.509 certificates. Such directories could provide lookup services by using properties of the X.500 names. For example, in theory such a directory service could answer a query such as: ''Give me a list of all people named Alice who work in sales departments anywhere in the U.S. or Canada.'' LDAP might be a candidate for holding such information. 

Revocation 

The real world is full of certificates, too, such as passports and drivers' licenses. Sometimes these certificates can be revoked, for example, drivers' licenses can be revoked for drunken driving and other driving offenses. The same problem occurs in the digital world: the grantor 

595




of a certificate may decide to revoke it because the person or organization holding it has abused it in some way. It can also be revoked if the subject's private key has been exposed, or worse yet, the CA's private key has been compromised. Thus, a PKI needs to deal with the issue of revocation. 

A first step in this direction is to have each CA periodically issue a 

CRL

 (

Certificate Revocation List

) giving the serial numbers of all certificates that it has revoked. Since certificates contain expiry times, the CRL need only contain the serial numbers of certificates that have not yet expired. Once its expiry time has passed, a certificate is automatically invalid, so no distinction is needed between those that just timed out and those that were actually revoked. In both cases, they cannot be used any more. 

Unfortunately, introducing CRLs means that a user who is about to use a certificate must now acquire the CRL to see if the certificate has been revoked. If it has been, it should not be used. However, even if the certificate is not on the list, it might have been revoked just after the list was published. Thus, the only way to really be sure is to ask the CA. And on the next use of the same certificate, the CA has to be asked again, since the certificate might have been revoked a few seconds ago. 

Another complication is that a revoked certificate could conceivably be reinstated, for example, if it was revoked for nonpayment of some fee that has since been paid. Having to deal with revocation (and possibly reinstatement) eliminates one of the best properties of certificates, namely, that they can be used without having to contact a CA. 

Where should CRLs be stored? A good place would be the same place the certificates themselves are stored. One strategy is for the CA to actively push out CRLs periodically and have the directories process them by simply removing the revoked certificates. If directories are not used for storing certificates, the CRLs can be cached at various convenient places around the network. Since a CRL is itself a signed document, if it is tampered with, that tampering can be easily detected. 

If certificates have long lifetimes, the CRLs will be long, too. For example, if credit cards are valid for 5 years, the number of revocations outstanding will be much longer than if new cards are issued every 3 months. A standard way to deal with long CRLs is to issue a master list infrequently, but issue updates to it more often. Doing this reduces the bandwidth needed for distributing the CRLs. 

8.6 Communication Security 

We have now finished our study of the tools of the trade. Most of the important techniques and protocols have been covered. The rest of the chapter is about how these techniques are applied in practice to provide network security, plus some thoughts about the social aspects of security at the end of the chapter. 

In the following four sections, we will look at communication security, that is, how to get the bits secretly and without modification from source to destination and how to keep unwanted bits outside the door. These are by no means the only security issues in networking, but they are certainly among the most important ones, making this a good place to start. 

8.6.1 IPsec 

IETF has known for years that security was lacking in the Internet. Adding it was not easy because a war broke out about where to put it. Most security experts believe that to be really secure, encryption and integrity checks have to be end to end (i.e., in the application layer). That is, the source process encrypts and/or integrity protects the data and sends that to the destination process where it is decrypted and/or verified. Any tampering done in between 

596




these two processes, including within either operating system, can then be detected. The trouble with this approach is that it requires changing all the applications to make them security aware. In this view, the next best approach is putting encryption in the transport layer or in a new layer between the application layer and the transport layer, making it still end to end but not requiring applications to be changed. 

The opposite view is that users do not understand security and will not be capable of using it correctly and nobody wants to modify existing programs in any way, so the network layer should authenticate and/or encrypt packets without the users being involved. After years of pitched battles, this view won enough support that a network layer security standard was defined. In part the argument was that having network layer encryption does not prevent security-aware users from doing it right and it does help security-unaware users to some extent. 

The result of this war was a design called 

IPsec

 (

IP security

), which is described in RFCs 2401, 2402, and 2406, among others. Not all users want encryption (because it is computationally expensive). Rather than make it optional, it was decided to require encryption all the time but permit the use of a null algorithm. The null algorithm is described and praised for its simplicity, ease of implementation, and great speed in RFC 2410. 

The complete IPsec design is a framework for multiple services, algorithms and granularities. The reason for multiple services is that not everyone wants to pay the price for having all the services all the time, so the services are available a la carte. The major services are secrecy, data integrity, and protection from replay attacks (intruder replays a conversation). All of these are based on symmetric-key cryptography because high performance is crucial. 

The reason for having multiple algorithms is that an algorithm that is now thought to be secure may be broken in the future. By making IPsec algorithm-independent, the framework can survive even if some particular algorithm is later broken. 

The reason for having multiple granularities is to make it possible to protect a single TCP connection, all traffic between a pair of hosts, or all traffic between a pair of secure routers, among other possibilities. 

One slightly surprising aspect of IPsec is that even though it is in the IP layer, it is connection oriented. Actually, that is not so surprising because to have any security, a key must be established and used for some period of time—in essence, a kind of connection. Also connections amortize the setup costs over many packets. A ''connection'' in the context of IPsec is called an 

SA

 (

security association

). An SA is a simplex connection between two end points and has a security identifier associated with it. If secure traffic is needed in both directions, two security associations are required. Security identifiers are carried in packets traveling on these secure connections and are used to look up keys and other relevant information when a secure packet arrives. 

Technically, IPsec has two principal parts. The first part describes two new headers that can be added to packets to carry the security identifier, integrity control data, and other information. The other part, 

ISAKMP

 (

Internet Security Association and Key Management Protocol

) deals with establishing keys. We will not deal with ISAKMP further because (1) it is extremely complex and (2) its main protocol, 

IKE

 (

Internet Key Exchange

), is deeply flawed and needs to be replaced (Perlman and Kaufman, 2000). 

IPsec can be used in either of two modes. In 

transport mode

, the IPsec header is inserted just after the IP header. The 

Protocol

 field in the IP header is changed to indicate that an IPsec header follows the normal IP header (before the TCP header). The IPsec header contains security information, primarily the SA identifier, a new sequence number, and possibly an integrity check of the payload. 

597




In 

tunnel mode

, the entire IP packet, header and all, is encapsulated in the body of a new IP packet with a completely new IP header. Tunnel mode is useful when the tunnel ends at a location other than the final destination. In some cases, the end of the tunnel is a security gateway machine, for example, a company firewall. In this mode, the firewall encapsulates and decapsulates packets as they pass though the firewall. By terminating the tunnel at this secure machine, the machines on the company LAN do not have to be aware of IPsec. Only the firewall has to know about it. 

Tunnel mode is also useful when a bundle of TCP connections is aggregated and handled as one encrypted stream because it prevents an intruder from seeing who is sending how many packets to whom. Sometimes just knowing how much traffic is going where is valuable information. For example, if during a military crisis, the amount of traffic flowing between the Pentagon and the White House drops sharply, but the amount of traffic between the Pentagon and some military installation deep in the Colorado Rocky Mountains increases by the same amount, an intruder might be able to deduce some useful information from this data. Studying the flow patterns of packets, even if they are encrypted, is called 

traffic analysis

. Tunnel mode provides a way to foil it to some extent. The disadvantage of tunnel mode is that it adds an extra IP header, thus increasing packet size substantially. In contrast, transport mode does not affect packet size as much. 

The first new header is 

AH

 (

Authentication Header

). It provides integrity checking and antireplay security, but not secrecy (i.e., no data encryption). The use of AH in transport mode is illustrated in 

Fig. 8-27

. In IPv4 it is interposed between the IP header (including any options) and the TCP header. In IPv6 it is just another extension header and treated as such. In fact, the format is close to that of a standard IPv6 extension header. The payload may have to be padded out to some particular length for the authentication algorithm, as shown. 

Figure 8-27. The IPsec authentication header in transport mode for IPv4. 

 

Let us now examine the AH header. The 

Next header field

 is used to store the previous value that the IP 

Protocol

 field had before it was replaced with 51 to indicate that an AH header follows. In most cases, the code for TCP (6) will go here. The 

Payload length

 is the number of 32-bit words in the AH header minus 2. 

The 

Security parameters index

 is the connection identifier. It is inserted by the sender to indicate a particular record in the receiver's database. This record contains the shared key used on this connection and other information about the connection. If this protocol had been invented by ITU rather than IETF, this field would have been called 

Virtual circuit number

. 

The 

Sequence number

 field is used to number all the packets sent on an SA. Every packet gets a unique number, even retransmissions. In other words, the retransmission of a packet gets a different number here than the original (even though its TCP sequence number is the same). The purpose of this field is to detect replay attacks. These sequence numbers may not wrap around. If all 2

32

 are exhausted, a new SA must be established to continue communication. 

598




Finally, we come to the 

Authentication data

, which is a variable-length field that contains the payload's digital signature. When the SA is established, the two sides negotiate which signature algorithm they are going to use. Normally, public-key cryptography is not used here because packets must be processed extremely rapidly and all known public-key algorithms are too slow. Since IPsec is based on symmetric-key cryptography and the sender and receiver negotiate a shared key before setting up an SA, the shared key is used in the signature computation. One simple way is to compute the hash over the packet plus the shared key. The shared key is not transmitted, of course. A scheme like this is called an 

HMAC

 (

Hashed Message Authentication Code

). It is much faster to compute than first running SHA-1 and then running RSA on the result. 

The AH header does not allow encryption of the data, so it is mostly useful when integrity checking is needed but secrecy is not needed. One noteworthy feature of AH is that the integrity check covers some of the fields in the IP header, namely, those that do not change as the packet moves from router to router. The 

Time to live

 field changes on each hop, for example, so it cannot be included in the integrity check. However, the IP source address is included in the check, making it impossible for an intruder to falsify the origin of a packet. 

The alternative IPsec header is 

ESP

 (

Encapsulating Security Payload

). Its use for both transport mode and tunnel mode is shown in 

Fig. 8-28

. 

Figure 8-28. (a) ESP in transport mode. (b) ESP in tunnel mode. 

 

The ESP header consists of two 32-bit words. They are the 

Security parameters index

 and 

Sequence number

 fields that we saw in AH. A third word that generally follows them (but is technically not part of the header) is the 

Initialization vector

 used for the data encryption, unless null encryption is used, in which case it is omitted. 

ESP also provides for HMAC integrity checks, as does AH, but rather than being included in the header, they come after the payload, as shown in 

Fig. 8-28

. Putting the HMAC at the end has an advantage in a hardware implementation. The HMAC can be calculated as the bits are going out over the network interface and appended to the end. This is why Ethernet and other LANs have their CRCs in a trailer, rather than in a header. With AH, the packet has to be buffered and the signature computed before the packet can be sent, potentially reducing the number of packets/sec that can be sent. 

Given that ESP can do everything AH can do and more and is more efficient to boot, the question arises: Why bother having AH at all? The answer is mostly historical. Originally, AH handled only integrity and ESP handled only secrecy. Later, integrity was added to ESP, but the people who designed AH did not want to let it die after all that work. Their only real argument, however, is that AH checks part of the IP header, which ESP does not, but it is a weak argument. Another weak argument is that a product supporting AH but not ESP might have less trouble getting an export license because it cannot do encryption. AH is likely to be phased out in the future. 

599




8.6.2 Firewalls 

The ability to connect any computer, anywhere, to any other computer, anywhere, is a mixed blessing. For individuals at home, wandering around the Internet is lots of fun. For corporate security managers, it is a nightmare. Most companies have large amounts of confidential information on-line—trade secrets, product development plans, marketing strategies, financial analyses, etc. Disclosure of this information to a competitor could have dire consequences. 

In addition to the danger of information leaking out, there is also a danger of information leaking in. In particular, viruses, worms, and other digital pests can breach security, destroy valuable data, and waste large amounts of administrators' time trying to clean up the mess they leave. Often they are imported by careless employees who want to play some nifty new game. 

Consequently, mechanisms are needed to keep ''good'' bits in and ''bad'' bits out. One method is to use IPsec. This approach protects data in transit between secure sites. However, IPsec does nothing to keep digital pests and intruders from getting onto the company LAN. To see how to accomplish this goal, we need to look at firewalls. 

Firewalls

 are just a modern adaptation of that old medieval security standby: digging a deep moat around your castle. This design forced everyone entering or leaving the castle to pass over a single drawbridge, where they could be inspected by the I/O police. With networks, the same trick is possible: a company can have many LANs connected in arbitrary ways, but all traffic to or from the company is forced through an electronic drawbridge (firewall), as shown in 

Fig. 8-29

. 

Figure 8-29. A firewall consisting of two packet filters and an application gateway. 

 

The firewall in this configuration has two components: two routers that do packet filtering and an application gateway. Simpler configurations also exist, but the advantage of this design is that every packet must transit two filters and an application gateway to go in or out. No other route exists. Readers who think that one security checkpoint is enough clearly have not made an international flight on a scheduled airline recently. 

Each 

packet filter

 is a standard router equipped with some extra functionality. The extra functionality allows every incoming or outgoing packet to be inspected. Packets meeting some criterion are forwarded normally. Those that fail the test are dropped. 

600




In 

Fig. 8-29

, most likely the packet filter on the inside LAN checks outgoing packets and the one on the outside LAN checks incoming packets. Packets crossing the first hurdle go to the application gateway for further examination. The point of putting the two packet filters on different LANs is to ensure that no packet gets in or out without having to pass through the application gateway: there is no path around it. 

Packet filters are typically driven by tables configured by the system administrator. These tables list sources and destinations that are acceptable, sources and destinations that are blocked, and default rules about what to do with packets coming from or going to other machines. 

In the common case of a TCP/IP setting, a source or destination consists of an IP address and a port. Ports indicate which service is desired. For example, TCP port 23 is for telnet, TCP port 79 is for finger, and TCP port 119 is for USENET news. A company could block incoming packets for all IP addresses combined with one of these ports. In this way, no one outside the company could log in via telnet or look up people by using the Finger daemon. Furthermore, the company would be spared from having employees spend all day reading USENET news. 

Blocking outgoing packets is trickier because although most sites stick to the standard port numbering conventions, they are not forced to do so. Furthermore, for some important services, such as FTP (File Transfer Protocol), port numbers are assigned dynamically. In addition, although blocking TCP connections is difficult, blocking UDP packets is even harder because so little is known a priori about what they will do. Many packet filters are configured to simply ban UDP traffic altogether. 

The second half of the firewall is the 

application gateway

. Rather than just looking at raw packets, the gateway operates at the application level. A mail gateway, for example, can be set up to examine each message going in or coming out. For each one, the gateway decides whether to transmit or discard the message based on header fields, message size, or even the content (e.g., at a military installation, the presence of words like ''nuclear'' or ''bomb'' might cause some special action to be taken). 

Installations are free to set up one or more application gateways for specific applications, but it is not uncommon for suspicious organizations to permit e-mail in and out, and perhaps permit use of the World Wide Web, but to ban everything else as too dicey. Combined with encryption and packet filtering, this arrangement offers a limited amount of security at the cost of some inconvenience. 

Even if the firewall is perfectly configured, plenty of security problems still exist. For example, if a firewall is configured to allow in packets from only specific networks (e.g., the company's other plants), an intruder outside the firewall can put in false source addresses to bypass this check. If an insider wants to ship out secret documents, he can encrypt them or even photograph them and ship the photos as JPEG files, which bypasses any word filters. And we have not even discussed the fact that 70% of all attacks come from inside the firewall, for example, from disgruntled employees (Schneier, 2000). 

In addition, there is a whole other class of attacks that firewalls cannot deal with. The basic idea of a firewall is to prevent intruders from getting in and secret data from getting out. Unfortunately, there are people who have nothing better to do than try to bring certain sites down. They do this by sending legitimate packets at the target in great numbers until it collapses under the load. For example, to cripple a Web site, an intruder can send a TCP 

SYN

 packet to establish a connection. The site will then allocate a table slot for the connection and send a 

SYN + ACK

 packet in reply. If the intruder does not respond, the table slot will be tied up for a few seconds until it times out. If the intruder sends thousands of connection requests, all the table slots will fill up and no legitimate connections will be able to get through. Attacks in which the intruder's goal is to shut down the target rather than steal data are called 

DoS

 

601




(

Denial of Service

) attacks. Usually, the request packets have false source addresses so the intruder cannot be traced easily. 

An even worse variant is one in which the intruder has already broken into hundreds of computers elsewhere in the world, and then commands all of them to attack the same target at the same time. Not only does this approach increase the intruder's firepower, it also reduces his chance of detection, since the packets are coming from a large number of machines belonging to unsuspecting users. Such an attack is called a 

DDoS

 (

Distributed Denial of Service

) attack. This attack is difficult to defend against. Even if the attacked machine can quickly recognize a bogus request, it does take some time to process and discard the request, and if enough requests per second arrive, the CPU will spend all its time dealing with them. 

8.6.3 Virtual Private Networks 

Many companies have offices and plants scattered over many cities, sometimes over multiple countries. In the olden days, before public data networks, it was common for such companies to lease lines from the telephone company between some or all pairs of locations. Some companies still do this. A network built up from company computers and leased telephone lines is called a 

private network

. An example private network connecting three locations is shown in 

Fig. 8-30(a)

. 

Figure 8-30. (a) A leased-line private network. (b) A virtual private network. 

 

Private networks work fine and are very secure. If the only lines available are the leased lines, no traffic can leak out of company locations and intruders have to physically wiretap the lines to break in, which is not easy to do. The problem with private networks is that leasing a single T1 line costs thousands of dollars a month and T3 lines are many times more expensive. When public data networks and later the Internet appeared, many companies wanted to move their data (and possibly voice) traffic to the public network, but without giving up the security of the private network. 

This demand soon led to the invention of 

VPN

s (

Virtual Private Networks

), which are overlay networks on top of public networks but with most of the properties of private networks. They are called ''virtual'' because they are merely an illusion, just as virtual circuits are not real circuits and virtual memory is not real memory. 

Although VPNs can be implemented on top of ATM (or frame relay), an increasingly popular approach is to build VPNs directly over the Internet. A common design is to equip each office with a firewall and create tunnels through the Internet between all pairs of offices, as illustrated in 

Fig. 8-30(b)

. If IPsec is used for the tunneling, then it is possible to aggregate all traffic between any two pairs of offices onto a single authenticated, encrypted SA, thus providing integrity control, secrecy, and even considerable immunity to traffic analysis. 

When the system is brought up, each pair of firewalls has to negotiate the parameters of its SA, including the services, modes, algorithms, and keys. Many firewalls have VPN capabilities 

602




built in, although some ordinary routers can do this as well. But since firewalls are primarily in the security business, it is natural to have the tunnels begin and end at the firewalls, providing a clear separation between the company and the Internet. Thus, firewalls, VPNs, and IPsec with ESP in tunnel mode are a natural combination and widely used in practice. 

Once the SAs have been established, traffic can begin flowing. To a router within the Internet, a packet traveling along a VPN tunnel is just an ordinary packet. The only thing unusual about it is the presence of the IPsec header after the IP header, but since these extra headers have no effect on the forwarding process, the routers do not care about this extra header. 

A key advantage of organizing a VPN this way is that it is completely transparent to all user software. The firewalls set up and manage the SAs. The only person who is even aware of this setup is the system administrator who has to configure and manage the firewalls. To everyone else, it is like having a leased-line private network again. For more about VPNs, see (Brown, 1999; and Izzo, 2000). 

8.6.4 Wireless Security 

It is surprisingly easy to design a system that is logically completely secure by using VPNs and firewalls, but that, in practice, leaks like a sieve. This situation can occur if some of the machines are wireless and use radio communication, which passes right over the firewall in both directions. The range of 802.11 networks is often a few hundred meters, so anyone who wants to spy on a company can simply drive into the employee parking lot in the morning, leave an 802.11-enabled notebook computer in the car to record everything it hears, and take off for the day. By late afternoon, the hard disk will be full of valuable goodies. Theoretically, this leakage is not supposed to happen. Theoretically, people are not supposed to rob banks, either. 

Much of the security problem can be traced to the manufacturers of wireless base stations (access points) trying to make their products user friendly. Usually, if the user takes the device out of the box and plugs it into the electrical power socket, it begins operating immediately—nearly always with no security at all, blurting secrets to everyone within radio range. If it is then plugged into an Ethernet, all the Ethernet traffic suddenly appears in the parking lot as well. Wireless is a snooper's dream come true: free data without having to do any work. It therefore goes without saying that security is even more important for wireless systems than for wired ones. In this section, we will look at some ways wireless networks handle security. Some additional information can be found in (Nichols and Lekkas, 2002). 

802.11 Security 

The 802.11 standard prescribes a data link-level security protocol called 

WEP

 (

Wired Equivalent Privacy

), which is designed to make the security of a wireless LAN as good as that of a wired LAN. Since the default for wired LANs is no security at all, this goal is easy to achieve, and WEP achieves it, as we shall see. 

When 802.11 security is enabled, each station has a secret key shared with the base station. How the keys are distributed is not specified by the standard. They could be preloaded by the manufacturer. They could be exchanged in advance over the wired network. Finally, either the base station or user machine could pick a random key and send it to the other one over the air encrypted with the other one's public key. Once established, keys generally remain stable for months or years. 

WEP encryption uses a stream cipher based on the RC4 algorithm. RC4 was designed by Ronald Rivest and kept secret until it leaked out and was posted to the Internet in 1994. As we have pointed out before, it is nearly impossible to keep algorithms secret, even when the goal is guarding intellectual property (as it was in this case) rather than security by obscurity 

603




(which was not the goal with RC4). In WEP, RC4 generates a keystream that is XORed with the plaintext to form the ciphertext. 

Each packet payload is encrypted using the method of 

Fig. 8-31

. First the payload is checksummed using the CRC-32 polynomial and the checksum appended to the payload to form the plaintext for the encryption algorithm. Then this plaintext is XORed with a chunk of keystream its own size. The result is the ciphertext. The IV used to start RC4 is sent along with the ciphertext. When the receiver gets the packet, it extracts the encrypted payload from it, generates the keystream from the shared secret key and the IV it just got, and XORs the keystream with the payload to recover the plaintext. It can then verify the checksum to see if the packet has been tampered with. 

Figure 8-31. Packet encryption using WEP. 

 

While this approach looks good at first glance, a method for breaking it has already been published (Borisov et al., 2001). Below we will summarize their results. First of all, surprisingly many installations use the same shared key for all users, in which case each user can read all the other users' traffic. This is certainly equivalent to Ethernet, but it is not very secure. 

But even if each user has a distinct key, WEP can still be attacked. Since keys are generally stable for long periods of time, the WEP standard recommends (but does not mandate) that IV be changed on every packet to avoid the keystream reuse attack we discussed in 

Sec. 8.2.3

. Unfortunately, many 802.11 cards for notebook computers reset IV to 0 when the card is inserted into the computer, and increment it by one on each packet sent. Since people often remove and reinsert these cards, packets with low IV values are common. If Trudy can collect several packets sent by the same user with the same IV value (which is itself sent in plaintext along with each packet), she can compute the XOR of two plaintext values and probably break the cipher that way. 

But even if the 802.11 card picks a random IV for each packet, the IVs are only 24 bits, so after 2

24

 packets have been sent, they have to be reused. Worse yet, with randomly chosen IVs, the expected number of packets that have to be sent before the same one is used twice is about 5000, due to the birthday attack described in 

Sec. 8.4.4

. Thus, if Trudy listens for a few minutes, she is almost sure to capture two packets with the same IV and same key. By XORing the ciphertexts she is able to obtain the XOR of the plaintexts. This bit sequence can be attacked in various ways to recover the plaintexts. With some more work, the keystream for that IV can also be obtained. Trudy can continue working like this for a while and compile a dictionary of keystreams for various IVs. Once an IV has been broken, all the packets sent with it in the future (but also in the past) can be fully decrypted. 

Furthermore, since IVs are used at random, once Trudy has determined a valid (IV, keystream) pair, she can use it to generate all the packets she wants to using it and thus actively interfere with communication. Theoretically, a receiver could notice that large numbers of packets suddenly all have the same IV, but (1) WEP allows this, and (2) nobody checks for this anyway. 

604




Finally, the CRC is not worth much, since it is possible for Trudy to change the payload and make the corresponding change to the CRC, without even having to remove the encryption In short, breaking 802.11's security is fairly straightforward, and we have not even listed all the attacks Borisov et al. found. 

In August 2001, a month after the Borisov et al. paper was presented, another devastating attack on WEP was published (Fluhrer et al., 2001). This one found cryptographic weaknesses in RC4 itself. Fluhrer et al. discovered that many of the keys have the property that it is possible to derive some key bits from the keystream. If this attack is performed repeatedly, it is possible to derive the entire key with a modest amount of effort. Being somewhat theoretically inclined, Fluhrer et al. did not actually try to break any 802.11 LANs. 

In contrast, when a summer student and two researchers at AT&T Labs learned about the Fluhrer et al. attack, they decided to try it out for real (Stubblefield et al., 2002). Within a week they had broken their first 128-bit key on a production 802.11 LAN, and most of the week was actually devoted to looking for the cheapest 802.11 card they could find, getting permission to buy it, installing it, and testing it. The programming took only two hours. 

When they announced their results, CNN ran a story entitled ''Off-the-Shelf Hack Breaks Wireless Encryption,'' in which some industry gurus tried to pooh-pooh their results by saying what they had done was trivial, given the Fluhrer et al. results. While that remark is technically true, the fact remains that the combined efforts of these two teams demonstrated a fatal flaw in WEP and 802.11. 

On September 7, 2001, IEEE responded to the fact that WEP was now completely broken by issuing a short statement making six points that can be roughly summarized as follows: 

1. We told you that WEP security was no better than Ethernet's. 

2. A much bigger threat is forgetting to enable security at all. 

3. Try using some other security (e.g., transport layer security). 

4. The next version, 802.11i, will have better security. 

5. Future certification will mandate the use of 802.11i. 

6. We will try to figure out what to do until 802.11i arrives. 

We have gone through this story in some detail to make the point that getting security right is not easy, even for experts. 

Bluetooth Security 

Bluetooth has a considerably shorter range than 802.11, so it cannot be attacked from the parking lot, but security is still an issue here. For example, imagine that Alice's computer is equipped with a wireless Bluetooth keyboard. In the absence of security, if Trudy happened to be in the adjacent office, she could read everything Alice typed in, including all her outgoing e-mail. She could also capture everything Alice's computer sent to the Bluetooth printer sitting next to it (e.g., incoming e-mail and confidential reports). Fortunately, Bluetooth has an elaborate security scheme to try to foil the world's Trudies. We will now summarize the main features of it below. 

Bluetooth has three security modes, ranging from nothing at all to full data encryption and integrity control. As with 802.11, if security is disabled (the default), there is no security. Most users have security turned off until a serious breach has occurred; then they turn it on. In the agricultural world, this approach is known as locking the barn door after the horse has escaped. 

Bluetooth provides security in multiple layers. In the physical layer, frequency hopping provides a tiny bit of security, but since any Bluetooth device that moves into a piconet has to be told the frequency hopping sequence, this sequence is obviously not a secret. The real 

605




security starts when the newly-arrived slave asks for a channel with the master. The two devices are assumed to share a secret key set up in advance. In some cases, both are hardwired by the manufacturer (e.g., for a headset and mobile phone sold as a unit). In other cases, one device (e.g., the headset) has a hardwired key and the user has to enter that key into the other device (e.g., the mobile phone) as a decimal number. These shared keys are called 

passkeys

. 

To establish a channel, the slave and master each check to see if the other one knows the passkey. If so, they negotiate whether that channel will be encrypted, integrity controlled, or both. Then they select a random 128-bit session key, some of whose bits may be public. The point of allowing this key weakening is to comply with government restrictions in various countries designed to prevent the export or use of keys longer than the government can break. 

Encryption uses a stream cipher called 

E

0

; integrity control uses 

SAFER+

. Both are traditional symmetric-key block ciphers. SAFER+ was submitted to the AES bake-off, but was eliminated in the first round because it was slower than the other candidates. Bluetooth was finalized before the AES cipher was chosen; otherwise it would most likely have used Rijndael. 

The actual encryption using the stream cipher is shown in 

Fig. 8-14

, with the plaintext XORed with the keystream to generate the ciphertext. Unfortunately, 

E

 

0

 itself (like RC4) may have fatal weaknesses (Jakobsson and Wetzel, 2001). While it was not broken at the time of this writing, its similarities to the A5/1 cipher, whose spectacular failure compromises all GSM telephone traffic, are cause for concern (Biryukov et al., 2000). It sometimes amazes people (including the author), that in the perennial cat-and-mouse game between cryptographers and cryptanalysts, the cryptanalysts are so often on the winning side. 

Another security issue is that Bluetooth authenticates only devices, not users, so theft of a Bluetooth device may give the thief access to the user's financial and other accounts. However, Bluetooth also implements security in the upper layers, so even in the event of a breach of link-level security, some security may remain, especially for applications that require a PIN code to be entered manually from some kind of keyboard to complete the transaction. 

WAP 2.0 Security 

For the most part, the WAP Forum learned its lesson from having a nonstandard protocol stack in WAP 1.0. WAP 2.0 largely uses standard protocols in all layers. Security is no exception. Since it is IP based, it supports full use of IPsec in the network layer. In the transport layer, TCP connections can be protected by TLS, an IETF standard we will study later in this chapter. Higher still, it uses HTTP client authentication, as defined in RFC 2617. Application-layer crypto libraries provide for integrity control and nonrepudiation. All in all, since WAP 2.0 is based on well-known standards, there is a chance that its security services, in particular, privacy, authentication, integrity control, and nonrepudiation may fare better than 802.11 and Bluetooth security. 

8.7 Authentication Protocols 

Authentication

 is the technique by which a process verifies that its communication partner is who it is supposed to be and not an imposter. Verifying the identity of a remote process in the face of a malicious, active intruder is surprisingly difficult and requires complex protocols based on cryptography. In this section, we will study some of the many authentication protocols that are used on insecure computer networks. 

As an aside, some people confuse authorization with authentication. Authentication deals with the question of whether you are actually communicating with a specific process. Authorization is concerned with what that process is permitted to do. For example, a client process contacts 

606




a file server and says: I am Scott's process and I want to delete the file 

cookbook.old

. From the file server's point of view, two questions must be answered: 

1. Is this actually Scott's process (authentication)? 

2. Is Scott allowed to delete 

cookbook.old

 (authorization)? 

Only after both of these questions have been unambiguously answered in the affirmative can the requested action take place. The former question is really the key one. Once the file server knows to whom it is talking, checking authorization is just a matter of looking up entries in local tables or databases. For this reason, we will concentrate on authentication in this section. 

The general model that all authentication protocols use is this. Alice starts out by sending a message either to Bob or to a trusted 

KDC

 (

Key Distribution Center

), which is expected to be honest. Several other message exchanges follow in various directions. As these messages are being sent Trudy may intercept, modify, or replay them in order to trick Alice and Bob or just to gum up the works. 

Nevertheless, when the protocol has been completed, Alice is sure she is talking to Bob and Bob is sure he is talking to Alice. Furthermore, in most of the protocols, the two of them will also have established a secret 

session key

 for use in the upcoming conversation. In practice, for performance reasons, all data traffic is encrypted using symmetric-key cryptography (typically AES or triple DES), although public-key cryptography is widely used for the authentication protocols themselves and for establishing the session key. 

The point of using a new, randomly-chosen session key for each new connection is to minimize the amount of traffic that gets sent with the users' secret keys or public keys, to reduce the amount of ciphertext an intruder can obtain, and to minimize the damage done if a process crashes and its core dump falls into the wrong hands. Hopefully, the only key present then will be the session key. All the permanent keys should have been carefully zeroed out after the session was established. 

8.7.1 Authentication Based on a Shared Secret Key 

For our first authentication protocol, we will assume that Alice and Bob already share a secret key, 

K

AB

 . This shared key might have been agreed upon on the telephone or in person, but, in any event, not on the (insecure) network. 

This protocol is based on a principle found in many authentication protocols: one party sends a random number to the other, who then transforms it in a special way and then returns the result. Such protocols are called 

challenge-response

 protocols. In this and subsequent authentication protocols, the following notation will be used: 

A

, 

B

 are the identities of Alice and Bob. 

R

i

's are the challenges, where the subscript identifies the challenger. 

K

i

 are keys, where 

i

 indicates the owner. 

K

S

 is the session key. 

The message sequence for our first shared-key authentication protocol is illustrated in 

Fig. 8-

32

. In message 1, Alice sends her identity, 

A

, to Bob in a way that Bob understands. Bob, of course, has no way of knowing whether this message came from Alice or from Trudy, so he chooses a challenge, a large random number, 

R

B

, and sends it back to ''Alice'' as message 2, in plaintext. Random numbers used just once in challenge-response protocols like this one are called 

nonces

. Alice then encrypts the message with the key she shares with Bob and sends 

607




the ciphertext, 

K

AB

 (

R

B

), back in message 3. When Bob sees this message, he immediately knows that it came from Alice because Trudy does not know 

K

AB

 and thus could not have generated it. Furthermore, since 

R

B

 was chosen randomly from a large space (say, 128-bit random numbers), it is very unlikely that Trudy would have seen 

R

B

 and its response from an earlier session. It is equally unlikely that she could guess the correct response to any challenge. 

Figure 8-32. Two-way authentication using a challenge-response protocol. 

 

At this point, Bob is sure he is talking to Alice, but Alice is not sure of anything. For all Alice knows, Trudy might have intercepted message 1 and sent back 

R

B

 in response. Maybe Bob died last night. To find out to whom she is talking, Alice picks a random number, 

R

A

 and sends it to Bob as plaintext, in message 4. When Bob responds with 

K

AB

 (

R

A

), Alice knows she is talking to Bob. If they wish to establish a session key now, Alice can pick one, 

K

S

, and send it to Bob encrypted with 

K

AB

. 

The protocol of 

Fig. 8-32

 contains five messages. Let us see if we can be clever and eliminate some of them. One approach is illustrated in 

Fig. 8-33

. Here Alice initiates the challenge-response protocol instead of waiting for Bob to do it. Similarly, while he is responding to Alice's challenge, Bob sends his own. The entire protocol can be reduced to three messages instead of five. 

Figure 8-33. A shortened two-way authentication protocol. 

 

Is this new protocol an improvement over the original one? In one sense it is: it is shorter. Unfortunately, it is also wrong. Under certain circumstances, Trudy can defeat this protocol by using what is known as a 

reflection attack

. In particular, Trudy can break it if it is possible to open multiple sessions with Bob at once. This situation would be true, for example, if Bob is a bank and is prepared to accept many simultaneous connections from teller machines at once. 

Trudy's reflection attack is shown in 

Fig. 8-34

. It starts out with Trudy claiming she is Alice and sending 

R

T

. Bob responds, as usual, with his own challenge, 

R

B

. Now Trudy is stuck. What can she do? She does not know 

K

AB

 (

R

B

). 

Figure 8-34. The reflection attack. 

608




 

She can open a second session with message 3, supplying the 

R

B

 taken from message 2 as her challenge. Bob calmly encrypts it and sends back 

K

AB

 (

R

B

)in message 4. We have shaded the messages on the second session to make them stand out. Now Trudy has the missing information, so she can complete the first session and abort the second one. Bob is now convinced that Trudy is Alice, so when she asks for her bank account balance, he gives it to her without question. Then when she asks him to transfer it all to a secret bank account in Switzerland, he does so without a moment's hesitation. 

The moral of this story is: 

Designing a correct authentication protocol is harder than it looks.

The following four general rules often help: 

1. Have the initiator prove who she is before the responder has to. In this case, Bob gives away valuable information before Trudy has to give any evidence of who she is. 

2. Have the initiator and responder use different keys for proof, even if this means having two shared keys, 

K

AB

 and 

K

'

AB

 . 

3. Have the initiator and responder draw their challenges from different sets. For example, the initiator must use even numbers and the responder must use odd numbers. 

4. Make the protocol resistant to attacks involving a second parallel session in which information obtained in one session is used in a different one. 

If even one of these rules is violated, the protocol can frequently be broken. Here, all four rules were violated, with disastrous consequences. 

Now let us go back and take a closer look at 

Fig. 8-32

. Surely that protocol is not subject to a reflection attack? Well, it depends. It is quite subtle. Trudy was able to defeat our protocol by using a reflection attack because it was possible to open a second session with Bob and trick him into answering his own questions. What would happen if Alice were a general-purpose computer that also accepted multiple sessions, rather than a person at a computer? Let us take a look what Trudy can do. 

To see how Trudy's attack works, see 

Fig. 8-35

. Alice starts out by announcing her identity in message 1. Trudy intercepts this message and begins her own session with message 2, claiming to be Bob. Again we have shaded the session 2 messages. Alice responds to message 2 by saying: You claim to be Bob? Prove it. in message 3. At this point Trudy is stuck because she cannot prove she is Bob. 

Figure 8-35. A reflection attack on the protocol of 

Fig. 8-32

. 

609




 

What does Trudy do now? She goes back to the first session, where it is her turn to send a challenge, and sends the 

R

A

 she got in message 3. Alice kindly responds to it in message 5, thus supplying Trudy with the information she needs to send message 6 in session 2. At this point, Trudy is basically home free because she has successfully responded to Alice's challenge in session 2. She can now cancel session 1, send over any old number for the rest of session 2, and she will have an authenticated session with Alice in session 2. 

But Trudy is nasty, and she really wants to rub it in. Instead of sending any old number over to complete session 2, she waits until Alice sends message 7, Alice's challenge for session 1. Of course, Trudy does not know how to respond, so she uses the reflection attack again, sending back 

R

A2

 as message 8. Alice conveniently encrypts 

R

A

2

 in message 9. Trudy now switches back to session 1 and sends Alice the number she wants in message 10, conveniently copied from what Alice sent in message 9. At this point Trudy has two fully authenticated sessions with Alice. 

This attack has a somewhat different result than the attack on the three-message protocol shown in 

Fig. 8-34

. This time, Trudy has two authenticated connections with Alice. In the previous example, she had one authenticated connection with Bob. Again here, if we had applied all the general authentication protocol rules discussed above, this attack could have been stopped. A detailed discussion of these kind of attacks and how to thwart them is given in (Bird et al., 1993). They also show how it is possible to systematically construct protocols that are provably correct. The simplest such protocol is nevertheless a bit complicated, so we will now show a different class of protocol that also works. 

The new authentication protocol is shown in 

Fig. 8-36

 (Bird et al., 1993). It uses an HMAC of the type we saw when studying IPsec. Alice starts out by sending Bob a nonce, 

R

A

 as message 1. Bob responds by selecting his own nonce, 

R

B

, and sending it back along with an HMAC. The HMAC is formed to building a data structure consisting of the Alice's nonce, Bob's nonce, their identities, and the shared secret key, 

K

AB

. This data structured is then hashed into the HMAC, for example using SHA-1. When Alice receives message 2, she now has 

R

A

 (which she picked herself), 

R

B

, which arrives as plaintext, the two identities, and the secret key, 

K

AB

, which has known all along, so she can compute the HMAC herself. If it agrees with the HMAC in the message, she knows she is talking to Bob because Trudy does not know 

K

AB

 and thus cannot figure out which HMAC to send. Alice responds to Bob with an HMAC containing just the two nonces. 

Figure 8-36. Authentication using HMACs. 

610




 

Can Trudy somehow subvert this protocol? No, because she cannot force either party to encrypt or hash a value of her choice, as happened in 

Fig. 8-34

 and Fig. 8-(Fo. Both HMACs include values chosen by the sending party, something which Trudy cannot control. 

Using HMACs is not the only way to use this idea. An alternative scheme that is often used instead of computing the HMAC over a series of items is to encrypt the items sequentially using cipher block chaining. 

8.7.2 Establishing a Shared Key: The Diffie-Hellman Key Exchange 

So far we have assumed that Alice and Bob share a secret key. Suppose that they do not (because so far there is no universally accepted PKI for signing and distributing certificates). How can they establish one? One way would be for Alice to call Bob and give him her key on the phone, but he would probably start out by saying: How do I know you are Alice and not Trudy? They could try to arrange a meeting, with each one bringing a passport, a drivers' license, and three major credit cards, but being busy people, they might not be able to find a mutually acceptable date for months. Fortunately, incredible as it may sound, there is a way for total strangers to establish a shared secret key in broad daylight, even with Trudy carefully recording every message. 

The protocol that allows strangers to establish a shared secret key is called the 

Diffie-Hellman key exchange

 (Diffie and Hellman, 1976) and works as follows. Alice and Bob have to agree on two large numbers, 

n

 and 

g

, where 

n

 is a prime, (

n

 - 1)

/

2 is also a prime and certain conditions apply to 

g

. These numbers may be public, so either one of them can just pick 

n

 and 

g

 and tell the other openly. Now Alice picks a large (say, 512-bit) number, 

x

, and keeps it secret. Similarly, Bob picks a large secret number, 

y

. 

Alice initiates the key exchange protocol by sending Bob a message containing (

n

, 

g

, 

g

x

 mod 

n

), as shown in 

Fig. 8-37

. Bob responds by sending Alice a message containing 

g

y

 mod 

n

. Now Alice raises the number Bob sent her to the 

x

th power modulo 

n

 to get (

g

y

 mod 

n

)

x

 

mod n

. Bob performs a similar operation to get (

g

x

 mod 

n

)

y

 

mod n

. By the laws of modular arithmetic, both calculations yield 

g

xy

 mod 

n

. Lo and behold, Alice and Bob suddenly share a secret key, 

g

xy

 mod 

n

. 

Figure 8-37. The Diffie-Hellman key exchange. 

 

611




Trudy, of course, has seen both messages. She knows 

g

 and 

n

 from message 1. If she could compute 

x

 and 

y

, she could figure out the secret key. The trouble is, given only 

g

x

 mod 

n

, she cannot find 

x

. No practical algorithm for computing discrete logarithms modulo a very large prime number is known. 

To make the above example more concrete, we will use the (completely unrealistic) values of 

n

 = 47 and 

g

 = 3. Alice picks 

x

 = 8 and Bob picks 

y

 = 10. Both of these are kept secret. Alice's message to Bob is (47, 3, 28) because 3

8

 mod 47 is 28. Bob's message to Alice is (17). Alice computes 17

8

 mod 47, which is 4. Bob computes 28

10

 mod 47, which is 4. Alice and Bob have independently determined that the secret key is now 4. Trudy has to solve the equation 3

x

 mod 47 = 28, which can be done by exhaustive search for small numbers like this, but not when all the numbers are hundreds of bits long. All currently-known algorithms simply take too long, even on massively parallel supercomputers. 

Despite the elegance of the Diffie-Hellman algorithm, there is a problem: when Bob gets the triple (47, 3, 28), how does he know it is from Alice and not from Trudy? There is no way he can know. Unfortunately, Trudy can exploit this fact to deceive both Alice and Bob, as illustrated in 

Fig. 8-38

. Here, while Alice and Bob are choosing 

x

 and 

y

, respectively, Trudy picks her own random number, 

z

. Alice sends message 1 intended for Bob. Trudy intercepts it and sends message 2 to Bob, using the correct 

g

 and 

n

 (which are public anyway) but with her own 

z

 instead of 

x

. She also sends message 3 back to Alice. Later Bob sends message 4 to Alice, which Trudy again intercepts and keeps. 

Figure 8-38. The bucket brigade or man-in-the-middle attack. 

 

Now everybody does the modular arithmetic. Alice computes the secret key as 

g

xz

 mod 

n

, and so does Trudy (for messages to Alice). Bob computes 

g

yz

 mod 

n

 and so does Trudy (for messages to Bob). Alice thinks she is talking to Bob so she establishes a session key (with Trudy). So does Bob. Every message that Alice sends on the encrypted session is captured by Trudy, stored, modified if desired, and then (optionally) passed on to Bob. Similarly, in the other direction. Trudy sees everything and can modify all messages at will, while both Alice and Bob are under the illusion that they have a secure channel to one another. This attack is known as the 

bucket brigade attack

, because it vaguely resembles an old-time volunteer fire department passing buckets along the line from the fire truck to the fire. It is also called the 

man-in-the-middle attack

. 

8.7.3 Authentication Using a Key Distribution Center 

Setting up a shared secret with a stranger almost worked, but not quite. On the other hand, it probably was not worth doing in the first place (sour grapes attack). To talk to 

n

 people this way, you would need 

n

 keys. For popular people, key management would become a real burden, especially if each key had to be stored on a separate plastic chip card. 

A different approach is to introduce a trusted key distribution center (KDC). In this model, each user has a single key shared with the KDC. Authentication and session key management now goes through the KDC. The simplest known KDC authentication protocol involving two parties and a trusted KDC is depicted in 

Fig. 8-39

. 

612




Figure 8-39. A first attempt at an authentication protocol using a KDC. 

 

The idea behind this protocol is simple: Alice picks a session key, 

K

S

, and tells the KDC that she wants to talk to Bob using 

K

S

. This message is encrypted with the secret key Alice shares (only) with the KDC, 

K

A

. The KDC decrypts this message, extracting Bob's identity and the session key. It then constructs a new message containing Alice's identity and the session key and sends this message to Bob. This encryption is done with 

K

B

, the secret key Bob shares with the KDC. When Bob decrypts the message, he learns that Alice wants to talk to him and which key she wants to use. 

The authentication here happens for free. The KDC knows that message 1 must have come from Alice, since no one else would have been able to encrypt it with Alice's secret key. Similarly, Bob knows that message 2 must have come from the KDC, whom he trusts, since no one else knows his secret key. 

Unfortunately, this protocol has a serious flaw. Trudy needs some money, so she figures out some legitimate service she can perform for Alice, makes an attractive offer, and gets the job. After doing the work, Trudy then politely requests Alice to pay by bank transfer. Alice then establishes a session key with her banker, Bob. Then she sends Bob a message requesting money to be transferred to Trudy's account. 

Meanwhile, Trudy is back to her old ways, snooping on the network. She copies both message 2 in 

Fig. 8-39

 and the money-transfer request that follows it. Later, she replays both of them to Bob. Bob gets them and thinks: Alice must have hired Trudy again. She clearly does good work. Bob then transfers an equal amount of money from Alice's account to Trudy's. Some time after the 50th message pair, Bob runs out of the office to find Trudy to offer her a big loan so she can expand her obviously successful business. This problem is called the 

replay attack

. 

Several solutions to the replay attack are possible. The first one is to include a timestamp in each message. Then if anyone receives an obsolete message, it can be discarded. The trouble with this approach is that clocks are never exactly synchronized over a network, so there has to be some interval during which a timestamp is valid. Trudy can replay the message during this interval and get away with it. 

The second solution is to put a nonce in each message. Each party then has to remember all previous nonces and reject any message containing a previously-used nonce. But nonces have to be remembered forever, lest Trudy try replaying a 5-year-old message. Also, if some machine crashes and it loses its nonce list, it is again vulnerable to a replay attack. Timestamps and nonces can be combined to limit how long nonces have to be remembered, but clearly the protocol is going to get a lot more complicated. 

A more sophisticated approach to mutual authentication is to use a multiway challenge-response protocol. A well-known example of such a protocol is the 

Needham-Schroeder authentication

 protocol (Needham and Schroeder, 1978), one variant of which is shown in 

Fig. 8-40

. 

Figure 8-40. The Needham-Schroeder authentication protocol. 

613




 

The protocol begins with Alice telling the KDC that she wants to talk to Bob. This message contains a large random number, 

R

A

, as a nonce. The KDC sends back message 2 containing Alice's random number, a session key, and a ticket that she can send to Bob. The point of the random number, 

R

A

, is to assure Alice that message 2 is fresh, and not a replay. Bob's identity is also enclosed in case Trudy gets any funny ideas about replacing 

B

 in message 1 with her own identity so the KDC will encrypt the ticket at the end of message 2 with 

K

T

 instead of 

K

B

. The ticket encrypted with 

K

B

 is included inside the encrypted message to prevent Trudy from replacing it with something else on the way back to Alice. 

Alice now sends the ticket to Bob, along with a new random number, 

R

A

2

, encrypted with the session key, 

K

S

. In message 4, Bob sends back 

K

S

(

R

A

2

 - 1) to prove to Alice that she is talking to the real Bob. Sending back 

K

S

(

R

A

2

) would not have worked, since Trudy could just have stolen it from message 3. 

After receiving message 4, Alice is now convinced that she is talking to Bob and that no replays could have been used so far. After all, she just generated 

R

A

2

 a few milliseconds ago. The purpose of message 5 is to convince Bob that it is indeed Alice he is talking to, and no replays are being used here either. By having each party both generate a challenge and respond to one, the possibility of any kind of replay attack is eliminated. 

Although this protocol seems pretty solid, it does have a slight weakness. If Trudy ever manages to obtain an old session key in plaintext, she can initiate a new session with Bob by replaying the message 3 corresponding to the compromised key and convince him that she is Alice (Denning and Sacco, 1981). This time she can plunder Alice's bank account without having to perform the legitimate service even once. 

Needham and Schroeder later published a protocol that corrects this problem (Needham and Schroeder, 1987). In the same issue of the same journal, Otway and Rees (1987) also published a protocol that solves the problem in a shorter way. 

Figure 8-41

 shows a slightly modified Otway-Rees protocol. 

Figure 8-41. The Otway-Rees authentication protocol (slightly simplified). 

 

In the Otway-Rees protocol, Alice starts out by generating a pair of random numbers, 

R

, which will be used as a common identifier, and 

R

A

, which Alice will use to challenge Bob. When Bob 

614




gets this message, he constructs a new message from the encrypted part of Alice's message and an analogous one of his own. Both the parts encrypted with 

K

A

 and 

K

B

 identify Alice and Bob, contain the common identifier, and contain a challenge. 

The KDC checks to see if the 

R

 in both parts is the same. It might not be because Trudy tampered with 

R

 in message 1 or replaced part of message 2. If the two 

R

s match, the KDC believes that the request message from Bob is valid. It then generates a session key and encrypts it twice, once for Alice and once for Bob. Each message contains the receiver's random number, as proof that the KDC, and not Trudy, generated the message. At this point both Alice and Bob are in possession of the same session key and can start communicating. The first time they exchange data messages, each one can see that the other one has an identical copy of 

K

S

, so the authentication is then complete. 

8.7.4 Authentication Using Kerberos 

An authentication protocol used in many real systems (including Windows 2000) is 

Kerberos

, which is based on a variant of Needham-Schroeder. It is named for a multiheaded dog in Greek mythology that used to guard the entrance to Hades (presumably to keep undesirables out). Kerberos was designed at M.I.T. to allow workstation users to access network resources in a secure way. Its biggest difference from Needham-Schroeder is its assumption that all clocks are fairly well synchronized. The protocol has gone through several iterations. V4 is the version most widely used in industry, so we will describe it. Afterward, we will say a few words about its successor, V5. For more information, see (Steiner et al., 1988). 

Kerberos involves three servers in addition to Alice (a client workstation): 

Authentication Server (AS): verifies users during login 

Ticket-Granting Server (TGS): issues ''proof of identity tickets'' 

Bob the server: actually does the work Alice wants performed 

AS is similar to a KDC in that it shares a secret password with every user. The TGS's job is to issue tickets that can convince the real servers that the bearer of a TGS ticket really is who he or she claims to be. 

To start a session, Alice sits down at an arbitrary public workstation and types her name. The workstation sends her name to the AS in plaintext, as shown in 

Fig. 8-42

. What comes back is a session key and a ticket, 

K

TGS

(

A

, 

K

S

), intended for the TGS. These items are packaged together and encrypted using Alice's secret key, so that only Alice can decrypt them. Only when message 2 arrives does the workstation ask for Alice's password. The password is then used to generate 

K

A

 in order to decrypt message 2 and obtain the session key and TGS ticket inside it. At this point, the workstation overwrites Alice's password to make sure that it is only inside the workstation for a few milliseconds at most. If Trudy tries logging in as Alice, the password she types will be wrong and the workstation will detect this because the standard part of message 2 will be incorrect. 

Figure 8-42. The operation of Kerberos V4. 

615




 

After she logs in, Alice may tell the workstation that she wants to contact Bob the file server. The workstation then sends message 3 to the TGS asking for a ticket to use with Bob. The key element in this request is 

K

TGS

(

A

, 

K

S

), which is encrypted with the TGS's secret key and used as proof that the sender really is Alice. The TGS responds by creating a session key, 

K

AB

, for Alice to use with Bob. Two versions of it are sent back. The first is encrypted with only 

K

S

, so Alice can read it. The second is encrypted with Bob's key, 

K

B

, so Bob can read it. 

Trudy can copy message 3 and try to use it again, but she will be foiled by the encrypted timestamp, 

t

, sent along with it. Trudy cannot replace the timestamp with a more recent one, because she does not know 

K

S

, the session key Alice uses to talk to the TGS. Even if Trudy replays message 3 quickly, all she will get is another copy of message 4, which she could not decrypt the first time and will not be able to decrypt the second time either. 

Now Alice can send 

K

AB

 to Bob to establish a session with him. This exchange is also timestamped. The response is proof to Alice that she is actually talking to Bob, not to Trudy. 

After this series of exchanges, Alice can communicate with Bob under cover of 

K

AB

. If she later decides she needs to talk to another server, Carol, she just repeats message 3 to the TGS, only now specifying 

C

 instead of 

B

. The TGS will promptly respond with a ticket encrypted with 

K

C

 that Alice can send to Carol and that Carol will accept as proof that it came from Alice. 

The point of all this work is that now Alice can access servers all over the network in a secure way and her password never has to go over the network. In fact, it only had to be in her own workstation for a few milliseconds. However, note that each server does its own authorization. When Alice presents her ticket to Bob, this merely proves to Bob who sent it. Precisely what Alice is allowed to do is up to Bob. 

Since the Kerberos designers did not expect the entire world to trust a single authentication server, they made provision for having multiple 

realms

, each with its own AS and TGS. To get a ticket for a server in a distant realm, Alice would ask her own TGS for a ticket accepted by the TGS in the distant realm. If the distant TGS has registered with the local TGS (the same way local servers do), the local TGS will give Alice a ticket valid at the distant TGS. She can then do business over there, such as getting tickets for servers in that realm. Note, however, that for parties in two realms to do business, each one must trust the other's TGS. 

Kerberos V5 is fancier than V4 and has more overhead. It also uses OSI ASN.1 (Abstract Syntax Notation 1) for describing data types and has small changes in the protocols. Furthermore, it has longer ticket lifetimes, allows tickets to be renewed, and will issue postdated tickets. In addition, at least in theory, it is not DES dependent, as V4 is, and supports multiple realms by delegating ticket generation to multiple ticket servers. 

8.7.5 Authentication Using Public-Key Cryptography 

Mutual authentication can also be done using public-key cryptography. To start with, Alice needs to get Bob's public key. If a PKI exists with a directory server that hands out certificates 

616




for public keys, Alice can ask for Bob's, as shown in 

Fig. 8-43

 as message 1. The reply, in message 2, is an X.509 certificate containing Bob's public key. When Alice verifies that the signature is correct, she sends Bob a message containing her identity and a nonce. 

Figure 8-43. Mutual authentication using public-key cryptography. 

 

When Bob receives this message, he has no idea whether it came from Alice or from Trudy, but he plays along and asks the directory server for Alice's public key (message 4) which he soon gets (message 5). He then sends Alice a message containing Alice's 

R

A

, his own nonce, 

R

B

, and a proposed session key, 

K

S

, as message 6. 

When Alice gets message 6, she decrypts it using her private key. She sees 

R

A

 in it, which gives her a warm feeling inside. The message must have come from Bob, since Trudy has no way of determining 

R

A

. Furthermore, it must be fresh and not a replay, since she just sent Bob 

R

A

. Alice agrees to the session by sending back message 7. When Bob sees 

R

B

 encrypted with the session key he just generated, he knows Alice got message 6 and verified 

R

A

. 

What can Trudy do to try to subvert this protocol? She can fabricate message 3 and trick Bob into probing Alice, but Alice will see an 

R

A

 that she did not send and will not proceed further. Trudy cannot forge message 7 back to Bob because she does not know 

R

B

 or 

K

S

 and cannot determine them without Alice's private key. She is out of luck. 

 

8.8 E-Mail Security 

When an e-mail message is sent between two distant sites, it will generally transit dozens of machines on the way. Any of these can read and record the message for future use. In practice, privacy is nonexistent, despite what many people think. Nevertheless, many people would like to be able to send e-mail that can be read by the intended recipient and no one else: not their boss and not even their government. This desire has stimulated several people and groups to apply the cryptographic principles we studied earlier to e-mail to produce secure e-mail. In the following sections we will study a widely-used secure e-mail system, PGP, and then briefly mention two others, PEM and S/MIME. For additional information about secure e-mail, see (Kaufman et al., 2002; and Schneier, 1995). 

8.8.1 PGP—Pretty Good Privacy 

Our first example, 

PGP

 (

Pretty Good Privacy

) is essentially the brainchild of one person, Phil Zimmermann (Zimmermann, 1995a, 1995b). Zimmermann is a privacy advocate whose motto is: If privacy is outlawed, only outlaws will have privacy. Released in 1991, PGP is a complete e-mail security package that provides privacy, authentication, digital signatures, and compression, all in an easy-to-use form. Furthermore, the complete package, including all the source code, is distributed free of charge via the Internet. Due to its quality, price (zero), and easy availability on UNIX, Linux, Windows, and Mac OS platforms, it is widely used today. 

617




PGP encrypts data by using a block cipher called 

IDEA

 (

International Data Encryption Algorithm

), which uses 128-bit keys. It was devised in Switzerland at a time when DES was seen as tainted and AES had not yet been invented. Conceptually, IDEA is similar to DES and AES: it mixes up the bits in a series of rounds, but the details of the mixing functions are different from DES and AES. Key management uses RSA and data integrity uses MD5, topics that we have already discussed. 

PGP has also been embroiled in controversy since day 1 (Levy, 1993). Because Zimmermann did nothing to stop other people from placing PGP on the Internet, where people all over the world could get it, the U.S. Government claimed that Zimmermann had violated U.S. laws prohibiting the export of munitions. The U.S. Government's investigation of Zimmermann went on for 5 years, but was eventually dropped, probably for two reasons. First, Zimmermann did not place PGP on the Internet himself, so his lawyer claimed that 

he

 never exported anything (and then there is the little matter of whether creating a Web site constitutes export at all). Second, the government eventually came to realize that winning a trial meant convincing a jury that a Web site containing a downloadable privacy program was covered by the arms-trafficking law prohibiting the export of war materiel such as tanks, submarines, military aircraft, and nuclear weapons. Years of negative publicity probably did not help much, either. 

As an aside, the export rules are bizarre, to put it mildly. The government considered putting code on a Web site to be an illegal export and harassed Zimmermann for 5 years about it. On the other hand, when someone published the complete PGP source code, in C, as a book (in a large font with a checksum on each page to make scanning it in easy) and then exported the book, that was fine with the government because books are not classified as munitions. The sword is mightier than the pen, at least for Uncle Sam. 

Another problem PGP ran into involved patent infringement. The company holding the RSA patent, RSA Security, Inc., alleged that PGP's use of the RSA algorithm infringed on its patent, but that problem was settled with releases starting at 2.6. Furthermore, PGP uses another patented encryption algorithm, IDEA, whose use caused some problems at first. 

Since PGP is open source, various people and groups have modified it and produced a number of versions. Some of these were designed to get around the munitions laws, others were focused on avoiding the use of patented algorithms, and still others wanted to turn it into a closed-source commercial product. Although the munitions laws have now been slightly liberalized (otherwise products using AES would not have been exportable from the U.S.), and the RSA patent expired in September 2000, the legacy of all these problems is that several incompatible versions of PGP are in circulation, under various names. The discussion below focuses on classic PGP, which is the oldest and simplest version. Another popular version, Open PGP, is described in RFC 2440. Yet another is the GNU Privacy Guard. 

PGP intentionally uses existing cryptographic algorithms rather than inventing new ones. It is largely based on algorithms that have withstood extensive peer review and were not designed or influenced by any government agency trying to weaken them. For people who tend to distrust government, this property is a big plus. 

PGP supports text compression, secrecy, and digital signatures and also provides extensive key management facilities, but oddly enough, not e-mail facilities. It is more of a preprocessor that takes plaintext as input and produces signed ciphertext in base64 as output. This output can then be e-mailed, of course. Some PGP implementations call a user agent as the final step to actually send the message. 

To see how PGP works, let us consider the example of 

Fig. 8-44

. Here, Alice wants to send a signed plaintext message, 

P

, to Bob in a secure way. Both Alice and Bob have private (

D

X

) and public (

E

X

) RSA keys. Let us assume that each one knows the other's public key; we will cover PGP key management shortly. 

618




Figure 8-44. PGP in operation for sending a message. 

 

Alice starts out by invoking the PGP program on her computer. PGP first hashes her message, 

P

, using MD5, and then encrypts the resulting hash using her private RSA key, 

D

A

. When Bob eventually gets the message, he can decrypt the hash with Alice's public key and verify that the hash is correct. Even if someone else (e.g., Trudy) could acquire the hash at this stage and decrypt it with Alice's known public key, the strength of MD5 guarantees that it would be computationally infeasible to produce another message with the same MD5 hash. 

The encrypted hash and the original message are now concatenated into a single message, 

P1

, and compressed using the ZIP program, which uses the Ziv-Lempel algorithm (Ziv and Lempel, 1977). Call the output of this step 

P1.Z

. 

Next, PGP prompts Alice for some random input. Both the content and the typing speed are used to generate a 128-bit IDEA message key, 

K

M

 (called a session key in the PGP literature, but this is really a misnomer since there is no session). 

K

M

 is now used to encrypt 

P1.Z

 with IDEA in cipher feedback mode. In addition, 

K

M

 is encrypted with Bob's public key, 

E

B

. These two components are then concatenated and converted to base64, as we discussed in the section on MIME in 

Chap. 7

. The resulting message then contains only letters, digits, and the symbols +, /, and =, which means it can be put into an RFC 822 body and be expected to arrive unmodified. 

When Bob gets the message, he reverses the base64 encoding and decrypts the IDEA key using his private RSA key. Using this key, he decrypts the message to get 

P1.Z

. After decompressing it, Bob separates the plaintext from the encrypted hash and decrypts the hash using Alice's public key. If the plaintext hash agrees with his own MD5 computation, he knows that 

P

 is the correct message and that it came from Alice. 

It is worth noting that RSA is only used in two places here: to encrypt the 128-bit MD5 hash and to encrypt the 128-bit IDEA key. Although RSA is slow, it has to encrypt only 256 bits, not a large volume of data. Furthermore, all 256 plaintext bits are exceedingly random, so a considerable amount of work will be required on Trudy's part just to determine if a guessed key is correct. The heavyduty encryption is done by IDEA, which is orders of magnitude faster than RSA. Thus, PGP provides security, compression, and a digital signature and does so in a much more efficient way than the scheme illustrated in 

Fig. 8-19

. 

PGP supports four RSA key lengths. It is up to the user to select the one that is most appropriate. The lengths are 

1. Casual (384 bits): can be broken easily today. 

2. Commercial (512 bits): breakable by three-letter organizations. 

3. Military (1024 bits): Not breakable by anyone on earth. 

619




4. Alien (2048 bits): Not breakable by anyone on other planets, either. 

Since RSA is only used for two small computations, everyone should use alien strength keys all the time. 

The format of a classic PGP message is shown in 

Fig. 8-45

. Numerous other formats are also in use. The message has three parts, containing the IDEA key, the signature, and the message, respectively. The key part contains not only the key, but also a key identifier, since users are permitted to have multiple public keys. 

Figure 8-45. A PGP message. 

 

The signature part contains a header, which will not concern us here. The header is followed by a timestamp, the identifier for the sender's public key that can be used to decrypt the signature hash, some type information that identifies the algorithms used (to allow MD6 and RSA2 to be used when they are invented), and the encrypted hash itself. 

The message part also contains a header, the default name of the file to be used if the receiver writes the file to the disk, a message creation timestamp, and, finally, the message itself. 

Key management has received a large amount of attention in PGP as it is the Achilles heel of all security systems. Key management works as follows. Each user maintains two data structures locally: a private key ring and a public key ring. The 

private key ring

 contains one or more personal private-public key pairs. The reason for supporting multiple pairs per user is to permit users to change their public keys periodically or when one is thought to have been compromised, without invalidating messages currently in preparation or in transit. Each pair has an identifier associated with it so that a message sender can tell the recipient which public key was used to encrypt it. Message identifiers consist of the low-order 64 bits of the public key. Users are responsible for avoiding conflicts in their public key identifiers. The private keys on disk are encrypted using a special (arbitrarily long) password to protect them against sneak attacks. 

The 

public key ring

 contains public keys of the user's correspondents. These are needed to encrypt the message keys associated with each message. Each entry on the public key ring contains not only the public key, but also its 64-bit identifier and an indication of how strongly the user trusts the key. 

The problem being tackled here is the following. Suppose that public keys are maintained on bulletin boards. One way for Trudy to read Bob's secret e-mail is to attack the bulletin board and replace Bob's public key with one of her choice. When Alice later fetches the key allegedly belonging to Bob, Trudy can mount a bucket brigade attack on Bob. 

To prevent such attacks, or at least minimize the consequences of them, Alice needs to know how much to trust the item called ''Bob's key'' on her public key ring. If she knows that Bob personally handed her a floppy disk containing the key, she can set the trust value to the 

620




highest value. It is this decentralized, user-controlled approach to public-key management that sets PGP apart from centralized PKI schemes. 

Nevertheless, people do sometimes obtain public keys by querying a trusted key server. For this reason, after X.509 was standardized, PGP supported these certificates as well as the traditional PGP public key ring mechanism. All current versions of PGP have X.509 support. 

8.8.2 PEM—Privacy Enhanced Mail 

In contrast to PGP, which was initially a one-man show, our second example, 

PEM

 (

Privacy Enhanced Mail

), developed in the late 1980s, is an official Internet standard and described in four RFCs: RFC 1421 through RFC 1424. Very roughly, PEM covers the same territory as PGP: privacy and authentication for RFC 822-based e-mail systems. Nevertheless, it also has some differences from PGP in approach and technology. 

Messages sent using PEM are first converted to a canonical form so they all have the same conventions about white space (e.g., tabs, trailing spaces). Next, a message hash is computed using MD2 or MD5. Then the concatenation of the hash and the message is encrypted using DES. In light of the known weakness of a 56-bit key, this choice is certainly suspect. The encrypted message can then be encoded with base64 coding and transmitted to the recipient. 

As in PGP, each message is encrypted with a one-time key that is enclosed along with the message. The key can be protected either with RSA or with triple DES using EDE. 

Key management is more structured than in PGP. Keys are certified by X.509 certificates issued by CAs, which are arranged in a rigid hierarchy starting at a single root. The advantage of this scheme is that certificate revocation is possible by having the root issue CRLs periodically. 

The only problem with PEM is that nobody ever used it and it has long-since gone to that big bit bin in the sky. The problem was largely political: who would operate the root and under what conditions? There was no shortage of candidates, but many people were afraid to trust any one company with the security of the whole system. The most serious candidate, RSA Security, Inc., wanted to charge per certificate issued. However, some organizations balked at this idea. In particular, the U.S. Government is allowed to use all U.S. patents for free, and companies outside the U.S. had become accustomed to using the RSA algorithm for free (the company forgot to patent it outside the U.S.). Neither was enthusiastic about suddenly having to pay RSA Security, Inc., for doing something that they had always done for free. In the end, no root could be found and PEM collapsed. 

8.8.3 S/MIME 

IETF's next venture into e-mail security, called 

S/MIME

 (

Secure/MIME

), is described in RFCs 2632 through 2643. Like PEM, it provides authentication, data integrity, secrecy, and nonrepudiation. It also is quite flexible, supporting a variety of cryptographic algorithms. Not surprisingly, given the name, S/MIME integrates well with MIME, allowing all kinds of messages to be protected. A variety of new MIME headers are defined, for example, for holding digital signatures. 

IETF definitely learned something from the PEM experience. S/MIME does not have a rigid certificate hierarchy beginning at a single root. Instead, users can have multiple trust anchors. As long as a certificate can be traced back to some trust anchor the user believes in, it is considered valid. S/MIME uses the standard algorithms and protocols we have been examining so far, so we will not discuss it any further here. For the details, please consult the RFCs. 

621




8.9 Web Security 

We have just studied two important areas where security is needed: communications and e-mail. You can think of these as the soup and appetizer. Now it is time for the main course: Web security. The Web is where most of the Trudies hang out nowadays and do their dirty work. In the following sections we will look at some of the problems and issues relating to Web security. 

Web security can be roughly divided into three parts. First, how are objects and resources named securely? Second, how can secure, authenticated connections be established? Third, what happens when a Web site sends a client a piece of executable code? After looking at some threats, we will examine all these issues. 

8.9.1 Threats 

One reads about Web site security problems in the newspaper almost weekly. The situation is really pretty grim. Let us look at a few examples of what has already happened. First, the home page of numerous organizations has been attacked and replaced by a new home page of the crackers' choosing. (The popular press calls people who break into computers ''hackers,'' but many programmers reserve that term for great programmers. We prefer to call these people ''crackers.'') Sites that have been cracked include Yahoo, the U.S. Army, the CIA, NASA, and the New York Times. In most cases, the crackers just put up some funny text and the sites were repaired within a few hours. 

Now let us look at some much more serious cases. Numerous sites have been brought down by denial-of-service attacks, in which the cracker floods the site with traffic, rendering it unable to respond to legitimate queries. Often the attack is mounted from a large number of machines that the cracker has already broken into (DDoS atacks). These attacks are so common that they do not even make the news any more, but they can cost the attacked site thousands of dollars in lost business. 

In 1999, a Swedish cracker broke into Microsoft's Hotmail Web site and created a mirror site that allowed anyone to type in the name of a Hotmail user and then read all of the person's current and archived e-mail. 

In another case, a 19-year-old Russian cracker named Maxim broke into an e-commerce Web site and stole 300,000 credit card numbers. Then he approached the site owners and told them that if they did not pay him $100,000, he would post all the credit card numbers to the Internet. They did not give in to his blackmail, and he indeed posted the credit card numbers, inflicting great damage to many innocent victims. 

In a different vein, a 23-year-old California student e-mailed a press release to a news agency falsely stating that the Emulex Corporation was going to post a large quarterly loss and that the C.E.O. was resigning immediately. Within hours, the company's stock dropped by 60%, causing stockholders to lose over $2 billion. The perpetrator made a quarter of a million dollars by selling the stock short just before sending the announcement. While this event was not a Web site break-in, it is clear that putting such an announcement on the home page of any big corporation would have a similar effect. 

We could (unfortunately) go on like this for many pages. But it is now time to examine some of the technical issues related to Web security. For more information about security problems of all kinds, see (Anderson, 2001; Garfinkel with Spafford, 2002; and Schneier, 2000). Searching the Internet will also turn up vast numbers of specific cases. 

622




8.9.2 Secure Naming 

Let us start with something very basic: Alice wants to visit Bob's Web site. She types Bob's URL into her browser and a few seconds later, a Web page appears. But is it Bob's? Maybe yes and maybe no. Trudy might be up to her old tricks again. For example, she might be intercepting all of Alice's outgoing packets and examining them. When she captures an HTTP 

GET

 request headed to Bob's Web site, she could go to Bob's Web site herself to get the page, modify it as she wishes, and return the fake page to Alice. Alice would be none the wiser. Worse yet, Trudy could slash the prices at Bob's e-store to make his goods look very attractive, thereby tricking Alice into sending her credit card number to ''Bob'' to buy some merchandise. 

One disadvantage to this classic man-in-the-middle attack is that Trudy has to be in a position to intercept Alice's outgoing traffic and forge her incoming traffic. In practice, she has to tap either Alice's phone line or Bob's, since tapping the fiber backbone is fairly difficult. While active wiretapping is certainly possible, it is a certain amount of work, and while Trudy is clever, she is also lazy. Besides, there are easier ways to trick Alice. 

DNS Spoofing 

For example, suppose Trudy is able to crack the DNS system, maybe just the DNS cache at Alice's ISP, and replace Bob's IP address (say, 36.1.2.3) with her (Trudy's) IP address (say, 42.9.9.9). That leads to the following attack. The way it is supposed to work is illustrated in 

Fig. 8-46(a)

. Here (1) Alice asks DNS for Bob's IP address, (2) gets it, (3) asks Bob for his home page, and (4) gets that, too. After Trudy has modified Bob's DNS record to contain her own IP address instead of Bob's, we get the situation of 

Fig. 8-46(b)

. Here, when Alice looks up Bob's IP address, she gets Trudy's, so all her traffic intended for Bob goes to Trudy. Trudy can now mount a man-in-the-middle attack without having to go to the trouble of tapping any phone lines. Instead, she has to break into a DNS server and change one record, a much easier proposition. 

Figure 8-46. (a) Normal situation. (b) An attack based on breaking into DNS and modifying Bob's record. 

 

How might Trudy fool DNS? It turns out to be relatively easy. Briefly summarized, Trudy can trick the DNS server at Alice's ISP into sending out a query to look up Bob's address. Unfortunately, since DNS uses UDP, the DNS server has no real way of checking who supplied the answer. Trudy can exploit this property by forging the expected reply and thus injecting a 

623




false IP address into the DNS server's cache. For simplicity, we will assume that Alice's ISP does not initially have an entry for Bob's Web site, 

bob.com

. If it does, Trudy can wait until it times out and try later (or use other tricks). 

Trudy starts the attack by sending a lookup request to Alice's ISP asking for the IP address of 

bob.com

. Since there is no entry for this DNS name, the cache server queries the top-level server for the 

com

 domain to get one. However, Trudy beats the 

com

 server to the punch and sends back a false reply saying: ''

bob.com

 is 42.9.9.9,'' where that IP address is hers. If her false reply gets back to Alice's ISP first, that one will be cached and the real reply will be rejected as an unsolicited reply to a query no longer outstanding. Tricking a DNS server into installing a false IP address is called 

DNS spoofing

. A cache that holds an intentionally false IP address like this is called a 

poisoned cache

. 

Actually, things are not quite that simple. First, Alice's ISP checks to see that the reply bears the correct IP source address of the top-level server. But since Trudy can put anything she wants in that IP field, she can defeat that test easily since the IP addresses of the top-level servers have to be public. 

Second, to allow DNS servers to tell which reply goes with which request, all requests carry a sequence number. To spoof Alice's ISP, Trudy has to know its current sequence number. The easiest way to learn the current sequence number is for Trudy to register a domain herself, say, 

trudy-the-intruder.com

. Let us assume its IP address is also 42.9.9.9. She also creates a DNS server for her newly-hatched domain, 

dns.trudy-the-intruder.com

. It, too, uses Trudy's 42.9.9.9 IP address, since Trudy has only one computer. Now she has to make Alice's ISP aware of her DNS server. That is easy to do. All she has to do is ask Alice's ISP for 

foobar.trudy-the-intruder.com

, which will cause Alice's ISP to find out who serves Trudy's new domain by asking the top-level 

com

 server. 

With 

dns.trudy-the-intruder.com

 safely in the cache at Alice's ISP, the real attack can start. Trudy now queries Alice's ISP for 

www.trudy-the-intruder.com

. The ISP naturally sends Trudy's DNS server a query asking for it. This query bears the sequence number that Trudy is looking for. Quick like a bunny, Trudy asks Alice's ISP to look up Bob. She immediately answers her own question by sending the ISP a forged reply, allegedly from the top-level 

com

 server saying: ''

bob.com

 is 42.9.9.9''. This forged reply carries a sequence number one higher than the one she just received. While she is at it, she can also send a second forgery with a sequence number two higher, and maybe a dozen more with increasing sequence numbers. One of them is bound to match. The rest will just be thrown out. When Alice's forged reply arrives, it is cached; when the real reply comes in later, it is rejected since no query is then outstanding. 

Now when Alice looks up 

bob.com

, she is told to use 42.9.9.9, Trudy's address. Trudy has mounted a successful man-in-the-middle attack from the comfort of her own living room. The various steps to this attack are illustrated in 

Fig. 8-47

. To make matters worse, this is not the only way to spoof DNS. There are many other ways as well. 

Figure 8-47. How Trudy spoofs Alice's ISP. 

624




 

Secure DNS 

This one specific attack can be foiled by having DNS servers use random IDs in their queries rather than just counting, but it seems that every time one hole is plugged, another one turns up. The real problem is that DNS was designed at a time when the Internet was a research facility for a few hundred universities and neither Alice, nor Bob, nor Trudy was invited to the party. Security was not an issue then; making the Internet work at all was the issue. The environment has changed radically over the years, so in 1994 IETF set up a working group to make DNS fundamentally secure. This project is known as 

DNSsec

 (

DNS security

); its output is presented in RFC 2535. Unfortunately, DNSsec has not been fully deployed yet, so numerous DNS servers are still vulnerable to spoofing attacks. 

DNSsec is conceptually extremely simple. It is based on public-key cryptography. Every DNS zone (in the sense of 

Fig. 7-4

) has a public/private key pair. All information sent by a DNS server is signed with the originating zone's private key, so the receiver can verify its authenticity. 

DNSsec offers three fundamental services: 

1. Proof of where the data originated. 

2. Public key distribution. 

3. Transaction and request authentication. 

The main service is the first one, which verifies that the data being returned has been approved by the zone's owner. The second one is useful for storing and retrieving public keys securely. The third one is needed to guard against playback and spoofing attacks. Note that secrecy is not an offered service since all the information in DNS is considered public. Since phasing in DNSsec is expected to take several years, the ability for security-aware servers to interwork with security-ignorant servers is essential, which implies that the protocol cannot be changed. Let us now look at some of the details. 

DNS records are grouped into sets called 

RRSets

 (

Resource Record Sets

), with all the records having the same name, class and type being lumped together in a set. An RRSet may contain multiple 

A

 records, for example, if a DNS name resolves to a primary IP address and a secondary IP address. The RRSets are extended with several new record types (discussed below). Each RRSet is cryptographically hashed (e.g., using MD5 or SHA-1). The hash is signed by the zone's private key (e.g., using RSA). The unit of transmission to clients is the signed RRSet. Upon receipt of a signed RRSet, the client can verify whether it was signed by the private key of the originating zone. If the signature agrees, the data are accepted. Since each RRSet contains its own signature, RRSets can be cached anywhere, even at untrustworthy servers, without endangering the security. 

DNSsec introduces several new record types. The first of these is the 

KEY

 record. This records holds the public key of a zone, user, host, or other principal, the cryptographic algorithm used 

625




for signing, the protocol used for transmission, and a few other bits. The public key is stored naked. X.509 certificates are not used due to their bulk. The algorithm field holds a 1 for MD5/RSA signatures (the preferred choice), and other values for other combinations. The protocol field can indicate the use of IPsec or other security protocols, if any. 

The second new record type is the 

SIG

 record. It holds the signed hash according to the algorithm specified in the 

KEY

 record The signature applies to all the records in the RRSet, including any 

KEY

 records present, but excluding itself. It also holds the times when the signature begins its period of validity and when it expires, as well as the signer's name and a few other items. 

The DNSsec design is such that a zone's private key can be kept off-line. Once or twice a day, the contents of a zone's database can be manually transported (e.g., on CD-ROM) to a disconnected machine on which the private key is located. All the RRSets can be signed there and the 

SIG

 records thus produced can be conveyed back to the zone's primary server on CD-ROM. In this way, the private key can be stored on a CD-ROM locked in a safe except when it is inserted into the disconnected machine for signing the day's new RRSets. After signing is completed, all copies of the key are erased from memory and the disk and the CD-ROM are returned to the safe. This procedure reduces electronic security to physical security, something people understand how to deal with. 

This method of presigning RRSets greatly speeds up the process of answering queries since no cryptography has to be done on the fly. The trade-off is that a large amount of disk space is needed to store all the keys and signatures in the DNS databases. Some records will increase tenfold in size due to the signature. 

When a client process gets a signed RRSet, it must apply the originating zone's public key to decrypt the hash, compute the hash itself, and compare the two values. If they agree, the data are considered valid. However, this procedure begs the question of how the client gets the zone's public key. One way is to acquire it from a trusted server, using a secure connection (e.g., using IPsec). 

However, in practice, it is expected that clients will be preconfigured with the public keys of all the top-level domains. If Alice now wants to visit Bob's Web site, she can ask DNS for the RRSet of 

bob.com

, which will contain his IP address and a 

KEY

 record containing Bob's public key. This RRSet will be signed by the top-level 

com

 domain, so Alice can easily verify its validity. An example of what this RRSet might contain is shown in 

Fig. 8-48

. 

Figure 8-48. An example RRSet for 

bob.com

. The 

KEY

 record is Bob's public key. The 

SIG

 record is the top-level 

com

 server's signed hash of the 

A

 and 

KEY

 records to verify their authenticity. 

 

Now armed with a verified copy of Bob's public key, Alice can ask Bob's DNS server (run by Bob) for the IP address of 

www.bob.com

. This RRSet will be signed by Bob's private key, so Alice can verify the signature on the RRSet Bob returns. If Trudy somehow manages to inject a false RRSet into any of the caches, Alice can easily detect its lack of authenticity because the 

SIG

 record contained in it will be incorrect. 

However, DNSsec also provides a cryptographic mechanism to bind a response to a specific query, to prevent the kind of spoof Trudy managed to pull off in 

Fig. 8-47

. This (optional) 

626




antispoofing measure adds to the response a hash of the query message signed with the respondent's private key. Since Trudy does not know the private key of the top-level 

com

 server, she cannot forge a response to a query Alice's ISP sent there. She can certainly get her response back first, but it will be rejected due to its invalid signature over the hashed query. 

DNSsec also supports a few other record types. For example, the 

CERT

 record can be used for storing (e.g., X.509) certificates. This record has been provided because some people want to turn DNS into a PKI. Whether this actually happens remains to be seen. We will stop our discussion of DNSsec here. For more details, please consult RFC 2535. 

Self-Certifying Names 

Secure DNS is not the only possibility for securing names. A completely different approach is used in the 

Secure File System

 (Mazières et al., 1999). In this project, the authors designed a secure, scalable, worldwide file system, without modifying (standard) DNS and without using certificates or assuming the existence of a PKI. In this section we will show how their ideas could be applied to the Web. Accordingly, in the description below, we will use Web terminology rather than the file system terminology used in the paper. But to avoid any possible confusion, while this scheme 

could

 be applied to the Web to achieve high security, it is not currently in use and would require substantial software changes to introduce it. 

We start out by assuming that each Web server has a public/private key pair. The essence of the idea is that each URL contains a cryptographic hash of the server's name and public key as part of the URL. For example, in 

Fig. 8-49

 we see the URL for Bob's photo. It starts out with the usual 

http

 scheme followed by the DNS name of the server (

www.bob.com

). Then comes a colon and 32-character hash. At the end is the name of the file, again as usual. Except for the hash, this is a standard URL. With the hash, it is a 

self-certifying URL

. 

Figure 8-49. A self-certifying URL containing a hash of server's name and public key. 

 

The obvious question is: What is the hash for? The hash is computed by concatenating the DNS name of the server with the server's public key and running the result through the SHA-1 function to get a 160-bit hash. In this scheme, the hash is represented as a sequence of 32 digits and lower-case letters, with the exception of the letters ''l'' and ''o'' and the digits ''1'' and ''0'', to avoid confusion. This leaves 32 digits and letters over. With 32 characters available, each one can encode a 5-bit string. A string of 32 characters can hold the 160-bit SHA-1 hash. Actually, it is not necessary to use a hash; the key itself could be used. The advantage of the hash is to reduce the length of the name. 

In the simplest (but least convenient) way to see Bob's photo, Alice just types the string of 

Fig. 

8-49

 to her browser. The browser sends a message to Bob's Web site asking him for his public key. When Bob's public key arrives, the browser concatenates the server name and public key and runs the hash algorithm. If the result agrees with the 32-character hash in the secure URL, the browser is sure it has Bob's public key. After all, due to the properties of SHA-1, even if Trudy intercepts the request and forges the reply, she has no way to find a public key that gives the expected hash. Any interference from her will thus be detected. Bob's public key can be cached for future use. 

Now Alice has to verify that Bob has the corresponding private key. She constructs a message containing a proposed AES session key, a nonce, and a timestamp. She then encrypts the message with Bob's public key and sends it to him. Since only Bob has the corresponding private key, only Bob is able to decrypt the message and send back the nonce encrypted with 

627




the AES key. Upon receiving the correct AES-encrypted nonce, Alice knows she is talking to Bob. Also, Alice and Bob now have an AES session key for subsequent 

GET

 requests and replies. 

Once Alice has Bob's photo (or any Web page), she can bookmark it, so she does not have to type in the full URL again. Furthermore, the URLs embedded in Web pages can also be self certifying, so they can be used by just clicking on them, but with the additional security of knowing that the page returned is the correct one. Other ways to avoid the initial typing of the self-certifying URLs are to get them over a secure connection to a trusted server or have them present in X.509 certificates signed by CAs. 

Another way to get self-certifying URLs would be to connect to a trusted search engine by typing in its self-certifying URL (the first time) and going through the same protocol as described above, leading to a secure, authenticated connection to the trusted search engine. The search engine could then be queried, with the results appearing on a signed page full of self-certifying URLs that could be clicked on without having to type in long strings. 

Let us now see how well this approach stands up to Trudy's DNS spoofing. If Trudy manages to poison the cache of Alice's ISP, Alice's request may be falsely delivered to Trudy rather than to Bob. But the protocol now requires the recipient of an initial message (i.e., Trudy) to return a public key that produces the correct hash. If Trudy returns her own public key, Alice will detect it immediately because the SHA-1 hash will not match the self-certifying URL. If Trudy returns Bob's public key, Alice will not detect the attack, but Alice will encrypt her next message, using Bob's key. Trudy will get the message, but she will have no way to decrypt it to extract the AES key and nonce. Either way, all spoofing DNS can do is provide a denial-of-service attack. 

8.9.3 SSL—The Secure Sockets Layer 

Secure naming is a good start, but there is much more to Web security. The next step is secure connections. We will now look at how secure connections can be achieved. 

When the Web burst into public view, it was initially used for just distributing static pages. However, before long, some companies got the idea of using it for financial transactions, such as purchasing merchandise by credit card, on-line banking, and electronic stock trading. These applications created a demand for secure connections. In 1995, Netscape Communications Corp, the then-dominant browser vendor, responded by introducing a security package called 

SSL

 (

Secure Sockets Layer

) to meet this demand. This software and its protocol is now widely used, also by Internet Explorer, so it is worth examining in some detail. 

SSL builds a secure connection between two sockets, including 

1. Parameter negotiation between client and server. 

2. Mutual authentication of client and server. 

3. Secret communication. 

4. Data integrity protection. 

We have seen these items before, so there is no need to elaborate on them. 

The positioning of SSL in the usual protocol stack is illustrated in 

Fig. 8-50

. Effectively, it is a new layer interposed between the application layer and the transport layer, accepting requests from the browser and sending them down to TCP for transmission to the server. Once the secure connection has been established, SSL's main job is handling compression and encryption. When HTTP is used over SSL, it is called 

HTTPS

 (

Secure HTTP

), even though it is the standard HTTP protocol. Sometimes it is available at a new port (443) instead of the standard port (80), though. As an aside, SSL is not restricted to being used only with Web browsers, but that is its most common application. 

628




Figure 8-50. Layers (and protocols) for a home user browsing with SSL. 

 

The SSL protocol has gone through several versions. Below we will discuss only version 3, which is the most widely used version. SSL supports a variety of different algorithms and options. These options include the presence or absence of compression, the cryptographic algorithms to be used, and some matters relating to export restrictions on cryptography. The last is mainly intended to make sure that serious cryptography is used only when both ends of the connection are in the United States. In other cases, keys are limited to 40 bits, which cryptographers regard as something of a joke. Netscape was forced to put in this restriction in order to get an export license from the U.S. Government. 

SSL consists of two subprotocols, one for establishing a secure connection and one for using it. Let us start out by seeing how secure connections are established. The connection establishment subprotocol is shown in 

Fig. 8-51

. It starts out with message 1 when Alice sends a request to Bob to establish a connection. The request specifies the SSL version Alice has and her preferences with respect to compression and cryptographic algorithms. It also contains a nonce, 

R

A

, to be used later. 

Figure 8-51. A simplified version of the SSL connection establishment subprotocol. 

 

Now it is Bob's turn. In message 2, Bob makes a choice among the various algorithms that Alice can support and sends his own nonce, 

R

B

. Then in message 3, he sends a certificate containing his public key. If this certificate is not signed by some well-known authority, he also sends a chain of certificates that can be followed back to one. All browsers, including Alice's, come preloaded with about 100 public keys, so if Bob can establish a chain anchored at one of these, Alice will be able to verify Bob's public key. At this point Bob may send some other 

629




messages (such as a request for Alice's public-key certificate). When Bob is done, he sends message 4 to tell Alice it is her turn. 

Alice responds by choosing a random 384-bit 

premaster key

 and sending it to Bob encrypted with his public key (message 5). The actual session key used for encrypting data is derived from the premaster key combined with both nonces in a complex way. After message 5 has been received, both Alice and Bob are able to compute the session key. For this reason, Alice tells Bob to switch to the new cipher (message 6) and also that she is finished with the establishment subprotocol (message 7). Bob then acknowledges her (messages 8 and 9). 

However, although Alice knows who Bob is, Bob does not know who Alice is (unless Alice has a public key and a corresponding certificate for it, an unlikely situation for an individual). Therefore, Bob's first message may well be a request for Alice to log in using a previously established login name and password. The login protocol, however, is outside the scope of SSL. Once it has been accomplished, by whatever means, data transport can begin. 

As mentioned above, SSL supports multiple cryptographic algorithms. The strongest one uses triple DES with three separate keys for encryption and SHA-1 for message integrity. This combination is relatively slow, so it is mostly used for banking and other applications in which the highest security is required. For ordinary e-commerce applications, RC4 is used with a 128-bit key for encryption and MD5 is used for message authentication. RC4 takes the 128-bit key as a seed and expands it to a much larger number for internal use. Then it uses this internal number to generate a keystream. The keystream is XORed with the plaintext to provide a classical stream cipher, as we saw in 

Fig. 8-14

. The export versions also use RC4 with 128-bit keys, but 88 of the bits are made public to make the cipher easy to break. 

For actual transport, a second subprotocol is used, as shown in 

Fig. 8-52

. Messages from the browser are first broken into units of up to 16 KB. If compression is enabled, each unit is then separately compressed. After that, a secret key derived from the two nonces and premaster key is concatenated with the compressed text and the result hashed with the agreed-on hashing algorithm (usually MD5). This hash is appended to each fragment as the MAC. The compressed fragment plus MAC is then encrypted with the agreed-on symmetric encryption algorithm (usually by XORing it with the RC4 keystream). Finally, a fragment header is attached and the fragment is transmitted over the TCP connection. 

Figure 8-52. Data transmission using SSL. 

 

A word of caution is in order, however. Since it has been shown that RC4 has some weak keys that can be easily cryptanalyzed, the security of SSL using RC4 is on shaky ground (Fluhrer et al., 2001). Browsers that allow the user to choose the cipher suite should be configured to use 

630




triple DES with 168-bit keys and SHA-1 all the time, even though this combination is slower than RC4 and MD5. 

Another problem with SSL is that the principals may not have certificates and even if they do, they do not always verify that the keys being used match them. 

In 1996, Netscape Communications Corp. turned SSL over to IETF for standardization. The result was 

TLS

 (

Transport Layer Security

). It is described in RFC 2246. 

The changes made to SSL were relatively small, but just enough that SSL version 3 and TLS cannot interoperate. For example, the way the session key is derived from the premaster key and nonces was changed to make the key stronger (i.e., harder to cryptanalyze). The TLS version is also known as SSL version 3.1. The first implementations appeared in 1999, but it is not clear yet whether TLS will replace SSL in practice, even though it is slightly stronger. The problem with weak RC4 keys remains, however. 

8.9.4 Mobile Code Security 

Naming and connections are two areas of concern related to Web security. But there are more. In the early days, when Web pages were just static HTML files, they did not contain executable code. Now they often contain small programs, including Java applets, ActiveX controls, and JavaScripts. Downloading and executing such 

mobile code

 is obviously a massive security risk, so various methods have been devised to minimize it. We will now take a quick peek at some of the issues raised by mobile code and some approaches to dealing with it. 

Java Applet Security 

Java applets are small Java programs compiled to a stack-oriented machine language called 

JVM

 (

Java Virtual Machine

). They can be placed on a Web page for downloading along with the page. After the page is loaded, the applets are inserted into a JVM interpreter inside the browser, as illustrated in 

Fig. 8-53

. 

Figure 8-53. Applets can be interpreted by a Web browser. 

 

The advantage of running interpreted code over compiled code is that every instruction is examined by the interpreter before being executed. This gives the interpreter the opportunity to check whether the instruction's address is valid. In addition, system calls are also caught and interpreted. How these calls are handled is a matter of the security policy. For example, if an applet is trusted (e.g., it came from the local disk), its system calls could be carried out without question. However, if an applet is not trusted (e.g., it came in over the Internet), it could be encapsulated in what is called a 

sandbox

 to restrict its behavior and trap its attempts to use system resources. 

When an applet tries to use a system resource, its call is passed to a security monitor for approval. The monitor examines the call in light of the local security policy and then makes a 

631




decision to allow or reject it. In this way, it is possible to give applets access to some resources but not all. Unfortunately, the reality is that the security model works badly and that bugs in it crop up all the time. 

ActiveX 

ActiveX controls are Pentium binary programs that can be embedded in Web pages. When one of them is encountered, a check is made to see if it should be executed, and it if passes the test, it is executed. It is not interpreted or sandboxed in any way, so it has as much power as any other user program and can potentially do great harm. Thus, all the security is in the decision whether to run the ActiveX control. 

The method that Microsoft chose for making this decision is based on the idea of 

code signing

. Each ActiveX control is accompanied by a digital signature—a hash of the code that is signed by its creator using public key cryptography. When an ActiveX control shows up, the browser first verifies the signature to make sure it has not been tampered with in transit. If the signature is correct, the browser then checks its internal tables to see if the program's creator is trusted or there is a chain of trust back to a trusted creator. If the creator is trusted, the program is executed; otherwise, it is not. The Microsoft system for verifying ActiveX controls is called 

Authenticode

. 

It is useful to contrast the Java and ActiveX approaches. With the Java approach, no attempt is made to determine who wrote the applet. Instead, a run-time interpreter makes sure it does not do things the machine owner has said applets may not do. In contrast, with code signing, there is no attempt to monitor the mobile code's run-time behavior. If it came from a trusted source and has not been modified in transit, it just runs. No attempt is made to see whether the code is malicious or not. If the original programmer 

intended

 the code to format the hard disk and then erase the flash ROM so the computer can never again be booted, and if the programmer has been certified as trusted, the code will be run and destroy the computer (unless ActiveX controls have been disabled in the browser). 

Many people feel that trusting an unknown software company is scary. To demonstrate the problem, a programmer in Seattle formed a software company and got it certified as trustworthy, which is easy to do. He then wrote an ActiveX control that did a clean shutdown of the machine and distributed his ActiveX control widely. It shut down many machines, but they could just be rebooted, so no harm was done. He was just trying to expose the problem to the world. The official response was to revoke the certificate for this specific ActiveX control, which ended a short episode of acute embarrassment, but the underlying problem is still there for an evil programmer to exploit (Garfinkel with Spafford, 2002). Since there is no way to police thousands of software companies that might write mobile code, the technique of code signing is a disaster waiting to happen. 

JavaScript 

JavaScript does not have any formal security model, but it does have a long history of leaky implementations. Each vendor handles security in a different way. For example, Netscape Navigator version 2 used something akin to the Java model, but by version 4 that had been abandoned for a code signing model. 

The fundamental problem is that letting foreign code run on your machine is asking for trouble. From a security standpoint, it is like inviting a burglar into your house and then trying to watch him carefully so he cannot escape from the kitchen into the living room. If something unexpected happens and you are distracted for a moment, bad things can happen. The tension here is that mobile code allows flashy graphics and fast interaction, and many Web site designers think that this is much more important than security, especially when it is somebody else's machine at risk. 

632




Viruses 

Viruses are another form of mobile code. Only unlike the examples above, viruses are not invited in at all. The difference between a virus and ordinary mobile code is that viruses are written to reproduce themselves. When a virus arrives, either via a Web page, an e-mail attachment, or some other way, it usually starts out by infecting executable programs on the disk. When one of these programs is run, control is transferred to the virus, which usually tries to spread itself to other machines, for example, by e-mailing copies of itself to everyone in the victim's e-mail address book. Some viruses infect the boot sector of the hard disk, so when the machine is booted, the virus gets to run. Viruses have become a huge problem on the Internet and have caused billions of dollars worth of damage. There is no obvious solution. Perhaps a whole new generation of operating systems based on secure microkernels and tight compartmentalization of users, processes, and resources might help. 

8.10 Social Issues 

The Internet and its security technology is an area where social issues, public policy, and technology meet head on, often with huge consequences. Below we will just briefly examine three areas: privacy, freedom of speech, and copyright. Needless to say, we can only scratch the surface here. For additional reading, see (Anderson, 2001; Garfinkel with Spafford, 2002; and Schneier, 2000). The Internet is also full of material. Just type words such as ''privacy,'' ''censorship,'' and ''copyright'' into any search engine. Also, see this book's Web site for some links. 

8.10.1 Privacy 

Do people have a right to privacy? Good question. The Fourth Amendment to the U.S. Constitution prohibits the government from searching people's houses, papers, and effects without good reason, and goes on to restrict the circumstances under which search warrants shall be issued. Thus, privacy has been on the public agenda for over 200 years, at least in the U.S. 

What has changed in the past decade is both the ease with which governments can spy on their citizens and the ease with which the citizens can prevent such spying. In the 18th century, for the government to search a citizen's papers, it had to send out a policeman on a horse to go to the citizen's farm demanding to see certain documents. It was a cumbersome procedure. Nowadays, telephone companies and Internet providers readily provide wiretaps when presented with search warrants. It makes life much easier for the policeman and there is no danger of falling off the horse. 

Cryptography changes all that. Anybody who goes to the trouble of downloading and installing PGP and who uses a well-guarded alien-strength key can be fairly sure that nobody in the known universe can read his e-mail, search warrant or no search warrant. Governments well understand this and do not like it. Real privacy means it is much harder for them to spy on criminals of all stripes, but it is also much harder to spy on journalists and political opponents. Consequently, some governments restrict or forbid the use or export of cryptography. In France, for example, prior to 1999, all cryptography was banned unless the government was given the keys. 

France was not alone. In April 1993, the U.S. Government announced its intention to make a hardware cryptoprocessor, the 

clipper chip

, the standard for all networked communication. In this way, it was said, citizens' privacy would be guaranteed. It also mentioned that the chip provided the government with the ability to decrypt all traffic via a scheme called 

key escrow

, which allowed the government access to all the keys. However, it promised only to snoop when it had a valid search warrant. Needless to say, a huge furor ensued, with privacy 

633




advocates denouncing the whole plan and law enforcement officials praising it. Eventually, the government backed down and dropped the idea. 

A large amount of information about electronic privacy is available at the Electronic Frontier Foundation's Web site, 

www.eff.org

. 

Anonymous Remailers 

PGP, SSL, and other technologies make it possible for two parties to establish secure, authenticated communication, free from third-party surveillance and interference. However, sometimes privacy is best served by 

not

 having authentication, in fact by making communication anonymous. The anonymity may be desired for point-to-point messages, newsgroups, or both. 

Let us consider some examples. First, political dissidents living under authoritarian regimes often wish to communicate anonymously to escape being jailed or killed. Second, wrongdoing in many corporate, educational, governmental, and other organizations has often been exposed by whistleblowers, who frequently prefer to remain anonymously to avoid retribution. Third, people with unpopular social, political, or religious views may wish to communicate with each other via e-mail or newsgroups without exposing themselves. Fourth, people may wish to discuss alcoholism, mental illness, sexual harassment, child abuse, or being a member of a persecuted minority in a newsgroup without having to go public. Numerous other examples exist, of course. 

Let us consider a specific example. In the 1990s, some critics of a nontraditional religious group posted their views to a USENET newsgroup via an 

anonymous remailer

. This server allowed users to create pseudonyms and send e-mail to the server, which then re-mailed or re-posted them using the pseudonym, so no one could tell where the message really came from. Some postings revealed what the religious group claimed were trade secrets and copyrighted documents. The religious group responded by telling local authorities that its trade secrets had been disclosed and its copyright infringed, both of which were crimes where the server was located. A court case followed and the server operator was compelled to turn over the mapping information which revealed the true identities of the persons who had made the postings. (Incidentally, this was not the first time that a religion was unhappy when someone leaked its secrets: William Tyndale was burned at the stake in 1536 for translating the Bible into English). 

A substantial segment of the Internet community was outraged by this breach of confidentiality. The conclusion that everyone drew is that an anonymous remailer that stores a mapping between real e-mail addresses and pseudonyms (called a type 1 remailer) is not worth much. This case stimulated various people into designing anonymous remailers that could withstand subpoena attacks. 

These new remailers, often called 

cypherpunk remailers

, work as follows. The user produces an e-mail message, complete with RFC 822 headers (except 

From:

, of course), encrypts it with the remailer's public key, and sends it to the remailer. There the outer RFC 822 headers are stripped off, the content is decrypted and the message is remailed. The remailer has no accounts and maintains no logs, so even if the server is later confiscated, it retains no trace of messages that have passed through it. 

Many users who wish anonymity chain their requests through multiple anonymous remailers, as shown in 

Fig. 8-54

. Here, Alice wants to send Bob a really, really, really anonymous Valentine's Day card, so she uses three remailers. She composes the message, 

M

, and puts a header on it containing Bob's e-mail address. Then she encrypts the whole thing with remailer 3's public key, 

E

3

. (indicated by horizontal hatching). To this she prepends a header with remailer 3's e-mail address in plaintext. This is the message shown between remailers 2 and 3 in the figure. 

634




Figure 8-54. How Alice uses 3 remailers to send Bob a message. 

 

Then she encrypts this message with remailer 2's public key, 

E

2

 (indicated by vertical hatching) and prepends a plaintext header containing remailer 2's e-mail address. This message is shown between 1 and 2 in 

Fig. 8-54

. Finally, she encrypts the entire message with remailer 1's public key, 

E

1

, and prepends a plaintext header with remailer 1's e-mail address. This is the message shown to the right of Alice in the figure and this is the message she actually transmits. 

When the message hits remailer 1, the outer header is stripped off. The body is decrypted and then e-mailed to remailer 2. Similar steps occur at the other two remailers. 

Although it is extremely difficult for anyone to trace the final message back to Alice, many remailers take additional safety precautions. For example, they may hold messages for a random time, add or remove junk at the end of a message, and reorder messages, all to make it harder for anyone to tell which message output by a remailer corresponds to which input, in order to thwart traffic analysis. For a description of a system that represents the state of the art in anonymous e-mail, see (Mazières and Kaashoek, 1998). 

Anonymity is not restricted to e-mail. Services also exist that allow anonymous Web surfing. The user configures his browser to use the anonymizer as a proxy. Henceforth, all HTTP requests go to the anonymizer, which requests the page and sends it back. The Web site sees the anonymizer as the source of the request, not the user. As long as the anonymizer refrains from keeping a log, after the fact no one can determine who requested which page. 

8.10.2 Freedom of Speech 

Privacy relates to individuals wanting to restrict what other people can see about them. A second key social issue is freedom of speech, and its opposite, censorship, which is about governments wanting to restrict what individuals can read and publish. With the Web containing millions and millions of pages, it has become a censor's paradise. Depending on the nature and ideology of the regime, banned material may include Web sites containing any of the following: 

1. Material inappropriate for children or teenagers. 

2. Hate aimed at various ethnic, religious, sexual or other groups. 

3. Information about democracy and democratic values. 

4. Accounts of historical events contradicting the government's version. 

5. Manuals for picking locks, building weapons, encrypting messages, etc. 

The usual response is to ban the bad sites. 

Sometimes the results are unexpected. For example, some public libraries have installed Web filters on their computers to make them child friendly by blocking pornography sites. The filters veto sites on their blacklists but also check pages for dirty words before displaying them. In 

635




one case in Loudoun County, Virginia, the filter blocked a patron's search for information on breast cancer because the filter saw the word ''breast.'' The library patron sued Loudoun county. However, in Livermore, California, a parent sued the public library for 

not

 installing a filter after her 12-year-old son was caught viewing pornography there. What's a library to do? 

It has escaped many people that the World Wide Web is a Worldwide Web. It covers the whole world. Not all countries agree on what should be allowed on the Web. For example, in November 2000, a French court ordered Yahoo, a California Corporation, to block French users from viewing auctions of Nazi memorabilia on Yahoo's Web site because owning such material violates French law. Yahoo appealed to a U.S. court, which sided with it, but the issue of whose laws apply where is far from settled. 

Just imagine. What would happen if some court in Utah instructed France to block Web sites dealing with wine because they do not comply with Utah's much stricter laws about alcohol? Suppose that China demanded that all Web sites dealing with democracy be banned as not in the interest of the State. Do Iranian laws on religion apply to more liberal Sweden? Can Saudi Arabia block Web sites dealing with women's rights? The whole issue is a veritable Pandora's box. 

A relevant comment from John Gilmore is: ''The net interprets censorship as damage and routes around it.'' For a concrete implementation, consider the 

eternity service

 (Anderson, 1996). Its goal is make sure published information cannot be depublished or rewritten, as was common in the Soviet Union during Josef Stalin's reign. To use the eternity service, the user specifies how long the material is to be preserved, pays a fee proportional to its duration and size, and uploads it. Thereafter, no one can remove or edit it, not even the uploader. 

How could such a service be implemented? The simplest model is to use a peer-to-peer system in which stored documents would be placed on dozens of participating servers, each of which gets a fraction of the fee, and thus an incentive to join the system. The servers should be spread over many legal jurisdictions for maximum resilience. Lists of 10 randomly-selected servers would be stored securely in multiple places, so that if some were compromised, others would still exist. An authority bent on destroying the document could never be sure it had found all copies. The system could also be made self-repairing in the sense that if it became known that some copies had been destroyed, the remaining sites would attempt to find new repositories to replace them. 

The eternity service was the first proposal for a censorship-resistant system. Since then, others have been proposed and, in some cases, implemented. Various new features have been added, such as encryption, anonymity, and fault tolerance. Often the files to be stored are broken up into multiple fragments, with each fragment stored on many servers. Some of these systems are Freenet (Clarke et al., 2002), PASIS (Wylie et al., 2000), and Publius (Waldman et al., 2000). Other work is reported in (Serjantov, 2002). 

Increasingly, many countries are now trying to regulate the export of intangibles, which often include Web sites, software, scientific papers, e-mail, telephone helpdesks, and more. Even the U.K., which has a centuries-long tradition of freedom of speech, is now seriously considering highly restrictive laws, which would, for example, define technical discussions between a British professor and his foreign student at the University of Cambridge as regulated export needing a government license (Anderson, 2002). Needless to say, such policies are controversial. 

Steganography 

In countries where censorship abounds, dissidents often try to use technology to evade it. Cryptography allows secret messages to be sent (although possibly not lawfully), but if the government thinks that Alice is a Bad Person, the mere fact that she is communicating with Bob may get him put in this category, too, as repressive governments understand the concept 

636




of transitive closure, even if they are short on mathematicians. Anonymous remailers can help, but if they are banned domestically and messages to foreign ones require a government export license, they cannot help much. But the Web can. 

People who want to communicate secretly often try to hide the fact that any communication at all is taking place. The science of hiding messages is called 

steganography

, from the Greek words for ''covered writing.'' In fact, the ancient Greeks used it themselves. Herodotus wrote of a general who shaved the head of a messenger, tattooed a message to his scalp, and let the hair grow back before sending him off. Modern techniques are conceptually the same, only they have a higher bandwidth and lower latency. 

As a case in point, consider 

Fig. 8-55(a)

. This photograph, taken by the author in Kenya, contains three zebras contemplating an acacia tree. 

Fig. 8-55(b)

 appears to be the same three zebras and acacia tree, but it has an extra added attraction. It contains the complete, unabridged text of five of Shakespeare's plays embedded in it: 

Hamlet

, 

King Lear

, 

Macbeth

, 

The Merchant of Venice

, and 

Julius Caesar

. Together, these plays total over 700 KB of text. 

Figure 8-55. (a) Three zebras and a tree. (b) Three zebras, a tree, and the complete text of five plays by William Shakespeare. 

 

How does this steganographic channel work? The original color image is 1024 x 768 pixels. Each pixel consists of three 8-bit numbers, one each for the red, green, and blue intensity of that pixel. The pixel's color is formed by the linear superposition of the three colors. The steganographic encoding method uses the low-order bit of each RGB color value as a covert channel. Thus, each pixel has room for 3 bits of secret information, one in the red value, one in the green value, and one in the blue value. With an image of this size, up to 1024 x 768 x 3 bits or 294,912 bytes of secret information can be stored in it. 

The full text of the five plays and a short notice add up to 734,891 bytes. This text was first compressed to about 274 KB using a standard compression algorithm. The compressed output was then encrypted using IDEA and inserted into the low-order bits of each color value. As can be seen (or actually, cannot be seen), the existence of the information is completely invisible. It is equally invisible in the large, full-color version of the photo. The eye cannot easily distinguish 21-bit color from 24-bit color. 

Viewing the two images in black and white with low resolution does not do justice to how powerful the technique is. To get a better feel for how steganography works, the author has prepared a demonstration, including the full-color high-resolution image of 

Fig. 8-55(b)

 with the five plays embedded in it. The demonstration, including tools for inserting and extracting text into images, can be found at the book's Web site. 

To use steganography for undetected communication, dissidents could create a Web site bursting with politically-correct pictures, such as photographs of the Great Leader, local sports, 

637




movie, and television stars, etc. Of course, the pictures would be riddled with steganographic messages. If the messages were first compressed and then encrypted, even someone who suspected their presence would have immense difficulty in distinguishing the messages from white noise. Of course, the images should be fresh scans; copying a picture from the Internet and changing some of the bits is a dead giveaway. 

Images are by no means the only carrier for steganographic messages. Audio files also work fine. Video files have a huge steganographic bandwidth. Even the layout and ordering of tags in an HTML file can carry information. 

Although we have examined steganography in the context of free speech, it has numerous other uses. One common use is for the owners of images to encode secret messages in them stating their ownership rights. If such an image is stolen and placed on a Web site, the lawful owner can reveal the steganographic message in court to prove whose image it is. This technique is called 

watermarking

. It is discussed in (Piva et al., 2002). 

For more on steganography, see (Artz, 2001; Johnson and Jajoda, 1998; Katzenbeisser and Petitcolas, 2000; and Wayner, 2002). 

8.10.3 Copyright 

Privacy and censorship are just two areas where technology meets public policy. A third one is copyright. 

Copyright

 is the granting to the creators of 

IP

 (

Intellectual Property

), including writers, artists, composers, musicians, photographers, cinematographers, choreographers, and others, the exclusive right to exploit their IP for some period of time, typically the life of the author plus 50 years or 75 years in the case of corporate ownership. After the copyright of a work expires, it passes into the public domain and anyone can use or sell it as they wish. The Gutenberg Project (

www.promo.net/pg

), for example, has placed thousands of public domain works (e.g., by Shakespeare, Twain, Dickens) on the Web. In 1998, the U.S. Congress extended copyright in the U.S. by another 20 years at the request of Hollywood, which claimed that without an extension nobody would create anything anymore. By way of contrast, patents last for only 20 years and people still invent things. 

Copyright came to the forefront when Napster, a music-swapping service, had 50 million members. Although Napster did not actually copy any music, the courts held that its holding a central database of who had which song was contributory infringement, that is, they helped other people infringe. While nobody seriously claims copyright is a bad idea (although many claim that the term is far too long, favoring big corporations over the public), the next generation of music sharing is already raising major ethical issues. 

For example, consider a peer-to-peer network in which people share legal files (public domain music, home videos, religious tracts that are not trade secrets, etc.) and perhaps a few that are copyrighted. Assume that everyone is on-line all the time via ADSL or cable. Each machine has an index of what is on the hard disk, plus a list of other members. Someone looking for a specific item can pick a random member and see if he has it. If not, he can check out all the members in that person's list, and all the members in their lists, and so on. Computers are very good at this kind of work. Having found the item, the requester just copies it. 

If the work is copyrighted, chances are the requester is infringing (although for international transfers, the question of whose law applies is unclear). But what about the supplier? Is it a crime to keep music you have paid for and legally downloaded on your hard disk where others might find it? If you have an unlocked cabin in the country and a IP thief sneaks in carrying a notebook computer and scanner, copies a copyrighted book, and sneaks out, are 

you

 guilty of the crime of failing to protect someone else's copyright? 

But there is more trouble brewing on the copyright front. There is a huge battle going on now between Hollywood and the computer industry. The former wants stringent protection of all 

638




intellectual property and the latter does not want to be Hollywood's policeman. In October 1998, Congress passed the 

DMCA

 (

Digital Millennium Copyright Act

) which makes it a crime to circumvent any protection mechanism present in a copyrighted work or to tell others how to circumvent it. Similar legislation is being set in place in the European Union. While virtually no one thinks that pirates in the Far East should be allowed to duplicate copyrighted works, many people think that the DMCA completely shifts the balance between the copyright owner's interest and the public interest. 

A case in point. In September 2000, a music industry consortium charged with building an unbreakable system for selling music on-line sponsored a contest inviting people to try to break the system (which is precisely the right thing to do with any new security system). A team of security researchers from several universities, led by Prof. Edward Felten of Princeton, took up the challenge and broke the system. They then wrote a paper about their findings and submitted it to a USENIX security conference, where it underwent peer review and was accepted. Before the paper was to be presented, Felten received a letter from the Recording Industry Association of America which threatened to sue the authors under the DMCA if they published the paper. 

Their response was to file suit asking a federal court to rule on whether publishing scientific papers on security research was still legal. Fearing a definitive court ruling against them, the industry withdrew its threat and the court dismissed Felten's suit. No doubt the industry was motivated by the weakness of its case: they had invited people to try to break their system and then threatened to sue some of them for accepting their challenge. With the threat withdrawn, the paper was published (Craver et al., 2001). A new confrontation is virtually certain. 

A related issue is the extent of the 

fair use doctrine

, which has been established by court rulings in various countries. This doctrine says that purchasers of a copyrighted work have certain limited rights to copy the work, including the right to quote parts of it for scientific purposes, use it as teaching material in schools or colleges, and in some cases make backup copies for personal use in case the original medium fails. The tests for what constitutes fair use include (1) whether the use is commercial, (2) what percentage of the whole is being copied, and (3) the effect of the copying on sales of the work. Since the DMCA and similar laws within the European Union prohibit circumvention of copy protection schemes, these laws also prohibit legal fair use. In effect, the DMCA takes away historical rights from users to give content sellers more power. A major show-down is inevitable. 

Another development in the works that dwarfs even the DMCA in its shifting of the balance between copyright owners and users is the 

TCPA

 (

Trusted Computing Platform Alliance

) led by Intel and Microsoft. The idea is to have the CPU chip and operating system carefully monitor user behavior in various ways (e.g., playing pirated music) and prohibit unwanted behavior. The system even allows content owners to remotely manipulate user PCs to change the rules when that is deemed necessary. Needless to say, the social consequences of this scheme are immense. It is nice that the industry is finally paying attention to security, but it is lamentable that it is entirely aimed at enforcing copyright law rather than dealing with viruses, crackers, intruders, and other security issues that most people are concerned about. 

In short, the lawmakers and lawyers will be busy balancing the economic interests of copyright owners with the public interest for years to come. Cyberspace is no different from meatspace: it constantly pits one group against another, resulting in power struggles, litigation, and (hopefully) eventually some kind of resolution, at least until some new disruptive technology comes along. 

8.11 Summary 

Cryptography is a tool that can be used to keep information confidential and to ensure its integrity and authenticity. All modern cryptographic systems are based on Kerckhoff's principle 

639




of having a publicly-known algorithm and a secret key. Many cryptographic algorithms use complex transformations involving substitutions and permutations to transform the plaintext into the ciphertext. However, if quantum cryptography can be made practical, the use of one-time pads may provide truly unbreakable cryptosystems. 

Cryptographic algorithms can be divided into symmetric-key algorithms and public-key algorithms. Symmetric-key algorithms mangle the bits in a series of rounds parameterized by the key to turn the plaintext into the ciphertext. Triple DES and Rijndael (AES) are the most popular symmetric-key algorithms at present. These algorithms can be used in electronic code book mode, cipher block chaining mode, stream cipher mode, counter mode, and others. 

Public-key algorithms have the property that different keys are used for encryption and decryption and that the decryption key cannot be derived from the encryption key. These properties make it possible to publish the public key. The main public-key algorithm is RSA, which derives its strength from the fact that it is very difficult to factor large numbers. 

Legal, commercial, and other documents need to be signed. Accordingly, various schemes have been devised for digital signatures, using both symmetric-key and public-key algorithms. Commonly, messages to be signed are hashed using algorithms such as MD5 or SHA-1, and then the hashes are signed rather than the original messages. 

Public-key management can be done using certificates, which are documents that bind a principal to a public key. Certificates are signed by a trusted authority or by someone (recursively) approved by a trusted authority. The root of the chain has to be obtained in advance, but browsers generally have many root certificates built into them. 

These cryptographic tools can be used to secure network traffic. IPsec operates in the network layer, encrypting packet flows from host to host. Firewalls can screen traffic going into or out of an organization, often based on the protocol and port used. Virtual private networks can simulate an old leased-line network to provide certain desirable security properties. Finally, wireless networks need good security and 802.11's WEP does not provide it, although 802.11i should improve matters considerably. 

When two parties establish a session, they have to authenticate each other and if need be, establish a shared session key. Various authentication protocols exist, including some that use a trusted third party, Diffie-Hellman, Kerberos, and public-key cryptography. 

E-mail security can be achieved by a combination of the techniques we have studied in this chapter. PGP, for example, compresses messages, then encrypts them using IDEA. It sends the IDEA key encrypted with the receiver's public key. In addition, it also hashes the message and sends the signed hash to verify message integrity. 

Web security is also an important topic, starting with secure naming. DNSsec provides a way to prevent DNS spoofing, as do self-certifying names. Most e-commerce Web sites use SSL to establish secure, authenticated sessions between the client and server. Various techniques are used to deal with mobile code, especially sandboxing and code signing. 

The Internet raises many issues in which technology interacts strongly with public policy. Some of the areas include privacy, freedom of speech, and copyright. 

Problems 

1. Break the following monoalphabetic cipher. The plaintext, consisting of letters only, is a well-known excerpt from a poem by Lewis Carroll. 

640




 kfd ktbd fzm eubd kfd pzyiom mztx ku kzyg ur bzha kfthcm ur mftnm zhx mfudm zhx mdzythc pzq ur ezsszcdm zhx gthcm zhx pfa kfd mdz tm sutythc fuk zhx pfdkfdi ntcm fzld pthcm sok pztk z stk kfd uamkdim eitdx sdruid pd fzld uoi efzk rui mubd ur om zid uok ur sidzkf zhx zyy ur om zid rzk hu foiia mztx kfd ezindhkdi kfda kfzhgdx ftb boef rui kfzk 

2. Break the following columnar transposition cipher. The plaintext is taken from a popular computer textbook, so ''computer'' is a probable word. The plaintext consists entirely of letters (no spaces). The ciphertext is broken up into blocks of five characters for readability. 

 aauan cvlre rurnn dltme aeepb ytust iceat npmey iicgo gorch srsoc nntii imiha oofpa gsivt tpsit lbolr otoex 

3. Find a 77-bit one-time pad that generates the text ''Donald Duck'' from the ciphertext of 

Fig. 8-4

. 

4. Quantum cryptography requires having a photon gun that can, on demand, fire a single photon carrying 1 bit. In this problem, calculate how many photons a bit carries on a 100-Gbps fiber link. Assume that the length of a photon is equal to its wavelength, which for purposes of this problem, is 1 micron. The speed of light in fiber is 20 cm/nsec. 

5. If Trudy captures and regenerates photons when quantum cryptography is in use, she will get some of them wrong and cause errors to appear in Bob's one-time pad. What fraction of Bob's one-time pad bits will be in error, on average? 

6. A fundamental cryptographic principle states that all messages must have redundancy. But we also know that redundancy helps an intruder tell if a guessed key is correct. Consider two forms of redundancy. First, the initial 

n

 bits of the plaintext contain a known pattern. Second, the final 

n

 bits of the message contain a hash over the message. From a security point of view, are these two equivalent? Discuss your answer. 

7. In 

Fig. 8-6

, the P-boxes and S-boxes alternate. Although this arrangement is esthetically pleasing, is it any more secure than first having all the P-boxes and then all the S-boxes? 

8. Design an attack on DES based on the knowledge that the plaintext consists exclusively of upper case ASCII letters, plus space, comma, period, semicolon, carriage return, and line feed. Nothing is known about the plaintext parity bits. 

9. In the text we computed that a cipher-breaking machine with a billion processors that could analyze a key in 1 picosecond would take only 10

10

 years to break the 128-bit version of AES. However, current machines might have 1024 processors and take 1 msec to analyze a key, so we need a factor of 10

15

 improvement in performance just to obtain the AES-breaking machine. If Moore's law (computing power doubles every 18 months) continues to hold, how many years will it take to even build the machine? 

10. AES supports a 256-bit key. How many keys does AES-256 have? See if you can find some number in physics, chemistry, or astronomy of about the same size. Use the Internet to help search for big numbers. Draw a conclusion from your research. 

11. Suppose that a message has been encrypted using DES in ciphertext block chaining mode. One bit of ciphertext in block 

C

i

 is accidentally transformed from a 0 to a 1 during transmission. How much plaintext will be garbled as a result? 

12. Now consider ciphertext block chaining again. Instead of a single 0 bit being transformed into a 1 bit, an extra 0 bit is inserted into the ciphertext stream after block 

C

i

. How much plaintext will be garbled as a result? 

13. Compare cipher block chaining with cipher feedback mode in terms of the number of encryption operations needed to transmit a large file. Which one is more efficient and by how much? 

14. Using the RSA public key cryptosystem, with 

a

 = 1, 

b

 = 2, etc., 

a. If 

p

 = 7 and 

q

 = 11, list five legal values for 

d

. 

641




b. If 

p

 = 13, 

q

 = 31, and 

d

 = 7, find 

e.

 

c. Using 

p

 = 5, 

q

 = 11, and 

d

 = 27, find 

e

 and encrypt ''abcdefghij''. 

15. Suppose a user, Maria, discovers that her private RSA key (

d

 1, 

n

 1) is same as the public RSA key (

e

 2, 

n

 2) of another user, Frances. In other words, 

d

 1 = 

e

 2 and 

n

 1 = 

n

 2. Should Maria consider changing her public and private keys? Explain your answer. 

16. Consider the use of counter mode, as shown in 

Fig. 8-15

, but with 

IV

 = 0. Does the use of 0 threaten the security of the cipher in general? 

17. The signature protocol of 

Fig. 8-18

 has the following weakness. If Bob crashes, he may lose the contents of his RAM. What problems does this cause and what can he do to prevent them? 

18. In 

Fig. 8-20

, we see how Alice can send Bob a signed message. If Trudy replaces 

P

, Bob can detect it. But what happens if Trudy replaces both 

P

 and the signature? 

19. Digital signatures have a potential weakness due to lazy users. In e-commerce transactions, a contract might be drawn up and the user asked to sign its SHA-1 hash. If the user does not actually verify that the contract and hash correspond, the user may inadvertently sign a different contract. Suppose that the Mafia try to exploit this weakness to make some money. They set up a pay Web site (e.g., pornography, gambling, etc.) and ask new customers for a credit card number. Then they send over a contract saying that the customer wishes to use their service and pay by credit card and ask the customer to sign it, knowing that most of them will just sign without verifying that the contract and hash agree. Show how the Mafia can buy diamonds from a legitimate Internet jeweler and charge them to unsuspecting customers. 

20. A math class has 20 students. What is the probability that at least two students have the same birthday? Assume that nobody was born on leap day, so there are 365 possible birthdays. 

21. After Ellen confessed to Marilyn about tricking her in the matter of Tom's tenure, Marilyn resolved to avoid this problem by dictating the contents of future messages into a dictating machine and having her new secretary just type them in. Marilyn then planned to examine the messages on her terminal after they had been typed in to make sure they contained her exact words. Can the new secretary still use the birthday attack to falsify a message, and if so, how? 

Hint

: She can. 

22. Consider the failed attempt of Alice to get Bob's public key in 

Fig. 8-23

. Suppose that Bob and Alice already share a secret key, but Alice still wants Bob's public key. Is there now a way to get it securely? If so, how? 

23. Alice wants to communicate with Bob, using public-key cryptography. She establishes a connection to someone she hopes is Bob. She asks him for his public key and he sends it to her in plaintext along with an X.509 certificate signed by the root CA. Alice already has the public key of the root CA. What steps does Alice carry out to verify that she is talking to Bob? Assume that Bob does not care who he is talking to (e.g., Bob is some kind of public service). 

24. Suppose that a system uses PKI based on a tree-structured hierarchy of CAs. Alice wants to communicate with Bob, and receives a certificate from Bob signed by a CA 

X

 after establishing a communication channel with Bob. Suppose Alice has never heard of 

X

. What steps does Alice take to verify that she is talking to Bob? 

25. Can IPsec using AH be used in transport mode if one of the machines is behind a NAT box? Explain your answer. 

26. Give one advantage of HMACs over using RSA to sign SHA-1 hashes. 

27. Give one reason why a firewall might be configured to inspect incoming traffic. Give one reason why it might be configured to inspect outgoing traffic. Do you think the inspections are likely to be successful? 

28. The WEP packet format is shown in 

Fig. 8-31

. Suppose that the checksum is 32 bits, computed by XORing all the 32-bit words in the payload together. Also suppose that the problems with RC4 are corrected by replacing it with a stream cipher having no weaknesses and that IV's are extended to 128 bits. Is there any way for an intruder to spy on or interfere with traffic without being detected? 

29. Suppose an organization uses VPN to securely connect its sites over the Internet. Is there a need for a user, Jim, in this organization to use encryption or any other security mechanism to communicate with another user Mary in the organization. 

642




30. Change one message in protocol of 

Fig. 8-34

 in a minor way to make it resistant to the reflection attack. Explain why your change works. 

31. The Diffie-Hellman key exchange is being used to establish a secret key between Alice and Bob. Alice sends Bob (719, 3, 191). Bob responds with (543). Alice's secret number, 

x

, is 16. What is the secret key? 

32. If Alice and Bob have never met, share no secrets, and have no certificates, they can nevertheless establish a shared secret key using the Diffie-Hellman algorithm. Explain why it is very hard to defend against a man-in-the-middle attack. 

33. In the protocol of 

Fig. 8-39

, why is 

A

 sent in plaintext along with the encrypted session key? 

34.

 In the protocol of 

Fig. 8-39

, we pointed out that starting each plaintext message with 32 zero bits is a security risk. Suppose that each message begins with a per-user random number, effectively a second secret key known only to its user and the KDC. Does this eliminate the known plaintext attack? Why? 

34. In the Needham-Schroeder protocol, Alice generates two challenges, 

R

A

 and 

R

A

2

. This seems like overkill. Would one not have done the job? 

35. Suppose an organization uses Kerberos for authentication. In terms of security and service availability, what is the effect if AS or TGS goes down? 

36. In the public-key authentication protocol of 

Fig. 8-43

, in message 7, 

R

B

 is encrypted with 

K

S

. Is this encryption necessary, or would it have been adequate to send it back in plaintext? Explain your answer. 

37. Point-of-sale terminals that use magnetic-stripe cards and PIN codes have a fatal flaw: a malicious merchant can modify his card reader to capture and store all the information on the card as well as the PIN code in order to post additional (fake) transactions in the future. The next generation of point-of-sale terminals will use cards with a complete CPU, keyboard, and tiny display on the card. Devise a protocol for this system that malicious merchants cannot break. 

38. Give 

two

 reasons why PGP compresses messages. 

39. Assuming that everyone on the Internet used PGP, could a PGP message be sent to an arbitrary Internet address and be decoded correctly by all concerned? Discuss your answer. 

40. The attack shown in 

Fig. 8-47

 leaves out one step. The step is not needed for the spoof to work, but including it might reduce potential suspicion after the fact. What is the missing step? 

41. It has been proposed to foil DNS spoofing using ID prediction by having the server put in a random ID rather than using a counter. Discuss the security aspects of this approach. 

42. The SSL data transport protocol involves two nonces as well as a premaster key. What value, if any, does using the nonces have? 

43. The image of 

Fig. 8-55(b)

 contains the ASCII text of five plays by Shakespeare. Would it be possible to hide music among the zebras instead of text? If so, how would it work and how much could you hide in this picture? If not, why not? 

44. Alice was a heavy user of a type 1 anonymous remailer. She would post many messages to her favorite newsgroup, 

alt.fanclub.alice

, and everyone would know they all came from Alice because they all bore the same pseudonym. Assuming that the remailer worked correctly, Trudy could not impersonate Alice. After type 1 remailers werer all shut down, Alice switched to a cypherpunk remailer and started a new thread in her newsgroup. Devise a way for her to prevent Trudy from posting new messages to the newsgroup, impersonating Alice. 

45. Search the Internet for an interesting case involving privacy and write a 1-page report on it. 

46. Search the Internet for some court case involving copyright versus fair use and write a 1-page report summarizing your findings. 

47. Write a program that encrypts its input by XORing it with a keystream. Find or write as good a random number generator as you can to generate the keystream. The program should act as a filter, taking plaintext on standard input and producing ciphertext on 

643




standard output (and vice versa). The program should take one parameter, the key that seeds the random number generator. 

48. Write a procedure that computes the SHA-1 hash of a block of data. The procedure should have two parameters: a pointer to the input buffer and a pointer to a 20-byte output buffer. To see the exact specification of SHA-1, search the Internet for FIPS 180-1, which is the full specification. 

 

644




Chapter 9. Reading List and Bibliography 

We have now finished our study of computer networks, but this is only the beginning. Many interesting topics have not been treated in as much detail as they deserve, and others have been omitted altogether for lack of space. In this chapter we provide some suggestions for further reading and a bibliography, for the benefit of readers who wish to continue their study of computer networks. 

9.1 Suggestions for Further Reading 

There is an extensive literature on all aspects of computer networks. Three journals that frequently publish papers in this area are 

IEEE Transactions on Communications

, 

IEEE Journal on Selected Areas in Communications

, and 

Computer Communication Review

. Many other journals also publish occasional papers on the subject. 

IEEE also publishes three magazines—

IEEE Internet Computing

, 

IEEE Network Magazine

, and 

IEEE Communications Magazine

—that contain surveys, tutorials, and case studies on networking. The first two emphasize architecture, standards, and software, and the last tends toward communications technology (fiber optics, satellites, and so on). 

In addition, there are a number of annual or biannual conferences that attract numerous papers on networks and distributed systems, in particular, 

SIGCOMM Annual Conference

, 

The International Conference on Distributed Computer Systems

. and 

The Symposium on Operating Systems Principles

, 

Below we list some suggestions for supplementary reading, keyed to the chapters of this book. Most of these are tutorials or surveys on the subject. A few are chapters from textbooks. 

9.1.1 Introduction and General Works 

Bi et al., "Wireless Mobile Communications at the Start of the 21st Century" 

A new century, a new technology. Sounds good. After some history of wireless, the major topics are covered here, including standards, applications, Internet, and technologies. 

Comer, 

The Internet Book

Anyone looking for an easy-going introduction to the Internet should look here. Comer describes the history, growth, technology, protocols, and services of the Internet in terms that novices can understand, but so much material is covered that the book is also of interest to more technical readers. 

Garber, "Will 3G Really Be the Next Big Wireless Technology?" 

Third-generation mobile phones are supposed to combine voice and data and provide data rates up to 2 Mbps. They have been slow to take off. The promises, pitfalls, technology, politics, and economics of using broadband wireless communication are all covered in this easy-to-read article. 

IEEE Internet Computing, Jan.-Feb. 2000 

The first issue of 

IEEE Internet Computing

 in the new millennium did exactly what you would expect: ask the people who helped create the Internet in the previous millennium to speculate 

645




on where it is going in the next one. The experts are Paul Baran, Lawrence Roberts, Leonard Kleinrock, Stephen Crocker, Danny Cohen, Bob Metcalfe, Bill Gates, Bill Joy, and others. For best results, wait 500 years, 

then

 read their predictions. 

Kipnis, "Beating the System: Abuses of the Standards Adoption Process" 

Standards committees try to be fair and vendor neutral in their work, but unfortunately there are companies that try to abuse the system. For example, it has happened repeatedly that a company helps develop a standard and after it is approved, announces that the standard is based on a patent it owns and which it will license to companies that it likes and not to companies that it does not like, and at prices that it alone determines. For a look at the dark side of standardization, this article is an excellent start. 

Kyas and Crawford, 

ATM Networks

ATM was once touted as the networking protocol of the future, and is still important within the telephone system. This book is an up-to-date guide to ATM's current status, with detailed information on ATM protocols and how they can integrate with IP-based networks. 

Kwok, "A Vision for Residential Broadband Service" 

If you want to know what Microsoft thought about delivering video on demand in 1995, read this article. Five years later the vision was hopelessly obsolete. The value of the article is to demonstrate that even highly-knowledgeable and well-motivated people cannot see even five years into the future with any accuracy at all. It should be a lesson for all of us. 

Naughton, 

A Brief History of the Future

Who invented the Internet, anyway? Many people have claimed credit. And rightly so, since many people had a hand in it, in different ways. This history of the Internet tells how it all happened, and in a witty and charming way, replete with anecdotes, such as AT&T's repeatedly making clear its belief that digital communication had no future. 

Perkins, "Mobile Networking in the Internet" 

For a good overview of mobile networking protocol layer by protocol layer, this is the place to look. The physical through transport layers are covered, as well as middleware, security, and ad hoc networking. 

Teger and Waks,"End-User Perspectives on Home Networking" 

Home networks are not like corporate networks. The applications are different (more multimedia intensive), the equipment comes from a wider range of suppliers, and the users have little technical training and no patience whatsoever for any failures. To find out more, look here. 

Varshney and Vetter, "Emerging Mobile and Wireless Networks" 

Another introduction to wireless communication. It covers wireless LANs, wireless local loops, and satellites, as well as some of the software and applications. 

Wetteroth, 

OSI Reference Model for Telecommunications

Though the OSI protocols themselves are not used any more, the seven-layer model has become very well-known. As well as explaining more about OSI, this book applies the model to 

646




telecom (as opposed to computer) networks, showing where common telephony and other voice protocols fit into the networking stack. 

9.1.2 The Physical Layer 

Abramson, "Internet Access Using VSATs" 

Small earth stations are becoming more popular for both rural telephony and corporate Internet access in developed countries. However, the nature of the traffic for these two cases differs dramatically, so different protocols are needed to handle the two cases. In this article, the inventor of the ALOHA system discusses numerous channel allocation methods that can be used for VSAT systems. 

Alkhatib et al., "Wireless Data Networks: Reaching the Extra Mile" 

For a quick introduction to wireless networking terms and technologies, including spread spectrum, this tutorial paper is a good starting place. 

Azzam and Ransom, 

Broadband Access Technologies

The telephone system, fiber, ADSL, cable networks, satellites, even power lines are covered here as network access technologies. Other topics include home networks, services, network performance, and standards. The book concludes with biographies of the major companies in the telecom and network business, but with the rate of change in the industry, this chapter may have a shorter shelf life than the technology chapters. 

Bellamy, 

Digital Telephony

Everything you ever wanted to know about the telephone system and more is contained in this authoritative book. Particularly interesting are the chapters on transmission and multiplexing, digital switching, fiber optics, mobile telephony, and DSL. 

Berezdivin et al., "Next-Generation Wireless Communications Concepts and Technologies" 

These folks are one step ahead of everyone else. The "next generation" in the title refers to fourth-generation wireless networks. These networks are expected to provide IP service everywhere with seamless connectivity to the Internet with high bandwidth and excellent quality of service. These goals are to be achieved through smart spectrum usage, dynamic resource management, and adaptive service. All this sounds visionary now, but mobile telephones sounded pretty visionary back in 1995. 

Dutta-Roy, "An Overview of Cable Modem Technology and Market Perspectives" 

Cable TV has gone from simple CATV to a complex distribution system for TV, Internet, and telephony. These changes have affected the cable infrastructure considerably. For a discussion of cable plant, standards, and marketing, with an emphasis on DOCSIS, this article is worth reading. 

Farserotu and Prasad, "A Survey of Future Broadband Multimedia Satellite Systems, Issues, and Trends" 

A variety of data communication satellites are in the sky or on the drawing board, including Astrolink, Cyberstar, Spaceway, Skybridge, Teledesic, and iSky. They use various techniques including bent pipe and satellite switching. For an overview of different satellite systems and techniques, this paper is a good starting place. 

647




Hu and Li, "Satellite-Based Internet: A Tutorial" 

Internet access via satellite is different from access via terrestrial lines. Not only is there the issue of delay, but routing and switching are also different. In this paper, the authors examine some of the issues related to using satellites for Internet access. 

Joel, "Telecommunications and the IEEE Communications Society" 

For a compact, but surprisingly comprehensive, history of telecommunications, starting with the telegraph and ending with 802.11, this article is the place to look. It also covers radio, telephones, analog and digital switching, submarine cables, digital transmission, ATM, television broadcasting, satellites, cable TV, optical communications, mobile phones, packet switching, the ARPANET, and the Internet. 

Metcalfe, "Computer/Network Interface Design: Lessons from Arpanet & Ethernet" 

Although engineers have been building network interfaces for decades now, one often wonders if they have learned anything from all this experience. In this paper, the designer of the Ethernet tells how to build a network interface and what to do with it once you have built it. He pulls no punches, telling what he did wrong as well as what he did right. 

Palais, 

Fiber Optic Communication

, 3rd ed. 

Books on fiber optic technology tend to be aimed at the specialist, but this one is more accessible than most. It covers waveguides, light sources, light detectors, couplers, modulation, noise, and many other topics. 

Pandya, "Emerging Mobile and Personal Communications Systems" 

For a short and sweet introduction to hand-held personal communication systems, this article is worth looking at. One of the nine pages contains a list of 70 acronyms used on the other eight pages. 

Sarikaya, "Packet Mode in Wireless Networks: Overview of Transition to Third Generation" 

The whole idea of third-generation cellular networks is wireless data transmission. To get an overview of how second-generation networks handle data and what the evolution to third generation will be, this is a good place to look. Topics covered include GPRS, IS-95B, WCDMA, and CDMA2000. 

9.1.3 The Data Link Layer 

Carlson, 

PPP Design, Implementation and Debugging

, 2nd ed. 

If you are interested in detailed information on all the protocols that make up the PPP suite, including CCP (compression) and ECP (encryption), this book is a good reference. There is a particular focus on ANU PPP-2.3, a popular implementation of PPP. 

Gravano, 

Introduction to Error Control Codes

Errors creep into nearly all digital communications, and many types of codes have been developed to detect and correct for them. This book explains some of the most important, from simple linear Hamming codes to more complex Galois fields and convolutional codes. It tries to do so with the minimum algebra necessary, but that is still a lot. 

648




Holzmann, 

Design and Validation of Computer Protocols

Readers interested in the more formal aspects of data link (and similar) protocols should look here. The specification, modeling, correctness, and testing of such protocols are all covered in this book. 

Peterson and Davie, 

Computer Networks: A Systems Approach

Chapter 2

 contains material about many data link issues, including framing, error detection, stop-and-wait protocols, sliding window protocols, and IEEE 802 LANs. 

Stallings, 

Data and Computer Communications

Chapter 7

 deals with the data link layer and covers flow control, error detection, and the basic data link protocols, including stop-and-wait and go back n. The HDLC-type protocols are also covered. 

9.1.4 The Medium Access Control Sublayer 

Bhagwat, "Bluetooth: Technology for Short-Range Wireless Apps" 

For a straightforward introduction to the Bluetooth system, this is a good place to start. The core protocols and profiles, radio, piconets, and links are discussed, followed by an introduction to the various protocols. 

Bisdikian, "An Overview of the Bluetooth Wireless Technology" 

Like the Bhagwat paper (above), this is also a good starting point for learning more about the Bluetooth system. The piconets, the protocol stack, and the profiles are all discussed, among other topics. 

Crow et al., "IEEE 802.11 Wireless Local Area Networks" 

For a simple introduction to the technology and protocols of 802.11, this is a good place to start. The emphasis is on the MAC sublayer. Both distributed control and centralized control are covered. The paper concludes with some simulation studies of the performance of 802.11 under various conditions. 

Eklund et al., "IEEE Standard 802.16: A Technical Overview of the Wireless MAN Air Interface for Broadband Wireless Access" 

The wireless local loop standardized by IEEE in 2002 as 802.16 may revolutionize telephone service, bringing broadband to the home. In this overview, the authors explain the main technological issues relating to this standard. 

Kapp, "802.11: Leaving the Wire Behind" 

This short introduction to 802.11 covers the basics, protocols, and relevant standards. 

Kleinrock, "On Some Principles of Nomadic Computing and Multi-Access Communications" 

Wireless access over a shared channel is more complex than having wired stations share a channel. Among other issues are dynamic topologies, routing, and power management. These and other issues related to channel access by mobile wireless devices are covered in this article. 

649




Miller and Cummins, 

LAN Technologies Explained

Need to know more about the technologies that can be used in a LAN? This book covers most of them, including FDDI and token ring as well as the everpopular Ethernet. While new installations of the first two are now rare, many existing networks still use them, and ring networks are still common (e.g., SONET). 

Perlman, 

Interconnections

, 2nd ed. 

For an authoritative, but entertaining, treatment of bridges, routers, and routing in general, Perlman's book is the place to look. The author designed the algorithms used in the IEEE 802 spanning tree bridge and is one of the world's leading authorities on various aspects of networking. 

Webb, "Broadband Fixed Wireless Access" 

Both the "why" and the "how" of fixed broadband wireless are examined in this paper. The "why" section argues that people do not want a home e-mail address, a work e-mail address, separate telephone numbers for home, work, and mobile, an instant messaging account, and perhaps a fax number or two. They want a single integrated system that works everywhere. The emphasis in the technology section is on the physical layer, including topics such as TDD versus FDD, adaptive versus fixed modulation, and number of carriers. 

9.1.5 The Network Layer 

Bhatti and Crowcroft, "QoS Sensitive Flows: Issues in IP Packet Handling" 

One of the ways to achieve better quality of service in a network is to schedule packet departures from each router carefully. In this paper, a variety of packet scheduling algorithms, as well as related issues, are discussed in some detail. 

Chakrabarti, "QoS Issues in Ad Hoc Wireless Networks" 

Routing in ad hoc networks of notebook computers that just happen to be near each other is hard enough without having to worry about quality of service as well. Nevertheless, people do care about quality of service, so attention needs to be paid to this topic. The nature of ad hoc networks and some of the issues related to routing and quality of service are discussed in this article. 

Comer, 

Internetworking with TCP/IP

, Vol. 1, 4th ed. 

Comer has written the definitive work on the TCP/IP protocol suite. Chapters 4 through 11 deal with IP and related protocols in the network layer. The other chapters deal primarily with the higher layers and are also worth reading. 

Huitema, 

Routing in the Internet

If you want to know everything there is to know about routing in the Internet, this is the book for you. Both pronounceable algorithms (e.g., RIP, CIDR, and MBONE) and unpronounceable algorithms (e.g., OSPF, IGRP, EGP, and BGP) are treated in great detail. New features, such as multicast, mobile IP, and resource reservation, are also here. 

Malhotra, 

IP Routing

For a detailed guide to IP routing, this book contains a lot of material. The protocols covered include RIP, RIP-2, IGRP, EIGRP, OSPF, and BGP-4. 

650




Metz, "Differentiated Services" 

Quality-of-service guarantees are important for many multimedia applications. Integrated services and differentiated services are two possible approaches to achieving them. Both are discussed here, with the emphasis on differentiated services. 

Metz, "IP Routers: New Tool for Gigabit Networking" 

Most of the other references for Chap. 5 are about routing algorithms. This one is different: it is about how routers actually work. They have gone through an evolutionary process from being general-purpose workstations to being highly special-purpose routing machines. If you want to know more, this article is a good starting place. 

Nemeth et al., 

UNIX System Administration Handbook

For a change of pace, Chap. 13 of this book deals with networking in a more practical way than most of our other references. Rather than just dealing with the abstract concepts, it gives much advice here about what to do if you are actually managing a real network. 

Perkins, "Mobile Networking through Mobile IP" 

As mobile computing devices become more and more common, Mobile IP is becoming more and more important. This tutorial gives a good introduction to it and related topics. 

Perlman, 

Interconnections: Bridges and Routers

, 2nd ed. 

In Chapters 12 through 15, Perlman describes many of the issues involved in unicast and multicast routing algorithm design, both for WANs and networks of LANs, and their implementation in various protocols. But the best part of the book is Chap. 18, in which the author distills her years of experience about network protocols into an informative and fun chapter. 

Puzmanova, 

Routing and Switching: Time of Convergence?

In the late 1990s, some networking equipment vendors began to call everything a switch, while many managers of large networks said that they were switching from routers to switches. As the title implies, this book predicts the future of both routers and switches and asks whether they really are converging. 

Royer and Toh, "A Review of Current Routing Protocols for Ad-Hoc Mobile Wireless Networks" 

The AODV ad hoc routing algorithm that we discussed in 

Chap. 5

 is not the only one known. A variety of other ones, including DSDV, CGSR, WRP, DSR, TORA, ABR, DRP, and SRP, are discussed here and compared with one another. Clearly, if you are planning to invent a new ad hoc routing protocol, step 1 is to think of a three- or four-letter acronym. 

Stevens, 

TCP/IP Illustrated

, Vol. 1 

Chapters 3-10 provide a comprehensive treatment of IP and related protocols (ARP, RARP, and ICMP) illustrated by examples. 

Striegel and Manimaran, "A Survey of QoS Multicasting Issues" 

Multicasting and quality of service are two increasing important topics as services such as internet radio and television begin to take off. In this survey paper, the authors discuss how routing algorithms can take both of these issues into account. 

651




Yang and Reddy, "A Taxonomy for Congestion Control Algorithms in Packet Switching Networks" 

The authors have devised a taxonomy for congestion control algorithms. The main categories are open loop with source control, open loop with destination control, closed loop with explicit feedback, and closed loop with implicit feedback. They use this taxonomy to describe and classify 23 existing algorithms. 

9.1.6 The Transport Layer 

Comer, 

Internetworking with TCP/IP

, Vol. 1, 4th ed. 

As mentioned above, Comer has written the definitive work on the TCP/IP protocol suite. Chap. 12 is about UDP; Chap. 13 is about TCP. 

Hall and Cerf, 

Internet Core Protocols: The Definitive Guide

If you like your information straight from the source, this is the place to learn more about TCP. After all, Cerf co-invented it. 

Chapter 7

 is a good reference on TCP, showing how to interpret the information supplied by protocol analysis and network management tools. Other chapters cover UDP, IGMP, ICMP and ARP. 

Kurose and Ross, 

Computer Networking: A Top-Down Approach Featuring the Internet

Chapter 3

 is about the transport layer and contains a fair amount of material on UDP and TCP. It also discusses the stop-and-wait and go back n protocols we examined in 

Chap. 3

. 

Mogul, "IP Network Performance" 

Despite the title of this article, it is at least, if not more, about TCP and network performance in general, than about IP performance in particular. It is full of useful guidelines and rules of thumb. 

Peterson and Davie, 

Computer Networks: A Systems Approach

Chapter 5

 is about UDP, TCP, and a few related protocols. Network performance is also covered briefly. 

Stevens, 

TCP/IP Illustrated

, Vol. 1 

Chapters 17-24 provide a comprehensive treatment of TCP illustrated by examples. 

9.1.7 The Application Layer 

Bergholz, "Extending Your Markup: An XML Tutorial" 

A short and straightforward introduction to XML for beginners. 

Cardellini et al., 

The State-of-the-Art in Locally Distributed Web-Server Systems"

As the Web gets more popular, some Web sites need to have large server farms to handle the traffic. The hard part of building a server farm is distributing the load among the machines. This tutorial paper discusses that subject at great length. 

Berners-Lee et al., "The World Wide Web" 

652




A perspective on the Web and where it is going by the person who invented it and some of his colleagues at CERN. The article focuses on the Web architecture, URLs, HTTP, and HTML, as well as future directions, and compares it to other distributed information systems. 

Choudbury et al., "Copyright Protection for Electronic Publishing on Computer Networks" 

Although numerous books and articles describe cryptographic algorithms, few describe how they could be used to prevent users from further distributing documents that they are allowed to decrypt. This paper describes a variety of mechanisms that might help protect authors' copyrights in the electronic era. 

Collins, "Carrier Grade Voice over IP" 

If you have read the Varshney et al. paper and now want to know all the details about voice over IP using H.323, this is a good place to look. Although the book is long and detailed, it is tutorial in nature and does not require any previous knowledge of telephone engineering. 

Davison, "A Web Caching Primer" 

As the Web grows, caching is becoming crucial to good performance. For a brief introduction to Web caching, this is a good place to look. 

Krishnamurthy and Rexford, 

Web Protocols and Practice

It would be hard to find a more comprehensive book about all aspects of the Web than this one. It covers clients, servers, proxies, and caching, as you might expect. But there are also chapters on Web traffic and measurements as well as chapters on current research and improving the Web. 

Rabinovich and Spatscheck, 

Web Caching and Replication

For a comprehensive treatment of Web caching and replication, this is a good bet. Proxies, caches, prefetching, content delivery networks, server selection, and much more are covered in great detail. 

Shahabi et al. "Yima: A Second-Generation Continuous Media Server" 

Multimedia servers are complex systems that have to manage CPU scheduling, disk file placement, stream synchronization and more. As time has gone on, people have learned how to design them better. An architectural overview of one recent system is presented in this paper. 

Tittel et al., 

Mastering XHTML

Two books in one large volume, covering the Web's new standard markup language. First, there's a text describing XHTML, focusing mostly on how it differs from regular HTML. Then there is a comprehensive reference guide to the tags, codes, and special characters used in XHTML 1.0. 

Varshney et al., "Voice over IP" 

How does voice over IP work and is it going to replace the public switched telephone network? Read and find out. 

653




9.1.8 Network Security 

Anderson, "Why Cryptosystems Fail" 

According to Anderson, security in banking systems is poor, but not due to clever intruders breaking DES on their PCs. The real problems range from dishonest employees (a bank clerk's changing a customer's mailing address to his own to intercept the bank card and PIN number) to programming errors (giving all customers the same PIN code). What is especially interesting is the arrogant response banks give when confronted with a clear problem: "Our systems are perfect and therefore all errors must be due to customer errors or fraud." 

Anderson, 

Security Engineering

To some extent, this book is a 600-page version of "Why Cryptosystems Fail." It is more technical than 

Secrets and Lies

 but less technical than 

Network Security

 (see below). After an introduction to the basic security techniques, entire chapters are devoted to various applications, including banking, nuclear command and control, security printing, biometrics, physical security, electronic warfare, telecom security, e-commerce, and copyright protection. The third part of the book is about policy, management, and system evaluation. 

Artz, "Digital Steganography" 

Steganography goes back to ancient Greece, where the wax was melted off blank tablets so secret messages could be applied to the underlying wood before the wax was reapplied. Nowadays different techniques are used, but the goal is the same. Various modern techniques for hiding information in images, audio, and other carriers are discussed here. 

Brands, 

Rethinking Public Key Infrastructures and Digital Certificates

More than a wide-ranging introduction to digital certificates, this is also a powerful work of advocacy. The author believes that current paper-based systems of identity verification are outdated and inefficient, and argues that digital certificates can be used for applications such as electronic voting, digital rights management and even as a replacement for cash. He also warns that without PKI and encryption, the Internet could become a large-scale surveillance tool. 

Kaufman et al., 

Network Security

, 2nd ed. 

This authoritative and witty book is the first place to look for more technical information on network security algorithms and protocols. Secret and public key algorithms and protocols, message hashes, authentication, Kerberos, PKI, IPsec, SSL/TLS, and e-mail security are all explained carefully and at considerable length, with many examples. Chapter 26 on security folklore is a real gem. In security, the devil is in the details. Anyone planning to design a security system that will actually be used will learn a lot from the real-world advice in this chapter. 

Pohlmann, 

Firewall Systems

Firewalls are most networks' first (and last) line of defense against attackers. This book explains how they work and what they do, from the simplest softwarebased firewall designed to protect a single PC to the advanced firewall appliances that sit between a private network and its Internet connection. 

Schneier, 

Applied Cryptography

, 2nd ed. 

654




This monumental compendium is NSA's worst nightmare: a single book that describes every known cryptographic algorithm. To make it worse (or better, depending on your point of view), the book contains most of the algorithms as runnable programs (in C). Furthermore, over 1600 references to the cryptographic literature are provided. This book is not for beginners, but if you 

really

 want to keep your files secret, read this book. 

Schneier, 

Secrets and Lies

If you read 

Applied Cryptography

 from cover to cover, you will know everything there is to know about cryptographic algorithms. If you then read 

Secrets and Lies

 cover to cover (which can be done in a lot less time), you will learn that cryptographic algorithms are not the whole story. Most security weaknesses are not due to faulty algorithms or even keys that are too short, but to flaws in the security environment. Endless examples are presented about threats, attacks, defenses, counterattacks, and much more. For a nontechnical and fascinating discussion of computer security in the broadest sense, this book is the one to read. 

Skoudis, 

Counter Hack

The best way to stop a hacker is to think like a hacker. This book shows how hackers see a network, and argues that security should be a function of the entire network's design, not an afterthought based on one specific technology. It covers almost all common attacks, including the "social engineering" types that take advantage of users who are not always familiar with computer security measures. 

 

9.2 Alphabetical Bibliography 

ABRAMSON, N.: 

"Internet Access Using VSATs,"

 

IEEE Commun. Magazine

, vol. 38, pp. 60-68, July 2000. 

ABRAMSON,N.: 

"Development of the ALOHANET,"

 

IEEE Trans. on Information Theory

, vol. IT-31, pp. 119-123, March 1985. 

ADAMS, M., and DULCHINOS, D.: 

"OpenCable,"

 

IEEE Commun. Magazine

, vol. 39, pp. 98-105, June 2001. 

ALKHATIB, H.S., BAILEY, C., GERLA, M., and MCRAE, J.: 

"Wireless Data Networks: Reaching the Extra Mile,"

 

Computer

, vol. 30, pp. 59-62, Dec. 1997. 

ANDERSON, R.J.: 

"Free Speech Online and Office,"

 

Computer

, vol. 25, pp. 28-30, June 2002. 

ANDERSON, R.J.: 

Security Engineering

, New York: Wiley, 2001. 

ANDERSON, R.J.: 

"The Eternity Service,"

 

Proc. First Int'l Conf. on Theory and Appl. of Cryptology

, CTU Publishing House, 1996. 

ANDERSON, R.J.: 

"Why Cryptosystems Fail,"

 

Commun. of the ACM

, vol. 37, pp. 32-40, Nov. 1994. 

ARTZ, D.: 

"Digital Steganography,"

 

IEEE Internet Computing

, vol. 5, pp. 75-80, 2001. 

AZZAM, A.A., and RANSOM, N.: 

Broadband Access Technologies

, New York: McGraw-Hill, 1999. 

655




BAKNE, A., and BADRINATH, B.R.: 

"I-TCP: Indirect TCP for Mobile Hosts,"

 

Proc. 15th Int'l Conf. on Distr. Computer Systems

, IEEE, pp. 136-143, 1995. 

BALAKRISHNAN, H., SESHAN, S., and KATZ, R.H.: 

"Improving Reliable Transport and Handoff Performance in Cellular Wireless Networks,"

 

Proc. ACM Mobile Computing and Networking Conf

., ACM, pp. 2-11, 1995. 

BALLARDIE, T., FRANCIS, P., and CROWCROFT, J.: 

"Core Based Trees (CBT),"

 

Proc. SIGCOMM '93 Conf.

, ACM, pp. 85-95, 1993. 

BARAKAT, C., ALTMAN, E., and DABBOUS, W.: 

"On TCP Performance in a Heterogeneous Network: A Survey,"

 

IEEE Commun. Magazine

, vol. 38, pp. 40-46, Jan. 2000. 

BELLAMY, J.: 

Digital Telephony

, 3rd ed., New York: Wiley, 2000. 

BELLMAN, R.E.: 

Dynamic Programming

, Princeton, NJ: Princeton University Press, 1957. 

BELSNES, D.: 

"Flow Control in the Packet Switching Networks,"

 

Communications Networks

, Uxbridge, England: Online, pp. 349-361, 1975. 

BENNET, C.H., and BRASSARD, G.: 

"Quantum Cryptography: Public Key Distribution and Coin Tossing,"

 

Int'l Conf. on Computer Systems and Signal Processing

, pp. 175-179, 1984. 

BEREZDIVIN, R., BREINIG, R., and TOPP, R.: 

"Next-Generation Wireless Communication Concepts and Technologies,"

 

IEEE Commun. Magazine

, vol. 40, pp. 108-116, March 2002. 

BERGHEL, H.L.: 

"Cyber Privacy in the New Millennium,"

 

Computer

, vol. 34, pp. 132-134, Jan. 2001. 

BERGHOLZ, A.: 

"Extending Your Markup: An XML Tutorial,"

 

IEEE Internet Computing

, vol. 4, pp. 74-79, July-Aug. 2000. 

BERNERS-LEE, T., CAILLIAU, A., LOUTONEN, A., NIELSEN, H.F., and SECRET, A.: 

"The World Wide Web,"

 

Commun. of the ACM

, vol. 37, pp. 76-82, Aug. 1994. 

BERTSEKAS, D., and GALLAGER, R.: 

Data Networks

, 2nd ed., Englewood Cliffs, NJ: Prentice Hall, 1992. 

BHAGWAT, P.: 

"Bluetooth: Technology for Short-Range Wireless Apps,"

 

IEEE Internet Computing

, vol. 5, pp. 96-103, May-June 2001. 

BHARGHAVAN, V., DEMERS, A., SHENKER, S., and ZHANG, L.: 

"MACAW: A Media Access Protocol for Wireless LANs,"

 

Proc. SIGCOMM '94 Conf.

, ACM, pp. 212-225, 1994. 

BHATTI, S.N., and CROWCROFT, J.: 

"QoS Sensitive Flows: Issues in IP Packet Han

dling," 

IEEE Internet Computing

, vol. 4, pp. 48-57, July-Aug. 2000. 

BI, Q., ZYSMAN, G.I., and MENKES, H.: 

"Wireless Mobile Communications at the Start of the 21st Century,"

 

IEEE Commun. Magazine

, vol. 39, pp. 110-116, Jan, 2001. 

BIHAM, E., and SHAMIR, A.: 

"Differential Cryptanalysis of the Data Encryption Standard,"

 

Proc. 17th Ann. Int'l Cryptology Conf.

, Berlin: Springer-Verlag LNCS 1294, pp. 513-525, 1997. 

BIRD, R., GOPAL, I., HERZBERG, A., JANSON, P.A., KUTTEN, S., MOLVA, R, and YUNG, M.: 

"Systematic Design of a Family of Attack-Resistant Authentication Protocols,"

 

IEEE J. on Selected Areas in Commun.

, vol. 11, pp. 679-693, June 1993. 

656




BIRRELL, A.D., and NELSON, B.J.: 

"Implementing Remote Procedure Calls,"

 

ACM Trans. on Computer Systems

, vol. 2, pp. 39-59, Feb. 1984. 

BIRYUKOV, A., SHAMIR, A., and WAGNER, D.: 

"Real Time Cryptanalysis of A5/1 on a PC,"

 

Proc. Seventh Int'l Workshop on Fast Software Encryption

, Berlin: Springer-Verlag LNCS 1978, p. 1, 2000. 

BISDIKIAN, C.: 

"An Overview of the Bluetooth Wireless Technology,"

 

IEEE Commun. Magazine

, vol. 39, pp. 86-94, Dec. 2001. 

BLAZE, M.: 

"Protocol Failure in the Escrowed Encryption Standard,"

 

Proc. Second ACM Conf. on Computer and Commun. Security

, ACM, pp. 59-67, 1994. 

BLAZE, M., and BELLOVIN, S.: 

"Tapping on My Network Door,"

 

Commun. of the ACM

, vol. 43, p. 136 , Oct. 2000. 

BOGINENI, K., SIVALINGAM, K.M., and DOWD, P.W.: 

"Low-Complexity Multiple Access Protocols for Wavelength-Division Multiplexed Photonic Networks,"

 

IEEE Journal on Selected Areas in Commun.

 , vol. 11, pp. 590-604, May 1993. 

BOLCSKEI, H., PAULRAJ, A.J., HARI, K.V.S., and NABAR, R.U.: 

"Fixed Broadband Wireless Access: State of the Art, Challenges, and Future Directions,"

 

IEEE Commun. Magazine

, vol. 39, pp. 100-108, Jan. 2001. 

BORISOV, N., GOLDBERG, I., and WAGNER, D.: 

"Intercepting Mobile Communications: The Insecurity of 802.11,"

 

Seventh Int'l Conf. on Mobile Computing and Networking

, ACM, pp. 180-188, 2001. 

BRANDS, S.: 

Rethinking Public Key Infrastructures and Digital Certificates

, Cambridge, MA: M.I.T. Press, 2000. 

BRAY, J., and STURMAN, C.F.: 

Bluetooth 1.1: Connect without Cables

, 2nd ed., Upper Saddle River, NJ: Prentice Hall, 2002. 

BREYER, R., and RILEY, S.: 

Switched, Fast, and Gigabit Ethernet

, Indianapolis, IN: New Riders, 1999. 

BROWN, S.: 

Implementing Virtual Private Networks

, New York: McGraw-Hill, 1999. 

BROWN, L., KWAN, M., PIEPRZYK, J., and SEBERRY, J.: 

"Improving Resistance to Differential Cryptanalysis and the Redesign of LOKI,"

 

ASIACRYPT '91 Abstracts

, pp. 25-30, 1991. 

BURNETT, S., and PAINE, S.: 

RSA Security's Official Guide to Cryptography

, Berkeley, CA: Osborne/McGraw-Hill, 2001. 

CAPETANAKIS, J.I.: 

"Tree Algorithms for Packet Broadcast Channels,"

 

IEEE Trans. on Information Theory

, vol. IT-25, pp. 505-515, Sept. 1979. 

CARDELLINI, V., CASALICCHIO, E., COLAJANNI, M., and YU, P.S.: 

"The State-of-the-Art in Locally Distributed Web-Server Systems,"

 

ACM Computing Surveys

, 

vol. 34

, 

pp. 263-311

, June 2002. 

CARLSON, J.: 

PPP Design, Implementation and Debugging

, 2nd ed., Boston: Addison-Wesley, 2001. 

657




CERF, V., and KAHN, R.: 

"A Protocol for Packet Network Interconnection,"

 

IEEE Trans. on Commun.

, vol. COM-22, pp. 637-648, May 1974. 

CHAKRABARTI, S.: 

"QoS Issues in Ad Hoc Wireless Networks,"

 

IEEE Commun. Magazine

, vol. 39, pp. 142-148, Feb. 2001. 

CHASE, J.S., GALLATIN, A.J., and YOCUM, K.G.: 

"End System Optimizations for High-Speed TCP,"

 

IEEE Commun. Magazine

, vol. 39, pp. 68-75, April 2001. 

CHEN, B., JAMIESON, K., BALAKRISHNAN, H., and MORRIS, R.: 

"Span: An Energy-Efficient Coordination Algorithm for Topology Maintenance in Ad Hoc Wireless Networks,"

 

ACM Wireless Networks

, vol. 8, Sept. 2002. 

CHEN, K.-C.: 

"Medium Access Control of Wireless LANs for Mobile Computing,"

 

IEEE Network Magazine

, vol. 8, pp. 50-63, Sept./Oct. 1994. 

CHOUDBURY, A.K., MAXEMCHUK, N.F., PAUL, S., and SCHULZRINNE, H.G.: 

"Copyright Protection for Electronic Publishing on Computer Networks,"

 

IEEE Network Magazine

, vol. 9, pp. 12-20, May/June, 1995. 

CHU, Y., RAO, S.G., and ZHANG, H.: 

"A Case for End System Multicast,"

 

Proc. Int'l Conf. on Measurements and Modeling of Computer Syst.

, ACM, pp. 1-12, 2000. 

CLARK, D.D.: 

"The Design Philosophy of the DARPA Internet Protocols,"

 

Proc. SIGCOMM '88 Conf.

, ACM, pp. 106-114, 1988. 

CLARK, D.D.: 

"Window and Acknowledgement Strategy in TCP,"

 RFC 813, July 1982. 

CLARK, D.D., DAVIE, B.S., FARBER, D.J., GOPAL, I.S., KADABA, B.K., SINCOSKIE, W.D., SMITH, J.M., and TENNENHOUSE, D.L.: 

"The Aurora Gigabit Testbed,"

 

Computer Networks and ISDN Systems

, vol. 25, pp. 599-621, Jan. 1993. 

CLARK, D.D., JACOBSON, V., ROMKEY, J., and SALWEN, H.: 

"An Analysis of TCP Processing Overhead,"

 

IEEE Commun. Magazine

, vol. 27, pp. 23-29, June 1989. 

CLARK, D.D., LAMBERT, M., and ZHANG, L.: 

"NETBLT: A High Throughput Transport Protocol,"

 

Proc. SIGCOMM '87 Conf.

, ACM, pp. 353-359, 1987. 

CLARKE, A.C.: 

"Extra-Terrestrial Relays,"

 

Wireless World

, 1945. 

CLARKE, I., MILLER, S.G., HONG, T.W., SANDBERG, O., and WILEY, B.: 

"Protecting Free Expression Online with Freenet,"

 

IEEE Internet Computing

, vol. 6, pp. 40-49, Jan.-Feb. 2002. 

COLLINS, D.: 

Carrier Grade Voice over IP

, New York: McGraw-Hill, 2001. 

COLLINS, D., and SMITH, C.: 

3G Wireless Networks

, New York: McGraw-Hill, 2001. 

COMER, D.E.: 

The Internet Book

, Englewood Cliffs, NJ: Prentice Hall, 1995. 

COMER, D.E.: 

Internetworking with TCP/IP

, vol. 1, 4th ed., Englewood Cliffs, NJ: Prentice Hall, 2000. 

COSTA, L.H.M.K., FDIDA, S., and DUARTE, O.C.M.B.: 

"Hop by Hop Multicast Routing Protocol,"

 

Proc. 2001 Conf. on Applications, Technologies, Architectures, and Protocols for Computer Commun.

, ACM, pp. 249-259, 2001. 

658




CRAVER, S.A., WU, M., LIU, B., STUBBLEFIELD, A., SWARTZLANDER, B., WALLACH, D.W., DEAN, D., and FELTEN, E.W.: 

"Reading Between the Lines: Lessons from the SDMI Challenge,"

 

Proc. 10th USENIX Security Symp.

, USENIX, 2001. 

CRESPO, P.M., HONIG, M.L., and SALEHI, J.A.: 

"Spread-Time Code-Division Multiple Access,"

 

IEEE Trans. on Commun.

, vol. 43, pp. 2139-2148, June 1995. 

CROW, B.P., WIDJAJA, I, KIM, J.G., and SAKAI, P.T.: 

"IEEE 802.11 Wireless Local Area Networks,"

 

IEEE Commun. Magazine

, vol. 35, pp. 116-126, Sept. 1997. 

CROWCROFT, J., WANG, Z., SMITH, A., and ADAMS, J.: 

"A Rough Comparison of the IETF and ATM Service Models,"

 

IEEE Network Magazine

, vol. 9, pp. 12-16, Nov./Dec. 1995. 

DABEK, F., BRUNSKILL, E., KAASHOEK, M.F., KARGER, D., MORRIS, R., STOICA, R., and BALAKRISHNAN, H.: 

"Building Peer-to-Peer Systems With Chord, a Distributed Lookup Service,"

 

Proc. 8th Workshop on Hot Topics in Operating Systems

, IEEE, pp. 71-76, 2001a. 

DABEK, F., KAASHOEK, M.F., KARGER, D., MORRIS, R., and STOICA, I.: 

"Wide-Area Cooperative Storage with CFS,"

 

Proc. 18th Symp. on Operating Systems Prin.

, ACM, pp. 202-15 , 2001b. 

DAEMEN, J., and RIJMEN, V.: 

The Design of Rijndael

, Berlin: Springer-Verlag, 2002. 

DANTHINE, A.A.S.: 

"Protocol Representation with Finite-State Models,"

 

IEEE Trans. on Commun.

, vol. COM-28, pp. 632-643, April 1980. 

DAVIDSON, J., and PETERS, J.: 

Voice over IP Fundamentals

, Indianapolis, IN: Cisco Press, 2000. 

DAVIE, B., and REKHTER, Y.: 

MPLS Technology and Applications

, San Francisco: Morgan Kaufmann, 2000. 

DAVIS, P.T., and MCGUFFIN, C.R.: 

Wireless Local Area Networks

, New York: McGraw-Hill, 1995. 

DAVISON, B.D.: 

"A Web Caching Primer,"

 

IEEE Internet Computing

, vol. 5, pp. 38-45, July-Aug. 2001. 

DAY, J.D.: 

"The (Un)Revised OSI Reference Model,"

 

Computer Commun. Rev.

, vol. 25, pp. 39-55, Oct. 1995. 

DAY, J.D., and ZIMMERMANN, H.: 

"The OSI Reference Model,"

 

Proc. of the IEEE

, vol. 71, pp. 1334-1340, Dec. 1983. 

DE VRIENDT, J., LAINE, P., LEROUGE, C, and XU, X.: 

"Mobile Network Evolution: A Revolution on the Move,"

 

IEEE Commun. Magazine

, vol. 40, pp. 104-111, April 2002. 

DEERING, S.E.: 

"SIP: Simple Internet Protocol,"

 

IEEE Network Magazine

, vol. 7, pp. 16-28, May/June 1993. 

DEMERS, A., KESHAV, S., and SHENKER, S.: 

"Analysis and Simulation of a Fair Queueing Algorithm,"

 

Internetwork: Research and Experience

, vol. 1, pp. 3-26, Sept. 1990. 

DENNING, D.E., and SACCO, G.M.: 

"Timestamps in Key Distribution Protocols,"

 

Commun. of the ACM

, vol. 24, pp. 533-536, Aug. 1981. 

659




DIFFIE, W., and HELLMAN, M.E.: 

"Exhaustive Cryptanalysis of the NBS Data Encryption Standard,"

 

Computer

, vol. 10, pp. 74-84, June 1977. 

DIFFIE, W., and HELLMAN, M.E.: 

"New Directions in Cryptography,"

 

IEEE Trans. on Information Theory

, vol. IT-22, pp. 644-654, Nov. 1976. 

DIJKSTRA, E.W.: 

"A Note on Two Problems in Connexion with Graphs,"

 

Numer. Math.

, vol. 1, pp. 269-271, Oct. 1959. 

DOBROWSKI, G., and GRISE, D.: 

ATM and SONET Basics

, Fuquay-Varina, NC: APDG Telecom Books, 2001. 

DONALDSON, G., and JONES, D.: 

"Cable Television Broadband Network Architectures,"

 

IEEE Commun. Magazine

, vol. 39, pp. 122-126, June 2001. 

DORFMAN, R.: 

"Detection of Defective Members of a Large Population,"

 

Annals Math. Statistics

, vol. 14, pp. 436-440, 1943. 

DOUFEXI, A., ARMOUR, S., BUTLER, M., NIX, A., BULL, D., MCGEEHAN, J., and KARLSSON, P.: 

"A Comparison of the HIPERLAN/2 and IEEE 802.11A Wireless LAN Standards,"

 

IEEE Commun. Magazine

, vol. 40, pp. 172-180, May 2002. 

DURAND, A.: 

"Deploying IPv6,"

 

IEEE Internet Computing

, vol. 5, pp. 79-81, Jan.-Feb. 2001. 

DUTCHER, B.: 

The NAT Handbook

, New York: Wiley, 2001. 

DUTTA-ROY, A.: 

"An Overview of Cable Modem Technology and Market Perspectives,"

 

IEEE Commun. Magazine

, vol. 39, pp. 81-88, June 2001. 

EASTTOM, C.: 

Learn JavaScript

, Ashburton, U.K.: Wordware Publishing, 2001. 

EL GAMAL, T.: 

"A Public-Key Cryptosystem and a Signature Scheme Based on Discrete Logarithms,"

 

IEEE Trans. on Information Theory

, vol. IT-31, pp. 469-472, July 1985. 

ELHANANY, I., KAHANE, M., and SADOT, D.: 

"Packet Scheduling in Next-Generation Multiterabit Networks,"

 

Computer

, vol. 34, pp. 104-106, April 2001. 

ELMIRGHANI, J.M.H., and MOUFTAH, H.T.: 

"Technologies and Architectures for Scalable Dynamic Dense WDM Networks,"

 

IEEE Commun. Magazine

, vol. 38, pp. 58-66, Feb. 2000. 

FARSEROTU, J., and PRASAD, R.: 

"A Survey of Future Broadband Multimedia Satellite Systems, Issues, and Trends,"

 

IEEE Commun. Magazine

, vol. 38, pp. 128-133, June 2000. 

FIORINI, D., CHIANI, M., TRALLI, V., and SALATI., C.: 

"Can we Trust HDLC?,"

 

Computer Commun. Rev.

, vol. 24, pp. 61-80, Oct. 1994. 

FLOYD, S., and JACOBSON, V.: 

"Random Early Detection for Congestion Avoidance,"

 

IEEE/ACM Trans. on Networking

, vol. 1, pp. 397-413, Aug. 1993. 

FLUHRER, S., MANTIN, I., and SHAMIR, A.: 

"Weakness in the Key Scheduling Algorithm of RC4,"

 

Proc. Eighth Ann. Workshop on Selected Areas in Cryptography

, 2001. 

FORD, L.R., Jr., and FULKERSON, D.R.: 

Flows in Networks

, Princeton, NJ: Princeton University Press, 1962. 

660




FORD, W., and BAUM, M.S.: 

Secure Electronic Commerce

, Upper Saddle River, NJ: Prentice Hall, 2000. 

FORMAN, G.H., and ZAHORJAN, J.: 

"The Challenges of Mobile Computing,"

 

Computer

, vol. 27, pp. 38-47, April 1994. 

FRANCIS, P.: 

"A Near-Term Architecture for Deploying Pip,"

 

IEEE Network Magazine

, vol. 7, pp. 30-37, May/June 1993. 

FRASER, A.G.: 

"Towards a Universal Data Transport System,"

 in

 Advances in Local Area Networks

, Kummerle, K., Tobagi, F., and Limb, J.O. (Eds.), New York: IEEE Press, 1987. 

FRENGLE, N.: 

I-Mode: A Primer

, New York: Hungry Minds, 2002. 

GADECKI, C., and HECKERT, C.: 

ATM for Dummies

, New York: Hungry Minds, 1997. 

GARBER, L.: 

"Will 3G Really Be the Next Big Wireless Technology?,"

 

Computer

, vol. 35, pp. 26-32, Jan. 2002. 

GARFINKEL, S., with SPAFFORD, G.: 

Web Security, Privacy, and Commerce

, Sebastopol, CA: O'Reilly, 2002. 

GEIER, J.: 

Wireless LANs

, 2nd ed., Indianapolis, IN: Sams, 2002. 

GEVROS, P., CROWCROFT, J., KIRSTEIN, P., and BHATTI, S.: 

"Congestion Control Mechanisms and the Best Effort Service Model,"

 

IEEE Network Magazine

, vol. 15, pp. 16-25, May-June 2001. 

GHANI, N., and DIXIT, S.: 

"TCP/IP Enhancements for Satellite Networks,"

 

IEEE Commun. Magazine

, vol. 37, pp. 64-72, 1999. 

GINSBURG, D.: 

ATM: Solutions for Enterprise Networking

, Boston: Addison-Wesley, 1996. 

GOODMAN, D.J.: 

"The Wireless Internet: Promises and Challenges,"

 

Computer

, vol. 33, pp. 36-41, July 2000. 

GORALSKI, W.J.: 

Optical Networking and WDM

, New York: McGraw-Hill, 2001. 

GORALSKI, W.J.: 

SONET

, 2nd ed., New York: McGraw-Hill, 2000. 

GORALSKI, W.J.: 

Introduction to ATM Networking

, New York: McGraw-Hill, 1995. 

GOSSAIN, H., DE MORAIS CORDEIRO, and AGRAWAL, D.P.: 

"Multicast: Wired to Wireless,"

 

IEEE Commun. Mag.

, vol. 40, pp. 116-123, June 2002. 

GRAVANO, S.: 

Introduction to Error Control Codes

, Oxford, U.K.: Oxford University Press, 2001. 

GUO, Y., and CHASKAR, H.: 

"Class-Based Quality of Service over Air Interfaces in 4G Mobile Networks,"

 

IEEE Commun. Magazine

, vol. 40, pp. 132-137, March 2002. 

HAARTSEN, J.: 

"The Bluetooth Radio System,"

 

IEEE Personal Commun. Magazine

, vol. 7, pp. 28-36, Feb. 2000. 

661




HAC, A.: 

"Wireless and Cellular Architecture and Services,"

 

IEEE Commun. Magazine

, vol. 33, pp. 98-104, Nov. 1995. 

HAC, A., and GUO, L.: 

"A Scalable Mobile Host Protocol for the Internet,"

 

Int'l J. of Network Mgmt

, vol. 10, pp. 115-134, May-June, 2000. 

HALL, E., and CERF, V.: 

Internet Core Protocols: The Definitive Guide

, Sebastopol, CA: O'Reilly, 2000. 

HAMMING, R.W.: 

"Error Detecting and Error Correcting Codes,"

 

Bell System Tech. J.

, vol. 29, pp. 147-160, April 1950. 

HANEGAN, K.: 

Custom CGI Scripting with Perl

, New York: Wiley, 2001. 

HARRIS, A.: 

JavaScript Programming for the Absolute Beginner

, Premier Press, 2001. 

HARTE, L., KELLOGG, S., DREHER, R., and SCHAFFNIT, T.: 

The Comprehensive Guide to Wireless Technology

, Fuquay-Varina, NC: APDG Publishing, 2000. 

HARTE, L., LEVINE, R., and KIKTA, R.: 

3G Wireless Demystified

, New York: McGraw-Hill, 2002. 

HAWLEY, G.T.: 

"Historical Perspectives on the U.S. Telephone System,"

 

IEEE Commun. Magazine

, vol. 29, pp. 24-28, March 1991. 

HECHT, J.: 

"Understanding Fiber Optics,"

 Upper Saddle River, NJ: Prentice Hall, 2001. 

HEEGARD, C., COFFEY, J.T., GUMMADI, S., MURPHY, P.A., PROVENCIO, R., ROSSIN, E.J., SCHRUM, S., and SHOEMAKER, M.B.: 

"High-Performance Wireless Ethernet,"

 

IEEE Commun. Magazine

, vol. 39, pp. 64-73, Nov. 2001. 

HELD, G.: 

The Complete Modem Reference

, 2nd ed., New York: Wiley, 1994. 

HELLMAN, M.E.: 

"A Cryptanalytic Time-Memory Tradeoff,"

 

IEEE Trans. on Information Theory

, vol. IT-26, pp. 401-406, July 1980. 

HILLS, A.: 

"Large-Scale Wireless LAN Design,"

 

IEEE Commun. Magazine

, vol. 39, pp. 98-104, Nov. 2001. 

HOLZMANN, G.J.: 

Design and Validation of Computer Protocols

, Englewood Cliffs, NJ: Prentice Hall, 1991. 

HU, Y., and LI, V.O.K.: 

"Satellite-Based Internet Access,"

 

IEEE Commun. Magazine

, vol. 39, pp. 155-162, March 2001. 

HU, Y.-C., and JOHNSON, D.B.: 

"Implicit Source Routes for On-Demand Ad Hoc Network Routing,"

 

Proc. ACM Int'l Symp. on Mobile Ad Hoc Networking & Computing

, ACM, pp. 1-10, 2001. 

HUANG, V., and ZHUANG, W.: 

"QoS-Oriented Access Control for 4G Mobile Multimedia CDMA Communications,"

 

IEEE Commun. Magazine

, vol. 40, pp. 118-125, March 2002. 

HUBER, J.F., WEILER, D., and BRAND, H.: 

"UMTS, the Mobile Multimedia Vision for IMT-2000: A Focus on Standardization,"

 

IEEE Commun. Magazine

, vol. 38, pp. 129-136, Sept. 2000. nr u 0 

662




HUI, J.: 

"A Broadband Packet Switch for Multi-rate Services,"

 

Proc. Int'l Conf. on Commun.

, IEEE, pp. 782-788, 1987. 

HUITEMA, C.: 

Routing in the Internet

, Englewood Cliffs, NJ: Prentice Hall, 1995. 

HULL, S.: 

Content Delivery Networks

, Berkeley, CA: Osborne/McGraw-Hill, 2002. 

HUMBLET, P.A., RAMASWAMI, R., and SIVARAJAN, K.N.: 

"An Efficient Communication Protocol for High-Speed Packet-Switched Multichannel Networks,"

 

Proc. SIGCOMM '92 Conf.

, ACM, pp. 2-13, 1992. 

HUNTER, D.K., and ANDONOVIC, I.: 

"Approaches to Optical Internet Packet Switching,"

 

IEEE Commun. Magazine

, vol. 38, pp. 116-122, Sept. 2000. 

HUSTON, G.: 

"TCP in a Wireless World,"

 

IEEE Internet Computing

, vol. 5, pp. 82-84, March-April, 2001. 

IBE, O.C.: 

Essentials of ATM Networks and Services

, Boston: Addison-Wesley, 1997. 

IRMER, T.: 

"Shaping Future Telecommunications: The Challenge of Global Standardization,"

 

IEEE Commun. Magazine

, vol. 32, pp. 20-28, Jan. 1994. 

IZZO, P.: 

Gigabit Networks

, New York: Wiley, 2000. 

JACOBSON, V.: 

"Congestion Avoidance and Control,"

 

Proc. SIGCOMM '88 Conf.

, ACM, pp. 314-329, 1988. 

JAIN, R.: 

"Congestion Control and Traffic Management in ATM Networks: Recent Advances and a Survey,"

 

Computer Networks and ISDN Systems

, vol. 27, Nov. 1995. 

JAIN, R.: 

FDDI Handbook—High-Speed Networking Using Fiber and Other Media

, Boston: Addison-Wesley, 1994. 

JAIN, R.: 

"Congestion Control in Computer Networks: Issues and Trends,"

 

IEEE Network Magazine

, vol. 4, pp. 24-30, May/June 1990. 

JAKOBSSON, M., and WETZEL, S.: 

"Security Weaknesses in Bluetooth,"

 

Topics in Cryptology: CT-RSA 2001

, Berlin: Springer-Verlag LNCS 2020, pp. 176-191, 2001. 

JOEL, A.: 

"Telecommunications and the IEEE Communications Society,"

 

IEEE Commun. Magazine

, 50th Anniversary Issue, pp. 6-14 and 162-167, May 2002. 

JOHANSSON, P., KAZANTZIDIS, M., KAPOOR, R., and GERLA, M.: 

"Bluetooth: An Enabler for Personal Area Networking,"

 

IEEE Network Magazine

, vol. 15, pp. 28-37, Sept.-Oct 2001. 

JOHNSON, D.B.: 

"Scalable Support for Transparent Mobile Host Internetworking,"

 

Wireless Networks

, vol. 1, pp. 311-321, Oct. 1995. 

JOHNSON, H.W.: 

Fast Ethernet—Dawn of a New Network

, Englewood Cliffs, NJ: Prentice Hall, 1996. 

JOHNSON, N.F., and JAJODA, S.: 

"Exploring Steganography: Seeing the Unseen,"

 

Computer

, vol. 31, pp. 26-34, Feb. 1998. 

KAHN, D.: 

"Cryptology Goes Public,"

 

IEEE Commun. Magazine

, vol. 18, pp. 19-28, March 1980. 

663




KAHN, D.: 

The Codebreakers

, 2nd ed., New York: Macmillan, 1995. 

KAMOUN, F., and KLEINROCK, L.: 

"Stochastic Performance Evaluation of Hierarchical Routing for Large Networks,"

 

Computer Networks

, vol. 3, pp. 337-353, Nov. 1979. 

KAPP, S.: 

"802.11: Leaving the Wire Behind,"

 

IEEE Internet Computing

, vol. 6, pp. 82-85, Jan.-Feb. 2002. 

KARN, P.: 

"MACA—A New Channel Access Protocol for Packet Radio,"

 

ARRL/CRRL Amateur Radio Ninth Computer Networking Conf.

, pp. 134-140, 1990. 

KARTALOPOULOS, S.: 

Introduction to DWDM Technology: Data in a Rainbow

, New York, NY: IEEE Communications Society, 1999. 

KASERA, S.K., HJALMTYSSON, G., TOWLSEY, D.F., and KUROSE, J.F.: 

"Scalable Reliable Multicast Using Multiple Multicast Channels,"

 

IEEE/ACM Trans. on Networking

, vol. 8, pp. 294-310, 2000. 

KATZ, D., and FORD, P.S.: 

"TUBA: Replacing IP with CLNP,"

 

IEEE Network Magazine

, vol. 7, pp. 38-47, May/June 1993. 

KATZENBEISSER, S., and PETITCOLAS, F.A.P.: 

Information Hiding Techniques for Steganography and Digital Watermarking

, London, Artech House, 2000. 

KAUFMAN, C., PERLMAN, R., and SPECINER, M.: 

Network Security

, 2nd ed., Englewood Cliffs, NJ: Prentice Hall, 2002. 

KELLERER, W., VOGEL, H.-J., and STEINBERG, K.-E.: 

"A Communication Gateway for Infrastructure-Independent 4G Wireless Access,"

 

IEEE Commun. Magazine

, vol. 40, pp. 126-131, March 2002. 

KERCKHOFF, A.: 

"La Cryptographie Militaire,"

 

J. des Sciences Militaires

, vol. 9, pp. 5-38, Jan. 1883 and pp. 161-191, Feb. 1883. 

KIM, J.B., SUDA, T., and YOSHIMURA, M.: 

"International Standardization of B-ISDN,"

 

Computer Networks and ISDN Systems

, vol. 27, pp. 5-27, Oct. 1994. 

KIPNIS, J.: 

"Beating the System: Abuses of the Standards Adoptions Process,"

 

IEEE Commun. Magazine

, vol. 38, pp. 102-105, July 2000. 

KLEINROCK, L.: 

"On Some Principles of Nomadic Computing and Multi-Access Communications,"

 

IEEE Commun. Magazine

, vol. 38, pp. 46-50, July 2000. 

KLEINROCK, L., and TOBAGI, F.: 

"Random Access Techniques for Data Transmission over Packet-Switched Radio Channels,"

 

Proc. Nat. Computer Conf.

, pp. 187-201, 1975. 

KRISHNAMURTHY, B., and REXFORD, J.: 

Web Protocols and Practice

, Boston: Addison-Wesley, 2001. 

KUMAR, V., KORPI, M., and SENGODAN, S.: 

IP Telephony with H.323

, New York: Wiley, 2001. 

KUROSE, J.F., and ROSS, K.W.: 

Computer Networking: A Top-Down Approach Featuring the Internet

, Boston: Addison-Wesley, 2001. 

KWOK, T.: 

"A Vision for Residential Broadband Service: ATM to the Home,"

 

IEEE Network Magazine

, vol. 9, pp. 14-28, Sept./Oct. 1995. 

664




KYAS, O., and CRAWFORD, G.: 

ATM Networks

, Upper Saddle River, NJ: Prentice Hall, 2002. 

LAM, C.K.M., and TAN, B.C.Y.: 

"The Internet Is Changing the Music Industry,"

 

Commun. of the ACM

, vol. 44, pp. 62-66, Aug. 2001. 

LANSFORD, J., STEPHENS, A, and NEVO, R.: 

"Wi-Fi (802.11b) and Bluetooth: Enabling Coexistence,"

 

IEEE Network Magazine

, vol. 15, pp. 20-27, Sept.-Oct 2001. 

LASH, D.A.: 

The Web Wizard's Guide to Perl and CGI

, Boston: Addison-Wesley, 2002. 

LAUBACH, M.E., FARBER, D.J., and DUKES, S.D.: 

Delivering Internet Connections over Cable

, New York: Wiley, 2001. 

LEE, J.S., and MILLER, L.E.: 

CDMA Systems Engineering Handbook

, London: Artech House, 1998. 

LEEPER, D.G.: 

"A Long-Term View of Short-Range Wireless,"

 

Computer

, vol. 34, pp. 39-44, June 2001. 

LEINER, B.M., COLE, R., POSTEL, J., and MILLS, D.: 

"The DARPA Internet Protocol Suite,"

 

IEEE Commun. Magazine

, vol. 23, pp. 29-34, March 1985. 

LEVINE, D.A., and AKYILDIZ, I.A.: 

"PROTON: A Media Access Control Protocol for Optical Networks with Star Topology,"

 

IEEE/ACM Trans. on Networking

, vol. 3, pp. 158-168, April 1995. 

LEVY, S.: 

"Crypto Rebels,"

 

Wired

, pp. 54-61, May/June 1993. 

LI, J., BLAKE, C., DE COUTO, D.S.J., LEE, H.I., and MORRIS, R.: 

"Capacity of Ad Hoc Wireless Networks,"

 

Proc. 7th Int'l Conf. on Mobile Computing and Networking

, ACM, pp. 61-69, 2001. 

LIN, F., CHU, P., and LIU, M.: 

"Protocol Verification Using Reachability Analysis: The State Space Explosion Problem and Relief Strategies,"

 

Proc. SIGCOMM '87 Conf.

, ACM, pp. 126-135, 1987. 

LIN, Y.-D., HSU, N.-B., and HWANG, R.-H.: 

"QoS Routing Granularity in MPLS Networks"

 , 

IEEE Commun. Magazine

, vol. 40, pp. 58-65, June 2002. 

LISTANI, M., ERAMO, V., and SABELLA, R.: 

"Architectural and Technological Issues for Future Optical Internet Networks,"

 

IEEE Commun. Magazine

, vol. 38, pp. 82-92, Sept. 2000. 

LIU, C.L., and LAYLAND, J.W.: 

"Scheduling Algorithms for Multiprogramming in a Hard Real-Time Environment,"

 

Journal of the ACM

, vol. 20, pp. 46-61, Jan. 1973. 

LIVINGSTON, D.: 

Essential XML for Web Professionals

, Upper Saddle River, NJ: Prentice Hall, 2002. 

LOSHIN, P.: 

IPv6 Clearly Explained

, San Francisco: Morgan Kaufmann, 1999. 

LOUIS, P.J.: 

Broadband Crash Course

, New York: McGraw-Hill, 2002. 

LU, W.: 

Broadband Wireless Mobile: 3G and Beyond

, New York: Wiley, 2002. 

MACEDONIA, M.R.: 

"Distributed File Sharing,"

 

Computer

, vol. 33, pp. 99-101, 2000. 

665




MADRUGA, E.L., and GARCIA-LUNA-ACEVES, J.J.: 

"Scalable Multicasting: the Core-Assisted Mesh Protocol,"

 

Mobile Networks and Applications

, vol. 6, pp. 151-165, April 2001. 

MALHOTRA, R.: 

IP Routing

, Sebastopol, CA: O'Reilly, 2002. 

MATSUI, M.: 

"Linear Cryptanalysis Method for DES Cipher,"

 

Advances in Cryptology— Eurocrypt '93 Proceedings

, Berlin: Springer-Verlag LNCS 765, pp. 386-397, 1994. 

MAUFER, T.A.: 

IP Fundamentals

, Upper Saddle River, NJ: Prentice Hall, 1999. 

MAZIERES, D., and KAASHOEK, M.F.: 

"The Design, Implementation, and Operation of an Email Pseudonym Server,"

 

Proc. Fifth Conf. on Computer and Commun. Security

, ACM, pp. 27-36, 1998. 

MAZIERES, D., KAMINSKY, M., KAASHOEK, M.F., and WITCHEL, E.: 

"Separating Key Management from File System Security,"

 

Proc. 17th Symp. on Operating Systems Prin.

, ACM, pp. 124-139, Dec. 1999. 

MCFEDRIES, P: 

Using JavaScript

, Indianapolis, IN: Que, 2001. 

MCKENNEY, P.E., and DOVE, K.F.: 

"Efficient Demultiplexing of Incoming TCP Packets,"

 

Proc. SIGCOMM '92 Conf.

, ACM, pp. 269-279, 1992. 

MELTZER, K., and MICHALSKI, B.: 

Writing CGI Applications with Perl

, Boston: Addison-Wesley, 2001. 

MENEZES, A.J., and VANSTONE, S.A.: 

"Elliptic Curve Cryptosystems and Their Implementation,"

 

Journal of Cryptology

, vol. 6, pp. 209-224, 1993. 

MERKLE, R.C.: 

"Fast Software Encryption Functions,"

 

Advances in Cryptology— CRYPTO '90 Proceedings

, Berlin: Springer-Verlag LNCS 473, pp. 476-501, 1991. 

MERKLE, R.C., and HELLMAN, M.: 

"On the Security of Multiple Encryption,"

 

Commun. of the ACM

, vol. 24, pp. 465-467, July 1981. 

MERKLE, R.C., and HELLMAN, M.: 

"Hiding and Signatures in Trapdoor Knapsacks,"

 

IEEE Trans. on Information Theory

, vol. IT-24, pp. 525-530, Sept. 1978. 

METCALFE, R.M.: 

"On Mobile Computing,"

 

Byte

, vol. 20, p. 110, Sept. 1995. 

METCALFE, R.M.: 

"Computer/Network Interface Design: Lessons from Arpanet and Ethernet,"

 

IEEE Journal on Selected Areas in Commun.

, vol. 11, pp. 173-179, Feb. 1993. 

METCALFE, R.M., and BOGGS, D.R.: 

"Ethernet: Distributed Packet Switching for Local Computer Networks,"

 

Commun. of the ACM

, vol. 19, pp. 395-404, July 1976. 

METZ, C: 

"Interconnecting ISP Networks,"

 

IEEE Internet Computing

, vol. 5, pp. 74-80, March-April 2001. 

METZ, C.: 

"Differentiated Services,"

 

IEEE Multimedia Magazine

, vol. 7, pp. 84-90, July-Sept. 2000. 

METZ, C.: 

"IP Routers: New Tool for Gigabit Networking,"

 

IEEE Internet Computing

, vol. 2, pp. 14-18, Nov.-Dec. 1998. 

666




MILLER, B.A., and BISDIKIAN, C.,: 

Bluetooth Revealed

, Upper Saddle River, NJ: Prentice Hall, 2001. 

MILLER, P., and CUMMINS, M.: 

LAN Technologies Explained

, Woburn, MA: Butterworth-Heinemann, 2000. 

MINOLI, D.: 

Video Dialtone Technology

, New York: McGraw-Hill, 1995. 

MINOLI, D., and VITELLA, M.: 

ATM & Cell Relay for Corporate Environments

, New York: McGraw-Hill, 1994. 

MISHRA, P.P., and KANAKIA, H.: 

"A Hop by Hop Rate-Based Congestion Control Scheme,"

 

Proc. SIGCOMM '92 Conf.

, ACM, pp. 112-123, 1992. 

MISRA, A., DAS, S., DUTTA, A., MCAULEY, A., and DAS, S.: 

"IDMP-Based Fast Handoffs and Paging in IP-Based 4G Mobile Networks,"

 

IEEE Commun. Magazine

, vol. 40, pp. 138-145, March 2002. 

MOGUL, J.C.: 

"IP Network Performance,"

 in 

Internet System Handbook

, Lynch, D.C. and Rose, M.T. (eds.), Boston: Addison-Wesley, pp. 575-675, 1993. 

MOK, A.K., and WARD, S.A.: 

"Distributed Broadcast Channel Access,"

 

Computer Networks

, vol. 3, pp. 327-335, Nov. 1979. 

MOY, J.: 

"Multicast Routing Extensions,"

 

Commun. of the ACM

, vol. 37, pp. 61-66, AUg. 1994. 

MULLINS, J.: 

"Making Unbreakable Code,"

 

IEEE Spectrum

, pp. 40-45, May 2002. 

NAGLE, J.: 

"On Packet Switches with Infinite Storage,"

 

IEEE Trans. on Commun.

, vol. COM-35, pp. 435-438, April 1987. 

NAGLE, J.: 

"Congestion Control in TCP/IP Internetworks,"

 

Computer Commun. Rev.

, vol. 14, pp. 11-17, Oct. 1984. 

NARAYANASWAMI, C., KAMIJOH, N., RAGHUNATH, M., INOUE, T., CIPOLLA, T., SANFORD, J., SCHLIG, E., VENTKITESWARAN, S., GUNIGUNTALA, D., KULKARNI, V., and YAMAZAKI, K.: 

"IBM's Linux Watch: The Challenge of Miniaturization,"

 

Computer

, vol. 35, pp. 33-41, Jan. 2002. 

NAUGHTON, J.: 

"A Brief History of the Future,"

 Woodstock, NY: Overlook Press, 2000. 

NEEDHAM, R.M., and SCHROEDER, M.D.: 

"Authentication Revisited,"

 

Operating Systems Rev.

, vol. 21, p. 7, Jan. 1987. 

NEEDHAM, R.M., and SCHROEDER, M.D.: 

"Using Encryption for Authentication in Large Networks of Computers,"

 

Commun. of the ACM

, vol. 21, pp. 993-999, Dec. 1978. 

NELAKUDITI, S., and ZHANG, Z.-L.: 

"A Localized Adaptive Proportioning Approach to QoS Routing,"

 

IEEE Commun. Magazine

 vol. 40, pp. 66-71, June 2002. 

NEMETH, E., SNYDER, G., SEEBASS, S., and HEIN, T.R.: 

UNIX System Administration Handbook

, 3rd ed., Englewood Cliffs, NJ: Prentice Hall, 2000. 

NICHOLS, R.K., and LEKKAS, P.C.: 

Wireless Security

, New York: McGraw-Hill, 2002. 

667




NIST: 

"Secure Hash Algorithm,"

 U.S. Government Federal Information Processing Standard 180, 1993. 

O'HARA, B., and PETRICK, A.: 

802.11 Handbook: A Designer's Companion

, New York: IEEE Press, 1999. 

OTWAY, D., and REES, O.: 

"Efficient and Timely Mutual Authentication,"

 

Operating Systems Rev.

, pp. 8-10, Jan. 1987. 

OVADIA, S.: 

Broadband Cable TV Access Networks: from Technologies to Applications

, Upper Saddle River, NJ: Prentice Hall, 2001. 

PALAIS, J.C.: 

Fiber Optic Commun

., 3rd ed., Englewood Cliffs, NJ: Prentice Hall, 1992. 

PAN, D.: 

"A Tutorial on MPEG/Audio Compression,"

 

IEEE Multimedia Magazine

, vol. 2, pp.60-74, Summer 1995. 

PANDYA, R.: 

"Emerging Mobile and Personal Communication Systems,"

 

IEEE Commun. Magazine

, vol. 33, pp. 44-52, June 1995. 

PARAMESWARAN, M., SUSARLA, A., and WHINSTON, A.B.: 

"P2P Networking: An Information-Sharing Alternative,"

 

Computer

, vol. 34, pp. 31-38, July 2001. 

PARK, J.S., and SANDHU, R.: 

"Secure Cookies on the Web,"

 

IEEE Internet Computing

, vol. 4, pp. 36-44, July-Aug. 2000. 

PARTRIDGE, C., HUGHES, J., and STONE, J.: 

"Performance of Checksums and CRCs over Real Data,"

 

Proc. SIGCOMM '95 Conf.

, ACM, pp. 68-76, 1995. 

PAXSON, V.: 

"Growth Trends in Wide-Area TCP Connections,"

 

IEEE Network Magazine

, vol. 8, pp. 8-17, July/Aug. 1994. 

PAXSON, V., and FLOYD, S.: 

"Wide-Area Traffic: The Failure of Poisson Modeling,"

 

Proc. SIGCOMM '94 Conf.

, ACM, pp. 257-268, 1995. 

PEPELNJAK, I., and GUICHARD, J.: 

MPLS and VPN Architectures

, Indianapolis, IN: Cisco Press, 2001. 

PERKINS, C.E.: 

RTP: Audio and Video for the Internet

, Boston: Addison-Wesley, 2002. 

PERKINS, C.

E.

 (ed.): 

Ad Hoc Networking

, Boston: Addison-Wesley, 2001. 

PERKINS, C.E.: 

Mobile IP Design Principles and Practices

, Upper Saddle River, NJ: Prentice Hall, 1998a. 

PERKINS, C.E.: 

"Mobile Networking in the Internet,"

 

Mobile Networks and Applications

, vol. 3, pp. 319-334, 1998b. 

PERKINS, C.E.: 

"Mobile Networking through Mobile IP,"

 

IEEE Internet Computing

, vol. 2, pp. 58-69, Jan.-Feb. 1998c. 

PERKINS, C.E., and ROYER, E.: 

"The Ad Hoc On-Demand Distance-Vector Protocol,"

 in 

Ad Hoc Networking

, edited by C. Perkins, Boston: Addison-Wesley, 2001. 

668




PERKINS, C.E., and ROYER, E.: 

"Ad-hoc On-Demand Distance Vector Routing,"

 

Proc. Second Ann. IEEE Workshop on Mobile Computing Systems and Applications

, IEEE, pp. 90-100, 1999. 

PERLMAN, R.: 

Interconnections

, 2nd ed., Boston: Addison-Wesley, 2000. 

PERLMAN, R.: 

Network Layer Protocols with Byzantine Robustness

, Ph.D. thesis, M.I.T., 1988. 

PERLMAN, R., and KAUFMAN, C.: 

"Key Exchange in IPsec,"

 

IEEE Internet Computing

, vol. 4, pp. 50-56, Nov.-Dec. 2000. 

PETERSON, L.L., and DAVIE, B.S.: 

Computer Networks: A Systems Approach

, San Francisco: Morgan Kaufmann, 2000. 

PETERSON, W.W., and BROWN, D.T.: 

"Cyclic Codes for Error Detection,"

 

Proc. IRE

, vol. 49, pp. 228-235, Jan. 1961. 

PICKHOLTZ, R.L., SCHILLING, D.L., and MILSTEIN, L.B.: 

"Theory of Spread Spectrum Communication—A Tutorial,"

 

IEEE Trans. on Commun.

, vol. COM-30, pp. 855-884, May 1982. 

PIERRE, G., KUZ, I., VAN STEEN, M., TANENBAUM, A.S.: 

"Differentiated Strategies for Replicating Web Documents,"

 

Computer Commun

., vol. 24, pp. 232-240, Feb. 2001. 

PIERRE, G., VAN STEEN, M., and TANENBAUM, A.S.: 

"Dynamically Selecting Optimal Distribution Strategies for Web Documents,"

 

IEEE Trans. on Computers

, vol. 51, pp., June 2002. 

PISCITELLO, D.M., and CHAPIN, A.L.: 

Open Systems Networking: TCP/IP and OSI

, Boston: Addison-Wesley, 1993. 

PITT, D.A.: 

"Bridging—The Double Standard,"

 

IEEE Network Magazine

, vol. 2, pp. 94-95, Jan. 1988. 

PIVA, A., BARTOLINI, F., and BARNI, M.: 

"Managing Copyrights in Open Networks,"

 

IEEE Internet Computing

, vol. 6, pp. 18-26, May-June 2002. 

POHLMANN, N.: 

Firewall Systems

, Bonn, Germany: MITP-Verlag, 2001. 

PUZMANOVA, R.: 

Routing and Switching: Time of Convergence?

, London: AddisonWesley, 2002. 

RABINOVICH, M., and SPATSCHECK, O,: 

Web Caching and Replication

, Boston: Addison-Wesley, 2002. 

RAJU, J., and GARCIA-LUNA-ACEVES, J.J.: 

"Scenario-based Comparison of Source-Tracing and Dynamic Source Routing Protocols for Ad-Hoc Networks,"

 

ACM Computer Communications Review

, vol. 31, October 2001. 

RAMANATHAN, R., and REDI, J.: 

"A Brief Overview of Ad Hoc Networks: Challenges and Directions,"

 

IEEE Commun. Magazine

, 50th Anniversary Issue, pp. 20-22, May 2002. 

RATNASAMY, S., FRANCIS, P., HANDLEY, M., KARP, R., and SHENKER, S.: 

"A Scalable Content-Addressable Network,"

 

Proc. SIGCOMM '01 Conf.

, ACM, pp. 1161-172, 2001. 

RIVEST, R.L.: 

"The MD5 Message-Digest Algorithm,"

 RFC 1320, April 1992. 

669




RIVEST, R.L., and SHAMIR, A.: 

"How to Expose an Eavesdropper,"

 

Commun. of the ACM

, vol. 27, pp. 393-395, April 1984. 

RIVEST, R.L., SHAMIR, A., and ADLEMAN, L.: 

"On a Method for Obtaining Digital Signatures and Public Key Cryptosystems,"

 

Commun. of the ACM

, vol. 21, pp. 120-126, Feb. 1978. 

ROBERTS, L.G.: 

"Dynamic Allocation of Satellite Capacity through Packet Reservation,"

 

Proc. NCC

, AFIPS, pp. 711-716, 1973. 

ROBERTS, L.G.: 

"Extensions of Packet Communication Technology to a Hand Held Personal Terminal,"

 

Proc. Spring Joint Computer Conference

, AFIPS, pp. 295-298, 1972. 

ROBERTS, L.G.: 

"Multiple Computer Networks and Intercomputer Communication,"

 

Proc. First Symp. on Operating Systems Prin.

, ACM, 1967. 

ROSE, M.T.: 

The Simple Book

, Englewood Cliffs, NJ: Prentice Hall, 1994. 

ROSE, M.T.: 

The Internet Message

, Englewood Cliffs, NJ: Prentice Hall, 1993. 

ROSE, M.T., and MCCLOGHRIE, K.: 

How to Manage Your Network Using SNMP

, Englewood Cliffs, NJ: Prentice Hall, 1995. 

ROWSTRON, A., and DRUSCHEL, P.: 

"Storage Management and Caching in PAST, a Large-Scale, Persistent Peer-to-Peer Storage Utility,"

 

Proc. 18th Symp. on Operating Systems Prin.

, ACM, pp. 188-201, 2001a. 

ROWSTRON, A., and DRUSCHEL, P.: 

"Pastry: Scalable, Distributed Object Location and Routing for Large-Scale Peer-to-Peer Storage Utility,"

 

Proc. 18th Int'l Conf. on Distributed Systems Platforms

, ACM/IFIP, 2001b. 

ROYER, E.M., and TOH, C.-K.: 

"A Review of Current Routing Protocols for Ad-Hoc Mobile Wireless Networks,"

 

IEEE Personal Commun. Magazine

, vol. 6, pp. 46-55, April 1999. 

RUIZ-SANCHEZ, M.A., BIERSACK, E.W., and DABBOUS, W.: 

"Survey and Taxonomy of IP Address Lookup Algorithms,"

 

IEEE Network Magazine

, vol. 15, pp. 8-23, March-April 2001. 

SAIRAM, K.V.S.S.S.S., GUNASEKARAN, N., and REDDY, S.R.: 

"Bluetooth in Wireless Communication,"

 

IEEE Commun. Mag.

, vol. 40, pp. 90-96, June 2002. 

SALTZER, J.H., REED, D.P., and CLARK, D.D.: 

"End-to-End Arguments in System Design,"

 

ACM Trans. on Computer Systems

, vol. 2, pp. 277-288, Nov. 1984. 

SANDERSON, D.W., and DOUGHERTY, D.: 

Smileys

, Sebastopol, CA: O'Reilly, 1993. 

SARI, H., VANHAVERBEKE, F., and MOENECLAEY, M.: 

"Extending the Capacity of Multiple Access Channels,"

 

IEEE Commun. Magazine

, vol. 38, pp. 74-82, Jan. 2000. 

SARIKAYA, B.: 

"Packet Mode in Wireless Networks: Overview of Transition to Third Generation,"

 

IEEE Commun. Magazine

, vol. 38, pp. 164-172, Sept. 2000. 

SCHNEIER, B.: 

Secrets and Lies

, New York: Wiley, 2000. 

SCHNEIER, B.: 

Applied Cryptography

, 2nd ed., New York: Wiley, 1996. 

SCHNEIER, B.: 

E-Mail Security

, New York: Wiley, 1995. 

670




SCHNEIER, B.: 

"Description of a New Variable-Length Key, 64-Bit Block Cipher [Blowfish],"

 

Proc. of the Cambridge Security Workshop

, Berlin: Springer-Verlag LNCS 809, pp. 191-204, 1994. 

SCHNORR, C.P.: 

"Efficient Signature Generation for Smart Cards,"

 

Journal of Cryptology

, vol. 4, pp. 161-174, 1991. 

SCHOLTZ, R.A.: 

"The Origins of Spread-Spectrum Communications,"

 

IEEE Trans. on Commun.

, vol. COM-30, pp. 822-854, May 1982. 

SCOTT, R.: 

"Wide Open Encryption Design Offers Flexible Implementations,"

 

Cryptologia

, vol. 9, pp. 75-90, Jan. 1985. 

SEIFERT, R.: 

The Switch Book

, Boston: Addison-Wesley, 2000. 

SEIFERT, R.: 

Gigabit Ethernet

, Boston: Addison-Wesley, 1998. 

SENN, J.A.: 

"The Emergence of M-Commerce,"

 

Computer

, vol. 33, pp. 148-150, Dec. 2000. 

SERJANTOV, A.: 

"Anonymizing Censorship Resistant Systems,"

 

Proc. First Int'l Workshop on Peer-to-Peer Systems

, Berlin: Springer-Verlag LNCS, 2002. 

SEVERANCE, C.: 

"IEEE 802.11: Wireless Is Coming Home,"

 

Computer

, vol. 32, pp. 126-127, Nov. 1999. 

SHAHABI, C., ZIMMERMANN, R., FU, K., and YAO, S.-Y.D.: 

"YIMA: A Second-Generation Continuous Media Server,"

 

Computer

, vol. 35, pp. 56-64, June 2002. 

SHANNON, C.: 

"A Mathematical Theory of Communication,"

 

Bell System Tech. J.

, vol. 27, pp. 379-423, July 1948; and pp. 623-656, Oct. 1948. 

SHEPARD, S.: 

SONET/SDH Demystified

, New York: McGraw-Hill, 2001. 

SHREEDHAR, M., and VARGHESE, G.: 

"Efficient Fair Queueing Using Deficit Round Robin,"

 

Proc. SIGCOMM '95 Conf.

, ACM, pp. 231-243, 1995. 

SKOUDIS, E.: 

Counter Hack

, Upper Saddle River, NJ: Prentice Hall, 2002. 

SMITH, D.K., and ALEXANDER, R.C.: 

Fumbling the Future

, New York: William Morrow, 1988. 

SMITH, R.W.: 

Broadband Internet Connections

, Boston: Addison Wesley, 2002. 

SNOEREN, A.C., and BALAKRISHNAN, H.: 

"An End-to-End Approach to Host Mobility,"

 

Int'l Conf. on Mobile Computing and Networking

 , ACM, pp. 155-166, 2000. 

SOBEL, D.L.: 

"Will Carnivore Devour Online Privacy,"

 

Computer

, vol. 34, pp. 87-88, May 2001. 

SOLOMON, J.D.: 

Mobile IP: The Internet Unplugged

, Upper Saddle River, NJ: Prentice Hall, 1998. 

SPOHN, M., and GARCIA-LUNA-ACEVES, J.J.: 

"Neighborhood Aware Source Routing,"

 

Proc. ACM MobiHoc 2001

, ACM, pp. 2001. 

SPURGEON, C.E.: 

Ethernet: The Definitive Guide

, Sebastopol, CA: O'Reilly, 2000. 

671




STALLINGS, W.: 

Data and Computer Communications

, 6th ed., Upper Saddle River, NJ: Prentice Hall, 2000. 

STEINMETZ, R., and NAHRSTEDT, K.: 

Multimedia Fundamentals. Vol. 1: Media Coding and Content Processing

, Upper Saddle River, NJ: Prentice Hall, 2002. 

STEINMETZ, R., and NAHRSTEDT, K.: 

Multimedia Fundamentals. Vol. 2: Media Processing and Communications

, Upper Saddle River, NJ: Prentice Hall, 2003a. 

STEINMETZ, R., and NAHRSTEDT, K.: 

Multimedia Fundamentals. Vol. 3: Documents, Security, and Applications

, Upper Saddle River, NJ: Prentice Hall, 2003b. 

STEINER, J.G., NEUMAN, B.C., and SCHILLER, J.I.: 

"Kerberos: An Authentication Service for Open Network Systems,"

 

Proc. Winter USENIX Conf.

, USENIX, pp. 191-201, 1988. 

STEVENS, W.R.: 

UNIX Network Programming, Volume 1: Networking APIs - Sockets and XTI

, Upper Saddle River, NJ: Prentice Hall, 1997. 

STEVENS, W.R.: 

TCP/IP Illustrated

, Vol. 1, Boston: Addison-Wesley, 1994. 

STEWART, R., and METZ, C.: 

"SCTP: New Transport Protocol for TCP/IP,"

 

IEEE Internet Computing

, vol. 5, pp. 64-69, Nov.-Dec. 2001. 

STINSON, D.R.: 

Cryptography Theory and Practice

, 2nd ed., Boca Raton, FL: CRC Press, 2002. 

STOICA, I., MORRIS, R., KARGER, D., KAASHOEK, M.F., and BALAKRISHNAN, H.: 

"Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications,"

 

Proc. SIGCOMM '01 Conf.

, ACM, pp. 149-160, 2001. 

STRIEGEL, A., and MANIMARAN, G.: 

"A Survey of QoS Multicasting Issues,"

 

IEEE Commun. Mag.

, vol. 40, pp. 82-87, June 2002. 

STUBBLEFIELD, A., IOANNIDIS, J., and RUBIN, A.D.: 

"Using the Fluhrer, Mantin, and Shamir Attack to Break WEP,"

 

Proc Network and Distributed Systems Security Symp.

, ISOC, pp. 1-11, 2002. 

SUMMERS, C.K.: 

ADSL: Standards, Implementation, and Architecture

, Boca Raton, FL: CRC Press, 1999. 

SUNSHINE, C.A., and DALAL, Y.K.: 

"Connection Management in Transport Protocols,"

 

Computer Networks

, vol. 2, pp. 454-473, 1978. 

TANENBAUM, A.S.: 

Modern Operating Systems

, Upper Saddle River, NJ: Prentice Hall, 2001. 

TANENBAUM, A.S., and VAN STEEN, M.: 

Distributed Systems: Principles and Paradigms

, Upper Saddle River, NJ: Prentice Hall, 2002. 

TEGER, S., and WAKS, D.J.: 

"End-User Perspectives on Home Networking,"

 

IEEE Commun. Magazine

, vol. 40, pp. 114-119, April 2002. 

THYAGARAJAN, A.S., and DEERING, S.E.: 

"Hierarchical Distance-Vector Multicast Routing for the MBone,"

 

Proc. SIGCOMM '95 Conf.

, ACM, pp. 60-66, 1995. 

TITTEL, E., VALENTINE, C., BURMEISTER, M., and DYKES, L.: 

Mastering XHTML

, Alameda, CA: Sybex, 2001. 

672




TOKORO, M., and TAMARU, K.: 

"Acknowledging Ethernet,"

 

Compcon

, IEEE, pp. 320-325, Fall 1977. 

TOMLINSON, R.S.: 

"Selecting Sequence Numbers,"

 

Proc. SIGCOMM/SIGOPS Inter-process Commun. Workshop

, ACM, pp. 11-23, 1975. 

TSENG, Y.-C., WU, S.-L., LIAO, W.-H., and CHAO, C.-M.: 

"Location Awareness in Ad Hoc Wireless Mobile Networks,"

 

Computer

, vol. 34, pp. 46-51, 2001. 

TUCHMAN, W.: 

"Hellman Presents No Shortcut Solutions to DES,"

 

IEEE Spectrum

, vol. 16, pp. 40-41, July 1979. 

TURNER, J.S.: 

"New Directions in Communications (or Which Way to the Information Age),"

 

IEEE Commun. Magazine

, vol. 24, pp. 8-15, Oct. 1986. 

VACCA, J.R.: 

I-Mode Crash Course

, New York: McGraw-Hill, 2002. 

VALADE, J.,: 

PHP & MySQL for Dummies

, New York: Hungry Minds, 2002. 

VARGHESE, G., and LAUCK, T.: 

"Hashed and Hierarchical Timing Wheels: Data Structures for the Efficient Implementation of a Timer Facility,"

 

Proc. 11th Symp. on Operating Systems Prin.

, ACM, pp. 25-38, 1987. 

VARSHNEY, U., SNOW, A., MCGIVERN, M., and HOWARD, C.: 

"Voice over IP,"

 

Commun. of the ACM

, vol. 45, pp. 89-96, 2002. 

VARSHNEY, U., and VETTER, R.: 

"Emerging Mobile and Wireless Networks,"

 

Commun. of the ACM

, vol. 43, pp. 73-81, June 2000. 

VETTER, P., GODERIS, D., VERPOOTEN, L., and GRANGER, A.: 

"Systems Aspects of APON/VDSL Deployment,"

 

IEEE Commun. Magazine

, vol. 38, pp. 66-72, May 2000. 

WADDINGTON, D.G., and CHANG, F.: 

"Realizing the Transition to IPv6,"

 

IEEE Commun. Mag.

, vol. 40, pp. 138-148, June 2002. 

WALDMAN, M., RUBIN, A.D., and CRANOR, L.F.: 

"Publius: A Robust, Tamper-Evident, Censorship-Resistant, Web Publishing System,"

 

Proc. Ninth USENIX Security Symp.

, USENIX, pp. 59-72, 2000. 

WANG, Y., and CHEN, W.: 

"Supporting IP Multicast for Mobile Hosts,"

 

Mobile Networks and Applications

, vol. 6, pp. 57-66, Jan.-Feb. 2001. 

WANG, Z.: 

Internet QoS

, San Francisco: Morgan Kaufmann, 2001. 

WARNEKE, B., LAST, M., LIEBOWITZ, B., and PISTER, K.S.J.: 

"Smart Dust: Communicating with a Cubic Millimeter Computer,"

 

Computer

, vol. 34, pp. 44-51, Jan. 2001. 

WAYNER, P.: 

Disappearing Cryptography: Information Hiding, Steganography, and Watermarking

, 2nd ed., San Francisco: Morgan Kaufmann, 2002. 

WEBB, W.: 

"Broadband Fixed Wireless Access as a Key Component of the Future Integrated Communications Environment,"

 

IEEE Commun. Magazine

, vol. 39, pp. 115-121, Sept. 2001. 

WEISER, M.: 

"Whatever Happened to the Next Generation Internet?,"

 

Commun. of the ACM

, vol. 44, pp. 61-68, Sept. 2001. 

673




WELTMAN, R., and DAHBURA, T.: 

LDAP Programming with Java

, Boston: AddisonWesley, 2000. 

WESSELS, D.: 

Web Caching

, Sebastopol, CA: O'Reilly, 2001. 

WETTEROTH, D.: 

OSI Reference Model for Telecommunications

, New York: McGraw-Hill, 2001. 

WILJAKKA, J.: 

"Transition to Ipv6 in GPRS and WCDMA Mobile Networks,"

 

IEEE Commun. Magazine

, vol. 40, pp. 134-140, April 2002. 

WILLIAMSON, H.: 

XML: The Complete Reference

, New York: McGraw-Hill, 2001. 

WILLINGER, W., TAQQU, M.S., SHERMAN, R., and WILSON, D.V.: 

"Self-Similarity through High Variability: Statistical Analysis of Ethernet LAN Traffic at the Source Level,"

 

Proc. SIGCOMM '95 Conf.

, ACM, pp. 100-113, 1995. 

WRIGHT, D.J.: 

Voice over Packet Networks

, New York: Wiley, 2001. 

WYLIE, J., BIGRIGG, M.W., STRUNK, J.D., GANGER, G.R., KILICCOTE, H., and KHOSLA, P.K.: 

"Survivable Information Storage Systems,"

 

Computer

, vol. 33, pp. 61-68, Aug. 2000. 

XYLOMENOS, G., POLYZOS, G.C., MAHONEN, P., and SAARANEN, M.: 

"TCP Performance Issues over Wireless Links"

 , 

IEEE Commun. Magazine

, vol. 39, pp. 52-58, April 2001. 

YANG, C.-Q., and REDDY, A.V.S.: 

"A Taxonomy for Congestion Control Algorithms in Packet Switching Networks,"

 

IEEE Network Magazine

, vol. 9, pp. 34-45, July/Aug. 1995. 

YUVAL, G.: 

"How to Swindle Rabin,"

 

Cryptologia

, vol. 3, pp. 187-190, July 1979. 

ZACKS, M.: 

"Antiterrorist Legislation Expands Electronic Snooping,"

 

IEEE Internet Computing

, vol. 5, pp. 8-9, Nov.-Dec. 2001. 

ZADEH, A.N., JABBARI, B., PICKHOLTZ, R., and VOJCIC, B.: 

"Self-Organizing Packet Radio Ad Hoc Networks with Overlay (SOPRANO),"

 

IEEE Commun. Mag.

, vol. 40, pp. 149-157, June 2002. 

ZHANG, L.: 

"Comparison of Two Bridge Routing Approaches,"

 

IEEE Network Magazine

, vol. 2, pp. 44-48, Jan./Feb. 1988. 

ZHANG, L.: 

"RSVP: A New Resource ReSerVation Protocol,"

 

IEEE Network Magazine

, vol. 7, pp. 8-18, Sept./Oct. 1993. 

ZHANG, Y., and RYU, B.: 

"Mobile and Multicast IP Services in PACS: System Architecture, Prototype, and Performance,"

 

Mobile Networks and Applications

, vol. 6, pp. 81-94, Jan.-Feb. 2001. 

ZIMMERMANN, P.R.: 

The Official PGP User's Guide

, Cambridge, MA: M.I.T. Press, 1995a. 

ZIMMERMANN, P.R.: 

PGP: Source Code and Internals

, Cambridge, MA: M.I.T. Press, 1995b. 

ZIPF, G.K.: 

Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology

, Boston: Addison-Wesley, 1949. 

ZIV, J., and LEMPEL, Z.: 

"A Universal Algorithm for Sequential Data Compression,"

 

IEEE Trans. on Information Theory

, vol. IT-23, pp. 337-343, May 1977. 

674






Chapter 1 Internetworking

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Describe how a network works  Describe the purpose and functions of various network devices   Select the components required to meet a network specification   Use the OSI and TCP/IP models and their associated protocols to explain how data flows in a network   Describe common networked applications including web applications   Describe the purpose and basic operation of the protocols in the OSI and TCP models   Describe the impact of applications (Voice Over IP and Video Over IP) on a network   Interpret network diagrams   Describe the components required for network and Internet communications   Identify and correct common network problems at layers 1, 2, 3 and 7 using a layered model approach   Differentiate between LAN/WAN operation and features   Configure, verify and troubleshoot a switch with VLANs and interswitch communications  Select the appropriate media, cables, ports, and connectors to connect switches to other network devices and hosts   Explain the technology and media access control method for Ethernet networks   Explain network segmentation and basic traffic management concepts 

 

10089.book  Page 1  Monday, July 23, 2007  3:17 PM




  Implement an IP addressing scheme and IP Services to meet network requirements in a medium-size Enterprise branch office network  Explain the operation and benefits of using DHCP and DNS   Configure, verify, and troubleshoot basic router operation and routing on Cisco devices  Select the appropriate media, cables, ports, and connectors to connect routers to other network devices and hosts 

 

10089.book  Page 2  Monday, July 23, 2007  3:17 PM




 Welcome to the exciting world of internetworking. This first chapter will really help you understand the basics of internet-working by focusing on how to connect networks together using Cisco routers and switches. First, you need to know exactly what an internetwork is, right? You create an internetwork when you connect two or more LANs or WANs via a router and configure a logical network addressing scheme with a protocol such as IP.I’ll be covering these four topics in this chapter:  Internetworking basics  Network segmentation  How bridges, switches, and routers are used to physically segment a network  How routers are employed to create an internetworkI’m also going to dissect the Open Systems Interconnection (OSI) model and describe each part to you in detail because you really need a good grasp of it for the solid founda-tion you’ll build your networking knowledge upon. The OSI model has seven hierarchical layers that were developed to enable different networks to communicate reliably between disparate systems. Since this book is centering upon all things CCNA, it’s crucial for you to understand the OSI model as Cisco sees it, so that’s how I’ll be presenting the seven layers to you.Since there are a bunch of different types of devices specified at the different layers of the OSI model, it’s also very important to understand the many types of cables and connectors used for connecting all those devices to a network. We’ll go over cabling Cisco devices, dis-cussing how to connect to a router or switch (along with Ethernet LAN technologies) and even how to connect a router or switch with a console connection.We’ll finish the chapter by discussing the three-layer hierarchical model that was developed by Cisco to help you design, implement, and troubleshoot internetworks.After you finish reading this chapter, you’ll encounter 20 review questions and four written labs. These are given to you to really lock the information from this chapter into your memory. So don’t skip them!

 To find up to the minute updates for this chapter, please see www.lammle.com  and/or www.sybex.com.

 

10089.book  Page 3  Monday, July 23, 2007  3:17 PM




 4 Chapter1  Internetworking Internetworking Basics Before we explore internetworking models and the specifications of the OSI reference model, you’ve got to understand the big picture and learn the answer to the key question, Why is it so important to learn Cisco internetworking?Networks and networking have grown exponentially over the last 15 years—understandably so. They’ve had to evolve at light speed just to keep up with huge increases in basic mission-critical user needs such as sharing data and printers as well as more advanced demands such as videoconferencing. Unless everyone who needs to share network resources is located in the same office area (an increasingly uncommon situation), the challenge is to connect the sometimes many relevant networks together so all users can share the networks’ wealth.Starting with a look at Figure 1.1, you get a picture of a basic LAN network that’s con-nected together using a hub. This network is actually one collision domain and one broadcast domain—but no worries if you have no idea what this means because I’m going to talk about both collision and broadcast domains so much throughout this whole chapter, you’ll probably even dream about them! FIGURE1.1 The basic network Okay, about Figure 1.1… How would you say the PC named Bob communicates with the PC named Sally? Well, they’re both on the same LAN connected with a multiport repeater (a hub). So does Bob just send out a data message, “Hey Sally, you there?” or does Bob use Sally’s IP address and put things more like, “Hey 192.168.0.3, are you there?” Hopefully, you picked the IP address option, but even if you did, the news is still bad—both answers are wrong! Why? Because Bob is actually going to use Sally’s MAC address (known as a hardware address), which is burned right into the network card of Sally’s PC, to get ahold of her.Great, but how does Bob get Sally’s MAC address since Bob knows only Sally’s name and doesn’t even have her IP address yet? Bob is going to start with name resolution (hostname to 

The basic network allows devices to share information. The term computer language refers to binary code (0s or 1s). The two hosts above communicate using hardware or MAC addresses. (Hub) 

BobSally

 

10089.book  Page 4  Monday, July 23, 2007  3:17 PM




 Internetworking Basics 5 IP address resolution), something that’s usually accomplished using Domain Name Service (DNS). And of note, if these two are on the same LAN, Bob can just broadcast to Sally asking her for the information (no DNS needed)—welcome to Microsoft Windows (Vista included)!Here’s an output from a network analyzer depicting a simple name resolution process from Bob to Sally:

    Time      Source     Destination   Protocol   Info

 53.892794  192.168.0.2  192.168.0.255  NBNS  Name query NB  SALLY <00> As I already mentioned, since the two hosts are on a local LAN, Windows (Bob) will just broadcast to resolve the name Sally (the destination 192.168.0.255 is a broadcast address). Let’s take a look at the rest of the information:

 EthernetII,Src:192.168.0.2(00:14:22:be:18:3b),Dst:Broadcast (ff:ff:ff:ff:ff:ff) What this output shows is that Bob knows his own MAC address and source IP address but not Sally’s IP address or MAC address, so Bob sends a broadcast address of all  f s for the MAC address (a Data Link layer broadcast) and an IP LAN broadcast of 192.168.0.255. Again, don’t freak—you’re going to learn all about broadcasts in Chapter 3, “Subnetting, Variable Length Subnet Masks (VLSMs), and Troubleshooting TCP/IP.”Before the name is resolved, the first thing Bob has to do is broadcast on the LAN to get Sally’s MAC address so he can communicate to her PC and resolve her name to an IP address:

 Time      Source     Destination Protocol Info5.153054  192.168.0.2 Broadcast  ARP Who has 192.168.0.3? Tell 192.168.0.2

 Next, check out Sally’s response:

 Time      Source     Destination   Protocol  Info5.153403 192.168.0.3 192.168.0.2  ARP  192.168.0.3 is at 00:0b:db:99:d3:5e5.53.89317  192.168.0.3 192.168.0.2 NBNS Name query response NB  192.168.0.3

   Okay sweet— Bob now has both Sally’s IP address and her MAC address! These are both listed as the source address at this point because this information was sent from Sally back to Bob. So,  finally , Bob has all the goods he needs to communicate with Sally. And just so you know, I’m going to tell you all about ARP and show you exactly how Sally’s IP address was resolved to a MAC address a little later in Chapter 6, “IP Routing.”By the way, I want you to understand that Sally still had to go through the same resolution processes to communicate back to Bob—sounds crazy, huh? Consider this a welcome to IPv4 and basic networking with Windows (and we haven’t even added a router yet!).To complicate things further, it’s also likely that at some point you’ll have to break up one large network into a bunch of smaller ones because user response will have dwindled to a slow crawl as the network grew and grew. And with all that growth, your LAN’s traffic congestion has reached epic proportions. The answer to this is breaking up a really big network into a number of smaller 

 

10089.book  Page 5  Monday, July 23, 2007  3:17 PM




 6 Chapter1  Internetworking ones—something called  network segmentation . You do this by using devices like  routers ,  switches , and  bridges . Figure 1.2 displays a network that’s been segmented with a switch so each network segment connected to the switch is now a separate collision domain. But make note of the fact that this network is still one broadcast domain. FIGURE1.2 A switch can replace the hub, breaking up collision domains. Keep in mind that the hub used in Figure 1.2 just extended the one collision domain from the switch port. Here’s a list of some of the things that commonly cause LAN traffic congestion:  Too many hosts in a broadcast domain  Broadcast storms  Multicasting  Low bandwidth  Adding hubs for connectivity to the network  A bunch of ARP or IPX traffic (IPX is a Novell protocol that is like IP, but really, really chatty. Typically not used in today’s networks.)Take another look at Figure 1.2—did you notice that I replaced the main hub from Figure 1.1 with a switch? Whether you did or didn’t, the reason I did that is because hubs don’t segment a net-work; they just connect network segments together. So basically, it’s an inexpensive way to connect a couple of PCs together, which is great for home use and troubleshooting, but that’s about it!Now routers are used to connect networks together and route packets of data from one network to another. Cisco became the de facto standard of routers because of its high-quality router products, great selection, and fantastic service. Routers, by default, break up a  broad-cast domain —the set of all devices on a network segment that hear all the broadcasts sent on that segment. Figure 1.3 shows a router in our little network that creates an internetwork and breaks up broadcast domains.

(Hub) Switch 

 

10089.book  Page 6  Monday, July 23, 2007  3:17 PM




 8 Chapter1  Internetworking frames, routers (layer 3 switches) use logical addressing and provide what is called packet switching. Routers can also provide packet filtering by using access lists, and when routers connect two or more networks together and use logical addressing (IP or IPv6), this is called an internetwork. Last, routers use a routing table (map of the internetwork) to make path selections and to forward packets to remote networks.Conversely, switches aren’t used to create internetworks (they do not break up broadcast domains by default); they’re employed to add functionality to a network LAN. The main pur-pose of a switch is to make a LAN work better—to optimize its performance—providing more bandwidth for the LAN’s users. And switches don’t forward packets to other networks as routers do. Instead, they only “switch” frames from one port to another within the switched network. Okay, you may be thinking, “Wait a minute, what are frames and packets?” I’ll tell you all about them later in this chapter, I promise!By default, switches break up  collision domains . This is an Ethernet term used to describe a network scenario wherein one particular device sends a packet on a network segment, forc-ing every other device on that same segment to pay attention to it. At the same time, a different device tries to transmit, leading to a collision, after which both devices must retransmit, one at a time. Not very efficient! This situation is typically found in a hub environment where each host segment connects to a hub that represents only one collision domain and only one broad-cast domain. By contrast, each and every port on a switch represents its own collision domain.

 Switches create separate collision domains but a single broadcast domain.  Routers provide a separate broadcast domain for each interface. The term  bridging  was introduced before routers and hubs were implemented, so it’s pretty common to hear people referring to bridges as switches. That’s because bridges and switches basically do the same thing—break up collision domains on a LAN (in reality, you cannot buy a physical bridge these days, only LAN switches, but they use bridging technologies, so Cisco still calls them multiport bridges).So what this means is that a switch is basically just a multiple-port bridge with more brain-power, right? Well, pretty much, but there are differences. Switches do provide this function, but they do so with greatly enhanced management ability and features. Plus, most of the time, bridges only had 2 or 4 ports. Yes, you could get your hands on a bridge with up to 16 ports, but that’s nothing compared to the hundreds available on some switches!

 You would use a bridge in a network to reduce collisions within broadcast domains and to increase the number of collision domains in your network. Doing this provides more bandwidth for users. And keep in mind that using hubs in your network can contribute to congestion on your Ethernet network.  As always, plan your network design carefully! Figure 1.4 shows how a network would look with all these internetwork devices in place. Remember that the router will not only break up broadcast domains for every LAN interface, it will break up collision domains as well.

 

10089.book  Page 8  Monday, July 23, 2007  3:17 PM




 Internetworking Basics 9 FIGURE1.4 Internetworking devices When you looked at Figure 1.4, did you notice that the router is found at center stage and that it connects each physical network together? We have to use this layout because of the older technologies involved–—bridges and hubs.On the top internetwork in Figure 1.4, you’ll notice that a bridge was used to connect the hubs to a router. The bridge breaks up collision domains, but all the hosts connected to both hubs are still crammed into the same broadcast domain. Also, the bridge only created two collision domains, so each device connected to a hub is in the same collision domain as every other device connected to that same hub. This is actually pretty lame, but it’s still better than having one collision domain for all hosts.Notice something else: The three hubs at the bottom that are connected also connect to the router, creating one collision domain and one broadcast domain. This makes the bridged net-work look much better indeed!

 Although bridges/switches are used to segment networks, they will not iso- late broadcast or multicast packets.

Router Bridge  Switch 

 

10089.book  Page 9  Monday, July 23, 2007  3:17 PM




 Internetworking Basics 7 FIGURE1.3 Routers create an internetwork. The network in Figure 1.3 is a pretty cool network. Each host is connected to its own collision domain, and the router has created two broadcast domains. And don’t forget that the router pro-vides connections to WAN services as well! The router uses something called a serial interface for WAN connections, specifically, a V.35 physical interface on a Cisco router.Breaking up a broadcast domain is important because when a host or server sends a network broadcast, every device on the network must read and process that broadcast—unless you’ve got a router. When the router’s interface receives this broadcast, it can respond by basically saying, “Thanks, but no thanks,” and discard the broadcast without forwarding it on to other net-works. Even though routers are known for breaking up broadcast domains by default, it’s important to remember that they break up collision domains as well.There are two advantages of using routers in your network:  They don’t forward broadcasts by default.  They can filter the network based on layer 3 (Network layer) information (e.g., IP address).Four router functions in your network can be listed as follows:  Packet switching  Packet filtering  Internetwork communication  Path selectionRemember that routers are really switches; they’re actually what we call layer 3 switches (we’ll talk about layers later in this chapter). Unlike layer 2 switches, which forward or filter 

A router creates an internetwork and provides connections to WAN services. Switch Switch Serial 0 

 

10089.book  Page 7  Monday, July 23, 2007  3:17 PM




 10 Chapter1  Internetworking The best network connected to the router is the LAN switch network on the left. Why? Because each port on that switch breaks up collision domains. But it’s not all good—all devices are still in the same broadcast domain. Do you remember why this can be a really bad thing? Because all devices must listen to all broadcasts transmitted, that’s why. And if your broadcast domains are too large, the users have less bandwidth and are required to process more broad-casts, and network response time will slow to a level that could cause office riots.Once we have only switches in our network, things change a lot! Figure 1.5 shows the net-work that is typically found today. FIGURE1.5 Switched networks creating an internetwork Okay, here I’ve placed the LAN switches at the center of the network world so the routers are connecting only logical networks together. If I implemented this kind of setup, I’ve created virtual LANs (VLANs), something I’m going to tell you about in Chapter 9, “Virtual LANs (VLANs).” So don’t stress. But it is really important to understand that even though you have a switched network, you still need a router to provide your inter-VLAN communication, or internetworking. Don’t forget that!Obviously, the best network is one that’s correctly configured to meet the business require-ments of the company it serves. LAN switches with routers, correctly placed in the network, are the best network design. This book will help you understand the basics of routers and switches so you can make tight, informed decisions on a case-by-case basis.Let’s go back to Figure 1.4 again. Looking at the figure, how many collision domains and broadcast domains are in this internetwork? Hopefully, you answered nine collision domains and three broadcast domains! The broadcast domains are definitely the easiest to see because only routers break up broadcast domains by default. And since there are three connections, that gives you three broadcast domains. But do you see the nine collision domains? Just in case that’s a no, I’ll explain. The all-hub network is one collision domain; the bridge network equals three collision domains. Add in the switch network of five collision domains—one for each switch port—and you’ve got a total of nine.

Router

 

10089.book  Page 10  Monday, July 23, 2007  3:17 PM




 Internetworking Models 11 Now, in Figure 1.5, each port on the switch is a separate collision domain and each VLAN is a separate broadcast domain. But you still need a router for routing between VLANs. How many collision domains do you see here? I’m counting 10—remember that connections between the switches are considered a collision domain!So now that you’ve gotten an introduction to internetworking and the various devices that live in an internetwork, it’s time to head into internetworking models. Internetworking Models When networks first came into being, computers could typically communicate only with comput-ers from the same manufacturer. For example, companies ran either a complete DECnet solution or an IBM solution—not both together. In the late 1970s, the  Open Systems Interconnection (OSI) reference model was created by the International Organization for Standardization (ISO) to break this barrier.The OSI model was meant to help vendors create interoperable network devices and soft-ware in the form of protocols so that different vendor networks could work with each other. Like world peace, it’ll probably never happen completely, but it’s still a great goal.The OSI model is the primary architectural model for networks. It describes how data and network information are communicated from an application on one computer through the network media to an application on another computer. The OSI reference model breaks this approach into layers.

Should I Just Replace All My Hubs with Switches?You’re a network administrator at a large company in San Jose. The boss comes to you and says that he got your requisition to buy a switch and is not sure about approving the expense; do you really need it?Well, if you can, sure—why not? Switches really add a lot of functionality to a network that hubs just don’t have. But most of us don’t have an unlimited budget. Hubs still can create a nice network—that is, of course, if you design and implement the network correctly.Let’s say that you have 40 users plugged into four hubs, 10 users each. At this point, the hubs are all connected together so that you have one large collision domain and one large broad-cast domain. If you can afford to buy just one switch and plug each hub into a switch port, as well as plug the servers into the switch, then you now have four collision domains and one broadcast domain. Not great, but for the price of one switch, your network is a much better thing. So, go ahead! Put that requisition in to buy all new switches. What do you have to lose?

10089.book  Page 11  Monday, July 23, 2007  3:17 PM




12Chapter1 InternetworkingIn the following section, I am going to explain the layered approach and how we can use this approach to help us troubleshoot our internetworks.The Layered ApproachA reference model is a conceptual blueprint of how communications should take place. It addresses all the processes required for effective communication and divides these processes into logical groupings called layers. When a communication system is designed in this manner, it’s known as layered architecture.Think of it like this: You and some friends want to start a company. One of the first things you’ll do is sit down and think through what tasks must be done, who will do them, the order in which they will be done, and how they relate to each other. Ultimately, you might group these tasks into departments. Let’s say you decide to have an order-taking department, an inventory department, and a shipping department. Each of your departments has its own unique tasks, keeping its staff members busy and requiring them to focus on only their own duties.In this scenario, I’m using departments as a metaphor for the layers in a communication system. For things to run smoothly, the staff of each department will have to trust and rely heavily upon the others to do their jobs and competently handle their unique responsibilities. In your planning sessions, you would probably take notes, recording the entire process to facil-itate later discussions about standards of operation that will serve as your business blueprint, or reference model.Once your business is launched, your department heads, each armed with the part of the blueprint relating to their own department, will need to develop practical methods to imple-ment their assigned tasks. These practical methods, or protocols, will need to be compiled into a standard operating procedures manual and followed closely. Each of the various procedures in your manual will have been included for different reasons and have varying degrees of importance and implementation. If you form a partnership or acquire another company, it will be imperative that its business protocols—its business blueprint—match yours (or at least be compatible with it).Similarly, software developers can use a reference model to understand computer com-munication processes and see what types of functions need to be accomplished on any one layer. If they are developing a protocol for a certain layer, all they need to concern them-selves with is that specific layer’s functions, not those of any other layer. Another layer and protocol will handle the other functions. The technical term for this idea is binding. The communication processes that are related to each other are bound, or grouped together, at a particular layer.Advantages of Reference ModelsThe OSI model is hierarchical, and the same benefits and advantages can apply to any layered model. The primary purpose of all such models, especially the OSI model, is to allow different vendors’ networks to interoperate.

10089.book  Page 12  Monday, July 23, 2007  3:17 PM




The OSI Reference Model13Advantages of using the OSI layered model include, but are not limited to, the following: It divides the network communication process into smaller and simpler components, thus aiding component development, design, and troubleshooting. It allows multiple-vendor development through standardization of network components. It encourages industry standardization by defining what functions occur at each layer of the model. It allows various types of network hardware and software to communicate. It prevents changes in one layer from affecting other layers, so it does not hamper development.The OSI Reference ModelOne of the greatest functions of the OSI specifications is to assist in data transfer between dis-parate hosts—meaning, for example, that they enable us to transfer data between a Unix host and a PC or a Mac.The OSI isn’t a physical model, though. Rather, it’s a set of guidelines that application developers can use to create and implement applications that run on a network. It also pro-vides a framework for creating and implementing networking standards, devices, and inter-networking schemes.The OSI has seven different layers, divided into two groups. The top three layers define how the applications within the end stations will communicate with each other and with users. The bottom four layers define how data is transmitted end to end. Figure 1.6 shows the three upper layers and their functions, and Figure 1.7 shows the four lower layers and their functions.FIGURE1.6The upper layers

  Provides a user interface  Presents data  Handles processing such as encryption  Keeps different applications’  data separate

Application

Presentation

Session

Transport

Network

Data Link

Physical

10089.book  Page 13  Monday, July 23, 2007  3:17 PM




14Chapter1 InternetworkingWhen you study Figure 1.6, understand that the user interfaces with the computer at the Application layer and also that the upper layers are responsible for applications communicat-ing between hosts. Remember that none of the upper layers knows anything about networking or network addresses. That’s the responsibility of the four bottom layers.In Figure 1.7, you can see that it’s the four bottom layers that define how data is transferred through a physical wire or through switches and routers. These bottom layers also determine how to rebuild a data stream from a transmitting host to a destination host’s application.FIGURE1.7The lower layersThe following network devices operate at all seven layers of the OSI model: Network management stations (NMSs) Web and application servers Gateways (not default gateways) Network hostsBasically, the ISO is pretty much the Emily Post of the network protocol world. Just as Ms. Post wrote the book setting the standards—or protocols—for human social interaction, the ISO developed the OSI reference model as the precedent and guide for an open network pro-tocol set. Defining the etiquette of communication models, it remains today the most popular means of comparison for protocol suites.The OSI reference model has seven layers: Application layer (layer 7) Presentation layer (layer 6) Session layer (layer 5) Transport layer (layer 4) Network layer (layer 3) Data Link layer (layer 2) Physical layer (layer 1)

  Combines packets into bytes and bytes into frames  Provides access to media using MAC address  Performs error detection not correction  Provides logical addressing,  which routers use for path determination  Provides reliable or unreliable delivery  Performs error correction before retransmit  Moves bits between devices  Specifies voltage, wire speed,  and pin-out of cables

Transport

Network

Data Link

Physical

10089.book  Page 14  Monday, July 23, 2007  3:17 PM




The OSI Reference Model15Figure 1.8 shows a summary of the functions defined at each layer of the OSI model. With this in hand, you’re now ready to explore each layer’s function in detail.FIGURE1.8Layer functionsThe Application LayerThe Application layer of the OSI model marks the spot where users actually communicate to the computer. This layer only comes into play when it’s apparent that access to the network is going to be needed soon. Take the case of Internet Explorer (IE). You could uninstall every trace of networking components from a system, such as TCP/IP, NIC card, and so on, and you could still use IE to view a local HTML document—no problem. But things would definitely get messy if you tried to do something like view an HTML document that must be retrieved using HTTP or nab a file with FTP or TFTP. That’s because IE will respond to requests such as those by attempting to access the Application layer. And what’s happening is that the Appli-cation layer is acting as an interface between the actual application program—which isn’t at all a part of the layered structure—and the next layer down by providing ways for the appli-cation to send information down through the protocol stack. In other words, IE doesn’t truly reside within the Application layer—it interfaces with Application layer protocols when it needs to deal with remote resources.The Application layer is also responsible for identifying and establishing the availability of the intended communication partner and determining whether sufficient resources for the intended communication exist.These tasks are important because computer applications sometimes require more than only desktop resources. Often, they’ll unite communicating components from more than one network application. Prime examples are file transfers and email, as well as enabling remote access, network management activities, client/server processes, and information location. Many network applications provide services for communication over enterprise networks, but for present and future internetworking, the need is fast developing to reach beyond the limits of current physical networking.

10089.book  Page 15  Monday, July 23, 2007  3:17 PM




16Chapter1 Internetworking

It’s important to remember that the Application layer is acting as an interface between the actual application programs. This means that Microsoft Word, for example, does not reside at the Application layer but instead interfaces with the Application layer protocols. Chapter 2 will present some programs that actually reside at the Application layer—for example, FTP and TFTP.The Presentation LayerThe Presentation layer gets its name from its purpose: It presents data to the Application layer and is responsible for data translation and code formatting.This layer is essentially a translator and provides coding and conversion functions. A suc-cessful data-transfer technique is to adapt the data into a standard format before transmission. Computers are configured to receive this generically formatted data and then convert the data back into its native format for actual reading (for example, EBCDIC to ASCII). By providing translation services, the Presentation layer ensures that data transferred from the Application layer of one system can be read by the Application layer of another one.The OSI has protocol standards that define how standard data should be formatted. Tasks like data compression, decompression, encryption, and decryption are associated with this layer. Some Presentation layer standards are involved in multimedia operations too.The Session LayerThe Session layer is responsible for setting up, managing, and then tearing down sessions between Presentation layer entities. This layer also provides dialog control between devices, or nodes. It coordinates communication between systems and serves to organize their communication by offering three different modes: simplex, half duplex, and full duplex. To sum up, the Session layer basically keeps different applications’ data separate from other applications’ data.The Transport LayerThe Transport layer segments and reassembles data into a data stream. Services located in the Transport layer segment and reassemble data from upper-layer applications and unite it into the same data stream. They provide end-to-end data transport services and can establish a logical connection between the sending host and destination host on an internetwork.Some of you are probably familiar with TCP and UDP already. (But if you’re not, no wor-ries—I’ll tell you all about them in Chapter 2.) If so, you know that both work at the Transport layer and that TCP is a reliable service and UDP is not. This means that application developers have more options because they have a choice between the two protocols when working with TCP/IP protocols.

10089.book  Page 16  Monday, July 23, 2007  3:17 PM




The OSI Reference Model17The Transport layer is responsible for providing mechanisms for multiplexing upper-layer applications, establishing sessions, and tearing down virtual circuits. It also hides details of any network-dependent information from the higher layers by providing transparent data transfer.

The term reliable networking can be used at the Transport layer. It means that acknowledgments, sequencing, and flow control will be used.The Transport layer can be connectionless or connection-oriented. However, Cisco is mostly concerned with you understanding the connection-oriented portion of the Transport layer. The following sections will provide the skinny on the connection-oriented (reliable) pro-tocol of the Transport layer.Flow ControlData integrity is ensured at the Transport layer by maintaining flow control and by allowing users to request reliable data transport between systems. Flow control prevents a sending host on one side of the connection from overflowing the buffers in the receiving host—an event that can result in lost data. Reliable data transport employs a connection-oriented communications session between systems, and the protocols involved ensure that the following will be achieved: The segments delivered are acknowledged back to the sender upon their reception. Any segments not acknowledged are retransmitted. Segments are sequenced back into their proper order upon arrival at their destination. A manageable data flow is maintained in order to avoid congestion, overloading, and data loss.

The purpose of flow control is to provide a means for the receiver to govern the amount of data sent by the sender.Connection-Oriented CommunicationIn reliable transport operation, a device that wants to transmit sets up a connection-oriented communication with a remote device by creating a session. The transmitting device first estab-lishes a connection-oriented session with its peer system, which is called a call setup or a three-way handshake. Data is then transferred; when the transfer is finished, a call termination takes place to tear down the virtual circuit.Figure 1.9 depicts a typical reliable session taking place between sending and receiving sys-tems. Looking at it, you can see that both hosts’ application programs begin by notifying their individual operating systems that a connection is about to be initiated. The two operating sys-tems communicate by sending messages over the network confirming that the transfer is approved and that both sides are ready for it to take place. After all of this required synchro-nization takes place, a connection is fully established and the data transfer begins (this virtual circuit setup is called overhead!).

10089.book  Page 17  Monday, July 23, 2007  3:17 PM




18Chapter1 InternetworkingFIGURE1.9Establishing a connection-oriented sessionWhile the information is being transferred between hosts, the two machines periodically check in with each other, communicating through their protocol software to ensure that all is going well and that the data is being received properly.Let me sum up the steps in the connection-oriented session—the three-way handshake—pictured in Figure 1.9: The first “connection agreement” segment is a request for synchronization. The second and third segments acknowledge the request and establish connection parameters—the rules—between hosts. These segments request that the receiver’s sequencing is synchronized here as well so that a bidirectional connection is formed. The final segment is also an acknowledgment. It notifies the destination host that the con-nection agreement has been accepted and that the actual connection has been established. Data transfer can now begin.Sounds pretty simple, but things don’t always flow so smoothly. Sometimes during a transfer, congestion can occur because a high-speed computer is generating data traffic a lot faster than the network can handle transferring. A bunch of computers simultaneously send-ing datagrams through a single gateway or destination can also botch things up nicely. In the latter case, a gateway or destination can become congested even though no single source caused the problem. In either case, the problem is basically akin to a freeway bottleneck—too much traffic for too small a capacity. It’s not usually one car that’s the problem; there are simply too many cars on that freeway.Okay, so what happens when a machine receives a flood of datagrams too quickly for it to process? It stores them in a memory section called a buffer. But this buffering action can solve 

SynchronizeNegotiate connectionSynchronizeAcknowledge

Connection establishedData transfer(Send segments)Sender

Receiver

10089.book  Page 18  Monday, July 23, 2007  3:17 PM




The OSI Reference Model19the problem only if the datagrams are part of a small burst. If not, and the datagram deluge continues, a device’s memory will eventually be exhausted, its flood capacity will be exceeded, and it will react by discarding any additional datagrams that arrive.No huge worries here, though. Because of the transport function, network flood control systems really work quite well. Instead of dumping resources and allowing data to be lost, the transport can issue a “not ready” indicator to the sender, or source, of the flood (as shown in Figure 1.10). This mechanism works kind of like a stoplight, signaling the sending device to stop transmitting segment traffic to its overwhelmed peer. After the peer receiver processes the segments already in its memory reservoir—its buffer—it sends out a “ready” transport indi-cator. When the machine waiting to transmit the rest of its datagrams receives this “go” indic-tor, it resumes its transmission.FIGURE1.10Transmitting segments with flow controlIn fundamental, reliable, connection-oriented data transfer, datagrams are delivered to the receiving host in exactly the same sequence they’re transmitted—and the transmission fails if this order is breached! If any data segments are lost, duplicated, or damaged along the way, a failure will transmit. This problem is solved by having the receiving host acknowledge that it has received each and every data segment.A service is considered connection-oriented if it has the following characteristics: A virtual circuit is set up (e.g., a three-way handshake). It uses sequencing. It uses acknowledgments. It uses flow control.

TransmitTransmitNot ready

—

STOP!GO!SegmentsprocessedBuffer fullSender

Receiver

10089.book  Page 19  Monday, July 23, 2007  3:17 PM




20Chapter1 Internetworking

The types of flow control are buffering, windowing, and congestion avoidance.WindowingIdeally, data throughput happens quickly and efficiently. And as you can imagine, it would be slow if the transmitting machine had to wait for an acknowledgment after sending each seg-ment. But because there’s time available after the sender transmits the data segment and before it finishes processing acknowledgments from the receiving machine, the sender uses the break as an opportunity to transmit more data. The quantity of data segments (measured in bytes) that the transmitting machine is allowed to send without receiving an acknowledgment for them is called a window.

Windows are used to control the amount of outstanding, unacknowledged data segments.So the size of the window controls how much information is transferred from one end to the other. While some protocols quantify information by observing the number of packets, TCP/IP measures it by counting the number of bytes.As you can see in Figure 1.11, there are two window sizes—one set to 1 and one set to 3.FIGURE1.11Windowing

10089.book  Page 20  Monday, July 23, 2007  3:17 PM




The OSI Reference Model21When you’ve configured a window size of 1, the sending machine waits for an acknowledg-ment for each data segment it transmits before transmitting another. If you’ve configured a win-dow size of 3, it’s allowed to transmit three data segments before an acknowledgment is received.In our simplified example, both the sending and receiving machines are workstations. In reality this is not done in simple numbers but in the amount of bytes that can be sent.

If a receiving host fails to receive all the segments that it should acknowledge, the host can improve the communication session by decreasing the window size.AcknowledgmentsReliable data delivery ensures the integrity of a stream of data sent from one machine to the other through a fully functional data link. It guarantees that the data won’t be duplicated or lost. This is achieved through something called positive acknowledgment with retransmis-sion—a technique that requires a receiving machine to communicate with the transmitting source by sending an acknowledgment message back to the sender when it receives data. The sender documents each segment it sends and waits for this acknowledgment before sending the next segment. When it sends a segment, the transmitting machine starts a timer and retrans-mits if it expires before an acknowledgment is returned from the receiving end.In Figure 1.12, the sending machine transmits segments 1, 2, and 3. The receiving node acknowledges it has received them by requesting segment 4. When it receives the acknowledg-ment, the sender then transmits segments 4, 5, and 6. If segment 5 doesn’t make it to the des-tination, the receiving node acknowledges that event with a request for the segment to be resent. The sending machine will then resend the lost segment and wait for an acknowledg-ment, which it must receive in order to move on to the transmission of segment 7.FIGURE1.12Transport layer reliable delivery

SenderSend 1

Receiver

1

2

3

4

5

6

1

2

3

4

5

6

Send 2

Send 3

Ack 4

Send 4Connection lost!

Send 5

Send 6

Ack 5

Send 5

Ack 7

10089.book  Page 21  Monday, July 23, 2007  3:17 PM




22Chapter1 InternetworkingThe Network LayerThe Network layer (also called layer 3) manages device addressing, tracks the location of devices on the network, and determines the best way to move data, which means that the Network layer must transport traffic between devices that aren’t locally attached. Routers (layer 3 devices) are specified at the Network layer and provide the routing services within an internetwork.It happens like this: First, when a packet is received on a router interface, the destination IP address is checked. If the packet isn’t destined for that particular router, it will look up the desti-nation network address in the routing table. Once the router chooses an exit interface, the packet will be sent to that interface to be framed and sent out on the local network. If the router can’t find an entry for the packet’s destination network in the routing table, the router drops the packet.Two types of packets are used at the Network layer: data and route updates.Data packetsUsed to transport user data through the internetwork. Protocols used to sup-port data traffic are called routed protocols; examples of routed protocols are IP and IPv6. You’ll learn about IP addressing in Chapters 2 and 3 and IPv6 in Chapter 13 .Route update packetsUsed to update neighboring routers about the networks connected to all routers within the internetwork. Protocols that send route update packets are called routing protocols; examples of some common ones are RIP, RIPv2, EIGRP, and OSPF. Route update packets are used to help build and maintain routing tables on each router.In Figure 1.13, I’ve given you an example of a routing table. The routing table used in a router includes the following information:Network addressesProtocol-specific network addresses. A router must maintain a routing table for individual routing protocols because each routing protocol keeps track of a network with a dif-ferent addressing scheme (IP, IPv6, and IPX, for example). Think of it as a street sign in each of the different languages spoken by the residents that live on a particular street. So, if there were Amer-ican, Spanish, and French folks on a street named Cat, the sign would read Cat/Gato/Chat.FIGURE1.13Routing table used in a router

1.01.32.1E0S02.23.3S0E03.0Routing TableMetric001INTE0S0S0NET123Routing TableMetric100INTS0S0E0NET123

1.1

1.2

3.1

3.2

10089.book  Page 22  Monday, July 23, 2007  3:17 PM




The OSI Reference Model23InterfaceThe exit interface a packet will take when destined for a specific network.MetricThe distance to the remote network. Different routing protocols use different ways of computing this distance. I’m going to cover routing protocols in Chapters 6 and 7, but for now, know that some routing protocols (namely RIP) use something called a hop count (the number of routers a packet passes through en route to a remote network), while others use bandwidth, delay of the line, or even tick count (1/18 of a second).And as I mentioned earlier, routers break up broadcast domains, which means that by default, broadcasts aren’t forwarded through a router. Do you remember why this is a good thing? Routers also break up collision domains, but you can also do that using layer 2 (Data Link layer) switches. Because each interface in a router represents a separate network, it must be assigned unique network identification numbers, and each host on the network connected to that router must use the same network number. Figure 1.14 shows how a router works in an internetwork.FIGURE1.14A router in an internetworkHere are some points about routers that you should really commit to memory: Routers, by default, will not forward any broadcast or multicast packets. Routers use the logical address in a Network layer header to determine the next hop router to forward the packet to. Routers can use access lists, created by an administrator, to control security on the types of packets that are allowed to enter or exit an interface. Routers can provide layer 2 bridging functions if needed and can simultaneously route through the same interface. Layer 3 devices (routers in this case) provide connections between virtual LANs (VLANs). Routers can provide quality of service (QoS) for specific types of network traffic.

Switching and VLANs and are covered in Chapter 8, “LAN Switching and STP,” and Chapter 9, “Virtual LANs (VLANs).”

FastEthernet0/1

Internet

FastEthernet0/0Serial0WAN ServicesEach router interface is a broadcast domain. Routers break up broadcast domains by default and provide WAN services.

10089.book  Page 23  Monday, July 23, 2007  3:17 PM




24Chapter1 InternetworkingThe Data Link LayerThe Data Link layer provides the physical transmission of the data and handles error notification, network topology, and flow control. This means that the Data Link layer will ensure that messages are delivered to the proper device on a LAN using hardware addresses and will translate messages from the Network layer into bits for the Physical layer to transmit.The Data Link layer formats the message into pieces, each called a data frame, and adds a cus-tomized header containing the hardware destination and source address. This added information forms a sort of capsule that surrounds the original message in much the same way that engines, navigational devices, and other tools were attached to the lunar modules of the Apollo project. These various pieces of equipment were useful only during certain stages of space flight and were stripped off the module and discarded when their designated stage was complete. Data traveling through networks is similar.Figure 1.15 shows the Data Link layer with the Ethernet and IEEE specifications. When you check it out, notice that the IEEE 802.2 standard is used in conjunction with and adds func-tionality to the other IEEE standards.FIGURE1.15Data Link layerIt’s important for you to understand that routers, which work at the Network layer, don’t care at all about where a particular host is located. They’re only concerned about where net-works are located and the best way to reach them—including remote ones. Routers are totally obsessive when it comes to networks. And for once, this is a good thing! It’s the Data Link layer that’s responsible for the actual unique identification of each device that resides on a local network.For a host to send packets to individual hosts on a local network as well as transmit packets between routers, the Data Link layer uses hardware addressing. Each time a packet is sent between routers, it’s framed with control information at the Data Link layer, but that information is stripped off at the receiving router and only the original packet is left completely intact. This fram-ing of the packet continues for each hop until the packet is finally delivered to the correct receiving host. It’s really important to understand that the packet itself is never altered along the route; it’s only encapsulated with the type of control information required for it to be properly passed on to the different media types.

10089.book  Page 24  Monday, July 23, 2007  3:17 PM




The OSI Reference Model25The IEEE Ethernet Data Link layer has two sublayers:Media Access Control (MAC) 802.3Defines how packets are placed on the media. Conten-tion media access is “first come/first served” access where everyone shares the same band-width—hence the name. Physical addressing is defined here, as well as logical topologies. What’s a logical topology? It’s the signal path through a physical topology. Line discipline, error notification (not correction), ordered delivery of frames, and optional flow control can also be used at this sublayer.Logical Link Control (LLC) 802.2Responsible for identifying Network layer protocols and then encapsulating them. An LLC header tells the Data Link layer what to do with a packet once a frame is received. It works like this: A host will receive a frame and look in the LLC header to find out where the packet is destined—say, the IP protocol at the Network layer. The LLC can also provide flow control and sequencing of control bits.The switches and bridges I talked about near the beginning of the chapter both work at the Data Link layer and filter the network using hardware (MAC) addresses. We will look at these in the following section.Switches and Bridges at the Data Link LayerLayer 2 switching is considered hardware-based bridging because it uses specialized hardware called an application-specific integrated circuit (ASIC). ASICs can run up to gigabit speeds with very low latency rates.

Latency is the time measured from when a frame enters a port to the time it exits a port.Bridges and switches read each frame as it passes through the network. The layer 2 device then puts the source hardware address in a filter table and keeps track of which port the frame was received on. This information (logged in the bridge’s or switch’s filter table) is what helps the machine determine the location of the specific sending device. Figure 1.16 shows a switch in an internetwork.The real estate business is all about location, location, location, and it’s the same way for both layer 2 and layer 3 devices. Though both need to be able to negotiate the network, it’s crucial to remember that they’re concerned with very different parts of it. Primarily, layer 3 machines (such as routers) need to locate specific networks, whereas layer 2 machines (switches and bridges) need to eventually locate specific devices. So, networks are to routers as individual devices are to switches and bridges. And routing tables that “map” the internetwork are for routers as filter tables that “map” individual devices are for switches and bridges.After a filter table is built on the layer 2 device, it will forward frames only to the segment where the destination hardware address is located. If the destination device is on the same seg-ment as the frame, the layer 2 device will block the frame from going to any other segments. If the destination is on a different segment, the frame can be transmitted only to that segment. This is called transparent bridging.

10089.book  Page 25  Monday, July 23, 2007  3:17 PM




26Chapter1 InternetworkingWhen a switch interface receives a frame with a destination hardware address that isn’t found in the device’s filter table, it will forward the frame to all connected segments. If the unknown device that was sent the “mystery frame” replies to this forwarding action, the switch updates its filter table regarding that device’s location. But in the event the destination address of the transmitting frame is a broadcast address, the switch will forward all broadcasts to every con-nected segment by default.FIGURE1.16A switch in an internetworkAll devices that the broadcast is forwarded to are considered to be in the same broadcast domain. This can be a problem; layer 2 devices propagate layer 2 broadcast storms that choke performance, and the only way to stop a broadcast storm from propagating through an inter-network is with a layer 3 device—a router.The biggest benefit of using switches instead of hubs in your internetwork is that each switch port is actually its own collision domain. (Conversely, a hub creates one large collision domain.) But even armed with a switch, you still can’t break up broadcast domains. Neither switches nor bridges will do that. They’ll typically simply forward all broadcasts instead.Another benefit of LAN switching over hub-centered implementations is that each device on every segment plugged into a switch can transmit simultaneously—at least, they can as long as there is only one host on each port and a hub isn’t plugged into a switch port. As you might have guessed, hubs allow only one device per network segment to communicate at a time.Binary to Decimal and Hexadecimal ConversionBefore we finish this chapter and move to discussing the TCP/IP protocol stack and IP address-ing in Chapter 2, it’s really important for you to truly understand the differences between binary, decimal, and hexadecimal numbers and how to convert one format into the other.

Each segment has its own collision domain.All segments are in the same broadcast domain.

1

2

3

4

10089.book  Page 26  Monday, July 23, 2007  3:17 PM




The OSI Reference Model27So we’ll start with binary numbering. It’s pretty simple, really. The digits used are limited to either a 1 (one) or a 0 (zero), and each digit is called 1 bit (short for binary digit). Typically, you count either 4 or 8 bits together, with these being referred to as a nibble and a byte, respectively.What interests us in binary numbering is the value represented in a decimal format—the typical decimal format being the base-10 number scheme that we’ve all used since kindergar-ten. The binary numbers are placed in a value spot: starting at the right and moving left, with each spot having double the value of the previous spot.Table 1.1 shows the decimal values of each bit location in a nibble and a byte. Remember, a nibble is 4 bits and a byte is 8 bits.What all this means is that if a one digit (1) is placed in a value spot, then the nibble or byte takes on that decimal value and adds it to any other value spots that have a 1. And if a zero (0) is placed in a bit spot, you don’t count that value.Let me clarify things. If we have a 1 placed in each spot of our nibble, we would then add up 8 + 4 + 2 + 1, to give us a maximum value of 15. Another example for our nibble values would be 1010; that means that the 8 bit and the 2 bit are turned on, which equals a decimal value of 10. If we have a nibble binary value of 0110, then our decimal value would be 6, because the 4 and 2 bits are turned on.But the byte values can add up to a value that’s significantly higher than 15. This is how: If we counted every bit as a one (1), then the byte binary value would look like this (remember, 8 bits equal a byte):11111111We would then count up every bit spot because each is turned on. It would look like this, which demonstrates the maximum value of a byte:128 + 64 + 32 + 16 + 8 + 4 + 2 + 1 = 255There are plenty of other decimal values that a binary number can equal. Let’s work through a few examples:10010110Which bits are on? The 128, 16, 4, and 2 bits are on, so we’ll just add them up: 128 + 16 + 4 + 2 = 150.01101100Which bits are on? The 64, 32, 8, and 4 bits are on, so we just need to add them up: 64 + 32 + 8 + 4 = 108.11101000Which bits are on? The 128, 64, 32, and 8 bits are on, so just add the values up: 128 + 64 + 32 + 8 = 232.TABLE1.1Binary ValuesNibble ValuesByte Values8 4 2 1128 64 32 16 8 4 2 1

10089.book  Page 27  Monday, July 23, 2007  3:17 PM




28Chapter1 InternetworkingTable 1.2 is a table you should memorize before braving the IP sections in Chapters 2 and 3.Hexadecimal addressing is completely different than binary or decimal—it’s converted by reading nibbles, not bytes. By using a nibble, we can convert these bits to hex pretty simply. First, understand that the hexadecimal addressing scheme uses only the numbers 0 through 9. And since the numbers 10, 11, 12, and so on can’t be used (because they are two-digit numbers), the letters A, B, C, D, E, and F are used to represent 10, 11, 12, 13, 14, and 15, respectively.

Hex is short for hexadecimal, which is a numbering system that uses the first six letters of the alphabet (A through F) to extend beyond the available 10 digits in the decimal system. Hexadecimal has a total of 16 digits.Table 1.3 shows both the binary value and the decimal value for each hexadecimal digit.TABLE1.2Binary to Decimal Memorization ChartBinary ValueDecimal Value1000000012811000000192111000002241111000024011111000248111111002521111111025411111111255TABLE1.3Hex to Binary to Decimal Chart Hexadecimal ValueBinary ValueDecimal Value000000100011200102

10089.book  Page 28  Monday, July 23, 2007  3:17 PM




The OSI Reference Model29Did you notice that the first 10 hexadecimal digits (0–9) are the same value as the decimal values? If not, look again. This handy fact makes those values super easy to convert.So suppose you have something like this: 0x6A. (Sometimes Cisco likes to put 0x in front of characters so you know that they are a hex value. It doesn’t have any other special meaning.) What are the binary and decimal values? All you have to remember is that each hex character is one nibble and two hex characters together make a byte. To figure out the binary value, we need to put the hex characters into two nibbles and then put them together into a byte. 6 = 0110 and A (which is 10 in hex) = 1010, so the complete byte would be 01101010.To convert from binary to hex, just take the byte and break it into nibbles. Here’s what I mean.Say you have the binary number 01010101. First, break it into nibbles—0101 and 0101—with the value of each nibble being 5 since the 1 and 4 bits are on. This makes the hex answer 0x55. And in decimal format, the binary number is 01010101, which converts to 64 + 16 + 4 + 1 = 85.300113401004501015601106701117810008910019A101010B101111C110012D110113E111014F111115TABLE1.3Hex to Binary to Decimal Chart(continued)Hexadecimal ValueBinary ValueDecimal Value

10089.book  Page 29  Monday, July 23, 2007  3:17 PM




30Chapter1 InternetworkingHere’s another binary number:11001100Your answer would be 1100 = 12 and 1100 = 12 (therefore, it’s converted to CC in hex). The decimal conversion answer would be 128 + 64 + 8 + 4 = 204.One more example, then we need to get working on the Physical layer. Suppose you had the following binary number:10110101The hex answer would be 0xB5, since 1011 converts to B and 0101 converts to 5 in hex value. The decimal equivalent is 128 + 32 + 16 + 4 + 1 = 181.

See Written Lab 1.4 for more practice with binary/hex/decimal conversion.The Physical LayerFinally arriving at the bottom, we find that the Physical layer does two things: It sends bits and receives bits. Bits come only in values of 1 or 0—a Morse code with numerical values. The Physical layer communicates directly with the various types of actual communication media. Different kinds of media represent these bit values in different ways. Some use audio tones, while others employ state transitions—changes in voltage from high to low and low to high. Specific protocols are needed for each type of media to describe the proper bit patterns to be used, how data is encoded into media signals, and the various qualities of the physical media’s attachment interface.The Physical layer specifies the electrical, mechanical, procedural, and functional require-ments for activating, maintaining, and deactivating a physical link between end systems. This layer is also where you identify the interface between the data terminal equipment (DTE) and the data communication equipment (DCE). (Some old phone-company employees still call DCE data circuit-terminating equipment.) The DCE is usually located at the service provider, while the DTE is the attached device. The services available to the DTE are most often accessed via a modem or channel service unit/data service unit (CSU/DSU).The Physical layer’s connectors and different physical topologies are defined by the OSI as standards, allowing disparate systems to communicate. The CCNA objectives are only inter-ested in the IEEE Ethernet standards.Hubs at the Physical LayerA hub is really a multiple-port repeater. A repeater receives a digital signal and reamplifies or regenerates that signal and then forwards the digital signal out all active ports without looking at any data. An active hub does the same thing. Any digital signal received from a segment on a hub port is regenerated or reamplified and transmitted out all ports on the hub. This means all devices plugged into a hub are in the same collision domain as well as in the same broadcast domain. Figure 1.17 shows a hub in a network.

10089.book  Page 30  Monday, July 23, 2007  3:17 PM




Ethernet Networking31FIGURE1.17A hub in a networkHubs, like repeaters, don’t examine any of the traffic as it enters and is then transmitted out to the other parts of the physical media. Every device connected to the hub, or hubs, must listen if a device transmits. A physical star network—where the hub is a central device and cables extend in all directions out from it—is the type of topology a hub creates. Visually, the design really does resemble a star, whereas Ethernet networks run a logical bus topology, meaning that the signal has to run through the network from end to end.

Hubs and repeaters can be used to enlarge the area covered by a single LAN segment, although I do not recommend this. LAN switches are affordable for almost every situation.Ethernet NetworkingEthernet is a contention media access method that allows all hosts on a network to share the same bandwidth of a link. Ethernet is popular because it’s readily scalable, meaning that it’s comparatively easy to integrate new technologies, such as Fast Ethernet and Gigabit Ethernet, into an existing network infrastructure. It’s also relatively simple to implement in the first place, and with it, troubleshooting is reasonably straightforward. Ethernet uses both Data Link and Physical layer specifications, and this section of the chapter will give you both the Data Link layer and Physical layer information you need to effectively implement, trouble-shoot, and maintain an Ethernet network.Ethernet networking uses Carrier Sense Multiple Access with Collision Detection (CSMA/CD), a protocol that helps devices share the bandwidth evenly without having two devices transmit at the same time on the network medium. CSMA/CD was created to overcome the problem of those collisions that occur when packets are transmitted simultaneously from different nodes. And trust me—good collision management is crucial, because when a node transmits in a CSMA/CD net-work, all the other nodes on the network receive and examine that transmission. Only bridges and routers can effectively prevent a transmission from propagating throughout the entire network!

All devices in the same collision domain.All devices in the same broadcast domain.Devices share the same bandwidth.

ABCD

10089.book  Page 31  Monday, July 23, 2007  3:17 PM




32Chapter1 InternetworkingSo, how does the CSMA/CD protocol work? Let’s start by taking a look at Figure 1.18.FIGURE1.18CSMA/CDWhen a host wants to transmit over the network, it first checks for the presence of a digital signal on the wire. If all is clear (no other host is transmitting), the host will then proceed with its transmission. But it doesn’t stop there. The transmitting host constantly monitors the wire to make sure no other hosts begin transmitting. If the host detects another signal on the wire, it sends out an extended jam signal that causes all nodes on the segment to stop sending data (think busy signal). The nodes respond to that jam signal by waiting a while before attempting to trans-mit again. Backoff algorithms determine when the colliding stations can retransmit. If collisions keep occurring after 15 tries, the nodes attempting to transmit will then timeout. Pretty clean!When a collision occurs on an Ethernet LAN, the following happens: A jam signal informs all devices that a collision occurred. The collision invokes a random backoff algorithm. Each device on the Ethernet segment stops transmitting for a short time until the timers expire. All hosts have equal priority to transmit after the timers have expired.

A B C D A B C D A B C D A B C D Carrier Sense Multiple Access with Collision Detection (CSMA/CD) 

Collision 

Jam   

Jam   

Jam   

Jam   

Jam   

Jam   

Jam   

Jam   

10089.book  Page 32  Monday, July 23, 2007  3:17 PM




Ethernet Networking33The following are the effects of having a CSMA/CD network sustaining heavy collisions: Delay Low throughput Congestion

Backoff on an 802.3 network is the retransmission delay that’s enforced when a collision occurs. When a collision occurs, a host will resume transmission after the forced time delay has expired. After this backoff delay period has expired, all stations have equal priority to transmit data.In the following sections, I am going to cover Ethernet in detail at both the Data Link layer (layer 2) and the Physical layer (layer 1).Half- and Full-Duplex EthernetHalf-duplex Ethernet is defined in the original 802.3 Ethernet; Cisco says it uses only one wire pair with a digital signal running in both directions on the wire. Certainly, the IEEE specifi-cations discuss the process of half duplex somewhat differently, but what Cisco is talking about is a general sense of what is happening here with Ethernet.It also uses the CSMA/CD protocol to help prevent collisions and to permit retransmitting if a collision does occur. If a hub is attached to a switch, it must operate in half-duplex mode because the end stations must be able to detect collisions. Half-duplex Ethernet—typically 10BaseT—is only about 30 to 40 percent efficient as Cisco sees it because a large 10BaseT net-work will usually only give you 3 to 4Mbps, at most.But full-duplex Ethernet uses two pairs of wires instead of one wire pair like half duplex. And full duplex uses a point-to-point connection between the transmitter of the transmitting device and the receiver of the receiving device. This means that with full-duplex data transfer, you get a faster data transfer compared to half duplex. And because the transmitted data is sent on a different set of wires than the received data, no collisions will occur.The reason you don’t need to worry about collisions is because now it’s like a freeway with multiple lanes instead of the single-lane road provided by half duplex. Full-duplex Ethernet is supposed to offer 100 percent efficiency in both directions—for example, you can get 20Mbps with a 10Mbps Ethernet running full duplex or 200Mbps for Fast Ethernet. But this rate is something known as an aggregate rate, which translates as “you’re supposed to get” 100 per-cent efficiency. No guarantees, in networking as in life.Full-duplex Ethernet can be used in three situations: With a connection from a switch to a host With a connection from a switch to a switch With a connection from a host to a host using a crossover cable

Full-duplex Ethernet requires a point-to-point connection when only two nodes are present. You can run full-duplex with just about any device except a hub.

10089.book  Page 33  Monday, July 23, 2007  3:17 PM




34Chapter1 InternetworkingNow, if it’s capable of all that speed, why wouldn’t it deliver? Well, when a full-duplex Ethernet port is powered on, it first connects to the remote end and then negotiates with the other end of the Fast Ethernet link. This is called an auto-detect mechanism. This mechanism first decides on the exchange capability, which means it checks to see if it can run at 10 or 100Mbps. It then checks to see if it can run full duplex, and if it can’t, it will run half duplex.

Remember that half-duplex Ethernet shares a collision domain and provides a lower effective throughput than full-duplex Ethernet, which typically has a private collision domain and a higher effective throughput.Lastly, remember these important points: There are no collisions in full-duplex mode. A dedicated switch port is required for each full-duplex node. The host network card and the switch port must be capable of operating in full-duplex mode.Now let’s take a look at how Ethernet works at the Data Link layer.Ethernet at the Data Link LayerEthernet at the Data Link layer is responsible for Ethernet addressing, commonly referred to as hardware addressing or MAC addressing. Ethernet is also responsible for framing packets received from the Network layer and preparing them for transmission on the local network through the Ethernet contention media access method.Ethernet AddressingHere’s where we get into how Ethernet addressing works. It uses the Media Access Control (MAC) address burned into each and every Ethernet network interface card (NIC). The MAC, or hardware, address is a 48-bit (6-byte) address written in a hexadecimal format.Figure 1.19 shows the 48-bit MAC addresses and how the bits are divided.FIGURE1.19Ethernet addressing using MAC addressesThe organizationally unique identifier (OUI) is assigned by the IEEE to an organization. It’s composed of 24 bits, or 3 bytes. The organization, in turn, assigns a globally administered address (24 bits, or 3 bytes) that is unique (supposedly, again—no guarantees) to each and every adapter it manufactures. Look closely at the figure. The high-order bit is the Individual/Group (I/G) bit. When it has a value of 0, we can assume that the address is the MAC address 

OrganizationallyUnique Identifier (OUI)(Assigned by IEEE)24 bits24 bitsVendor assignedG/LI/G4647

10089.book  Page 34  Monday, July 23, 2007  3:17 PM




Ethernet Networking35of a device and may well appear in the source portion of the MAC header. When it is a 1, we can assume that the address represents either a broadcast or multicast address in Ethernet or a broadcast or functional address in TR and FDDI (who really knows about FDDI?).The next bit is the global/local bit, or just G/L bit (also known as U/L, where U means univer-sal).When set to 0, this bit represents a globally administered address (as by the IEEE). When the bit is a 1, it represents a locally governed and administered address (as in what DECnet used to do). The low-order 24 bits of an Ethernet address represent a locally administered or manufacturer-assigned code. This portion commonly starts with 24 0s for the first card made and continues in order until there are 24 1s for the last (16,777,216th) card made. You’ll find that many manufac-turers use these same six hex digits as the last six characters of their serial number on the same card.Ethernet FramesThe Data Link layer is responsible for combining bits into bytes and bytes into frames. Frames are used at the Data Link layer to encapsulate packets handed down from the Network layer for transmission on a type of media access.The function of Ethernet stations is to pass data frames between each other using a group of bits known as a MAC frame format. This provides error detection from a cyclic redundancy check (CRC). But remember—this is error detection, not error correction. The 802.3 frames and Ethernet frame are shown in Figure 1.20.

Encapsulating a frame within a different type of frame is called tunneling.FIGURE1.20802.3 and Ethernet frame formats

Preamble8 bytesPreamble8 bytesDA6 bytesSA6 bytesLength2 bytesDataFCSDA6 bytesSA6 bytesType2 bytesDataFCS4 bytesEthernet_II802.3_Ethernet

10089.book  Page 35  Monday, July 23, 2007  3:17 PM




36Chapter1 InternetworkingFollowing are the details of the different fields in the 802.3 and Ethernet frame types:PreambleAn alternating 1,0 pattern provides a 5MHz clock at the start of each packet, which allows the receiving devices to lock the incoming bit stream.Start Frame Delimiter (SFD)/SynchThe preamble is seven octets and the SFD is one octet (synch). The SFD is 10101011, where the last pair of 1s allows the receiver to come into the alternating 1,0 pattern somewhere in the middle and still sync up and detect the beginning of the data.Destination Address (DA)This transmits a 48-bit value using the least significant bit (LSB) first. The DA is used by receiving stations to determine whether an incoming packet is addressed to a particular node. The destination address can be an individual address or a broadcast or multicast MAC address. Remember that a broadcast is all 1s (or Fs in hex) and is sent to all devices but a multicast is sent only to a similar subset of nodes on a network.Source Address (SA)The SA is a 48-bit MAC address used to identify the transmitting device, and it uses the LSB first. Broadcast and multicast address formats are illegal within the SA field.Length or Type802.3 uses a Length field, but the Ethernet frame uses a Type field to identify the Network layer protocol. 802.3 cannot identify the upper-layer protocol and must be used with a proprietary LAN—IPX, for example.DataThis is a packet sent down to the Data Link layer from the Network layer. The size can vary from 64 to 1,500 bytes.Frame Check Sequence (FCS)FCS is a field at the end of the frame that’s used to store the CRC.Let’s pause here for a minute and take a look at some frames caught on our trusty OmniPeek network analyzer. You can see that the frame below has only three fields: Destination, Source, and Type (shown as Protocol Type on this analyzer):

Destination:   00:60:f5:00:1f:27Source:        00:60:f5:00:1f:2c

Protocol Type: 08-00 IPThis is an Ethernet_II frame. Notice that the type field is IP, or 08-00 (mostly just referred to as 0x800) in hexadecimal.The next frame has the same fields, so it must be an Ethernet_II frame too:

Destination:   ff:ff:ff:ff:ff:ff Ethernet BroadcastSource:        02:07:01:22:de:a4

Protocol Type: 08-00 IPDid you notice that this frame was a broadcast? You can tell because the destination hardware address is all 1s in binary, or all Fs in hexadecimal.Let’s take a look at one more Ethernet_II frame. I’ll talk about this next example again when we use IPv6 in Chapter 13, but you can see that the Ethernet frame is the same Ethernet_II frame 

10089.book  Page 36  Monday, July 23, 2007  3:17 PM




Ethernet Networking37we use with the IPv4 routed protocol but the type field has 0x86dd when we are carrying IPv6 data, and when we have IPv4 data, we use 0x0800 in the protocol field:

Destination: IPv6-Neighbor-Discovery_00:01:00:03 (33:33:00:01:00:03)Source: Aopen_3e:7f:dd (00:01:80:3e:7f:dd)

Type: IPv6 (0x86dd)This is the beauty of the Ethernet_II frame. Because of the protocol field, we can run any Network layer routed protocol and it will carry the data because it can identify the Network layer protocol.Ethernet at the Physical LayerEthernet was first implemented by a group called DIX (Digital, Intel, and Xerox). They created and implemented the first Ethernet LAN specification, which the IEEE used to create the IEEE 802.3 Committee. This was a 10Mbps network that ran on coax and then eventually twisted-pair and fiber physical media.The IEEE extended the 802.3 Committee to two new committees known as 802.3u (Fast Ethernet) and 802.3ab (Gigabit Ethernet on category 5) and then finally 802.3ae (10Gbps over fiber and coax).Figure 1.21 shows the IEEE 802.3 and original Ethernet Physical layer specifications.When designing your LAN, it’s really important to understand the different types of Ether-net media available to you. Sure, it would be great to run Gigabit Ethernet to each desktop and 10Gbps between switches, and although this might happen one day, justifying the cost of that network today would be pretty difficult. But if you mix and match the different types of Ether-net media methods currently available, you can come up with a cost-effective network solution that works great.FIGURE1.21Ethernet Physical layer specificationsThe EIA/TIA (Electronic Industries Association and the newer Telecommunications Indus-try Alliance) is the standards body that creates the Physical layer specifications for Ethernet. The EIA/TIA specifies that Ethernet use a registered jack (RJ) connector with a 4 5 wiring sequence on unshielded twisted-pair (UTP) cabling (RJ45). However, the industry is moving toward calling this just an 8-pin modular connector.Each Ethernet cable type that is specified by the EIA/TIA has inherent attenuation, which is defined as the loss of signal strength as it travels the length of a cable and is measured in decibels 

Data Link(MAC layer)PhysicalEthernet802.310Base210Base510BaseT10BaseF100BaseTX100BaseFX100BaseT4

10089.book  Page 37  Monday, July 23, 2007  3:17 PM




38Chapter1 Internetworking(dB). The cabling used in corporate and home markets is measured in categories. A higher-quality cable will have a higher-rated category and lower attenuation. For example, category 5 is better than category 3 because category 5 cables have more wire twists per foot and therefore less crosstalk. Crosstalk is the unwanted signal interference from adjacent pairs in the cable.Here are the original IEEE 802.3 standards:10Base210Mbps, baseband technology, up to 185 meters in length. Known as thinnet and can support up to 30 workstations on a single segment. Uses a physical and logical bus with AUI connectors. The 10 means 10Mbps, Base means baseband technology (which is a signal-ing method for communication on the network), and the 2 means almost 200 meters. 10Base2 Ethernet cards use BNC (British Naval Connector, Bayonet Neill Concelman, or Bayonet Nut Connector) and T-connectors to connect to a network.10Base510Mbps, baseband technology, up to 500 meters in length. Known as thicknet. Uses a physical and logical bus with AUI connectors. Up to 2,500 meters with repeaters and 1,024 users for all segments.10BaseT10Mbps using category 3 UTP wiring. Unlike with the 10Base2 and 10Base5 net-works, each device must connect into a hub or switch, and you can have only one host per seg-ment or wire. Uses an RJ45 connector (8-pin modular connector) with a physical star topology and a logical bus.Each of the 802.3 standards defines an Attachment Unit Interface (AUI), which allows a one-bit-at-a-time transfer to the Physical layer from the Data Link media access method. This allows the MAC to remain constant but means the Physical layer can support any existing and new technologies. The original AUI interface was a 15-pin connector, which allowed a trans-ceiver (transmitter/receiver) that provided a 15-pin-to-twisted-pair conversion.The thing is, the AUI interface cannot support 100Mbps Ethernet because of the high fre-quencies involved. So 100BaseT needed a new interface, and the 802.3u specifications created one called the Media Independent Interface (MII), which provides 100Mbps throughput. The MII uses a nibble, defined as 4 bits. Gigabit Ethernet uses a Gigabit Media Independent Inter-face (GMII) and transmits 8 bits at a time.802.3u (Fast Ethernet) is compatible with 802.3 Ethernet because they share the same physical characteristics. Fast Ethernet and Ethernet use the same maximum transmission unit (MTU), use the same MAC mechanisms, and preserve the frame format that is used by 10BaseT Ethernet. Basi-cally, Fast Ethernet is just based on an extension to the IEEE 802.3 specification, except that it offers a speed increase of 10 times that of 10BaseT.Here are the expanded IEEE Ethernet 802.3 standards:100BaseTX (IEEE 802.3u)EIA/TIA category 5, 6, or 7 UTP two-pair wiring. One user per segment; up to 100 meters long. It uses an RJ45 connector with a physical star topology and a logical bus.100BaseFX (IEEE 802.3u)Uses fiber cabling 62.5/125-micron multimode fiber. Point-to-point topology; up to 412 meters long. It uses an ST or SC connector, which are media-interface connectors.1000BaseCX (IEEE 802.3z)Copper twisted-pair called twinax (a balanced coaxial pair) that can only run up to 25 meters.

10089.book  Page 38  Monday, July 23, 2007  3:17 PM




Ethernet Cabling391000BaseT (IEEE 802.3ab)Category 5, four-pair UTP wiring up to 100 meters long.1000BaseSX (IEEE 802.3z)MMF using 62.5- and 50-micron core; uses an 850 nano-meter laser and can go up to 220 meters with 62.5-micron, 550 meters with 50-micron.1000BaseLX (IEEE 802.3z)Single-mode fiber that uses a 9-micron core and 1300 nano-meter laser and can go from 3 kilometers up to 10 kilometers.

If you want to implement a network medium that is not susceptible to elec-tromagnetic interference (EMI), fiber-optic cable provides a more secure, long-distance cable that is not susceptible to EMI at high speeds.Ethernet CablingEthernet cabling is an important discussion, especially if you are planning on taking the Cisco exams. Three types of Ethernet cables are available: Straight-through cable Crossover cable Rolled cableWe will look at each in the following sections.Straight-Through CableThe straight-through cable is used to connect Host to switch or hub Router to switch or hubFour wires are used in straight-through cable to connect Ethernet devices. It is relatively simple to create this type; Figure 1.22 shows the four wires used in a straight-through Ethernet cable.Notice that only pins 1, 2, 3, and 6 are used. Just connect 1 to 1, 2 to 2, 3 to 3, and 6 to 6 and you’ll be up and networking in no time. However, remember that this would be an Ethernet-only cable and wouldn’t work with voice, Token Ring, ISDN, and so on.FIGURE1.22Straight-through Ethernet cable

Hub/Switch1236Host1236

10089.book  Page 39  Monday, July 23, 2007  3:17 PM




40Chapter1 InternetworkingCrossover CableThe crossover cable can be used to connect Switch to switch Hub to hub Host to host Hub to switch Router direct to hostThe same four wires are used in this cable as in the straight-through cable; we just connect dif-ferent pins together. Figure 1.23 shows how the four wires are used in a crossover Ethernet cable.Notice that instead of connecting 1 to 1, 2 to 2, and so on, here we connect pins 1 to 3 and 2 to 6 on each side of the cable.FIGURE1.23Crossover Ethernet cableRolled CableAlthough rolled cable isn’t used to connect any Ethernet connections together, you can use a rolled Ethernet cable to connect a host to a router console serial communication (com) port.If you have a Cisco router or switch, you would use this cable to connect your PC running HyperTerminal to the Cisco hardware. Eight wires are used in this cable to connect serial devices, although not all eight are used to send information, just as in Ethernet networking. Figure 1.24 shows the eight wires used in a rolled cable.FIGURE1.24Rolled Ethernet cable

Hub/SwitchHub/Switch11

223366

Host12347568Router/Switch12347568

10089.book  Page 40  Monday, July 23, 2007  3:17 PM




Ethernet Cabling41These are probably the easiest cables to make because you just cut the end off on one side of a straight-through cable, turn it over, and put it back on (with a new connector, of course).Once you have the correct cable connected from your PC to the Cisco router or switch, you can start HyperTerminal to create a console connection and configure the device. Set the con-figuration as follows:1.Open HyperTerminal and enter a name for the connection. It is irrelevant what you name it, but I always just use Cisco. Then click OK.2.Choose the communications port—either COM1 or COM2, whichever is open on your PC.3.Now set the port settings. The default values (2400bps and no flow control hardware) will not work; you must set the port settings as shown in Figure 1.25.Notice that the bit rate is now set to 9600 and the flow control is set to None. At this point, you can click OK and press the Enter key and you should be connected to your Cisco device console port.We’ve taken a look at the various RJ45 unshielded twisted pair (UTP) cables. Keeping this in mind, what cable is used between the switches in Figure 1.26?In order for host A to ping host B, you need a crossover cable to connect the two switches together. But what types of cables are used in the network shown in Figure 1.27?In Figure 1.27, there are a variety of cables in use. For the connection between the switches, we’d obviously use a crossover cable like we saw in Figure 1.23. The trouble is, we have a con-sole connection that uses a rolled cable. Plus, the connection from the router to the switch is a straight-through cable, as is true for the hosts to the switches. Keep in mind that if we had a serial connection (which we don’t), it would be a V.35 that we’d use to connect us to a WAN.

10089.book  Page 41  Monday, July 23, 2007  3:17 PM




42Chapter1 InternetworkingFIGURE1.25Port settings for a rolled cable connectionFIGURE1.26RJ45 UTP cable question #1FIGURE1.27RJ45 UTP cable question #2

? Switch Switch A B 

ConsoleRouter 

10089.book  Page 42  Monday, July 23, 2007  3:17 PM




Data Encapsulation43Data EncapsulationWhen a host transmits data across a network to another device, the data goes through encap-sulation: It is wrapped with protocol information at each layer of the OSI model. Each layer communicates only with its peer layer on the receiving device.To communicate and exchange information, each layer uses Protocol Data Units (PDUs). These hold the control information attached to the data at each layer of the model. They are usu-ally attached to the header in front of the data field but can also be in the trailer, or end, of it.Each PDU attaches to the data by encapsulating it at each layer of the OSI model, and each has a specific name depending on the information provided in each header. This PDU infor-mation is read only by the peer layer on the receiving device. After it’s read, it’s stripped off and the data is then handed to the next layer up.Figure 1.28 shows the PDUs and how they attach control information to each layer. This figure demonstrates how the upper-layer user data is converted for transmission on the net-work. The data stream is then handed down to the Transport layer, which sets up a virtual cir-cuit to the receiving device by sending over a synch packet. Next, the data stream is broken up into smaller pieces, and a Transport layer header (a PDU) is created and attached to the header of the data field; now the piece of data is called a segment. Each segment is sequenced so the data stream can be put back together on the receiving side exactly as it was transmitted.Each segment is then handed to the Network layer for network addressing and routing through the internetwork. Logical addressing (for example, IP) is used to get each segment to the correct network. The Network layer protocol adds a control header to the segment handed down from the Transport layer, and what we have now is called a packet or datagram. Remem-ber that the Transport and Network layers work together to rebuild a data stream on a receiving host, but it’s not part of their work to place their PDUs on a local network segment—which is the only way to get the information to a router or host.FIGURE1.28Data encapsulation

Application

Presentation

Session

Transport

Network

Data Link

PhysicalSegmentPDUPacketFrameBits

Upper layer data

TCP header

Data

IP header

Data

LLC header

Data

MAC header

0101110101001000010

Upper layer data

FCS

FCS

10089.book  Page 43  Monday, July 23, 2007  3:17 PM




44Chapter1 InternetworkingIt’s the Data Link layer that’s responsible for taking packets from the Network layer and placing them on the network medium (cable or wireless). The Data Link layer encapsulates each packet in a frame, and the frame’s header carries the hardware address of the source and destination hosts. If the destination device is on a remote network, then the frame is sent to a router to be routed through an internetwork. Once it gets to the destination network, a new frame is used to get the packet to the destination host.To put this frame on the network, it must first be put into a digital signal. Since a frame is really a logical group of 1s and 0s, the Physical layer is responsible for encoding these digits into a digital signal, which is read by devices on the same local network. The receiving devices will synchronize on the digital signal and extract (decode) the 1s and 0s from the digital signal. At this point, the devices build the frames, run a CRC, and then check their answer against the answer in the frame’s FCS field. If it matches, the packet is pulled from the frame and what’s left of the frame is discarded. This process is called de-encapsulation. The packet is handed to the Network layer, where the address is checked. If the address matches, the segment is pulled from the packet and what’s left of the packet is discarded. The segment is processed at the Transport layer, which rebuilds the data stream and acknowledges to the transmitting station that it received each piece. It then happily hands the data stream to the upper-layer application.At a transmitting device, the data encapsulation method works like this:1.User information is converted to data for transmission on the network.2.Data is converted to segments and a reliable connection is set up between the transmitting and receiving hosts.3.Segments are converted to packets or datagrams, and a logical address is placed in the header so each packet can be routed through an internetwork.4.Packets or datagrams are converted to frames for transmission on the local network. Hard-ware (Ethernet) addresses are used to uniquely identify hosts on a local network segment.5.Frames are converted to bits, and a digital encoding and clocking scheme is used.6.To explain this in more detail using the layer addressing, I’ll use Figure 1.29.FIGURE1.29PDU and layer addressing

Source IP

DestinationMAC

Source Port

DestinationPort

. . .

Data

DestinationIP

Protocol

. . .

Segment

Source MAC

Ether-Field

Packet

FCSSegmentPacketFrameBit1011011100011110000

10089.book  Page 44  Monday, July 23, 2007  3:17 PM




Data Encapsulation45Remember that a data stream is handed down from the upper layer to the Transport layer. As technicians, we really don’t care who the data stream comes from because that’s really a programmer’s problem. Our job is to rebuild the data stream reliably and hand it to the upper layers on the receiving device.Before we go further in our discussion of Figure 1.29, let’s discuss port numbers and make sure we understand them. The Transport layer uses port numbers to define both the virtual circuit and the upper-layer process, as you can see from Figure 1.30.FIGURE1.30Port numbers at the Transport layerThe Transport layer takes the data stream, makes segments out of it, and establishes a reli-able session by creating a virtual circuit. It then sequences (numbers) each segment and uses acknowledgments and flow control. If you’re using TCP, the virtual circuit is defined by the source port number. Remember, the host just makes this up starting at port number 1024 (0 through 1023 are reserved for well-known port numbers). The destination port number defines the upper-layer process (application) that the data stream is handed to when the data stream is reliably rebuilt on the receiving host.Now that you understand port numbers and how they are used at the Transport layer, let’s go back to Figure 1.30. Once the Transport layer header information is added to the piece of data, it becomes a segment and is handed down to the Network layer along with the destina-tion IP address. (The destination IP address was handed down from the upper layers to the Transport layer with the data stream, and it was discovered through a name resolution method at the upper layers—probably DNS.)The Network layer adds a header, and adds the logical addressing (IP addresses), to the front of each segment. Once the header is added to the segment, the PDU is called a packet. The packet has a protocol field that describes where the segment came from (either UDP or TCP) so it can hand the segment to the correct protocol at the Transport layer when it reaches the receiving host.

Source Port

DestinationPort

. . .

1028

23Host A

Host Z

Defines upper layerprocess or application

. . .

DPSPDefines Virtual Circuit

10089.book  Page 45  Monday, July 23, 2007  3:17 PM




46Chapter1 InternetworkingThe Network layer is responsible for finding the destination hardware address that dictates where the packet should be sent on the local network. It does this by using the Address Res-olution Protocol (ARP)—something I’ll talk about more in Chapter 2. IP at the Network layer looks at the destination IP address and compares that address to its own source IP address and subnet mask. If it turns out to be a local network request, the hardware address of the local host is requested via an ARP request. If the packet is destined for a remote host, IP will look for the IP address of the default gateway (router) instead.The packet, along with the destination hardware address of either the local host or default gateway, is then handed down to the Data Link layer. The Data Link layer will add a header to the front of the packet and the piece of data then becomes a frame. (We call it a frame because both a header and a trailer are added to the packet, which makes the data resemble bookends or a frame, if you will.) This is shown in Figure 1.29. The frame uses an Ether-Type field to describe which protocol the packet came from at the Network layer. Now a cyclic redundancy check (CRC) is run on the frame, and the answer to the CRC is placed in the Frame Check Sequence field found in the trailer of the frame.The frame is now ready to be handed down, one bit at a time, to the Physical layer, which will use bit timing rules to encode the data in a digital signal. Every device on the network segment will synchronize with the clock and extract the 1s and 0s from the digital signal and build a frame. After the frame is rebuilt, a CRC is run to make sure the frame is okay. If everything turns out to be all good, the hosts will check the destination address to see if the frame is for them.If all this is making your eyes cross and your brain freeze, don’t freak. I’ll be going over exactly how data is encapsulated and routed through an internetwork in Chapter 6.The Cisco Three-Layer Hierarchical ModelMost of us were exposed to hierarchy early in life. Anyone with older siblings learned what it was like to be at the bottom of the hierarchy. Regardless of where you first discovered hier-archy, today most of us experience it in many aspects of our lives. It is hierarchy that helps us understand where things belong, how things fit together, and what functions go where. It brings order and understandability to otherwise complex models. If you want a pay raise, for instance, hierarchy dictates that you ask your boss, not your subordinate. That is the person whose role it is to grant (or deny) your request. So basically, understanding hierarchy helps us discern where we should go to get what we need.Hierarchy has many of the same benefits in network design that it does in other areas of life. When used properly, it makes networks more predictable. It helps us define which areas should perform certain functions. Likewise, you can use tools such as access lists at certain levels in hierarchical networks and avoid them at others.Let’s face it: Large networks can be extremely complicated, with multiple protocols, detailed configurations, and diverse technologies. Hierarchy helps us summarize a complex collection of details into an understandable model. Then, as specific configurations are needed, the model dictates the appropriate manner in which to apply them.

10089.book  Page 46  Monday, July 23, 2007  3:17 PM




The Cisco Three-Layer Hierarchical Model47The Cisco hierarchical model can help you design, implement, and maintain a scalable, reliable, cost-effective hierarchical internetwork. Cisco defines three layers of hierarchy, as shown in Figure 1.31, each with specific functions.FIGURE1.31The Cisco hierarchical modelThe following are the three layers and their typical functions: The core layer: backbone The distribution layer: routing The access layer: switchingEach layer has specific responsibilities. Remember, however, that the three layers are logical and are not necessarily physical devices. Consider the OSI model, another logical hierarchy. The seven layers describe functions but not necessarily protocols, right? Sometimes a protocol maps to more than one layer of the OSI model, and sometimes multiple protocols communicate within a single layer. In the same way, when we build physical implementations of hierarchical net-works, we may have many devices in a single layer, or we might have a single device performing functions at two layers. The definition of the layers is logical, not physical.Now, let’s take a closer look at each of the layers.The Core LayerThe core layer is literally the core of the network. At the top of the hierarchy, the core layer is responsible for transporting large amounts of traffic both reliably and quickly. The only purpose of the network’s core layer is to switch traffic as fast as possible. The traffic transported across 

CorelayerDistributionlayerAccesslayer

10089.book  Page 47  Monday, July 23, 2007  3:17 PM




48Chapter1 Internetworkingthe core is common to a majority of users. However, remember that user data is processed at the distribution layer, which forwards the requests to the core if needed.If there is a failure in the core, every single user can be affected. Therefore, fault tolerance at this layer is an issue. The core is likely to see large volumes of traffic, so speed and latency are driving concerns here. Given the function of the core, we can now consider some design specifics. Let’s start with some things we don’t want to do: Don’t do anything to slow down traffic. This includes using access lists, routing between virtual local area networks (VLANs), and implementing packet filtering. Don’t support workgroup access here. Avoid expanding the core (i.e., adding routers) when the internetwork grows. If perfor-mance becomes an issue in the core, give preference to upgrades over expansion.Now, there are a few things that we want to do as we design the core: Design the core for high reliability. Consider data-link technologies that facilitate both speed and redundancy, such as FDDI, Fast Ethernet (with redundant links), or even ATM. Design with speed in mind. The core should have very little latency. Select routing protocols with lower convergence times. Fast and redundant data-link con-nectivity is no help if your routing tables are shot!The Distribution LayerThe distribution layer is sometimes referred to as the workgroup layer and is the communica-tion point between the access layer and the core. The primary functions of the distribution layer are to provide routing, filtering, and WAN access and to determine how packets can access the core, if needed. The distribution layer must determine the fastest way that network service requests are handled—for example, how a file request is forwarded to a server. After the distribution layer determines the best path, it forwards the request to the core layer if nec-essary. The core layer then quickly transports the request to the correct service.The distribution layer is the place to implement policies for the network. Here you can exercise considerable flexibility in defining network operation. There are several actions that generally should be done at the distribution layer: Routing Implementing tools (such as access lists), packet filtering, and queuing Implementing security and network policies, including address translation and firewalls Redistributing between routing protocols, including static routing Routing between VLANs and other workgroup support functions Defining broadcast and multicast domainsThings to avoid at the distribution layer are limited to those functions that exclusively belong to one of the other layers.

10089.book  Page 48  Monday, July 23, 2007  3:17 PM




Exam Essentials49The Access LayerThe access layer controls user and workgroup access to internetwork resources. The access layer is sometimes referred to as the desktop layer. The network resources most users need will be available locally. The distribution layer handles any traffic for remote services. The follow-ing are some of the functions to be included at the access layer: Continued (from distribution layer) use of access control and policies Creation of separate collision domains (segmentation) Workgroup connectivity into the distribution layerTechnologies such as DDR and Ethernet switching are frequently seen in the access layer. Static routing (instead of dynamic routing protocols) is seen here as well.As already noted, three separate levels does not imply three separate routers. There could be fewer, or there could be more. Remember, this is a layered approach.SummaryWhew! I know this seemed like the chapter that wouldn’t end, but it did—and you made it through! You’re now armed with a ton of fundamental information; you’re ready to build upon it and are well on your way to certification.I started by discussing simple, basic networking and the differences between collision and broadcast domains. I also discussed the various devices used in an internetwork.I then discussed the OSI model—the seven-layer model used to help application developers design applications that can run on any type of system or network. Each layer has its special jobs and select responsibilities within the model to ensure that solid, effective communications do, in fact, occur. I provided you with complete details of each layer and discussed how Cisco views the specifications of the OSI model.In addition, each layer in the OSI model specifies different types of devices. I described the dif-ferent devices, cables, and connectors used at each layer. Remember that hubs are Physical layer devices and repeat the digital signal to all segments except the one from which it was received. Switches segment the network using hardware addresses and break up collision domains. Routers break up broadcast domains (and collision domains) and use logical addressing to send packets through an internetwork.Last, this chapter covered the Cisco three-layer hierarchical model. I described in detail the three layers and how each is used to help design and implement a Cisco internetwork. We are now going to move on to IP addressing in the next chapter.Exam EssentialsRemember the possible causes of LAN traffic congestion.Too many hosts in a broadcast domain, broadcast storms, multicasting, and low bandwidth are all possible causes of LAN traffic congestion.

10089.book  Page 49  Monday, July 23, 2007  3:17 PM




50Chapter1 InternetworkingUnderstand the difference between a collision domain and a broadcast domain.Collision domain is an Ethernet term used to describe a network collection of devices in which one par-ticular device sends a packet on a network segment, forcing every other device on that same segment to pay attention to it. On a broadcast domain, a set of all devices on a network seg-ment hear all broadcasts sent on that segment.Understand the difference between a hub, a bridge, a switch, and a router.Hubs create one col-lision domain and one broadcast domain. Bridges break up collision domains but create one large broadcast domain. They use hardware addresses to filter the network. Switches are really just multiple port bridges with more intelligence. They break up collision domains but create one large broadcast domain by default. Switches use hardware addresses to filter the network. Routers break up broadcast domains (and collision domains) and use logical addressing to filter the network.Remember the difference between connection-oriented and connectionless network services.Connection-oriented services use acknowledgments and flow control to create a reliable session. More overhead is used than in a connectionless network service. Connectionless services are used to send data with no acknowledgments or flow control. This is considered unreliable.Remember the OSI layers.You must remember the seven layers of the OSI model and what function each layer provides. The Application, Presentation, and Session layers are upper layers and are responsible for communicating from a user interface to an application. The Transport layer provides segmentation, sequencing, and virtual circuits. The Network layer provides log-ical network addressing and routing through an internetwork. The Data Link layer provides framing and placing of data on the network medium. The Physical layer is responsible for taking 1s and 0s and encoding them into a digital signal for transmission on the network segment.Remember the types of Ethernet cabling and when you would use them.The three types of cables that can be created from an Ethernet cable are straight-through (to connect a PC’s or a router’s Ethernet interface to a hub or switch), crossover (to connect hub to hub, hub to switch, switch to switch, or PC to PC), and rolled (for a console connection from a PC to a router or switch).Understand how to connect a console cable from a PC to a router and start HyperTerminal.Take a rolled cable and connect it from the COM port of the host to the console port of a router. Start HyperTerminal and set the BPS to 9600 and flow control to None.Remember the three layers in the Cisco three-layer model.The three layers in the Cisco hierarchical model are the core, distribution, and access layers.Written Lab 1In this section, you’ll complete the following labs to make sure you’ve got the information and concepts contained within them fully dialed in: Lab 1.1: OSI Questions Lab 1.2: Defining the OSI Layers and Devices

10089.book  Page 50  Monday, July 23, 2007  3:17 PM




Written Lab 151 Lab 1.3: Identifying Collision and Broadcast Domains Lab 1.4: Binary/Decimal/Hexadecimal Conversion(The answers to the written labs can be found following the answers to the review questions for this chapter.)Written Lab 1.1: OSI QuestionsAnswer the following questions about the OSI model:1.Which layer chooses and determines the availability of communicating partners along with the resources necessary to make the connection, coordinates partnering applications, and forms a consensus on procedures for controlling data integrity and error recovery?2.Which layer is responsible for converting data packets from the Data Link layer into electrical signals?3.At which layer is routing implemented, enabling connections and path selection between two end systems?4.Which layer defines how data is formatted, presented, encoded, and converted for use on the network?5.Which layer is responsible for creating, managing, and terminating sessions between applications?6.Which layer ensures the trustworthy transmission of data across a physical link and is primarily concerned with physical addressing, line discipline, network topology, error notification, ordered delivery of frames, and flow control?7.Which layer is used for reliable communication between end nodes over the network and provides mechanisms for establishing, maintaining, and terminating virtual circuits; transport-fault detection and recovery; and controlling the flow of information?8.Which layer provides logical addressing that routers will use for path determination?9.Which layer specifies voltage, wire speed, and pinout cables and moves bits between devices?10.Which layer combines bits into bytes and bytes into frames, uses MAC addressing, and provides error detection?11.Which layer is responsible for keeping the data from different applications separate on the network?12.Which layer is represented by frames?13.Which layer is represented by segments?14.Which layer is represented by packets?15.Which layer is represented by bits?16.Put the following in order of encapsulation: Packets Frames

10089.book  Page 51  Monday, July 23, 2007  3:17 PM




52Chapter1 Internetworking Bits Segments17.Which layer segments and reassembles data into a data stream?18.Which layer provides the physical transmission of the data and handles error notification, network topology, and flow control?19.Which layer manages device addressing, tracks the location of devices on the network, and determines the best way to move data?20.What is the bit length and expression form of a MAC address?Written Lab 1.2: Defining the OSI Layers and DevicesFill in the blanks with the appropriate layer of the OSI or hub, switch, or router device.DescriptionDevice or OSI LayerThis device sends and receives information about the Network layer. This layer creates a virtual circuit before transmitting between two end stations. This layer uses service access points. This device uses hardware addresses to filter a network. Ethernet is defined at these layers. This layer supports flow control and sequencing. This device can measure the distance to a remote network. Logical addressing is used at this layer. Hardware addresses are defined at this layer. This device creates one big collision domain and one large broadcast domain. This device creates many smaller collision domains, but the network is still one large broadcast domain. This device can never run full duplex. This device breaks up collision domains and broadcast domains. 

10089.book  Page 52  Monday, July 23, 2007  3:17 PM




Written Lab 153Written Lab 1.3: Identifying Collision and Broadcast DomainsIn the following exhibit, identify the number of collision domains and broadcast domains in each specified device. Each device is represented by a letter:A.HubB.BridgeC.SwitchD.RouterWritten Lab 1.4: Binary/Decimal/Hexadecimal Conversion1.Convert from decimal IP address to binary format.Complete the following table to express 192.168.10.15 in binary format.1286432168421Binary                                     

ABDC

Router

SwitchBridge

Hub

10089.book  Page 53  Monday, July 23, 2007  3:17 PM




54Chapter1 InternetworkingComplete the following table to express 172.16.20.55 in binary format.Complete the following table to express 10.11.12.99 in binary format.2.Convert the following from binary format to decimal IP address.Complete the following table to express 11001100.00110011.10101010.01010101 in decimal IP address format.Complete the following table to express 11000110.11010011.00111001.11010001 in decimal IP address format.1286432168421Binary                                     1286432168421Binary                                     1286432168421Decimal                                     1286432168421Decimal                                     

10089.book  Page 54  Monday, July 23, 2007  3:17 PM




Written Lab 155Complete the following table to express 10000100.11010010.10111000.10100110 in decimal IP address format.3.Convert the following from binary format to hexadecimal.Complete the following table to express 11011000.00011011.00111101.01110110 in hexadecimal.Complete the following table to express 11001010.11110101.10000011.11101011 in hexadecimal.Complete the following table to express 10000100.11010010.01000011.10110011 in hexadecimal.1286432168421Decimal                                     1286432168421Hexadecimal                                     1286432168421Hexadecimal                                     1286432168421Hexadecimal                                     

10089.book  Page 55  Monday, July 23, 2007  3:17 PM




56Chapter1 InternetworkingReview Questions

The following questions are designed to test your understanding of this chapter’s material. For more information on how to get additional ques-tions, please see this book’s Introduction.1.A receiving host has failed to receive all of the segments that it should acknowledge. What can the host do to improve the reliability of this communication session?A.Send a different source port number.B.Restart the virtual circuit.C.Decrease the sequence number.D.Decrease the window size.2.Which fields are contained within an IEEE Ethernet frame header? (Choose two.)A.Source and destination MAC addressB.Source and destination network addressC.Source and destination MAC address and source and destination network addressD.FCS field3.Which layer 1 devices can be used to enlarge the area covered by a single LAN segment? (Choose two.)A.SwitchB.NICC.HubD.RepeaterE.RJ45 transceiver4.Segmentation of a data stream happens at which layer of the OSI model?A.PhysicalB.Data LinkC.NetworkD.Transport5.Which of the following describe router functions? (Choose four.)A.Packet switchingB.Collision preventionC.Packet filteringD.Broadcast domain enlargementE.Internetwork communicationF.Broadcast forwardingG.Path selection

10089.book  Page 56  Monday, July 23, 2007  3:17 PM




Review Questions576.Routers operate at layer __. LAN switches operate at layer __. Ethernet hubs operate at layer __. Word processing operates at layer __.A.3, 3, 1, 7B.3, 2, 1, noneC.3, 2, 1, 7D.2, 3, 1, 7E.3, 3, 2, none7.When data is encapsulated, which is the correct order?A.Data, frame, packet, segment, bitB.Segment, data, packet, frame, bitC.Data, segment, packet, frame, bitD.Data, segment, frame, packet, bit8.Why does the data communication industry use the layered OSI reference model? (Choose two.)A.It divides the network communication process into smaller and simpler components, thus aiding component development, design, and troubleshooting.B.It enables equipment from different vendors to use the same electronic components, thus saving research and development funds.C.It supports the evolution of multiple competing standards and thus provides business opportunities for equipment manufacturers.D.It encourages industry standardization by defining what functions occur at each layer of the model.E.It provides a framework by which changes in functionality in one layer require changes in other layers.9.What are two purposes for segmentation with a bridge?A.To add more broadcast domainsB.To create more collision domainsC.To add more bandwidth for usersD.To allow more broadcasts for users10.Which of the following are unique characteristics of half-duplex Ethernet when compared to full-duplex Ethernet? (Choose two.)A.Half-duplex Ethernet operates in a shared collision domain.B.Half-duplex Ethernet operates in a private collision domain.C.Half-duplex Ethernet has higher effective throughput.D.Half-duplex Ethernet has lower effective throughput.E.Half-duplex Ethernet operates in a private broadcast domain.

10089.book  Page 57  Monday, July 23, 2007  3:17 PM




58Chapter1 Internetworking11.You want to implement a network medium that is not susceptible to EMI. Which type of cabling should you use?A.Thicknet coaxB.Thinnet coaxC.Category 5 UTP cableD.Fiber-optic cable12.Acknowledgments, sequencing, and flow control are characteristics of which OSI layer?A.Layer 2B.Layer 3C.Layer 4D.Layer 713.Which of the following are types of flow control? (Choose all that apply.)A.BufferingB.Cut-throughC.WindowingD.Congestion avoidanceE.VLANs14.Which of the following types of connections can use full duplex? (Choose three.)A.Hub to hubB.Switch to switchC.Host to hostD.Switch to hubE.Switch to host15.What is the purpose of flow control?A.To ensure that data is retransmitted if an acknowledgment is not receivedB.To reassemble segments in the correct order at the destination deviceC.To provide a means for the receiver to govern the amount of data sent by the senderD.To regulate the size of each segment16.Which three statements are true about the operation of a full-duplex Ethernet network?A.There are no collisions in full-duplex mode.B.A dedicated switch port is required for each full-duplex node.C.Ethernet hub ports are preconfigured for full-duplex mode.D.In a full-duplex environment, the host network card must check for the availability of the network media before transmitting.E.The host network card and the switch port must be capable of operating in full-duplex mode.

10089.book  Page 58  Monday, July 23, 2007  3:17 PM




Review Questions5917.What type of RJ45 UTP cable is used between switches?A.Straight-throughB.Crossover cableC.Crossover with a CSU/DSUD.Crossover with a router in between the two switches18.How does a host on an Ethernet LAN know when to transmit after a collision has occurred? (Choose two.)A.In a CSMA/CD collision domain, multiple stations can successfully transmit data simultaneously.B.In a CSMA/CD collision domain, stations must wait until the media is not in use before transmitting.C.You can improve the CSMA/CD network by adding more hubs.D.After a collision, the station that detected the collision has first priority to resend the lost data.E.After a collision, all stations run a random backoff algorithm. When the backoff delay period has expired, all stations have equal priority to transmit data.F.After a collision, all stations involved run an identical backoff algorithm and then synchro-nize with each other prior to transmitting data.19.What type of RJ45 UTP cable do you use to connect a PC’s COM port to a router or switch console port?A.Straight-throughB.Crossover cableC.Crossover with a CSU/DSUD.Rolled20.You have the following binary number:10110111What are the decimal and hexadecimal equivalents?A.69/0x2102B.183/B7C.173/A6D.83/0xC5

10089.book  Page 59  Monday, July 23, 2007  3:17 PM




60Chapter1 InternetworkingAnswers to Review Questions1.D. A receiving host can control the transmitter by using flow control (TCP uses Windowing by default). By decreasing the window size, the receiving host can slow down the transmitting host so the receiving host does not overflow its buffers.2.A, D. An Ethernet frame has source and destination MAC addresses, an Ether-Type field to identify the Network layer protocol, the data, and the FCS field that holds the answer to the CRC.3.C, D. Not that you really want to enlarge a single collision domain, but a hub (multiport repeater) will provide this for you.4.D. The Transport layer receives large data streams from the upper layers and breaks these up into smaller pieces called segments.5.A, C, E, G. Routers provide packet switching, packet filtering, internetwork communication, and path selection.6.B. Routers operate at layer 3. LAN switches operate at layer 2. Ethernet hubs operate at layer 1. Word processing applications communicate to the Application layer interface, but do not operate at layer 7, so the answer would be none.7.C. The encapsulation method is data, segment, packet, frame, bit.8.A, D. The main advantage of a layered model is that it can allow application developers to change aspects of a program in just one layer of the layer model’s specifications. Advantages of using the OSI layered model include, but are not limited to, the following: It divides the network communication process into smaller and simpler components, thus aiding compo-nent development, design, and troubleshooting; it allows multiple-vendor development through standardization of network components; it encourages industry standardization by defining what functions occur at each layer of the model; it allows various types of network hardware and software to communicate; and it prevents changes in one layer from affecting other layers, so it does not hamper development.9.B, C. Bridges break up collision domains, which allow more bandwidth for users.10.A, D. Unlike full duplex, half-duplex Ethernet operates in a shared collision domain, and it has a lower effective throughput than full duplex.11.D. Fiber-optic cable provides a more secure, long-distance cable that is not susceptible to EMI interference at high speeds.12.C. A reliable Transport layer connection uses acknowledgments to make sure all data is trans-mitted and received reliably. A reliable connection is defined by a virtual circuit that uses acknowledgments, sequencing, and flow control, which are characteristics of the Transport layer (layer 4).13.A, C, D. The common types of flow control are buffering, windowing, and congestion avoidance.

10089.book  Page 60  Monday, July 23, 2007  3:17 PM




Answers to Review Questions6114.B, C, E. Hubs cannot run full-duplex Ethernet. Full duplex must be used on a point-to-point connection between two devices capable of running full duplex. Switches and hosts can run full duplex between each other, but a hub can never run full duplex.15.C. Flow control allows the receiving device to control the transmitter so the receiving device’s buffer does not overflow.16.A, B, E. Full-duplex means you are using both wire pairs simultaneously to send and receive data. You must have a dedicated switch port for each node, which means you will not have collisions. Both the host network card and the switch port must be capable and set to work in full-duplex mode.17.B. To connect two switches together, you would use a RJ45 UTP crossover cable.18.B, E. Once transmitting stations on an Ethernet segment hear a collision, they send an extended jam signal to ensure that all stations recognize the collision. After the jamming is complete, each sender waits a predetermined amount of time, plus a random time. After both timers expire, they are free to transmit, but they must make sure the media is clear before transmitting and that they all have equal priority.19.D. To connect to a router or switch console port, you would use an RJ45 UTP rolled cable.20.B. You must be able to take a binary number and convert it into both decimal and hexadecimal. To convert to decimal, just add up the 1s using their values. The values that are turned on with the binary number of 10110111 are 128 + 32 + 16 + 4 + 2 + 1 = 183. To get the hexadecimal equivalent, you need to break the eight binary digits into nibbles (4 bits), 1011 and 0111. By add-ing up these values, you get 11 and 7. In hexadecimal, 11 is B, so the answer is 0xB7.

10089.book  Page 61  Monday, July 23, 2007  3:17 PM




62Chapter1 InternetworkingAnswers to Written Lab 11.The Application layer is responsible for finding the network resources broadcast from a server and adding flow control and error control (if the application developer chooses).2.The Physical layer takes frames from the Data Link layer and encodes the 1s and 0s into a digital signal for transmission on the network medium.3.The Network layer provides routing through an internetwork and logical addressing.4.The Presentation layer makes sure that data is in a readable format for the Application layer.5.The Session layer sets up, maintains, and terminates sessions between applications.6.PDUs at the Data Link layer are called frames. As soon as you see frame in a question, you know the answer.7.The Transport layer uses virtual circuits to create a reliable connection between two hosts.8.The Network layer provides logical addressing, typically IP addressing and routing.9.The Physical layer is responsible for the electrical and mechanical connections between devices.10.The Data Link layer is responsible for the framing of data packets.11.The Session layer creates sessions between different hosts’ applications.12.The Data Link layer frames packets received from the Network layer.13.The Transport layer segments user data.14.The Network layer creates packets out of segments handed down from the Transport layer.15.The Physical layer is responsible for transporting 1s and 0s in a digital signal.16.Segments, packets, frames, bits17.Transport18.Data Link19.Network20.48 bits (6 bytes) expressed as a hexadecimal number

10089.book  Page 62  Monday, July 23, 2007  3:17 PM




Answers to Written Lab 1.363Answer to Written Lab 1.2Answers to Written Lab 1.31.Hub: One collision domain, one broadcast domain2.Bridge: Two collision domains, one broadcast domain3.Switch: Four collision domains, one broadcast domain4.Router: Three collision domains, three broadcast domainsDescriptionDevice or OSI LayerThis device sends and receives information about the Network layer.RouterThis layer creates a virtual circuit before transmitting between two end stations.TransportThis layer uses service access points.Data Link (LLC sublayer)This device uses hardware addresses to filter a network.Bridge or switchEthernet is defined at these layers.Data Link and PhysicalThis layer supports flow control and sequencing.TransportThis device can measure the distance to a remote network.RouterLogical addressing is used at this layer.NetworkHardware addresses are defined at this layer.Data Link (MAC sublayer)This device creates one big collision domain and one large broadcast domain.HubThis device creates many smaller collision domains, but the network is still one large broadcast domain.Switch or bridgeThis device can never run full duplexHubThis device breaks up collision domains and broadcast domains.Router

10089.book  Page 63  Monday, July 23, 2007  3:17 PM




64Chapter1 InternetworkingAnswers to Written Lab 1.41.Convert from decimal IP address to binary format.Complete the following table to express 192.168.10.15 in binary format.Complete the following table to express 172.16.20.55 in binary format.Complete the following table to express 10.11.12.99 in binary format.2.Convert the following from binary format to decimal IP address.Complete the following table to express 11001100.00110011.10101010.01010101 in decimal IP address format.Decimal1286432168421Binary 19211000000110000001681010100010101000100000101000001010150000111100001111Decimal1286432168421Binary 172?1010110010101100160001000000010000200001010000010100550011011100110111Decimal1286432168421Binary 100000101000001010110000101100001011120000110000001100990110001101100011Binary1286432168421Decimal 11001100110011002040011001100110011511010101010101010170010101010101010185

10089.book  Page 64  Monday, July 23, 2007  3:17 PM




Answers to Written Lab 1.465Complete the following table to express 11000110.11010011.00111001.11010001 in decimal IP address format.Complete the following table to express 10000100.11010010.10111000.10100110 in decimal IP address format.3.Convert the following from binary format to hexadecimal.Complete the following table to express 11011000.00011011.00111101.01110110 in hexadecimal.Complete the following table to express 11001010.11110101.10000011.11101011 in hexadecimal.Binary1286432168421Decimal 110001101100011019811010011110100112110011100100111001571101000111010001209Binary1286432168421Decimal 1000010010000100132110100101101001021010111000101110001841010011010100110166Binary1286432168421Hexadecimal 1101100011011000D800011011000110111B00111101001111013D011101100111011076Binary1286432168421Hexadecimal 1100101011001010CA1111010111110101F51000001110000011831110101111101011EB

10089.book  Page 65  Monday, July 23, 2007  3:17 PM




66Chapter1 InternetworkingComplete the following table to express 10000100.11010010.01000011.10110011 in hexadecimal.Binary1286432168421Hexadecimal 1000010010000100841101001011010010D20100001101000011431011001110110011B3

10089.book  Page 66  Monday, July 23, 2007  3:17 PM




 

Chapter 2 Introduction to TCP/IP

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Describe how a network works  Describe the purpose and basic operation of the protocols in the OSI and TCP models  Identify and correct common network problems at layers 1, 2, 3 and 7 using a layered model approach   Implement an IP addressing scheme and IP Services to meet network requirements in a medium-size Enterprise branch office network  Describe the operation and benefits of using private and public IP addressing 

 

10089.book  Page 67  Monday, July 23, 2007  3:17 PM




 The  Transmission Control Protocol/Internet Protocol (TCP/IP)  suite was created by the Department of Defense (DoD) to ensure and preserve data integrity, as well as maintain communications in the event of catastrophic war. So it follows that if designed and implemented correctly, a TCP/IP network can be a truly dependable and resilient one. In this chapter, I’ll cover the pro-tocols of TCP/IP, and throughout this book, you’ll learn how to create a marvelous TCP/IP network—using Cisco routers, of course.We’ll begin by taking a look at the DoD’s version of TCP/IP and then compare this version and its protocols with the OSI reference model discussed in Chapter 1, “Internetworking.”Once you understand the protocols used at the various levels of the DoD model, I’ll cover IP addressing and the different classes of addresses used in networks today.

 Subnetting will be covered in Chapter 3, “IP Subnetting, Variable Length  Subnet Masks (VLSMs), and Troubleshooting TCP/IP.” Last, because broadcast addresses are so important to understanding IP addressing, as well as subnetting, and VLSM, an understanding of the various flavors of broadcast addresses is critical. I’ll finish with the various types of broadcast addresses that you just must know.Internet Protocol version 6 will not be discussed in this chapter; this chapter will focus solely on IPv4. IPv6 will be covered in Chapter 13, “IP Version 6 (IPv6).” Also, when discuss-ing Internet Protocol Version 4, you’ll see it written as just IP, not typically IPv4.

 For up-to-the-minute updates for this chapter, please see  www.lammle.com   and/or  www.sybex.com . TCP/IP and the DoD Model The DoD model is basically a condensed version of the OSI model—it’s composed of four, instead of seven, layers:  Process/Application layer  Host-to-Host layer  Internet layer  Network Access layer

 

10089.book  Page 68  Monday, July 23, 2007  3:17 PM




 TCP/IP and the DoD Model 69 Figure 2.1 shows a comparison of the DoD model and the OSI reference model. As you can see, the two are similar in concept, but each has a different number of layers with different names. FIGURE2.1 The DoD and OSI models

 When the different protocols in the IP stack are discussed, the layers of the OSI and DoD models are interchangeable. In other words, the Internet layer and the Network layer describe the same thing, as do the Host-to-Host layer and the  Transport layer. A vast array of protocols combine at the DoD model’s  Process/Application layer  to inte-grate the various activities and duties spanning the focus of the OSI’s corresponding top three layers (Application, Presentation, and Session). We’ll be looking closely at those protocols in the next part of this chapter. The Process/Application layer defines protocols for node-to-node application communication and also controls user-interface specifications.The  Host-to-Host layer  parallels the functions of the OSI’s Transport layer, defining pro-tocols for setting up the level of transmission service for applications. It tackles issues such as creating reliable end-to-end communication and ensuring the error-free delivery of data. It handles packet sequencing and maintains data integrity.The  Internet layer  corresponds to the OSI’s Network layer, designating the protocols relating to the logical transmission of packets over the entire network. It takes care of the addressing of hosts by giving them an IP (Internet Protocol) address, and it handles the routing of packets among multiple networks.At the bottom of the DoD model, the  Network Access layer  monitors the data exchange between the host and the network. The equivalent of the Data Link and Physical layers of the OSI model, the Network Access layer oversees hardware addressing and defines protocols for the physical transmission of data.

 

10089.book  Page 69  Monday, July 23, 2007  3:17 PM




 70 Chapter2  Introduction to TCP/IP The DoD and OSI models are alike in design and concept and have similar functions in similar layers. Figure 2.2 shows the TCP/IP protocol suite and how its protocols relate to the DoD model layers. FIGURE2.2 The TCP/IP protocol suite In the following sections, we will look at the different protocols in more detail, starting with the Process/Application layer protocols. The Process/Application Layer Protocols In this section, I’ll describe the different applications and services typically used in IP networks. The following protocols and applications are covered in this section:  Telnet  FTP  TFTP  NFS  SMTP  LPD  X Window  SNMP  DNS  DHCP/BootP

 

10089.book  Page 70  Monday, July 23, 2007  3:17 PM




 TCP/IP and the DoD Model 71 Telnet Telnet  is the chameleon of protocols—its specialty is terminal emulation. It allows a user on a remote client machine, called the Telnet client, to access the resources of another machine, the Telnet server. Telnet achieves this by pulling a fast one on the Telnet server and making the client machine appear as though it were a terminal directly attached to the local network. This projection is actually a software image—a virtual terminal that can interact with the chosen remote host.These emulated terminals are of the text-mode type and can execute refined procedures such as displaying menus that give users the opportunity to choose options and access the applications on the duped server. Users begin a Telnet session by running the Telnet client soft-ware and then logging into the Telnet server. File Transfer Protocol (FTP) File Transfer Protocol (FTP)  is the protocol that actually lets us transfer files, and it can accomplish this between any two machines using it. But FTP isn’t just a protocol; it’s also a program. Operating as a protocol, FTP is used by applications. As a program, it’s employed by users to perform file tasks by hand. FTP also allows for access to both directories and files and can accomplish certain types of directory operations, such as relocating into different ones. FTP teams up with Telnet to transparently log you into the FTP server and then provides for the transfer of files.Accessing a host through FTP is only the first step, though. Users must then be subjected to an authentication login that’s probably secured with passwords and usernames implemented by system administrators to restrict access. You can get around this somewhat by adopting the user-name  anonymous —though what you’ll gain access to will be limited.Even when employed by users manually as a program, FTP’s functions are limited to listing and manipulating directories, typing file contents, and copying files between hosts. It can’t execute remote files as programs. Trivial File Transfer Protocol (TFTP) Trivial File Transfer Protocol (TFTP)  is the stripped-down, stock version of FTP, but it’s the protocol of choice if you know exactly what you want and where to find it, plus it’s so easy to use and it’s fast too! It doesn’t give you the abundance of functions that FTP does, though. TFTP has no directory-browsing abilities; it can do nothing but send and receive files. This compact little protocol also skimps in the data department, sending much smaller blocks of data than FTP, and there’s no authentication as with FTP, so it’s insecure. Few sites support it because of the inherent security risks. Network File System (NFS) Network File System (NFS)  is a jewel of a protocol specializing in file sharing. It allows two dif-ferent types of file systems to interoperate. It works like this: Suppose the NFS server software is running on an NT server and the NFS client software is running on a Unix host. NFS allows for a portion of the RAM on the NT server to transparently store Unix files, which can, in turn, be used by Unix users. Even though the NT file system and Unix file system are unlike—they have different case sensitivity, filename lengths, security, and so on—both Unix users and NT users can access that same file with their normal file systems, in their normal way.

 

10089.book  Page 71  Monday, July 23, 2007  3:17 PM




 72 Chapter2  Introduction to TCP/IP Simple Mail Transfer Protocol (SMTP) Simple Mail Transfer Protocol (SMTP) , answering our ubiquitous call to email, uses a spooled, or queued, method of mail delivery. Once a message has been sent to a destination, the message is spooled to a device—usually a disk. The server software at the destination posts a vigil, regu-larly checking the queue for messages. When it detects them, it proceeds to deliver them to their destination. SMTP is used to send mail; POP3 is used to receive mail. Line Printer Daemon (LPD) The Line Printer Daemon (LPD) protocol is designed for printer sharing. The LPD, along with the Line Printer (LPR) program, allows print jobs to be spooled and sent to the network’s printers using TCP/IP. X Window Designed for client/server operations,  X Window  defines a protocol for writing client/server applications based on a graphical user interface (GUI). The idea is to allow a program, called a client, to run on one computer and have it display things through a window server on another computer. Simple Network Management Protocol (SNMP) Simple Network Management Protocol (SNMP)  collects and manipulates valuable network information. It gathers data by polling the devices on the network from a management station at fixed or random intervals, requiring them to disclose certain information. When all is well, 

 When Should You Use FTP? The folks at your San Francisco office needs a 50MB file emailed to them right away. What do you do? Most email servers would reject the email because they have size limits. Even if there’s no size limit on the server, it still would take a while to send this big file to SF. FTP to the rescue!If you need to give someone a large file or you need to get a large file from someone, FTP is a nice choice. Smaller files (less than 5MB) can just be sent via email if you have the bandwidth of DSL or a cable modem. However, most ISPs don’t allow files larger then 5MB to be emailed, so FTP is an option you should consider if you are in need of sending and receiving large files. (Who isn’t these days?) To use FTP, you will need to set up an FTP server on the Internet so that the files can be shared.Besides, FTP is faster than email, which is another reason to use FTP for sending or receiving large files. In addition, because it uses TCP and is connection-oriented, if the session dies, FTP can sometimes start up where it left off. Try that with your email client!

 

10089.book  Page 72  Monday, July 23, 2007  3:17 PM




 TCP/IP and the DoD Model 73 SNMP receives something called a  baseline —a report delimiting the operational traits of a healthy network. This protocol can also stand as a watchdog over the network, quickly noti-fying managers of any sudden turn of events. These network watchdogs are called  agents , and when aberrations occur, agents send an alert called a  trap  to the management station. Domain Name Service (DNS) Domain Name Service (DNS)  resolves hostnames—specifically, Internet names, such as  www.routersim.com . You don’t have to use DNS; you can just type in the IP address of any device you want to communicate with. An IP address identifies hosts on a network and the Internet as well. However, DNS was designed to make our lives easier. Think about this: What would happen if you wanted to move your web page to a different service provider? The IP address would change and no one would know what the new one was. DNS allows you to use a domain name to specify an IP address. You can change the IP address as often as you want and no one will know the difference.DNS is used to resolve a  fully qualified domain name (FQDN) —for example,  www.lammle.com  or  todd.lammle.com . An FQDN is a hierarchy that can logically locate a system based on its domain identifier.If you want to resolve the name  todd , you either must type in the FQDN of  todd.lammle.com  or have a device such as a PC or router add the suffix for you. For example, on a Cisco router, you can use the command  ip   domain-name   lammle.com  to append each request with the  lammle.com  domain. If you don’t do that, you’ll have to type in the FQDN to get DNS to resolve the name.

 An important thing to remember about DNS is that if you can ping a device with an IP address but cannot use its FQDN, then you might have some type  of DNS configuration failure. Dynamic Host Configuration Protocol (DHCP)/Bootstrap Protocol (BootP) Dynamic Host Configuration Protocol (DHCP)  assigns IP addresses to hosts. It allows easier administration and works well in small to even very large network environments. All types of hardware can be used as a DHCP server, including a Cisco router.DHCP differs from BootP in that BootP assigns an IP address to a host but the host’s hard-ware address must be entered manually in a BootP table. You can think of DHCP as a dynamic BootP. But remember that BootP is also used to send an operating system that a host can boot from. DHCP can’t do that.But there is a lot of information a DHCP server can provide to a host when the host is requesting an IP address from the DHCP server. Here’s a list of the information a DHCP server can provide:  IP address  Subnet mask

 

10089.book  Page 73  Monday, July 23, 2007  3:17 PM




 74 Chapter2  Introduction to TCP/IP  Domain name  Default gateway (routers)  DNS  WINS informationA DHCP server can give us even more information than this, but the items in the list are the most common.A client that sends out a DHCP Discover message in order to receive an IP address sends out a broadcast at both layer 2 and layer 3. The layer 2 broadcast is all  F s in hex, which looks like this: FF:FF:FF:FF:FF:FF. The layer 3 broadcast is 255.255.255.255, which means all networks and all hosts. DHCP is connectionless, which means it uses User Datagram Protocol (UDP) at the Transport layer, also known as the Host-to-Host layer, which we’ll talk about next.In case you don’t believe me, here’s an example of output from my trusty Ethereal analyzer:

 Ethernet II, Src: 192.168.0.3 (00:0b:db:99:d3:5e), Dst: Broadcast ?  (ff:ff:ff:ff:ff:ff)Internet Protocol, Src: 0.0.0.0 (0.0.0.0), Dst: 255.255.255.255 ?

  (255.255.255.255) The Data Link and Network layers are both sending out “all hands” broadcasts saying, “Help—I don’t know my IP address!”

 Broadcast addresses will be discussed in more detail at the end of  this chapter.The Host-to-Host Layer ProtocolsThe main purpose of the Host-to-Host layer is to shield the upper-layer applications from the complexities of the network. This layer says to the upper layer, “Just give me your data stream, with any instructions, and I’ll begin the process of getting your information ready to send.”The following sections describe the two protocols at this layer: Transmission Control Protocol (TCP) User Datagram Protocol (UDP)In addition, we’ll look at some of the key host-to-host protocol concepts, as well as the port numbers.

Remember, this is still considered layer 4, and Cisco really likes the way layer 4 can use acknowledgments, sequencing, and flow control.

10089.book  Page 74  Monday, July 23, 2007  3:17 PM




TCP/IP and the DoD Model75Transmission Control Protocol (TCP)Transmission Control Protocol (TCP) takes large blocks of information from an application and breaks them into segments. It numbers and sequences each segment so that the destination’s TCP stack can put the segments back into the order the application intended. After these segments are sent, TCP (on the transmitting host) waits for an acknowledgment of the receiving end’s TCP virtual circuit session, retransmitting those that aren’t acknowledged.Before a transmitting host starts to send segments down the model, the sender’s TCP stack contacts the destination’s TCP stack to establish a connection. What is created is known as a virtual circuit. This type of communication is called connection-oriented. During this initial handshake, the two TCP layers also agree on the amount of information that’s going to be sent before the recipient’s TCP sends back an acknowledgment. With everything agreed upon in advance, the path is paved for reliable communication to take place.TCP is a full-duplex, connection-oriented, reliable, and accurate protocol, but establishing all these terms and conditions, in addition to error checking, is no small task. TCP is very compli-cated and, not surprisingly, costly in terms of network overhead. And since today’s networks are much more reliable than those of yore, this added reliability is often unnecessary.TCP Segment FormatSince the upper layers just send a data stream to the protocols in the Transport layers, I’ll dem-onstrate how TCP segments a data stream and prepares it for the Internet layer. When the Internet layer receives the data stream, it routes the segments as packets through an internet-work. The segments are handed to the receiving host’s Host-to-Host layer protocol, which rebuilds the data stream to hand to the upper-layer applications or protocols.Figure 2.3 shows the TCP segment format. The figure shows the different fields within the TCP header.FIGURE2.3TCP segment format

Bit 0Bit 15Source port (16)Destination port (16)Window (16)Urgent (16)Code bits (6)Reserved (6)Checksum (16)Headerlength (4)Sequence number (32)Acknowledgment number (32)Options (0 or 32 if any)Data (varies)Bit 16Bit 31

24 bytes

10089.book  Page 75  Monday, July 23, 2007  3:17 PM




76Chapter2 Introduction to TCP/IPThe TCP header is 20 bytes long, or up to 24 bytes with options. You need to understand what each field in the TCP segment is:Source portThe port number of the application on the host sending the data. (Port numbers will be explained a little later in this section.)Destination portThe port number of the application requested on the destination host.Sequence numberA number used by TCP that puts the data back in the correct order or retransmits missing or damaged data, a process called sequencing.Acknowledgment numberThe TCP octet that is expected next.Header lengthThe number of 32-bit words in the TCP header. This indicates where the data begins. The TCP header (even one including options) is an integral number of 32 bits in length.ReservedAlways set to zero.Code bitsControl functions used to set up and terminate a session.WindowThe window size the sender is willing to accept, in octets.ChecksumThe cyclic redundancy check (CRC), because TCP doesn’t trust the lower layers and checks everything. The CRC checks the header and data fields.UrgentA valid field only if the Urgent pointer in the code bits is set. If so, this value indicates the offset from the current sequence number, in octets, where the first segment of non-urgent data begins.OptionsMay be 0 or a multiple of 32 bits, if any. What this means is that no options have to be present (option size of 0). However, if any options are used that do not cause the option field to total a multiple of 32 bits, padding of 0s must be used to make sure the data begins on a 32-bit boundary.DataHanded down to the TCP protocol at the Transport layer, which includes the upper-layer headers.Let’s take a look at a TCP segment copied from a network analyzer:

TCP - Transport Control Protocol Source Port:      5973 Destination Port: 23 Sequence Number:  1456389907 Ack Number:       1242056456 Offset:           5 Reserved:         %000000 Code:             %011000      Ack is valid      Push Request Window:           61320 Checksum:         0x61a6

10089.book  Page 76  Monday, July 23, 2007  3:17 PM




TCP/IP and the DoD Model77 Urgent Pointer:   0 No TCP Options TCP Data Area: vL.5.+.5.+.5.+.5  76 4c 19 35 11 2b 19 35 11 2b 19 35 11  2b 19 35 +. 11 2b 19

Frame Check Sequence: 0x0d00000fDid you notice that everything I talked about earlier is in the segment? As you can see from the number of fields in the header, TCP creates a lot of overhead. Application developers may opt for efficiency over reliability to save overhead, so User Datagram Protocol was also defined at the Transport layer as an alternative.User Datagram Protocol (UDP)If you were to compare User Datagram Protocol (UDP) with TCP, the former is basically the scaled-down economy model that’s sometimes referred to as a thin protocol. Like a thin per-son on a park bench, a thin protocol doesn’t take up a lot of room—or in this case, much band-width on a network.UDP doesn’t offer all the bells and whistles of TCP either, but it does do a fabulous job of transporting information that doesn’t require reliable delivery—and it does so using far fewer network resources. (UDP is covered thoroughly in Request for Comments 768.)

The Requests for Comments (RFCs) form a series of notes, started in 1969, about the Internet (originally the ARPAnet). The notes discuss many aspects of computer communication; they focus on networking protocols, procedures, programs, and concepts but also include meeting notes, opinion, and some-times humor.There are some situations in which it would definitely be wise for developers to opt for UDP rather than TCP. Remember the watchdog SNMP up there at the Process/Application layer? SNMP monitors the network, sending intermittent messages and a fairly steady flow of status updates and alerts, especially when running on a large network. The cost in overhead to estab-lish, maintain, and close a TCP connection for each one of those little messages would reduce what would be an otherwise healthy, efficient network to a dammed-up bog in no time!Another circumstance calling for UDP over TCP is when reliability is already handled at the Process/Application layer. Network File System (NFS) handles its own reliability issues, making the use of TCP both impractical and redundant. But ultimately, it’s up to the application devel-oper to decide whether to use UDP or TCP, not the user who wants to transfer data faster.UDP does not sequence the segments and does not care in which order the segments arrive at the destination. But after that, UDP sends the segments off and forgets about them. It doesn’t follow through, check up on them, or even allow for an acknowledgment of safe arrival—complete abandonment. Because of this, it’s referred to as an unreliable protocol. This does not mean that UDP is ineffective, only that it doesn’t handle issues of reliability.

10089.book  Page 77  Monday, July 23, 2007  3:17 PM




78Chapter2 Introduction to TCP/IPFurther, UDP doesn’t create a virtual circuit, nor does it contact the destination before delivering information to it. Because of this, it’s also considered a connectionless protocol. Since UDP assumes that the application will use its own reliability method, it doesn’t use any. This gives an application developer a choice when running the Internet Protocol stack: TCP for reliability or UDP for faster transfers.So if you’re using Voice over IP (VoIP), for example, you really don’t want to use UDP, because if the segments arrive out of order (very common in IP networks), they’ll just be passed up to the next OSI (DoD) layer in whatever order they’re received, resulting in some seriously garbled data. On the other hand, TCP sequences the segments so they get put back together in exactly the right order—something UDP just can’t do.UDP Segment FormatFigure 2.4 clearly illustrates UDP’s markedly low overhead as compared to TCP’s hungry usage. Look at the figure carefully—can you see that UDP doesn’t use windowing or provide for acknowledgments in the UDP header?FIGURE2.4UDP segmentIt’s important for you to understand what each field in the UDP segment is:Source portPort number of the application on the host sending the dataDestination portPort number of the application requested on the destination hostLengthLength of UDP header and UDP dataChecksumChecksum of both the UDP header and UDP data fieldsDataUpper-layer dataUDP, like TCP, doesn’t trust the lower layers and runs its own CRC. Remember that the Frame Check Sequence (FCS) is the field that houses the CRC, which is why you can see the FCS information.The following shows a UDP segment caught on a network analyzer:

UDP - User Datagram Protocol Source Port:      1085 Destination Port: 5136 Length:           41 Checksum:         0x7a3c

Bit 0Bit 15Source port (16)Destination port (16)Length (16)Checksum (16)Data (if any)Bit 16Bit 318 bytes

10089.book  Page 78  Monday, July 23, 2007  3:17 PM




TCP/IP and the DoD Model79 UDP Data Area: ..Z......00 01 5a 96 00 01 00 00 00 00 00 11 0000 00...C..2._C._C  2e 03 00 43 02 1e 32 0a 00 0a 00 80 43 00 80

Frame Check Sequence: 0x00000000Notice that low overhead! Try to find the sequence number, ack number, and window size in the UDP segment. You can’t because they just aren’t there!Key Concepts of Host-to-Host ProtocolsSince you’ve seen both a connection-oriented (TCP) and connectionless (UDP) protocol in action, it would be good to summarize the two here. Table 2.1 highlights some of the key concepts that you should keep in mind regarding these two protocols. You should memorize this table.A telephone analogy could really help you understand how TCP works. Most of us know that before you speak to someone on a phone, you must first establish a connection with that other person—wherever they are. This is like a virtual circuit with the TCP protocol. If you were giving someone important information during your conversation, you might say, “You know?” or ask, “Did you get that?” Saying something like this is a lot like a TCP acknowl-edgment—it’s designed to get you verification. From time to time (especially on cell phones), people also ask, “Are you still there?” They end their conversations with a “Goodbye” of some kind, putting closure on the phone call. TCP also performs these types of functions.Alternately, using UDP is like sending a postcard. To do that, you don’t need to contact the other party first. You simply write your message, address the postcard, and mail it. This is analogous to UDP’s connectionless orientation. Since the message on the postcard is probably not a matter of life or death, you don’t need an acknowledgment of its receipt. Similarly, UDP does not involve acknowledgments.Let’s take a look at another figure, one that includes TCP, UDP, and the applications asso-ciated to each protocol, Figure 2.5.TABLE2.1Key Features of TCP and UDPTCPUDPSequencedUnsequencedReliableUnreliableConnection-orientedConnectionlessVirtual circuitLow overheadAcknowledgments No acknowledgmentWindowing flow controlNo windowing or flow control

10089.book  Page 79  Monday, July 23, 2007  3:17 PM




80Chapter2 Introduction to TCP/IPPort NumbersTCP and UDP must use port numbers to communicate with the upper layers because they’re what keep track of different conversations crossing the network simultaneously. Originating-source port numbers are dynamically assigned by the source host and will equal some number starting at 1024. 1023 and below are defined in RFC 3232 (or just see www.iana.org), which discusses what are called well-known port numbers.Virtual circuits that don’t use an application with a well-known port number are assigned port numbers randomly from a specific range instead. These port numbers identify the source and destination application or process in the TCP segment.Figure 2.5 illustrates how both TCP and UDP use port numbers.FIGURE2.5Port numbers for TCP and UDPThe different port numbers that can be used are explained next: Numbers below 1024 are considered well-known port numbers and are defined in RFC 3232. Numbers 1024 and above are used by the upper layers to set up sessions with other hosts and by TCP to use as source and destination addresses in the TCP segment.In the following sections, we’ll take a look at an analyzer output showing a TCP session.TCP Session: Source PortThe following listing shows a TCP session captured with OmniPeek analyzer software:

TCP - Transport Control Protocol Source Port:      5973 Destination Port: 23 Sequence Number:  1456389907 Ack Number:       1242056456 Offset:           5 Reserved:         %000000 Code:             %011000      Ack is valid      Push Request Window:           61320

FTPTelnetDoomTFTPPOP3DNS

TCPTransportlayerApplicationlayerPortnumbersUDPNews

119

110

69

53

666

23

21

10089.book  Page 80  Monday, July 23, 2007  3:17 PM




TCP/IP and the DoD Model81 Checksum:         0x61a6 Urgent Pointer:   0 No TCP Options TCP Data Area: vL.5.+.5.+.5.+.5  76 4c 19 35 11 2b 19 35 11 2b 19 35 11  2b 19 35 +. 11 2b 19

Frame Check Sequence: 0x0d00000fNotice that the source host makes up the source port, which in this case is 5973. The des-tination port is 23, which is used to tell the receiving host the purpose of the intended connec-tion (Telnet).By looking at this session, you can see that the source host makes up the source port by using numbers from 1024 to 65535. But why does the source make up a port number? To dif-ferentiate between sessions with different hosts, my friend. How would a server know where information is coming from if it didn’t have a different number from a sending host? TCP and the upper layers don’t use hardware and logical addresses to understand the sending host’s address as the Data Link and Network layer protocols do. Instead, they use port numbers. And it’s easy to imagine the receiving host getting thoroughly confused if all the hosts used the same source port number to get to FTP!TCP Session: Destination PortYou’ll sometimes look at an analyzer and see that only the source port is above 1024 and the destination port is a well-known port, as shown in the following trace:

TCP - Transport Control Protocol Source Port:      1144 Destination Port: 80 World Wide Web HTTP Sequence Number:  9356570 Ack Number:       0 Offset:           7 Reserved:         %000000 Code:             %000010      Synch Sequence Window:           8192 Checksum:         0x57E7 Urgent Pointer:   0 TCP Options:  Option Type: 2 Maximum Segment Size    Length:    4    MSS:       536  Option Type: 1 No Operation  Option Type: 1 No Operation  Option Type: 4

10089.book  Page 81  Monday, July 23, 2007  3:17 PM




82Chapter2 Introduction to TCP/IP    Length:    2    Opt Value:  No More HTTP Data

Frame Check Sequence: 0x43697363And sure enough, the source port is over 1024, but the destination port is 80, or HTTP service. The server, or receiving host, will change the destination port if it needs to.In the preceding trace, a “syn” packet is sent to the destination device. The syn sequence is what’s telling the remote destination device that it wants to create a session.TCP Session: Syn Packet AcknowledgmentThe next trace shows an acknowledgment to the syn packet:

TCP - Transport Control Protocol Source Port:      80 World Wide Web HTTP Destination Port: 1144 Sequence Number:  2873580788 Ack Number:       9356571 Offset:           6 Reserved:         %000000 Code:             %010010      Ack is valid      Synch Sequence Window:           8576 Checksum:         0x5F85 Urgent Pointer:   0 TCP Options:  Option Type: 2 Maximum Segment Size    Length:    4    MSS:       1460  No More HTTP Data

Frame Check Sequence: 0x6E203132Notice the Ack is valid, which means that the source port was accepted and the device agreed to create a virtual circuit with the originating host.And here again, you can see that the response from the server shows the source is 80 and the destination is the 1144 sent from the originating host—all’s well.Table 2.2 gives you a list of the typical applications used in the TCP/IP suite, their well-known port numbers, and the Transport layer protocols used by each application or process. It’s important that you study and memorize this table.Notice that DNS uses both TCP and UDP. Whether it opts for one or the other depends on what it’s trying to do. Even though it’s not the only application that can use both protocols, it’s certainly one that you should remember in your studies.

10089.book  Page 82  Monday, July 23, 2007  3:17 PM




TCP/IP and the DoD Model83

What makes TCP reliable is sequencing, acknowledgments, and flow control (windowing). UDP does not have reliability.The Internet Layer ProtocolsIn the DoD model, there are two main reasons for the Internet layer’s existence: routing and providing a single network interface to the upper layers.None of the other upper- or lower-layer protocols have any functions relating to routing—that complex and important task belongs entirely to the Internet layer. The Internet layer’s sec-ond duty is to provide a single network interface to the upper-layer protocols. Without this layer, application programmers would need to write “hooks” into every one of their applica-tions for each different Network Access protocol. This would not only be a pain in the neck, but it would lead to different versions of each application—one for Ethernet, another one for Token Ring, and so on. To prevent this, IP provides one single network interface for the upper-layer protocols. That accomplished, it’s then the job of IP and the various Network Access protocols to get along and work together.All network roads don’t lead to Rome—they lead to IP. And all the other protocols at this layer, as well as all those at the upper layers, use it. Never forget that. All paths through the DoD model go through IP. The following sections describe the protocols at the Internet layer: Internet Protocol (IP) Internet Control Message Protocol (ICMP) Address Resolution Protocol (ARP) Reverse Address Resolution Protocol (RARP) Proxy ARPTABLE2.2Key Protocols That Use TCP and UDPTCPUDPTelnet 23SNMP 161SMTP 25TFTP 69HTTP 80DNS 53FTP 21 DNS 53 HTTPS 443 

10089.book  Page 83  Monday, July 23, 2007  3:17 PM




84Chapter2 Introduction to TCP/IPInternet Protocol (IP)Internet Protocol (IP) essentially is the Internet layer. The other protocols found here merely exist to support it. IP holds the big picture and could be said to “see all,” in that it’s aware of all the inter-connected networks. It can do this because all the machines on the network have a software, or log-ical, address called an IP address, which I’ll cover more thoroughly later in this chapter.IP looks at each packet’s address. Then, using a routing table, it decides where a packet is to be sent next, choosing the best path. The protocols of the Network Access layer at the bot-tom of the DoD model don’t possess IP’s enlightened scope of the entire network; they deal only with physical links (local networks).Identifying devices on networks requires answering these two questions: Which network is it on? And what is its ID on that network? The first answer is the software address, or logical address (the correct street). The second answer is the hardware address (the correct mailbox). All hosts on a network have a logical ID called an IP address. This is the software, or logical, address and contains valuable encoded information, greatly simplifying the complex task of routing. (IP is discussed in RFC 791.)IP receives segments from the Host-to-Host layer and fragments them into datagrams (packets) if necessary. IP then reassembles datagrams back into segments on the receiving side. Each datagram is assigned the IP address of the sender and of the recipient. Each router (layer 3 device) that receives a datagram makes routing decisions based on the packet’s des-tination IP address.Figure 2.6 shows an IP header. This will give you an idea of what the IP protocol has to go through every time user data is sent from the upper layers and is to be sent to a remote network.FIGURE2.6IP header

Bit 0Bit 15Total length (16)Header checksum (16)Time to Live (8)Protocol (8)Version(4)Flags(3)Headerlength (4)Priority andType of Service (8)Identification (16)Fragment offset (13)Options (0 or 32 if any)Destination IP address (32)Source IP address (32)Data (varies if any)Bit 16Bit 31

20 bytes

10089.book  Page 84  Monday, July 23, 2007  3:17 PM




TCP/IP and the DoD Model85The following fields make up the IP header:VersionIP version number.Header lengthHeader length (HLEN) in 32-bit words.Priority and Type of ServiceType of Service tells how the datagram should be handled. The first 3 bits are the priority bits.Total lengthLength of the packet including header and data.IdentificationUnique IP-packet value.FlagsSpecifies whether fragmentation should occur.Fragment offsetProvides fragmentation and reassembly if the packet is too large to put in a frame. It also allows different maximum transmission units (MTUs) on the Internet.Time to LiveThe time to live is set into a packet when it is originally generated. If it doesn’t get to where it wants to go before the TTL expires, boom—it’s gone. This stops IP packets from continuously circling the network looking for a home.ProtocolPort of upper-layer protocol (TCP is port 6 or UDP is port 17 [hex]). Also supports Network layer protocols, like ARP and ICMP. Can be called Type field in some analyzers. We’ll talk about this field in more detail in a minute.Header checksumCyclic redundancy check (CRC) on header only.Source IP address32-bit IP address of sending station.Destination IP address32-bit IP address of the station this packet is destined for.OptionsUsed for network testing, debugging, security, and more.DataAfter the IP option field will be the upper-layer data.Here’s a snapshot of an IP packet caught on a network analyzer (notice that all the header information discussed previously appears here):

IP Header - Internet Protocol Datagram Version:             4 Header Length:       5 Precedence:          0 Type of Service:     %000 Unused:              %00 Total Length:        187 Identifier:          22486 Fragmentation Flags: %010 Do Not Fragment Fragment Offset:     0 Time To Live:        60 IP Type:             0x06 TCP Header Checksum:     0xd031 Source IP Address:   10.7.1.30

10089.book  Page 85  Monday, July 23, 2007  3:17 PM




86Chapter2 Introduction to TCP/IP Dest. IP Address:    10.7.1.10

 No Internet Datagram OptionsThe Type field—it’s typically a Protocol field, but this analyzer sees it as an IP Type field—is important. If the header didn’t carry the protocol information for the next layer, IP wouldn’t know what to do with the data carried in the packet. The preceding example tells IP to hand the segment to TCP.Figure 2.7 demonstrates how the Network layer sees the protocols at the Transport layer when it needs to hand a packet to the upper-layer protocols.FIGURE2.7The Protocol field in an IP headerIn this example, the Protocol field tells IP to send the data to either TCP port 6 or UDP port 17 (both hex addresses). But it will only be UDP or TCP if the data is part of a data stream headed for an upper-layer service or application. It could just as easily be destined for Internet Control Message Protocol (ICMP), Address Resolution Protocol (ARP), or some other type of Network layer protocol.Table 2.3 is a list of some other popular protocols that can be specified in the Protocol field.TABLE2.3Possible Protocols Found in the Protocol Field of an IP HeaderProtocolProtocol NumberICMP1IP in IP (tunneling)4IGRP9EIGRP88OSPF89IPv641GRE47Layer 2 tunnel (L2TP) 115

TCPUDPProtocolnumbers

IPTransport layerInternet layer

17

6

10089.book  Page 86  Monday, July 23, 2007  3:17 PM




TCP/IP and the DoD Model87

You can find a complete list of protocol field numbers at www.iana.org/assignments/protocol-numbers.Internet Control Message Protocol (ICMP)Internet Control Message Protocol (ICMP) works at the Network layer and is used by IP for many different services. ICMP is a management protocol and messaging service provider for IP. Its messages are carried as IP datagrams. RFC 1256 is an annex to ICMP, which affords host’s extended capability in discovering routes to gateways.ICMP packets have the following characteristics: They can provide hosts with information about network problems. They are encapsulated within IP datagrams.The following are some common events and messages that ICMP relates to:Destination UnreachableIf a router can’t send an IP datagram any further, it uses ICMP to send a message back to the sender, advising it of the situation. For example, take a look at Figure 2.8, which shows that interface E0 of the Lab_B router is down.FIGURE2.8ICMP error message is sent to the sending host from the remote router.When Host A sends a packet destined for Host B, the Lab_B router will send an ICMP desti-nation unreachable message back to the sending device (Host A in this example).Buffer FullIf a router’s memory buffer for receiving incoming datagrams is full, it will use ICMP to send out this message until the congestion abates.HopsEach IP datagram is allotted a certain number of routers, called hops, to pass through. If it reaches its limit of hops before arriving at its destination, the last router to receive that datagram 

EO on Lab B is down. Host A is trying to communicate to Host B. What happens?Lab_A Lab_B E0 E0 Host A Host B icmp 

10089.book  Page 87  Monday, July 23, 2007  3:17 PM




88Chapter2 Introduction to TCP/IPdeletes it. The executioner router then uses ICMP to send an obituary message, informing the send-ing machine of the demise of its datagram.PingPacket Internet Groper (Ping) uses ICMP echo request and reply messages to check the physical and logical connectivity of machines on an internetwork.TracerouteUsing ICMP time-outs, Traceroute is used to discover the path a packet takes as it traverses an internetwork.

Both Ping and Traceroute (also just called Trace; Microsoft Windows uses tracert) allow you to verify address configurations in your internetwork.The following data is from a network analyzer catching an ICMP echo request:

Flags:         0x00 Status:        0x00 Packet Length: 78 Timestamp:     14:04:25.967000 12/20/03Ethernet Header Destination: 00:a0:24:6e:0f:a8 Source:      00:80:c7:a8:f0:3d Ether-Type:  08-00 IPIP Header - Internet Protocol Datagram Version:             4 Header Length:       5 Precedence:          0 Type of Service:     %000 Unused:              %00 Total Length:        60 Identifier:          56325 Fragmentation Flags: %000 Fragment Offset:     0 Time To Live:        32 IP Type:             0x01 ICMP Header Checksum:     0x2df0 Source IP Address:   100.100.100.2 Dest. IP Address:    100.100.100.1 No Internet Datagram OptionsICMP - Internet Control Messages Protocol ICMP Type:       8 Echo Request Code:            0 Checksum:        0x395c Identifier:      0x0300 Sequence Number: 4352

10089.book  Page 88  Monday, July 23, 2007  3:17 PM




TCP/IP and the DoD Model89 ICMP Data Area: abcdefghijklmnop  61 62 63 64 65 66 67 68 69 6a 6b 6c 6d qrstuvwabcdefghi  71 72 73 74 75 76 77 61 62 63 64 65 66

Frame Check Sequence: 0x00000000Notice anything unusual? Did you catch the fact that even though ICMP works at the Inter-net (Network) layer, it still uses IP to do the Ping request? The Type field in the IP header is 0x01, which specifies that the data we’re carrying is owned by the ICMP protocol. Remember, just as all roads lead to Rome, all segments or data must go through IP!

The Ping program uses the alphabet in the data portion of the packet as just a payload, 100 bytes by default, unless, of course, you are pinging from a Windows device, which thinks the alphabet stops at the letter W and doesn’t’ include X, Y, or Z and then starts at A again. Go figure!If you remember reading about the Data Link layer and the different frame types in Chapter 1, you should be able to look at the preceding trace and tell what type of Ethernet frame this is. The only fields are destination hardware address, source hardware address, and Ether-Type. The only frame that uses an Ether-Type field exclusively is an Ethernet_II frame.But before we get into the ARP protocol, let’s take another look at ICMP in action. Figure 2.9 shows an internetwork (it has a router, so it’s an internetwork, right?).FIGURE2.9ICMP in action

Server110.1.2.2/2410.1.3.2/2410.1.4.2/2410.1.5.2/2410.1.5.23/2410.1.5.4/2410.1.5.5/24

10089.book  Page 89  Monday, July 23, 2007  3:17 PM




90Chapter2 Introduction to TCP/IPServer1 (10.1.2.2) telnets to 10.1.1.5 from a DOS prompt. What do you think Server1 will receive as a response? Since Server1 will send the Telnet data to the default gateway, which is the router, the router will drop the packet because there isn’t a network 10.1.1.0 in the routing table. Because of this, Server1 will receive a destination unreachable back from ICMP.Address Resolution Protocol (ARP)Address Resolution Protocol (ARP) finds the hardware address of a host from a known IP address. Here’s how it works: When IP has a datagram to send, it must inform a Network Access protocol, such as Ethernet or Token Ring, of the destination’s hardware address on the local network. (It has already been informed by upper-layer protocols of the destination’s IP address.) If IP doesn’t find the destination host’s hardware address in the ARP cache, it uses ARP to find this information.As IP’s detective, ARP interrogates the local network by sending out a broadcast asking the machine with the specified IP address to reply with its hardware address. So basically, ARP translates the software (IP) address into a hardware address—for example, the destination machine’s Ethernet board address—and from it, deduces its whereabouts on the LAN by broadcasting for this address. Figure 2.10 shows how an ARP looks to a local network.

ARP resolves IP addresses to Ethernet (MAC) addresses.FIGURE2.10Local ARP broadcastThe following trace shows an ARP broadcast—notice that the destination hardware address is unknown and is all Fs in hex (all 1s in binary)—and is a hardware address broadcast:

 Flags:         0x00 Status:        0x00

I need the Ethernetaddress of 10.1.1.2

I heard that broadcast.The message is for me.Here is my Ethernet address.

10.1.1.110.1.1.2

IP: 10.1.1.2 = ???IP: 10.1.1.2Ethernet: 4523.7985.7734

10089.book  Page 90  Monday, July 23, 2007  3:17 PM




TCP/IP and the DoD Model91 Packet Length: 64 Timestamp:     09:17:29.574000 12/06/03Ethernet Header Destination:   FF:FF:FF:FF:FF:FF Ethernet Broadcast Source:        00:A0:24:48:60:A5 Protocol Type: 0x0806 IP ARPARP - Address Resolution Protocol Hardware:                1 Ethernet (10Mb) Protocol:                0x0800 IP Hardware Address Length: 6 Protocol Address Length: 4 Operation:               1 ARP Request Sender Hardware Address: 00:A0:24:48:60:A5 Sender Internet Address: 172.16.10.3 Target Hardware Address: 00:00:00:00:00:00 (ignored) Target Internet Address: 172.16.10.10Extra bytes (Padding): ................ 0A 0A 0A 0A 0A 0A 0A 0A 0A 0A 0A 0A 0A  0A 0A 0A 0A 0A

Frame Check Sequence: 0x00000000Reverse Address Resolution Protocol (RARP)When an IP machine happens to be a diskless machine, it has no way of initially knowing its IP address. But it does know its MAC address. Reverse Address Resolution Protocol (RARP) discovers the identity of the IP address for diskless machines by sending out a packet that includes its MAC address and a request for the IP address assigned to that MAC address. A designated machine, called a RARP server, responds with the answer and the identity crisis is over. RARP uses the information it does know about the machine’s MAC address to learn its IP address and complete the machine’s ID portrait.Figure 2.11 shows a diskless workstation asking for its IP address with a RARP broadcast.

RARP resolves Ethernet (MAC) addresses to IP addresses.Proxy Address Resolution Protocol (Proxy ARP)On a network, your hosts can’t have more then one default gateway configured. Think about this…What if the default gateway (router) happens to go down? The host won’t just start sending to another router automatically—you’ve got to reconfigure that host. But Proxy ARP can actually help machines on a subnet reach remote subnets without configuring routing or even a default gateway.

10089.book  Page 91  Monday, July 23, 2007  3:17 PM




92Chapter2 Introduction to TCP/IPFIGURE2.11RARP broadcast exampleOne advantage of using Proxy ARP is that it can be added to a single router on a network without disturbing the routing tables of all the other routers that live there too. But there’s a serious downside to using Proxy ARP. Using Proxy ARP will definitely increase the amount of traffic on your network segment, and hosts will have a larger ARP table than usual in order to handle all the IP-to-MAC-address mappings. And Proxy ARP is configured on all Cisco routers by default—you should disable it if you don’t think you’re going to use it.One last thought on Proxy ARP: Proxy ARP isn’t really a separate protocol. It is a service run by routers on behalf of other devices (usually PC’s) that are separated from their query to another device by a router, although they think they share the subnet with the remote device.

If you can afford it, use Cisco’s Hot Standby Router Protocol (HSRP) instead. It means you have to buy two or more of your Cisco device(s), but it is well worth it. Check out the Cisco website for more information on HSRP.IP AddressingOne of the most important topics in any discussion of TCP/IP is IP addressing. An IP address is a numeric identifier assigned to each machine on an IP network. It designates the specific location of a device on the network.An IP address is a software address, not a hardware address—the latter is hard-coded on a network interface card (NIC) and used for finding hosts on a local network. IP addressing was designed to allow hosts on one network to communicate with a host on a different net-work regardless of the type of LANs the hosts are participating in.

What’s my IPaddress?

I heard that broadcast.Your IP addressis 192.168.10.3

Ethernet: 4523.7985.7734 IP = ????Ethernet: 4523.7985.7734IP: 192.168.10.3

10089.book  Page 92  Monday, July 23, 2007  3:17 PM




IP Addressing93Before we get into the more complicated aspects of IP addressing, you need to understand some of the basics. First I’m going to explain some of the fundamentals of IP addressing and its termi-nology. Then you’ll learn about the hierarchical IP addressing scheme and private IP addresses.IP TerminologyThroughout this chapter you’ll learn several important terms vital to your understanding of the Internet Protocol. Here are a few to get you started:BitA bit is one digit, either a 1 or a 0.ByteA byte is 7 or 8 bits, depending on whether parity is used. For the rest of this chapter, always assume a byte is 8 bits.OctetAn octet, made up of 8 bits, is just an ordinary 8-bit binary number. In this chapter, the terms byte and octet are completely interchangeable.Network addressThis is the designation used in routing to send packets to a remote net-work—for example, 10.0.0.0, 172.16.0.0, and 192.168.10.0.Broadcast addressThe address used by applications and hosts to send information to all nodes on a network is called the broadcast address. Examples include 255.255.255.255, which is all networks, all nodes; 172.16.255.255, which is all subnets and hosts on net-work 172.16.0.0; and 10.255.255.255, which broadcasts to all subnets and hosts on network 10.0.0.0.The Hierarchical IP Addressing SchemeAn IP address consists of 32 bits of information. These bits are divided into four sections, referred to as octets or bytes, each containing 1 byte (8 bits). You can depict an IP address using one of three methods: Dotted-decimal, as in 172.16.30.56 Binary, as in 10101100.00010000.00011110.00111000 Hexadecimal, as in AC.10.1E.38All these examples truly represent the same IP address. Hexadecimal isn’t used as often as dotted-decimal or binary when IP addressing is discussed, but you still might find an IP address stored in hexadecimal in some programs. The Windows Registry is a good example of a pro-gram that stores a machine’s IP address in hex.The 32-bit IP address is a structured or hierarchical address, as opposed to a flat or nonhi-erarchical address. Although either type of addressing scheme could have been used, hierar-chical addressing was chosen for a good reason. The advantage of this scheme is that it can handle a large number of addresses, namely 4.3 billion (a 32-bit address space with two pos-sible values for each position—either 0 or 1—gives you 232, or 4,294,967,296). The disadvan-tage of the flat addressing scheme, and the reason it’s not used for IP addressing, relates to routing. If every address were unique, all routers on the Internet would need to store the 

10089.book  Page 93  Monday, July 23, 2007  3:17 PM




94Chapter2 Introduction to TCP/IPaddress of each and every machine on the Internet. This would make efficient routing impos-sible, even if only a fraction of the possible addresses were used.The solution to this problem is to use a two- or three-level hierarchical addressing scheme that is structured by network and host or by network, subnet, and host.This two- or three-level scheme is comparable to a telephone number. The first section, the area code, designates a very large area. The second section, the prefix, narrows the scope to a local calling area. The final segment, the customer number, zooms in on the specific connec-tion. IP addresses use the same type of layered structure. Rather than all 32 bits being treated as a unique identifier, as in flat addressing, a part of the address is designated as the network address and the other part is designated as either the subnet and host or just the node address.In the following sections, I’m going to discuss IP network addressing and the different classes of address we can use to address our networks.Network AddressingThe network address (which can also be called the network number) uniquely identifies each network. Every machine on the same network shares that network address as part of its IP address. In the IP address 172.16.30.56, for example, 172.16 is the network address.The node address is assigned to, and uniquely identifies, each machine on a network. This part of the address must be unique because it identifies a particular machine—an individual—as opposed to a network, which is a group. This number can also be referred to as a host address. In the sample IP address 172.16.30.56, the 30.56 is the node address.The designers of the Internet decided to create classes of networks based on network size. For the small number of networks possessing a very large number of nodes, they created the rank Class A network. At the other extreme is the Class C network, which is reserved for the numer-ous networks with a small number of nodes. The class distinction for networks between very large and very small is predictably called the Class B network.Subdividing an IP address into a network and node address is determined by the class des-ignation of one’s network. Figure 2.12 summarizes the three classes of networks—a subject I’ll explain in much greater detail throughout this chapter.FIGURE2.12Summary of the three classes of networks

Network

Host

Host

Host

Network

Network

Host

Host

Network

Network

Network

HostMulticastResearchClass A:Class B:Class C:Class D:Class E:8 bits8 bits8 bits8 bits

10089.book  Page 94  Monday, July 23, 2007  3:17 PM




IP Addressing95To ensure efficient routing, Internet designers defined a mandate for the leading-bits sec-tion of the address for each different network class. For example, since a router knows that a Class A network address always starts with a 0, the router might be able to speed a packet on its way after reading only the first bit of its address. This is where the address schemes define the difference between a Class A, a Class B, and a Class C address. In the next sec-tions, I’ll discuss the differences between these three classes, followed by a discussion of the Class D and Class E addresses (Classes A, B, and C are the only ranges that are used to address hosts in our networks).Network Address Range: Class AThe designers of the IP address scheme said that the first bit of the first byte in a Class A net-work address must always be off, or 0. This means a Class A address must be between 0 and 127 in the first byte, inclusive.Consider the following network address:

0xxxxxxxIf we turn the other 7 bits all off and then turn them all on, we’ll find the Class A range of net-work addresses:

00000000 = 0

01111111 = 127So, a Class A network is defined in the first octet between 0 and 127, and it can’t be less or more. (Yes, I know 0 and 127 are not valid in a Class A network. I’ll talk about reserved addresses in a minute.)Network Address Range: Class BIn a Class B network, the RFCs state that the first bit of the first byte must always be turned on but the second bit must always be turned off. If you turn the other 6 bits all off and then all on, you will find the range for a Class B network:

10000000 = 128

10111111 = 191As you can see, a Class B network is defined when the first byte is configured from 128 to 191.Network Address Range: Class CFor Class C networks, the RFCs define the first 2 bits of the first octet as always turned on, but the third bit can never be on. Following the same process as the previous classes, convert from binary to decimal to find the range. Here’s the range for a Class C network:

11000000 = 192

11011111 = 223So, if you see an IP address that starts at 192 and goes to 223, you’ll know it is a Class C IP address.

10089.book  Page 95  Monday, July 23, 2007  3:17 PM




96Chapter2 Introduction to TCP/IPNetwork Address Ranges: Classes D and EThe addresses between 224 to 255 are reserved for Class D and E networks. Class D (224–239) is used for multicast addresses and Class E (240–255) for scientific purposes, but I’m not going into these types of addresses in this book (and you don’t need to know them).Network Addresses: Special PurposeSome IP addresses are reserved for special purposes, so network administrators can’t ever assign these addresses to nodes. Table 2.4 lists the members of this exclusive little club and the reasons why they’re included in it.Class A AddressesIn a Class A network address, the first byte is assigned to the network address and the three remaining bytes are used for the node addresses. The Class A format is as follows:

network.node.node.nodeFor example, in the IP address 49.22.102.70, the 49 is the network address and 22.102.70 is the node address. Every machine on this particular network would have the distinctive net-work address of 49.TABLE2.4Reserved IP AddressesAddressFunctionNetwork address of all 0sInterpreted to mean “this network or segment.”Network address of all 1sInterpreted to mean “all networks.”Network 127.0.0.1Reserved for loopback tests. Designates the local node and allows that node to send a test packet to itself without generating network traffic.Node address of all 0sInterpreted to mean “network address” or any host on specified network.Node address of all 1sInterpreted to mean “all nodes” on the specified network; for example, 128.2.255.255 means “all nodes” on network 128.2 (Class B address).Entire IP address set to all 0sUsed by Cisco routers to designate the default route. Could also mean “any network.” Entire IP address set to all 1s (same as 255.255.255.255)Broadcast to all nodes on the current network; sometimes called an “all 1s broadcast” or limited broadcast. 

10089.book  Page 96  Monday, July 23, 2007  3:17 PM




IP Addressing97Class A network addresses are 1 byte long, with the first bit of that byte reserved and the 7 remaining bits available for manipulation (addressing). As a result, the maximum number of Class A networks that can be created is 128. Why? Because each of the 7 bit positions can be either a 0 or a 1, thus 27, or 128.To complicate matters further, the network address of all 0s (0000 0000) is reserved to des-ignate the default route (see Table 2.4 in the previous section). Additionally, the address 127, which is reserved for diagnostics, can’t be used either, which means that you can really only use the numbers 1 to 126 to designate Class A network addresses. This means the actual num-ber of usable Class A network addresses is 128 minus 2, or 126.

The IP address 127.0.0.1 is used to test the IP stack on an individual node and cannot be used as a valid host address.Each Class A address has 3 bytes (24-bit positions) for the node address of a machine. This means there are 224—or 16,777,216—unique combinations and, therefore, precisely that many possible unique node addresses for each Class A network. Because node addresses with the two patterns of all 0s and all 1s are reserved, the actual maximum usable number of nodes for a Class A network is 224 minus 2, which equals 16,777,214. Either way, that’s a huge amount of hosts on a network segment!Class A Valid Host IDsHere’s an example of how to figure out the valid host IDs in a Class A network address: All host bits off is the network address: 10.0.0.0. All host bits on is the broadcast address: 10.255.255.255.The valid hosts are the numbers in between the network address and the broadcast address: 10.0.0.1 through 10.255.255.254. Notice that 0s and 255s can be valid host IDs. All you need to remember when trying to find valid host addresses is that the host bits can’t all be turned off or all be on at the same time.Class B AddressesIn a Class B network address, the first 2 bytes are assigned to the network address and the remaining 2 bytes are used for node addresses. The format is as follows:

network.network.node.nodeFor example, in the IP address 172.16.30.56, the network address is 172.16 and the node address is 30.56.With a network address being 2 bytes (8 bits each), there would be 216 unique combinations. But the Internet designers decided that all Class B network addresses should start with the binary digit 1, then 0. This leaves 14 bit positions to manipulate, therefore 16,384 (that is, 214) unique Class B network addresses.A Class B address uses 2 bytes for node addresses. This is 216 minus the two reserved pat-terns (all 0s and all 1s), for a total of 65,534 possible node addresses for each Class B network.

10089.book  Page 97  Monday, July 23, 2007  3:17 PM




98Chapter2 Introduction to TCP/IPClass B Valid Host IDsHere’s an example of how to find the valid hosts in a Class B network: All host bits turned off is the network address: 172.16.0.0. All host bits turned on is the broadcast address: 172.16.255.255.The valid hosts would be the numbers in between the network address and the broadcast address: 172.16.0.1 through 172.16.255.254.Class C AddressesThe first 3 bytes of a Class C network address are dedicated to the network portion of the address, with only 1 measly byte remaining for the node address. Here’s the format:

network.network.network.nodeUsing the example IP address 192.168.100.102, the network address is 192.168.100 and the node address is 102.In a Class C network address, the first three bit positions are always the binary 110. The calculation is as follows: 3 bytes, or 24 bits, minus 3 reserved positions leaves 21 positions. Hence, there are 221, or 2,097,152, possible Class C networks.Each unique Class C network has 1 byte to use for node addresses. This leads to 28 or 256, minus the two reserved patterns of all 0s and all 1s, for a total of 254 node addresses for each Class C network.Class C Valid Host IDsHere’s an example of how to find a valid host ID in a Class C network: All host bits turned off is the network ID: 192.168.100.0. All host bits turned on is the broadcast address: 192.168.100.255.The valid hosts would be the numbers in between the network address and the broadcast address: 192.168.100.1 through 192.168.100.254.Private IP AddressesThe people who created the IP addressing scheme also created what we call private IP addresses. These addresses can be used on a private network, but they’re not routable through the Internet. This is designed for the purpose of creating a measure of well-needed security, but it also con-veniently saves valuable IP address space.If every host on every network had to have real routable IP addresses, we would have run out of IP addresses to hand out years ago. But by using private IP addresses, ISPs, corpora-tions, and home users only need a relatively tiny group of bona fide IP addresses to connect their networks to the Internet. This is economical because they can use private IP addresses on their inside networks and get along just fine.To accomplish this task, the ISP and the corporation—the end user, no matter who they are—need to use something called Network Address Translation (NAT), which basically takes 

10089.book  Page 98  Monday, July 23, 2007  3:17 PM




 IP Addressing 99 a private IP address and converts it for use on the Internet. (NAT is covered in Chapter 11, “Network Address Translation.”) Many people can use the same real IP address to transmit out onto the Internet. Doing things this way saves megatons of address space—good for us all!The reserved private addresses are listed in Table 2.5.

 You must know your private address space!

 So, What Private IP Address Should I Use? That’s a really great question: Should you use Class A, Class B, or even Class C private address-ing when setting up your network? Let’s take Acme Corporation in SF as an example. This com-pany is moving into a new building and needs a whole new network (what a treat this is!). It has 14 departments, with about 70 users in each. You could probably squeeze one or two Class C addresses to use, or maybe you could use a Class B, or even a Class A just for fun.The rule of thumb in the consulting world is, when you’re setting up a corporate network—regardless of how small it is—you should use a Class A network address because it gives you the most flexibility and growth options. For example, if you used the 10.0.0.0 network address with a /24 mask, then you’d have 65,536 networks, each with 254 hosts. Lots of room for growth with that network!But if you’re setting up a home network, you’d opt for a Class C address because it is the easiest for people to understand and configure. Using the default Class C mask gives you one network with 254 hosts—plenty for a home network.With the Acme Corporation, a nice 10.1. x .0 with a /24 mask (the  x  is the subnet for each depart-ment) makes this easy to design, install, and troubleshoot. TABLE2.5 Reserved IP Address Space Address ClassReserved Address Space Class A10.0.0.0 through 10.255.255.255Class B172.16.0.0 through 172.31.255.255Class C192.168.0.0 through 192.168.255.255

 

10089c02.fm  Page 99  Wednesday, February 27, 2008  3:04 PM




100Chapter2 Introduction to TCP/IPBroadcast AddressesMost people use the term broadcast as a generic term, and most of the time, we understand what they mean. But not always. For example, you might say, “The host broadcasted through a router to a DHCP server,” but, well, it’s pretty unlikely that this would ever really happen. What you probably mean—using the correct technical jargon—is, “The host broadcasted for an IP address; a router then forwarded this as a unicast packet to the DHCP server.” Oh, and remember that with IPv4, broadcasts are pretty important, but with IPv6, there aren’t any broadcasts sent at all—now there’s something to get you excited about when you get to Chapter 13!Okay, I’ve referred to broadcast addresses throughout Chapters 1 and 2, and even showed you some examples. But I really haven’t gone into the different terms and uses associated with them yet, and it’s about time I did. So here are the four different broadcast (generic term broadcast) types that I’d like to define for you:Layer 2 broadcastsThese are sent to all nodes on a LAN.Broadcasts (layer 3)These are sent to all nodes on the network.UnicastThese are sent to a single destination host.MulticastThese are packets sent from a single source and transmitted to many devices on different networks.First, understand that layer 2 broadcasts are also known as hardware broadcasts—they only go out on a LAN, and they don’t go past the LAN boundary (router). The typical hard-ware address is 6 bytes (48 bits) and looks something like 0c.43.a4.f3.12.c2. The broadcast would be all 1s in binary, which would be all Fs in hexadecimal, as in FF.FF.FF.FF.FF.FF.Then there’s the plain old broadcast addresses at layer 3. Broadcast messages are meant to reach all hosts on a broadcast domain. These are the network broadcasts that have all host bits on. Here’s an example that you’re already familiar with: The network address of 172.16.0.0 255.255.0.0 would have a broadcast address of 172.16.255.255—all host bits on. Broadcasts can also be “all networks and all hosts,” as indicated by 255.255.255.255. A good example of a broadcast message is an Address Resolution Protocol (ARP) request. When a host has a packet, it knows the logical address (IP) of the destination. To get the packet to the destina-tion, the host needs to forward the packet to a default gateway if the destination resides on a different IP network. If the destination is on the local network, the source will forward the packet directly to the destination. Because the source doesn’t have the MAC address to which it needs to forward the frame, it sends out a broadcast, something that every device in the local broadcast domain will listen to. This broadcast says, in essence, “If you are the owner of IP address 192.168.2.3, please forward your MAC address to me,” with the source giving the appropriate information.A unicast is different because it’s a broadcast packet that goes from 255.255.255.255 to an actual destination IP address—in other words, it’s directed to a specific host. A DHCP client request is a good example of how a unicast works. Here’s an example: Your host on a LAN sends out an FF.FF.FF.FF.FF.FF layer 2 broadcast and 255.255.255.255 layer 3 destination broadcast looking for a DHCP server on the LAN. The router will see that this is a broadcast 

10089.book  Page 100  Monday, July 23, 2007  3:17 PM




Summary101meant for the DHCP server because it has a destination port number of 67 (BootP server) and will forward the request to the IP address of the DHCP server on another LAN. So, basically, if your DHCP server IP address is 172.16.10.1, your host just sends out a 255.255.255.255 DHCP client broadcast request, and the router changes that broadcast to the specific destina-tion address of 172.16.10.1. (In order for the router to provide this service, you need to con-figure the interfaces with the ip helper-address command—this is not a default service.)Multicast is a different beast entirely. At first glance, it appears to be a hybrid of unicast and broadcast communication, but that isn’t quite the case. Multicast does allow point-to-multipoint communication, which is similar to broadcasts, but it happens in a different man-ner. The crux of multicast is that it enables multiple recipients to receive messages without flooding the messages to all hosts on a broadcast domain.Multicast works by sending messages or data to IP multicast group addresses. Routers then forward copies (unlike broadcasts, which are not forwarded) of the packet out every interface that has hosts subscribed to that group address. This is where multicast differs from broadcast messages—with multicast communication, copies of packets, in theory, are sent only to sub-scribed hosts. When I say in theory, this means that the hosts will receive, for example, a mul-ticast packet destined for 224.0.0.9 (this is an EIGRP packet and only a router running the EIGRP protocol will read these). All hosts on the broadcast LAN (Ethernet is a broadcast multi-access LAN technology) will pick up the frame, read the destination address, and imme-diately discard the frame, unless they are in the multicast group. This saves PC processing, not LAN bandwidth. Multicasting can cause severe LAN congestion, in some instances, if not implemented carefully.There are several different groups that users or applications can subscribe to. The range of multicast addresses starts with 224.0.0.0 and goes through 239.255.255.255. As you can see, this range of addresses falls within IP Class D address space based on classful IP assignment.SummaryIf you made it this far and understood everything the first time through, you should be proud of yourself. We really covered a lot of ground in this chapter, but understand that the infor-mation in this chapter is key to being able to navigate through the rest of this book. And even if you didn’t get a complete understanding the first time around, don’t stress. It really wouldn’t hurt you to read this chapter more than once. There is still a lot of ground to cover, so make sure you’ve got it all down, and get ready for more. What we’re doing is building a foundation, and you want a strong foundation, right?After you learned about the DoD model, the layers, and associated protocols, you learned about the oh-so-important IP addressing. I discussed in detail the difference between each class of address and how to find a network address, broadcast address, and valid host range, which is critical information to understand before going on to Chapter 3.Since you’ve already come this far, there’s no reason to stop now and waste all those brain-waves and new neurons. So don’t stop—go through the written lab and review questions at the end of this chapter and make sure you understand each answer’s explanation. The best is yet to come!

10089.book  Page 101  Monday, July 23, 2007  3:17 PM




102Chapter2 Introduction to TCP/IPExam EssentialsRemember the Process/Application layer protocols.Telnet is a terminal emulation program that allows you to log into a remote host and run programs. File Transfer Protocol (FTP) is a connection-oriented service that allows you to transfer files. Trivial FTP (TFTP) is a connec-tionless file transfer program. Simple Mail Transfer Protocol (SMTP) is a send-mail program.Remember the Host-to-Host layer protocols.Transmission Control Protocol (TCP) is a connection-oriented protocol that provides reliable network service by using acknowledg-ments and flow control. User Datagram Protocol (UDP) is a connectionless protocol that pro-vides low overhead and is considered unreliable.Remember the Internet layer protocols.Internet Protocol (IP) is a connectionless protocol that provides network address and routing through an internetwork. Address Resolution Pro-tocol (ARP) finds a hardware address from a known IP address. Reverse ARP (RARP) finds an IP address from a known hardware address. Internet Control Message Protocol (ICMP) provides diagnostics and destination unreachable messages.Remember the Class A range.The IP range for a Class A network is 1–126. This provides 8 bits of network addressing and 24 bits of host addressing by default.Remember the Class B range.The IP range for a Class B network is 128–191. Class B addressing provides 16 bits of network addressing and 16 bits of host addressing by default.Remember the Class C range.The IP range for a Class C network is 192–223. Class C addressing provides 24 bits of network addressing and 8 bits of host addressing by default.Remember the Private IP ranges.Class A private address range is 10.0.0.0 through 10.255.255.255.Class B private address range is 172.16.0.0 through 172.31.255.255.Class C private address range is 192.168.0.0 through 192.168.255.255.Written Lab 2Answer the following questions about TCP/IP:1.What is the Class C address range in decimal and in binary?2.What layer of the DoD model is equivalent to the Transport layer of the OSI model?3.What is the valid range of a Class A network address?4.What is the 127.0.0.1 address used for?5.How do you find the network address from a listed IP address?6.How do you find the broadcast address from a listed IP address?7.What is the Class A private IP address space?

10089.book  Page 102  Monday, July 23, 2007  3:17 PM




Written Lab 21038.What is the Class B private IP address space?9.What is the Class C private IP address space?10.What are all the available characters that you can use in hexadecimal addressing?(The answers to Written Lab 2 can be found following the answers to the review questions for this chapter.)

10089.book  Page 103  Monday, July 23, 2007  3:17 PM




104Chapter2 Introduction to TCP/IPReview Questions

The following questions are designed to test your understanding of this chap-ter’s material. For more information on how to get additional questions, please see this book’s Introduction.1.What are the decimal and hexadecimal equivalents of the binary number 10011101? (Choose two.)A.159B.157C.185D.0x9DE.0xD9F.0x1592.Which of the following allows a router to respond to an ARP request that is intended for a remote host?A.Gateway DPB.Reverse ARP (RARP)C.Proxy ARPD.Inverse ARP (IARP)E.Address Resolution Protocol (ARP)3.You want to implement a mechanism that automates the IP configuration, including IP address, subnet mask, default gateway, and DNS information. Which protocol will you use to accomplish this?A.SMTPB.SNMPC.DHCPD.ARP4.What protocol is used to find the hardware address of a local device?A.RARPB.ARPC.IPD.ICMPE.BootP

10089.book  Page 104  Monday, July 23, 2007  3:17 PM




Review Questions1055.Which of the following are layers in the TCP/IP model? (Choose three.)A.ApplicationB.SessionC.TransportD.InternetE.Data LinkF.Physical6.Which class of IP address provides a maximum of only 254 host addresses per network ID?A.Class AB.Class BC.Class CD.Class DE.Class E7.Which of the following describe the DHCP Discover message? (Choose two.)A.It uses FF:FF:FF:FF:FF:FF as a layer 2 broadcast.B.It uses UDP as the Transport layer protocol.C.It uses TCP as the Transport layer protocol.D.It does not use a layer 2 destination address.8.Which layer 4 protocol is used for a Telnet connection?A.IPB.TCPC.TCP/IPD.UDPE.ICMP9.Which statements are true regarding ICMP packets? (Choose two.)A.They acknowledge receipt of a TCP segment.B.They guarantee datagram delivery.C.They can provide hosts with information about network problems.D.They are encapsulated within IP datagrams.E.They are encapsulated within UDP datagrams.10.Which of the following services use TCP? (Choose three.)A.DHCPB.SMTPC.SNMPD.FTPE.HTTPF.TFTP

10089.book  Page 105  Monday, July 23, 2007  3:17 PM




106Chapter2 Introduction to TCP/IP11.Which of the following services use UDP? (Choose three.)A.DHCPB.SMTPC.SNMPD.FTPE.HTTPF.TFTP12.Which of the following are TCP/IP protocols used at the Application layer of the OSI model? (Choose three.)A.IPB.TCPC.TelnetD.FTPE.TFTP13.The following illustration shows a data structure header. What protocol is this header from?A.IPB.ICMPC.TCPD.UDPE.ARPF.RARP

Bit 0Bit 15Source port (16)Destination port (16)Window (16)Urgent (16)Code bits (6)Reserved (6)Checksum (16)Headerlength (4)Sequence number (32)Acknowledgment number (32)Options (0 or 32 if any)Data (varies)Bit 16Bit 31

24 bytes

10089.book  Page 106  Monday, July 23, 2007  3:17 PM




Review Questions10714.If you use either Telnet or FTP, which is the highest layer you are using to transmit data?A.ApplicationB.PresentationC.SessionD.Transport15.The DoD model (also called the TCP/IP stack) has four layers. Which layer of the DoD model is equivalent to the Network layer of the OSI model?A.ApplicationB.Host-to-HostC.InternetD.Network Access16.Which two of the following are private IP addresses?A.12.0.0.1B.168.172.19.39C.172.20.14.36D.172.33.194.30E.192.168.24.4317.What layer in the TCP/IP stack is equivalent to the Transport layer of the OSI model?A.ApplicationB.Host-to-HostC.InternetD.Network Access18.Which statements are true regarding ICMP packets? (Choose two)A.ICMP guarantees datagram delivery.B.ICMP can provide hosts with information about network problems.C.ICMP is encapsulated within IP datagrams.D.ICMP is encapsulated within UDP datagrams.19.What is the address range of a Class B network address in binary?A.01xxxxxxB.0xxxxxxxC.10xxxxxxD.110xxxxx20.Which of the following protocols uses both TCP and UDP?A.FTPB.SMTPC.TelnetD.DNS

10089.book  Page 107  Monday, July 23, 2007  3:17 PM




108Chapter2 Introduction to TCP/IPAnswers to Review Questions1.B, D. To turn a binary number into decimal, you just have to add the values of each bit that is a 1. The values of 10011101 are 128, 16, 8, 4, and 1. 128 + 16 + 8 + 4 + 1 = 157. Hexadec-imal is a base-16 number system. The values of hexadecimal are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F—16 characters total, from which to create all the numbers you’ll ever need. So, if 1001 in binary is 9, then the hexadecimal equivalent is 9. Since we then have 1101, which is 13 in binary, the hexadecimal answer is D and the complete hexadecimal answer is 0x9D. Even though binary/hex numbers were discussed in Chapter 1, a good review is necessary here.2.C. Proxy ARP can help machines on a subnet reach remote subnets without configuring rout-ing or a default gateway.3.C. Dynamic Host Configuration Protocol (DHCP) is used to provide IP information to hosts on your network. DHCP can provide a lot of information, but the most common is IP address, subnet mask, default gateway, and DNS information.4.B. Address Resolution Protocol (ARP) is used to find the hardware address from a known IP address.5.A, C, D. This seems like a hard question at first because it doesn’t make sense. The listed answers are from the OSI model and the question asked about the TCP/IP protocol stack (DoD model). However, let’s just look for what is wrong. First, the Session layer is not in the TCP/IP model; neither are the Data Link and Physical layers. This leaves us with the Transport layer (Host-to-Host in the DoD model), Internet layer (Network layer in the OSI), and Application layer (Application/Process in the DoD).6.C. A Class C network address has only 8 bits for defining hosts: 28 – 2 = 254.7.A, B. A client that sends out a DHCP Discover message in order to receive an IP address sends out a broadcast at both layer 2 and layer 3. The layer 2 broadcast is all Fs in hex, or FF:FF:FF:FF:FF:FF. The layer 3 broadcast is 255.255.255.255, which means all networks and all hosts. DHCP is connectionless, which means it uses User Datagram Protocol (UDP) at the Transport layer, also called the Host-to-Host layer.8.B. Although Telnet does use TCP and IP (TCP/IP), the question specifically asks about layer 4, and IP works at layer 3. Telnet uses TCP at layer 4.9.C, D. Internet Control Message Protocol (ICMP) is used to send error messages through the network, but they do not work alone. Every segment or ICMP payload must be encapsulated within an IP datagram (or packet).10.B, D, E. SMTP, FTP, and HTTP use TCP.11.A, C, F. Explanation:DHCP, SNMP, and TFTP use UDP. SMTP, FTP, and HTTP use TCP.12.C, D, E. Telnet, File Transfer Protocol (FTP), and Trivial FTP (TFTP) are all Application layer protocols. IP is a Network layer protocol. Transmission Control Protocol (TCP) is a Transport layer protocol.

10089.book  Page 108  Monday, July 23, 2007  3:17 PM




Answers to Review Questions10913.C. First, you should know easily that only TCP and UDP work at the Transport layer, so now you have a 50/50 shot. However, since the header has sequencing, acknowledgment, and win-dow numbers, the answer can only be TCP.14.A. Both FTP and Telnet use TCP at the Transport layer; however, they both are Application layer protocols, so the Application layer is the best answer for this question.15.C. The four layers of the DoD model are Application/Process, Host-to-Host, Internet, and Net-work Access. The Internet layer is equivalent to the Network layer of the OSI model.16.C, E. Class A private address range is 10.0.0.0 through 10.255.255.255. Class B private address range is 172.16.0.0 through 172.31.255.255, and Class C private address range is 192.168.0.0 through 192.168.255.255.17.B. The four layers of the TCP/IP stack (also called the DoD model) are Application/Process, Host-to-Host, Internet, and Network Access. The Host-to-Host layer is equivalent to the Transport layer of the OSI model.18.B, C. ICMP is used for diagnostics and destination unreachable messages. ICMP is encapsu-lated within IP datagrams, and because it is used for diagnostics, it will provide hosts with information about network problems.19.C. The range of a Class B network address is 128–191. This makes our binary range 10xxxxxx.20.D. DNS uses TCP for zone exchanges between servers and UDP when a client is trying to resolve a hostname to an IP address.

10089.book  Page 109  Monday, July 23, 2007  3:17 PM




110Chapter2 Introduction to TCP/IPAnswers to Written Lab 21.192–223, 110xxxxx2.Host-to-Host3.1–1264.Loopback or diagnostics5.Turn all host bits off.6.Turn all host bits on.7.10.0.0.0 through 10.255.255.2558.172.16.0.0 through 172.31.255.2559.192.168.0.0 through 192.168.255.25510.0–9 and A, B, C, D, E, and F

10089.book  Page 110  Monday, July 23, 2007  3:17 PM




 

Chapter 3 Subnetting, Variable Length Subnet Masks (VLSMs), and Troubleshooting TCP/IP

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Describe how a network works  Interpret network   Implement an IP addressing scheme and IP Services to meet network requirements in a medium-size Enterprise branch office network  Describe the operation and benefits of using private and public IP addressing   Implement static and dynamic addressing services for hosts in a LAN environment   Calculate and apply an addressing scheme including VLSM IP addressing design to a network   Determine the appropriate classless addressing scheme using VLSM and summarization to satisfy addressing requirements in a LAN/WAN environment   Identify and correct common problems associated with IP addressing and host configurations 

 

10089c03.fm  Page 111  Thursday, August 30, 2007  12:33 PM




 This chapter will pick up right where we left off in the last chapter. We will continue our discussion of IP addressing.We’ll start with subnetting an IP network. You’re going to have to really apply yourself, because subnetting takes time and practice in order to nail it. So be patient. Do whatever it takes to get this stuff dialed in. This chapter truly is important—possibly the most important chapter in this book for you to understand.I’ll thoroughly cover IP subnetting from the very beginning. I know this might sound weird to you, but I think you’ll be much better off if you can try to forget everything you’ve learned about subnetting before reading this chapter—especially if you’ve been to a Microsoft class!After our discussion of IP subnetting, I’m going to tell you all about Variable Length Subnet Masks (VLSMs), as well as show you how to design and implement a network using VLSM networks.Once you have mastered VLSM design and implementation, I’ll show you how to summarize classful boundaries. We’ll go into this further in Chapter 7, “Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF),” where I’ll demonstrate summarizing using EIGRP and OSPF routing protocols.I’ll wrap up the chapter by going over IP address troubleshooting and take you through the steps Cisco recommends when troubleshooting an IP network.So get psyched—you’re about to go for quite a ride! This chapter will truly help you under-stand IP addressing and networking, so don’t get discouraged or give up. If you stick with it, I promise that one day you’ll look back on this and you’ll be really glad you decided to hang on. It’s one of those things that after you understand it, you’ll wonder why you once thought it was so hard. Ready? Let’s go!

 For up-to-the-minute updates for this chapter, please see  www.lammle.com   and/or  www.sybex.com . Subnetting Basics In Chapter 2, you learned how to define and find the valid host ranges used in a Class A, Class B, and Class C network address by turning the host bits all off and then all on. This is very good, but here’s the catch: You were defining only one network. What happens if you wanted to take one network address and create six networks from it? You would have to do something called  sub-netting , because that’s what allows you to take one larger network and break it into a bunch of smaller networks.

 

10089c03.fm  Page 112  Thursday, August 30, 2007  12:33 PM




 Subnetting Basics 113 There are loads of reasons in favor of subnetting, including the following benefits: Reduced network traffic We all appreciate less traffic of any kind. Networks are no differ-ent. Without trusty routers, packet traffic could grind the entire network down to a near standstill. With routers, most traffic will stay on the local network; only packets destined for other networks will pass through the router. Routers create broadcast domains. The more broadcast domains you create, the smaller the broadcast domains and the less network traffic on each network segment. Optimized network performance This is a result of reduced network traffic. Simplified management It’s easier to identify and isolate network problems in a group of smaller connected networks than within one gigantic network. Facilitated spanning of large geographical distances Because WAN links are considerably slower and more expensive than LAN links, a single large network that spans long distances can create problems in every area previously listed. Connecting multiple smaller networks makes the system more efficient.In the following sections, I am going to move to subnetting a network address. This is the good part—ready? IP Subnet-Zero IP subnet-zero  is not a new command, but in the past, Cisco courseware, and Cisco exam objec-tives, didn’t cover it—but it certainly does now! This command allows you to use the first and last subnet in your network design. For example, the Class C mask of 192 provides subnets 64 and 128 (discussed thoroughly later in this chapter), but with the  ip subnet-zero  command, you now get to use subnets 0, 64, 128, and 192. That is two more subnets for every subnet mask we use. Even though we don’t discuss the command line interface (CLI) until the next chapter, “Cisco’s Internetworking Operating System (IOS) and Security Device Manager (SDM),” it’s important for you to be familiar with this command:

 P1R1# sh running-config Building configuration...Current configuration : 827 bytes!hostname Pod1R1!ip subnet-zero

 ! This router output shows that the command  ip subnet-zero  is enabled on the router. Cisco has turned this command on by default starting with Cisco IOS version 12. x .

 When studying for your Cisco exams, make sure you read very carefully and understand if Cisco is asking you  not  to use  ip subnet-zero . There are  instances where this may happen.

 

10089c03.fm  Page 113  Thursday, August 30, 2007  12:33 PM




 114 Chapter3  Subnetting, VLSMs, and Troubleshooting TCP/IP How to Create Subnets To create subnetworks, you take bits from the host portion of the IP address and reserve them to define the subnet address. This means fewer bits for hosts, so the more subnets, the fewer bits available for defining hosts.Later in this chapter, you’ll learn how to create subnets, starting with Class C addresses. But before you actually implement subnetting, you need to determine your current require-ments as well as plan for future conditions.

 Before we move on to designing and creating a subnet mask, you need to understand that in this first section, we will be discussing classful routing, which means that all hosts (all nodes) in the network use the exact same sub-net mask. When we move on to Variable Length Subnet Masks (VLSMs), I’ll discuss classless routing, which means that each network segment  can  use a  different subnet mask. To create a subnet follow these steps: 1. Determine the number of required network IDs:  One for each subnet  One for each wide area network connection 2. Determine the number of required host IDs per subnet:  One for each TCP/IP host  One for each router interface 3. Based on the above requirements, create the following:  One subnet mask for your entire network  A unique subnet ID for each physical segment  A range of host IDs for each subnet

 Understanding the Powers of 2 Powers of 2 are important to understand and memorize for use with IP subnetting. To review powers of 2, remember that when you see a number with another number to its upper right (called an exponent), this means you should multiply the number by itself as many times as the upper number specifies. For example, 2 3  is 2  ×  2  ×  2, which equals 8. Here’s a list of powers of 2 that you should commit to memory:2 1  = 22 2  = 42 3  = 8

 

10089c03.fm  Page 114  Thursday, August 30, 2007  12:33 PM




 Subnetting Basics 115 Subnet Masks For the subnet address scheme to work, every machine on the network must know which part of the host address will be used as the subnet address. This is accomplished by assigning a  subnet mask  to each machine. A subnet mask is a 32-bit value that allows the recipient of IP packets to distinguish the network ID portion of the IP address from the host ID portion of the IP address.The network administrator creates a 32-bit subnet mask composed of 1s and 0s. The 1s in the subnet mask represent the positions that refer to the network or subnet addresses.Not all networks need subnets, meaning they use the default subnet mask. This is basically the same as saying that a network doesn’t have a subnet address. Table 3.1 shows the default subnet masks for Classes A, B, and C. These default masks cannot change. In other words, you can’t make a Class B subnet mask read 255.0.0.0. If you try, the host will read that address as invalid and usually won’t even let you type it in. For a Class A network, you can’t change the first byte in a subnet mask; it must read 255.0.0.0 at a minimum. Similarly, you cannot assign 255.255.255.255, as this is all 1s—a broadcast address. A Class B address must start with 255.255.0.0, and a Class C has to start with 255.255.255.0.

 2 4  = 162 5  = 322 6  = 642 7  = 1282 8  = 2562 9  = 5122 10  = 1,0242 11  = 2,0482 12  = 4,0962 13  = 8,1922 14  = 16,384Before you get stressed out about knowing all these exponents, remember that it’s helpful to know them, but it’s not absolutely necessary. Here’s a little trick since you’re working with 2s: Each successive power of 2 is double the previous one.For example, all you have to do to remember the value of 2 9  is to first know that 2 8  = 256. Why? Because when you double 2 to the eighth power (256), you get 2 9  (or 512). To determine the value of 2 10 , simply start at 2 8  = 256, and then double it twice.You can go the other way as well. If you needed to know what 2 6  is, for example, you just cut 256 in half two times: once to reach 2 7  and then one more time to reach 2 6 .

 

10089c03.fm  Page 115  Thursday, August 30, 2007  12:33 PM




 116 Chapter3  Subnetting, VLSMs, and Troubleshooting TCP/IP Classless Inter-Domain Routing (CIDR) Another term you need to familiarize yourself with is  Classless Inter-Domain Routing (CIDR) . It’s basically the method that ISPs (Internet service providers) use to allocate a number of addresses to a company, a home—a customer. They provide addresses in a certain block size, something I’ll be going into in greater detail later in this chapter.When you receive a block of addresses from an ISP, what you get will look something like this: 192.168.10.32/28. This is telling you what your subnet mask is. The slash notation (/) means how many bits are turned on (1s). Obviously, the maximum could only be /32 because a byte is 8 bits and there are 4 bytes in an IP address: (4  ×  8 = 32). But keep in mind that the largest subnet mask available (regardless of the class of address) can only be a /30 because you’ve got to keep at least 2 bits for host bits.Take, for example, a Class A default subnet mask, which is 255.0.0.0. This means that the first byte of the subnet mask is all ones (1s), or 11111111. When referring to a slash notation, you need to count all the 1s bits to figure out your mask. The 255.0.0.0 is considered a /8 because it has 8 bits that are 1s—that is, 8 bits that are turned on.A Class B default mask would be 255.255.0.0, which is a /16 because 16 bits are ones (1s): 11111111.11111111.00000000.00000000.Table 3.2 has a listing of every available subnet mask and its equivalent CIDR slash notation. TABLE3.1 Default Subnet Mask ClassFormatDefault Subnet Mask A network.node.node.node 255.0.0.0B network.network.node.node 255.255.0.0C network.network.network.node 255.255.255.0 TABLE3.2 CIDR Values  Subnet MaskCIDR Value 255.0.0.0/8255.128.0.0/9255.192.0.0/10255.224.0.0/11255.240.0.0/12

 

10089c03.fm  Page 116  Thursday, August 30, 2007  12:33 PM




 Subnetting Basics 117 The /8 through /15 can only be used with Class A network addresses. /16 through /23 can be used by Class A and B network addresses. /24 through /30 can be used by Class A, B, and C network addresses. This is a big reason why most companies use Class A network addresses. Since they can use all subnet masks, they get the maximum flexibility in network design. 255.248.0.0/13255.252.0.0/14255.254.0.0/15255.255.0.0/16255.255.128.0/17255.255.192.0/18255.255.224.0/19255.255.240.0/20255.255.248.0/21255.255.252.0/22255.255.254.0/23255.255.255.0/24255.255.255.128/25255.255.255.192/26255.255.255.224/27255.255.255.240/28255.255.255.248/29255.255.255.252/30 TABLE3.2 CIDR Values (continued) Subnet MaskCIDR Value

 

10089c03.fm  Page 117  Thursday, August 30, 2007  12:33 PM




 118 Chapter3  Subnetting, VLSMs, and Troubleshooting TCP/IP

 No, you cannot configure a Cisco router using this slash format. But wouldn’t that be nice? Nevertheless, it’s  really  important for you to know subnet masks  in the slash notation (CIDR). Subnetting Class C Addresses There are many different ways to subnet a network. The right way is the way that works best for you. In a Class C address, only 8 bits are available for defining the hosts. Remember that subnet bits start at the left and go to the right, without skipping bits. This means that the only Class C subnet masks can be the following:

 Binary     Decimal  CIDR---------------------------------------------------------00000000 = 0        /2410000000 = 128      /25 11000000 = 192      /2611100000 = 224      /2711110000 = 240      /2811111000 = 248      /29

 11111100 = 252      /30 We can’t use a /31 or /32 because we have to have at least 2 host bits for assigning IP addresses to hosts. In the past, I never discussed the /25 in a Class C network. Cisco always had been con-cerned with having at least 2 subnet bits, but now, because of Cisco recognizing the  ip subnet-zero  command in its curriculum and exam objectives, we can use just 1 subnet bit.In the following sections, I’m going to teach you an alternate method of subnetting that makes it easier to subnet larger numbers in no time. Trust me, you need to be able to subnet fast! Subnetting a Class C Address: The Fast Way!When you’ve chosen a possible subnet mask for your network and need to determine the num-ber of subnets, valid hosts, and broadcast addresses of a subnet that the mask provides, all you need to do is answer five simple questions: How many subnets does the chosen subnet mask produce? How many valid hosts per subnet are available? What are the valid subnets? What’s the broadcast address of each subnet? What are the valid hosts in each subnet?

10089c03.fm  Page 118  Thursday, August 30, 2007  12:33 PM




Subnetting Basics119At this point, it’s important that you both understand and have memorized your powers of 2. Please refer to the sidebar “Understanding the Powers of 2” earlier in this chapter if you need some help. Here’s how you get the answers to those five big questions: How many subnets? 2x = number of subnets. x is the number of masked bits, or the 1s. For example, in 11000000, the number of 1s gives us 22 subnets. In this example, there are 4 subnets. How many hosts per subnet? 2y – 2 = number of hosts per subnet. y is the number of unmasked bits, or the 0s. For example, in 11000000, the number of 0s gives us 26 – 2 hosts. In this example, there are 62 hosts per subnet. You need to subtract 2 for the subnet address and the broadcast address, which are not valid hosts. What are the valid subnets? 256 – subnet mask = block size, or increment number. An example would be 256 – 192 = 64. The block size of a 192 mask is always 64. Start count-ing at zero in blocks of 64 until you reach the subnet mask value and these are your sub-nets. 0, 64, 128, 192. Easy, huh? What’s the broadcast address for each subnet? Now here’s the really easy part. Since we counted our subnets in the last section as 0, 64, 128, and 192, the broadcast address is always the number right before the next subnet. For example, the 0 subnet has a broadcast address of 63 because the next subnet is 64. The 64 subnet has a broadcast address of 127 because the next subnet is 128. And so on. And remember, the broadcast address of the last subnet is always 255. What are the valid hosts? Valid hosts are the numbers between the subnets, omitting the all 0s and all 1s. For example, if 64 is the subnet number and 127 is the broadcast address, then 65–126 is the valid host range—it’s always the numbers between the subnet address and the broadcast address.I know this can truly seem confusing. But it really isn’t as hard as it seems to be at first—just hang in there! Why not try a few and see for yourself?Subnetting Practice Examples: Class C AddressesHere’s your opportunity to practice subnetting Class C addresses using the method I just described. Exciting, isn’t it! We’re going to start with the first Class C subnet mask and work through every subnet that we can using a Class C address. When we’re done, I’ll show you how easy this is with Class A and B networks too!Practice Example #1C: 255.255.255.128 (/25)Since 128 is 10000000 in binary, there is only 1 bit for subnetting and 7 bits for hosts. We’re going to subnet the Class C network address 192.168.10.0.192.168.10.0 = Network address255.255.255.128 = Subnet maskNow, let’s answer the big five: How many subnets? Since 128 is 1 bit on (10000000), the answer would be 21 = 2. How many hosts per subnet? We have 7 host bits off (10000000), so the equation would be 27 – 2 = 126 hosts.

10089c03.fm  Page 119  Thursday, August 30, 2007  12:33 PM




120Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IP What are the valid subnets? 256 – 128 = 128. Remember, we’ll start at zero and count in our block size, so our subnets are 0, 128. What’s the broadcast address for each subnet? The number right before the value of the next subnet is all host bits turned on and equals the broadcast address. For the zero sub-net, the next subnet is 128, so the broadcast of the 0 subnet is 127. What are the valid hosts? These are the numbers between the subnet and broadcast address. The easiest way to find the hosts is to write out the subnet address and the broad-cast address. This way, the valid hosts are obvious. The following table shows the 0 and 128 subnets, the valid host ranges of each, and the broadcast address of both subnets:Before moving on to the next example, take a look at Figure 3.1. Okay, looking at a Class C /25, it’s pretty clear there are two subnets. But so what—why is this significant? Well actually, it’s not, but that’s not the right question. What you really want to know is what you would do with this information!FIGURE3.1Implementing a Class C /25 logical networkI know this isn’t exactly everyone’s favorite pastime, but it’s really important, so just hang in there; we’re going to talk about subnetting—period. You need to know that the key to understand-ing subnetting is to understand the very reason you need to do it. And I’m going to demonstrate this by going through the process of building a physical network—and let’s add a router. (We now have an internetwork, as I truly hope you already know!) All right, because we added that router, in order for the hosts on our internetwork to communicate, they must now have a logical network addressing scheme. We could use IPX or IPv6, but IPv4 is still the most popular, and it also just happens to be what we’re studying at the moment, so that’s what we’re going with. Okay—now take a look back to Figure 3.1. There are two physical networks, so we’re going to implement a log-ical addressing scheme that allows for two logical networks. As always, it’s a really good idea to Subnet0128First host1129Last host126254Broadcast127255

.2 

.3 

.4 

.130 

.131 

.132 

Router#show ip route[output cut]C 192.168.10.0 is directly connected to Ethernet 0.C 192.168.10.128 is directly connected to Ethernet 1.192.168.10.0 .129 .1 192.168.10.128 

10089c03.fm  Page 120  Thursday, August 30, 2007  12:33 PM




Subnetting Basics121look ahead and consider likely growth scenarios—both short and long term, but for this example, a /25 will do the trick.Practice Example #2C: 255.255.255.192 (/26)In this second example, we’re going to subnet the network address 192.168.10.0 using the subnet mask 255.255.255.192.192.168.10.0 = Network address255.255.255.192 = Subnet maskNow, let’s answer the big five: How many subnets? Since 192 is 2 bits on (11000000), the answer would be 22 = 4 subnets. How many hosts per subnet? We have 6 host bits off (11000000), so the equation would be 26 – 2 = 62 hosts. What are the valid subnets? 256 – 192 = 64. Remember, we start at zero and count in our block size, so our subnets are 0, 64, 128, and 192. What’s the broadcast address for each subnet? The number right before the value of the next subnet is all host bits turned on and equals the broadcast address. For the zero subnet, the next subnet is 64, so the broadcast address for the zero subnet is 63. What are the valid hosts? These are the numbers between the subnet and broadcast address. The easiest way to find the hosts is to write out the subnet address and the broadcast address. This way, the valid hosts are obvious. The following table shows the 0, 64, 128, and 192 sub-nets, the valid host ranges of each, and the broadcast address of each subnet:Okay, again, before getting into the next example, you can see that we can now subnet a /26. And what are you going to do with this fascinating information? Implement it! We’ll use Figure 3.2 to practice a /26 network implementation.The /26 mask provides four subnetworks, and we need a subnet for each router interface. With this mask, in this example, we actually have room to add another router interface.Practice Example #3C: 255.255.255.224 (/27)This time, we’ll subnet the network address 192.168.10.0 and subnet mask 255.255.255.224.192.168.10.0 = Network address255.255.255.224 = Subnet mask How many subnets? 224 is 11100000, so our equation would be 23 = 8. How many hosts? 25 – 2 = 30. What are the valid subnets? 256 – 224 = 32. We just start at zero and count to the subnet mask value in blocks (increments) of 32: 0, 32, 64, 96, 128, 160, 192, and 224.The subnets (do this first)064128192Our first host (perform host addressing last)165129193Our last host62126190254The broadcast address (do this second)63127191255

10089c03.fm  Page 121  Thursday, August 30, 2007  12:33 PM




122Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IP What’s the broadcast address for each subnet (always the number right before the next subnet)? What are the valid hosts (the numbers between the subnet number and the broadcast address)?FIGURE3.2Implementing a Class C /26 logical networkTo answer the last two questions, first just write out the subnets, then write out the broad-cast addresses—the number right before the next subnet. Last, fill in the host addresses. The following table gives you all the subnets for the 255.255.255.224 Class C subnet mask:Practice Example #4C: 255.255.255.240 (/28)Let’s practice on another one:192.168.10.0 = Network address255.255.255.240 = Subnet mask Subnets? 240 is 11110000 in binary. 24 = 16. Hosts? 4 host bits, or 24 – 2 = 14. Valid subnets? 256 – 240 = 16. Start at 0: 0 + 16 = 16. 16 + 16 = 32. 32 + 16 = 48. 48 + 16 = 64. 64 + 16 = 80. 80 + 16 = 96. 96 + 16 = 112. 112 + 16 = 128. 128 + 16 = 144. 144 + 16 = 160. 160 + 16 = 176. 176 + 16 = 192. 192 + 16 = 208. 208 + 16 = 224. 224 + 16 = 240.The subnet address0326496128160192224The first valid host1336597129161193225The last valid host306294126158190222254The broadcast address316395127159191223255

.66 

.67 

.68 

.130 

.131 

.132 

Router#show ip route[output cut]C 192.168.10.0 is directly connected to Ethernet 0 C 192.168.10.64 is directly connected to Ethernet 1C 192.168.10.128 is directly connected to Ethernet 2192.168.10.64 

.2 

.3 

.4 

.5 

192.168.10.0 .129 .65 192.168.10.128 .1 

10089c03.fm  Page 122  Thursday, August 30, 2007  12:33 PM




Subnetting Basics123 Broadcast address for each subnet? Valid hosts?To answer the last two questions, check out the following table. It gives you the subnets, valid hosts, and broadcast addresses for each subnet. First, find the address of each subnet using the block size (increment). Second, find the broadcast address of each subnet increment (it’s always the number right before the next valid subnet), then just fill in the host addresses. The following table shows the available subnets, hosts, and broadcast addresses provided from a Class C 255.255.255.240 mask:

Cisco has figured out that most people cannot count in 16s and therefore have a hard time finding valid subnets, hosts, and broadcast addresses with the Class C 255.255.255.240 mask. You’d be wise to study this mask.Practice Example #5C: 255.255.255.248 (/29)Let’s keep practicing:192.168.10.0 = Network address255.255.255.248 = Subnet mask Subnets? 248 in binary = 11111000. 25 = 32. Hosts? 23 – 2 = 6. Valid subnets? 256 – 248 = 0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, and 248. Broadcast address for each subnet? Valid hosts?Take a look at the following table. It shows some of the subnets (first four and last four only), valid hosts, and broadcast addresses for the Class C 255.255.255.248 mask:Subnet0163248648096112128144160176192208224240First host1173349658197113129145161177193209225241Last host143046627894110126142158174190206222238254Broadcast153147637995111127143159175191207223239255Subnet081624 …224232240248First host191725 …225233241249Last host6142230 …230238246254Broadcast7152331 …231239247255

10089c03.fm  Page 123  Thursday, August 30, 2007  12:33 PM




124Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPPractice Example #6C: 255.255.255.252 (/30)Just one more:192.168.10.0 = Network address255.255.255.252 = Subnet mask Subnets? 64. Hosts? 2. Valid subnets? 0, 4, 8, 12, etc., all the way to 252. Broadcast address for each subnet (always the number right before the next subnet)? Valid hosts (the numbers between the subnet number and the broadcast address)?The following table shows you the subnet, valid host, and broadcast address of the first four and last four subnets in the 255.255.255.252 Class C subnet: Subnetting in Your Head: Class C AddressesIt really is possible to subnet in your head. Even if you don’t believe me, I’ll show you how. And it’s not all that hard either—take the following example:192.168.10.33 = Node address255.255.255.224 = Subnet maskSubnet04812 …240244248252First host15913 …241245249253Last host261014 …242246250254Broadcast371115 …243247251255

Should We Really Use This Mask That Provides Only Two Hosts?You are the network administrator for Acme Corporation in San Francisco, with dozens of WAN links connecting to your corporate office. Right now your network is a classful network, which means that the same subnet mask is on each host and router interface. You’ve read about classless routing where you can have different size masks but don’t know what to use on your point-to-point WAN links. Is the 255.255.255.252 (/30) a helpful mask in this situation?Yes, this is a very helpful mask in wide area networks.If you use the 255.255.255.0 mask, then each network would have 254 hosts, but you only use 2 addresses with a WAN link! That is a waste of 252 hosts per subnet. If you use the 255.255.255.252 mask, then each subnet has only 2 hosts and you don’t waste precious addresses. This is a really important subject, one that we’ll address in a lot more detail in the section on VLSM network design later in this chapter.

10089c03.fm  Page 124  Thursday, August 30, 2007  12:33 PM




Subnetting Basics125First, determine the subnet and broadcast address of the above IP address. You can do this by answering question 3 of the big five questions: 256 – 224 = 32. 0, 32, 64. The address of 33 falls between the two subnets of 32 and 64 and must be part of the 192.168.10.32 subnet. The next subnet is 64, so the broadcast address of the 32 subnet is 63. (Remember that the broadcast address of a subnet is always the number right before the next subnet.) The valid host range is 33–62 (the numbers between the subnet and broadcast address). This is too easy!Okay, let’s try another one. We’ll subnet another Class C address:192.168.10.33 = Node address255.255.255.240 = Subnet maskWhat subnet and broadcast address is the above IP address a member of? 256 – 240 = 16. 0, 16, 32, 48. Bingo—the host address is between the 32 and 48 subnets. The subnet is 192.168.10.32, and the broadcast address is 47 (the next subnet is 48). The valid host range is 33–46 (the numbers between the subnet number and the broadcast address).Okay, we need to do more, just to make sure you have this down.You have a node address of 192.168.10.174 with a mask of 255.255.255.240. What is the valid host range?The mask is 240, so we’d do a 256 – 240 = 16. This is our block size. Just keep adding 16 until we pass the host address of 174, starting at zero, of course: 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176. The host address of 174 is between 160 and 176, so the subnet is 160. The broadcast address is 175; the valid host range is 161–174. That was a tough one.One more—just for fun. This is the easiest one of all Class C subnetting:192.168.10.17 = Node address255.255.255.252 = Subnet maskWhat subnet and broadcast address is the above IP address a member of? 256 – 252 = 0 (always start at zero unless told otherwise), 4, 8, 12, 16, 20, etc. You’ve got it! The host address is between the 16 and 20 subnets. The subnet is 192.168.10.16, and the broadcast address is 19. The valid host range is 17–18.Now that you’re all over Class C subnetting, let’s move on to Class B subnetting. But before we do, let’s have a quick review.What Do We Know?Okay—here’s where you can really apply what you’ve learned so far, and begin committing it all to memory. This is a very cool section that I’ve been using in my classes for years. It will really help you nail down subnetting!When you see a subnet mask or slash notation (CIDR), you should know the following:/25What do we know about a /25? 128 mask 1 bits on and 7 bits off (10000000) Block size of 128 2 subnets, each with 126 hosts

10089c03.fm  Page 125  Thursday, August 30, 2007  12:33 PM




126Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IP/26What do we know about a /26? 192 mask 2 bits on and 6 bits off (11000000) Block size of 64 4 subnets, each with 62 hosts/27What do we know about a /27? 224 mask 3 bits on and 5 bits off (11100000) Block size of 32 8 subnets, each with 30 hosts/28What do we know about a /28? 240 mask 4 bits on and 4 bits off Block size of 16 16 subnets, each with 14 hosts/29What do we know about a /29? 248 mask 5 bits on and 3 bits off Block size of 8 32 subnets, each with 6 hosts/30What do we know about a /30? 252 mask 6 bits on and 2 bits off Block size of 4 64 subnets, each with 2 hostsRegardless of whether you have a Class A, Class B, or Class C address, the /30 mask will provide you with only two hosts, ever. This mask is suited almost exclusively—as well as sug-gested by Cisco—for use on point-to-point links.If you can memorize this “What Do We Know?” section, you’ll be much better off in your day-to-day job and in your studies. Try saying it out loud, which helps you memorize things—yes, your significant other and/or coworkers will think you’ve lost it, but they probably already do if you are in the networking field. And if you’re not yet in the networking field but are studying all this to break into it, you might as well have people start thinking you’re an odd bird now since they will eventually anyway.

10089c03.fm  Page 126  Thursday, August 30, 2007  12:33 PM




 Subnetting Basics 127 It’s also helpful to write these on some type of flashcards and have people test your skill. You’d be amazed at how fast you can get subnetting down if you memorize block sizes as well as this “What Do We Know?” section. Subnetting Class B Addresses Before we dive into this, let’s look at all the possible Class B subnet masks first. Notice that we have a lot more possible subnet masks than we do with a Class C network address:

 255.255.0.0    (/16)255.255.128.0  (/17)      255.255.255.0    (/24)255.255.192.0  (/18)      255.255.255.128  (/25)255.255.224.0  (/19)      255.255.255.192  (/26)255.255.240.0  (/20)      255.255.255.224  (/27)255.255.248.0  (/21)      255.255.255.240  (/28)255.255.252.0  (/22)      255.255.255.248  (/29)

 255.255.254.0  (/23)      255.255.255.252  (/30) We know the Class B network address has 16 bits available for host addressing. This means we can use up to 14 bits for subnetting (because we have to leave at least 2 bits for host addressing). Using a /16 means you are not subnetting with class B, but it is a mask you can use.

 By the way, do you notice anything interesting about that list of subnet val-ues—a pattern, maybe? Ah ha! That’s exactly why I had you memorize the binary-to-decimal numbers at the beginning of this section. Since subnet mask bits start on the left and move to the right and bits can’t be skipped, the numbers are always the same regardless of the class of address. Memorize  this pattern. The process of subnetting a Class B network is pretty much the same as it is for a Class C, except that you just have more host bits and you start in the third octet.Use the same subnet numbers for the third octet with Class B that you used for the fourth octet with Class C, but add a zero to the network portion and a 255 to the broadcast section in the fourth octet. The following table shows you an example host range of two subnets used in a Class B 240 (/20) subnet mask:Just add the valid hosts between the numbers, and you’re set!

 The preceding example is true only until you get up to /24. After that, it’s  numerically exactly like Class C. First subnet 16.031.255 Second subnet 32.047.255

 

10089c03.fm  Page 127  Wednesday, February 27, 2008  4:51 PM




128Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPSubnetting Practice Examples: Class B AddressesThis section will give you an opportunity to practice subnetting Class B addresses. Again, I have to mention that this is the same as subnetting with Class C, except we start in the third octet—with the exact same numbers!Practice Example #1B: 255.255.128.0 (/17)172.16.0.0 = Network address255.255.128.0 = Subnet mask Subnets? 21 = 2 (same as Class C). Hosts? 215 – 2 = 32,766 (7 bits in the third octet, and 8 in the fourth). Valid subnets? 256 – 128 = 128. 0, 128. Remember that subnetting is performed in the third octet, so the subnet numbers are really 0.0 and 128.0, as shown in the next table. These are the exact numbers we used with Class C; we use them in the third octet and add a 0 in the fourth octet for the network address. Broadcast address for each subnet? Valid hosts?The following table shows the two subnets available, the valid host range, and the broad-cast address of each:Okay, notice that we just added the fourth octet’s lowest and highest values and came up with the answers. And again, it’s done exactly the same way as for a Class C subnet. We just use the same numbers in the third octet and added 0 and 255 in the fourth octet—pretty simple huh! I really can’t say this enough: It’s just not hard; the numbers never change; we just use them in different octets!Practice Example #2B: 255.255.192.0 (/18)172.16.0.0 = Network address255.255.192.0 = Subnet mask Subnets? 22 = 4. Hosts? 214 – 2 = 16,382 (6 bits in the third octet, and 8 in the fourth). Valid subnets? 256 – 192 = 64. 0, 64, 128, 192. Remember that the subnetting is per-formed in the third octet, so the subnet numbers are really 0.0, 64.0, 128.0, and 192.0, as shown in the next table. Broadcast address for each subnet? Valid hosts?Subnet0.0128.0First host0.1128.1Last host127.254255.254Broadcast127.255255.255

10089c03.fm  Page 128  Thursday, August 30, 2007  12:33 PM




Subnetting Basics129The following table shows the four subnets available, the valid host range, and the broad-cast address of each:Again, it’s pretty much the same as it is for a Class C subnet—we just added 0 and 255 in the fourth octet for each subnet in the third octet.Practice Example #3B: 255.255.240.0 (/20)172.16.0.0 = Network address255.255.240.0 = Subnet mask Subnets? 24 = 16. Hosts? 212 – 2 = 4094. Valid subnets? 256 – 240 = 0, 16, 32, 48, etc., up to 240. Notice that these are the same numbers as a Class C 240 mask – we just put them in the third octet and add a 0 and 255 in the fourth octet. Broadcast address for each subnet? Valid hosts?The following table shows the first four subnets, valid hosts, and broadcast addresses in a Class B 255.255.240.0 mask:Practice Example #4B: 255.255.254.0 (/23)172.16.0.0 = Network address255.255.254.0 = Subnet mask Subnets? 27 = 128. Hosts? 29 – 2 = 510. Valid subnets? 256 – 254 = 0, 2, 4, 6, 8, etc., up to 254. Broadcast address for each subnet? Valid hosts?Subnet0.064.0128.0192.0First host0.164.1128.1192.1Last host63.254127.254191.254255.254Broadcast63.255127.255191.255255.255Subnet0.016.032.048.0First host0.116.132.148.1Last host15.25431.25447.25463.254Broadcast15.25531.25547.25563.255

10089c03.fm  Page 129  Thursday, August 30, 2007  12:33 PM




130Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPThe following table shows the first five subnets, valid hosts, and broadcast addresses in a Class B 255.255.254.0 mask:Practice Example #5B: 255.255.255.0 (/24)Contrary to popular belief, 255.255.255.0 used with a Class B network address is not called a Class B network with a Class C subnet mask. It’s amazing how many people see this mask used in a Class B network and think it’s a Class C subnet mask. This is a Class B subnet mask with 8 bits of subnetting—it’s considerably different from a Class C mask. Subnetting this address is fairly simple:172.16.0.0 = Network address255.255.255.0 = Subnet mask Subnets? 28 = 256. Hosts? 28 – 2 = 254. Valid subnets? 256 – 255 = 1. 0, 1, 2, 3, etc., all the way to 255. Broadcast address for each subnet? Valid hosts?The following table shows the first four and last two subnets, the valid hosts, and the broadcast addresses in a Class B 255.255.255.0 mask:Practice Example #6B: 255.255.255.128 (/25)This is one of the hardest subnet masks you can play with. And worse, it actually is a really good subnet to use in production because it creates over 500 subnets with 126 hosts for each subnet—a nice mixture. So, don’t skip over it!172.16.0.0 = Network address255.255.255.128 = Subnet maskSubnet0.02.04.06.08.0First host0.12.14.16.18.1Last host1.2543.2545.2547.2549.254Broadcast1.2553.2555.2557.2559.255Subnet0.01.02.03.0 ...254.0255.0First host0.11.12.13.1 ...254.1255.1Last host0.2541.2542.2543.254 ...254.254255.254Broadcast0.2551.2552.2553.255 ...254.255255.255

10089c03.fm  Page 130  Thursday, August 30, 2007  12:33 PM




Subnetting Basics131 Subnets? 29 = 512. Hosts? 27 – 2 = 126. Valid subnets? Okay, now for the tricky part. 256 – 255 = 1. 0, 1, 2, 3, etc. for the third octet. But you can’t forget the one subnet bit used in the fourth octet. Remember when I showed you how to figure one subnet bit with a Class C mask? You figure this the same way. (Now you know why I showed you the 1-bit subnet mask in the Class C section—to make this part easier.) You actually get two subnets for each third octet value, hence the 512 subnets. For example, if the third octet is showing subnet 3, the two subnets would actually be 3.0 and 3.128. Broadcast address for each subnet? Valid hosts?The following table shows how you can create subnets, valid hosts, and broadcast addresses using the Class B 255.255.255.128 subnet mask (the first eight subnets are shown, and then the last two subnets):Practice Example #7B: 255.255.255.192 (/26)Now, this is where Class B subnetting gets easy. Since the third octet has a 255 in the mask section, whatever number is listed in the third octet is a subnet number. However, now that we have a subnet number in the fourth octet, we can subnet this octet just as we did with Class C subnetting. Let’s try it out:172.16.0.0 = Network address255.255.255.192 = Subnet mask Subnets? 210 = 1024. Hosts? 26 – 2 = 62. Valid subnets? 256 – 192 = 64. The subnets are shown in the following table. Do these numbers look familiar? Broadcast address for each subnet? Valid hosts?Subnet0.00.1281.01.1282.02.1283.03.128 ...255.0255.128First host0.10.1291.11.1292.12.1293.13.129 ...255.1255.129Last host0.1260.2541.1261.2542.1262.2543.1263.254 ...255.126255.254Broad-cast0.1270.2551.1271.2552.1272.2553.1273.255 ...255.127255.255

10089c03.fm  Page 131  Thursday, August 30, 2007  12:33 PM




132Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPThe following table shows the first eight subnet ranges, valid hosts, and broadcast addresses:Notice that for each subnet value in the third octet, you get subnets 0, 64, 128, and 192 in the fourth octet.Practice Example #8B: 255.255.255.224 (/27)This is done the same way as the preceding subnet mask, except that we just have more subnets and fewer hosts per subnet available.172.16.0.0 = Network address255.255.255.224 = Subnet mask Subnets? 211 = 2048. Hosts? 25 – 2 = 30. Valid subnets? 256 – 224 = 32. 0, 32, 64, 96, 128, 160, 192, 224. Broadcast address for each subnet? Valid hosts?The following table shows the first eight subnets:This next table shows the last eight subnets:Subnet0.00.640.1280.1921.01.641.1281.192First host0.10.650.1290.1931.11.651.1291.193Last host0.620.1260.1900.2541.621.1261.1901.254Broadcast0.630.1270.1910.2551.631.1271.1911.255Subnet0.00.320.640.960.1280.1600.1920.224First host0.10.330.650.970.1290.1610.1930.225Last host0.300.620.940.1260.1580.1900.2220.254Broadcast0.310.630.950.1270.1590.1910.2230.255Subnet255.0255.32255.64255.96255.128255.160255.192255.224First host255.1255.33255.65255.97255.129255.161255.193255.225Last host255.30255.62255.94255.126255.158255.190255.222255.254Broadcast255.31255.63255.95255.127255.159255.191255.223255.255

10089c03.fm  Page 132  Thursday, August 30, 2007  12:33 PM




 Subnetting Basics 133 Subnetting in Your Head: Class B Addresses Are you nuts? Subnet Class B addresses in our heads? It’s actually easier than writing it out—I’m not kidding! Let me show you how: Question:  What subnet and broadcast address is the IP address 172.16.10.33 255.255.255.224 (/27) a member of? Answer:  The interesting octet is the fourth octet. 256 – 224 = 32. 32 + 32 = 64. Bingo: 33 is between 32 and 64. However, remember that the third octet is considered part of the subnet, so the answer would be the 10.32 subnet. The broadcast is 10.63, since 10.64 is the next subnet. That was a pretty easy one. Question:  What subnet and broadcast address is the IP address 172.16.66.10 255.255.192.0 (/18) a member of? Answer:  The interesting octet is the third octet instead of the fourth octet. 256 – 192 = 64. 0, 64, 128. The subnet is 172.16.64.0. The broadcast must be 172.16.127.255 since 128.0 is the next subnet. Question:  What subnet and broadcast address is the IP address 172.16.50.10 255.255.224.0 (/19) a member of? Answer:  256 – 224 = 0, 32, 64 (remember, we always start counting at zero (0)). The subnet is 172.16.32.0, and the broadcast must be 172.16.63.255 since 64.0 is the next subnet. Question:  What subnet and broadcast address is the IP address 172.16.46.255 255.255.240.0 (/20) a member of? Answer:  256 – 240 = 16. The third octet is interesting to us. 0, 16, 32, 48. This subnet address must be in the 172.16.32.0 subnet, and the broadcast must be 172.16.47.255 since 48.0 is the next subnet. So, yes, 172.16.46.255 is a valid host. Question:  What subnet and broadcast address is the IP address 172.16.45.14 255.255.255.252 (/30) a member of? Answer:  Where is the interesting octet? 256 – 252 = 0, 4, 8, 12, 16 (in the fourth octet). The subnet is 172.16.45.12, with a broadcast of 172.16.45.15 because the next subnet is 172.16.45.16. Question:  What is the subnet and broadcast address of the host 172.16.88.255/20? Answer:  What is a /20? If you can’t answer this, you can’t answer this question, can you? A /20 is 255.255.240.0, which gives us a block size of 16 in the third octet, and since no subnet bits are on in the fourth octet, the answer is always 0 and 255 in the fourth octet. 0, 16, 32, 48, 64, 80, 96…bingo. 88 is between 80 and 96, so the subnet is 80.0 and the broadcast address is 95.255. Question:  A router receives a packet on an interface with a destination address of 172.16.46.191/26. What will the router do with this packet? Answer:  Discard it. Do you know why? 172.16.46.191/26 is a 255.255.255.192 mask, which gives us a block size of 64. Our subnets are then 0, 64, 128, 192. 191 is the broad-cast address of the 128 subnet, so a router, by default, will discard any broadcast packets.

 

10089c03.fm  Page 133  Wednesday, February 27, 2008  4:50 PM




134Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPSubnetting Class A AddressesClass A subnetting is not performed any differently than Classes B and C, but there are 24 bits to play with instead of the 16 in a Class B address and the 8 in a Class C address.Let’s start by listing all the Class A masks:

255.0.0.0    (/8)255.128.0.0  (/9)           255.255.240.0  (/20)255.192.0.0  (/10)          255.255.248.0  (/21)255.224.0.0  (/11)          255.255.252.0  (/22)255.240.0.0  (/12)          255.255.254.0  (/23)255.248.0.0  (/13)          255.255.255.0  (/24)255.252.0.0  (/14)          255.255.255.128  (/25)255.254.0.0  (/15)          255.255.255.192  (/26)255.255.0.0  (/16)          255.255.255.224  (/27)255.255.128.0  (/17)        255.255.255.240  (/28)255.255.192.0  (/18)        255.255.255.248  (/29)

255.255.224.0  (/19)        255.255.255.252  (/30)That’s it. You must leave at least 2 bits for defining hosts. And I hope you can see the pat-tern by now. Remember, we’re going to do this the same way as a Class B or C subnet. It’s just that, again, we simply have more host bits and we just use the same subnet numbers we used with Class B and C, but we start using these numbers in the second octet.Subnetting Practice Examples: Class A AddressesWhen you look at an IP address and a subnet mask, you must be able to distinguish the bits used for subnets from the bits used for determining hosts. This is imperative. If you’re still struggling with this concept, please reread the section “IP Addressing” in Chapter 2. It shows you how to determine the difference between the subnet and host bits and should help clear things up.Practice Example #1A: 255.255.0.0 (/16)Class A addresses use a default mask of 255.0.0.0, which leaves 22 bits for subnetting since you must leave 2 bits for host addressing. The 255.255.0.0 mask with a Class A address is using 8 subnet bits. Subnets? 28 = 256. Hosts? 216 – 2 = 65,534. Valid subnets? What is the interesting octet? 256 – 255 = 1. 0, 1, 2, 3, etc. (all in the second octet). The subnets would be 10.0.0.0, 10.1.0.0, 10.2.0.0, 10.3.0.0, etc., up to 10.255.0.0.

10089c03.fm  Page 134  Thursday, August 30, 2007  12:33 PM




Subnetting Basics135 Broadcast address for each subnet? Valid hosts?The following table shows the first two and last two subnets, valid host range, and broad-cast addresses for the private Class A 10.0.0.0 network:Practice Example #2A: 255.255.240.0 (/20)255.255.240.0 gives us 12 bits of subnetting and leaves us 12 bits for host addressing. Subnets? 212 = 4096. Hosts? 212 – 2 = 4094. Valid subnets? What is your interesting octet? 256 – 240 = 16. The subnets in the second octet are a block size of 1 and the subnets in the third octet are 0, 16, 32, etc. Broadcast address for each subnet? Valid hosts?The following table shows some examples of the host ranges—the first three and the last subnets:Practice Example #3A: 255.255.255.192 (/26)Let’s do one more example using the second, third, and fourth octets for subnetting. Subnets? 218 = 262,144. Hosts? 26 – 2 = 62. Valid subnets? In the second and third octet, the block size is 1, and in the fourth octet, the block size is 64. Broadcast address for each subnet? Valid hosts?Subnet10.0.0.010.1.0.0…10.254.0.010.255.0.0First host10.0.0.110.1.0.1…10.254.0.110.255.0.1Last host10.0.255.25410.1.255.254…10.254.255.25410.255.255.254Broadcast10.0.255.25510.1.255.255…10.254.255.25510.255.255.255Subnet10.0.0.010.0.16.010.0.32.0…10.255.240.0First host10.0.0.110.0.16.110.0.32.1…10.255.240.1Last host10.0.15.25410.0.31.25410.0.47.254…10.255.255.254Broadcast10.0.15.25510.0.31.25510.0.47.255…10.255.255.255

10089c03.fm  Page 135  Thursday, August 30, 2007  12:33 PM




136Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPThe following table shows the first four subnets and their valid hosts and broadcast addresses in the Class A 255.255.255.192 mask:The following table shows the last four subnets and their valid hosts and broadcast addresses:Subnetting in Your Head: Class A AddressesThis sounds hard, but as with Class C and Class B, the numbers are the same; we just start in the second octet. What makes this easy? You only need to worry about the octet that has the largest block size (typically called the interesting octet; one that is something other than 0 or 255)—for example, 255.255.240.0 (/20) with a Class A network. The second octet has a block size of 1, so any number listed in that octet is a subnet. The third octet is a 240 mask, which means we have a block size of 16 in the third octet. If your host ID is 10.20.80.30, what is your subnet, broadcast address, and valid host range?The subnet in the second octet is 20 with a block size of 1, but the third octet is in block sizes of 16, so we’ll just count them out: 0, 16, 32, 48, 64, 80, 96…voilà! (By the way, you can count by 16s by now, right?) This makes our subnet 10.20.80.0, with a broadcast of 10.20.95.255 because the next subnet is 10.20.96.0. The valid host range is 10.20.80.1 through 10.20.95.254. And yes, no lie! You really can do this in your head if you just get your block sizes nailed!Okay, let’s practice on one more, just for fun!Host IP: 10.1.3.65/23First, you can’t answer this question if you don’t know what a /23, is. It’s 255.255.254.0. The interesting octet here is the third one: 256 – 254 = 2. Our subnets in the third octet are 0, 2, 4, 6, etc. The host in this question is in subnet 2.0, and the next subnet is 4.0, so that makes the broadcast address 3.255. And any address between 10.1.2.1 and 10.1.3.254 is considered a valid host.Subnet10.0.0.010.0.0.6410.0.0.12810.0.0.192First host10.0.0.110.0.0.6510.0.0.12910.0.0.193Last host10.0.0.6210.0.0.12610.0.0.19010.0.0.254Broadcast10.0.0.6310.0.0.12710.0.0.19110.0.0.255Subnet10.255.255.010.255.255.6410.255.255.12810.255.255.192First host10.255.255.110.255.255.6510.255.255.12910.255.255.193Last host10.255.255.6210.255.255.12610.255.255.19010.255.255.254Broadcast10.255.255.6310.255.255.12710.255.255.19110.255.255.255

10089c03.fm  Page 136  Thursday, August 30, 2007  12:33 PM




Variable Length Subnet Masks (VLSMs)137Variable Length Subnet Masks (VLSMs)I could easily devote an entire chapter to Variable Length Subnet Masks (VLSMs), but instead I’m going to show you a simple way to take one network and create many networks using sub-net masks of different lengths on different types of network designs. This is called VLSM net-working, and it does bring up another subject I mentioned at the beginning of this chapter: classful and classless networking.Neither RIPv1 nor IGRP routing protocols have a field for subnet information, so the sub-net information gets dropped. What this means is that if a router running RIP has a subnet mask of a certain value, it assumes that all interfaces within the classful address space have the same subnet mask. This is called classful routing, and RIP and IGRP are both considered class-ful routing protocols. (I’ll be talking more about RIP and IGRP in Chapter 6, “IP Routing.”) If you mix and match subnet mask lengths in a network running RIP or IGRP, that network just won’t work!Classless routing protocols, however, do support the advertisement of subnet information. Therefore, you can use VLSM with routing protocols such as RIPv2, EIGRP, and OSPF. (EIGRP and OSPF will be discussed in Chapter 7.) The benefit of this type of network is that you save a bunch of IP address space with it.As the name suggests, with VLSMs we can have different subnet masks for different router interfaces. Look at Figure 3.3 to see an example of why classful network designs are inefficient.Looking at this figure, you’ll notice that we have two routers, each with two LANs and con-nected together with a WAN serial link. In a typical classful network design (RIP or IGRP routing protocols), you could subnet a network like this:192.168.10.0 = Network255.255.255.240 (/28) = MaskFIGURE3.3Typical classful network

.34 

.35 

.66 

.67 

192.168.10.32/28 

.18 

.19 

.2 

.3 

.65 .50 .49 .33 192.168.10.64/28 192.168.10.16/28 192.168.10.0/28 .1 .17 (6 hosts) (10 hosts) (2 hosts) 192.168.10.48/28 (25 hosts) (12 hosts) 

10089c03.fm  Page 137  Thursday, August 30, 2007  12:33 PM




 138 Chapter3  Subnetting, VLSMs, and Troubleshooting TCP/IP Our subnets would be (you know this part, right?) 0, 16, 32, 48, 64, 80, etc. This allows us to assign 16 subnets to our internetwork. But how many hosts would be available on each net-work? Well, as you probably know by now, each subnet provides only 14 hosts. This means that each LAN has 14 valid hosts available—one LAN doesn’t even have enough addresses needed for all the hosts! But the point-to-point WAN link also has 14 valid hosts. It’s too bad we can’t just nick some valid hosts from that WAN link and give them to our LANs! All hosts and router interfaces have the same subnet mask—again, this is called classful routing. And if we want this network to be more efficient, we definitely need to add different masks to each router interface.But there’s still another problem—the link between the two routers will never use more than two valid hosts! This wastes valuable IP address space, and it’s the big reason I’m going to talk to you about VLSM network design. VLSM Design Let’s take Figure 3.3 and use a classless design…which will become the new network shown in Figure 3.4. In the previous example, we wasted address space—one LAN didn’t have enough addresses because every router interface and host used the same subnet mask. Not so good. What would be good is to provide only the needed number of hosts on each router interface. To do this, we use what are referred to as Variable Length Subnet Masks (VLSMs).Now remember that we can use different size masks on each router interface. And if we use a /30 on our WAN links and a /27, /28, and /29 on our LANs, we’ll get 2 hosts per WAN inter-face, and 30, 14, and 6 hosts per LAN interface—nice! This makes a huge difference—not only can we get just the right amount of hosts on each LAN, we still have room to add more WANs and LANs using this same network! FIGURE3.4 Classless network design

.66

.67

.50

.51

192.168.10.64/29

.45

.35

.2

.3

.49.74.73.65192.168.10.48/28192.168.10.32/28192.168.10.0/27.1.33(6 hosts)(10 hosts)(2 hosts)192.168.10.72/30

(25 hosts)(12 hosts)

 

10089c03.fm  Page 138  Wednesday, February 27, 2008  4:49 PM




Variable Length Subnet Masks (VLSMs)139

Remember, in order to implement a VLSM design on your network, you need to have a routing protocol that sends subnet mask information with the route updates. This would be RIPv2, EIGRP, and OSPF. RIPv1 and IGRP will not work in classless networks and are considered classful routing protocols.Implementing VLSM NetworksTo create VLSMs quickly and efficiently, you need to understand how block sizes and charts work together to create the VLSM masks. Table 3.3 shows you the block sizes used when 

Why Bother with VLSM Design?You have just been hired by a new company and need to add on to the existing network. There is no problem with starting over with a new IP address scheme. Should you use a VLSM class-less network or a classful network?Let’s just say you happen to have plenty of address space because you are using the Class A 10.0.0.0 private network address in your corporate environment and can’t even come close to imagining that you’d ever run out of IP addresses. Why would you want to bother with the VLSM design process?Good question. There’s a good answer too!Because by creating contiguous blocks of addresses to specific areas of your network, you can then easily summarize your network and keep route updates with a routing protocol to a minimum. Why would anyone want to advertise hundreds of networks between buildings when you can just send one summary route between buildings and achieve the same result?If you’re confused about what summary routes are, let me explain. Summarization, also called supernetting, provides route updates in the most efficient way possible by advertising many routes in one advertisement instead of individually. This saves a ton of bandwidth and minimizes router processing. As always, you use blocks of addresses (remember that block sizes are used in all sorts of networks) to configure your summary routes and watch your net-work’s performance hum.But know that summarization works only if you design your network carefully. If you care-lessly hand out IP subnets to any location on the network, you’ll notice straight away that you no longer have any summary boundaries. And you won’t get very far with creating summary routes without those, so watch your step!

10089c03.fm  Page 139  Thursday, August 30, 2007  12:33 PM




140Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPcreating VLSMs with Class C networks. For example, if you need 25 hosts, then you’ll need a block size of 32. If you need 11 hosts, you’ll use a block size of 16. Need 40 hosts? Then you’ll need a block of 64. You cannot just make up block sizes—they’ve got to be the block sizes shown in Table 3.3. So memorize the block sizes in this table—it’s easy. They’re the same numbers we used with subnetting!The next step is to create a VLSM table. Figure 3.5 shows you the table used in creating a VLSM network. The reason we use this table is so we don’t accidentally overlap networks.You’ll find the sheet shown in Figure 3.5 very valuable because it lists every block size you can use for a network address. Notice that the block sizes are listed starting from a block size of 4 all the way to a block size of 128. If you have two networks with block sizes of 128, you’ll quickly see that you can have only two networks. With a block size of 64, you can have only four networks, and so on, all the way to having 64 networks if you use only block sizes of 4. Remember that this takes into account that you are using the command ip subnet-zero in your network design.Now, just fill in the chart in the lower-left corner, and then add the subnets to the work-sheet and you’re good to go.So let’s take what we’ve learned so far about our block sizes and VLSM table and create a VLSM using a Class C network address 192.168.10.0 for the network in Figure 3.6. Then fill out the VLSM table, as shown in Figure 3.7.In Figure 3.6, we have four WAN links and four LANs connected together. We need to create a VLSM network that will allow us to save address space. Looks like we have two block sizes of 32, a block size of 16, and a block size of 8, and our WANs each have a block size of 4. Take a look and see how I filled out our VLSM chart in Figure 3.7.TABLE3.3Block SizesPrefixMaskHostsBlock Size/25128126128/261926264/272243032/282401416/2924868/3025224

10089c03.fm  Page 140  Thursday, August 30, 2007  12:33 PM




 Variable Length Subnet Masks (VLSMs) 141 FIGURE3.5 The VLSM table

Variable Length Subnet Masks Worksheet

SubnetMaskSubnetsHostsBlock/26/27/28/29/30192224240248252481632646230146264/25128212612832168404812162024283236404448525660646872768084889296100104108112116120124128132136140144148152156160154158172176180184188192196200204208212216220224228232236240244248252256

Class C Network           192.168.10.0NetworkHostsBlockSubnetMaskABCDEFGHIJKL

 

10089c03.fm  Page 141  Friday, November 7, 2008  10:43 PM




142Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPFIGURE3.6VLSM network example 1We still have plenty of room for growth with this VLSM network design.We never could accomplish that with one subnet mask using classful routing. Let’s do another one. Figure 3.8 shows a network with 11 networks, two block sizes of 64, one of 32, five of 16, and three of 4.First, create your VLSM table and use your block size chart to fill in the table with the sub-nets you need. Figure 3.9 shows a possible solution.Notice that we filled in this entire chart and only have room for one more block size of 4! Only with a VLSM network can you provide this type of address space savings.Keep in mind that it doesn’t matter where you start your block sizes as long as you always count from zero. For example, if you had a block size of 16, you must start at 0 and count from there—0, 16, 32, 48, etc. You can’t start a block size of 16 from, say, 40 or anything other than increments of 16.Here’s another example. If you had block sizes of 32, you must start at zero like this: 0, 32, 64, 96, etc. Just remember that you don’t get to start wherever you want; you must always start counting from zero. In the example in Figure 3.9, I started at 64 and 128, with my two block sizes of 64. I didn’t have much choice, because my options are 0, 64, 128, and 192. However, I added the block size of 32, 16, 8, and 4 wherever I wanted just as long as they were in the correct increments of that block size.Okay—you have three locations you need to address, and the IP network you have received is 192.168.55.0 to use as the addressing for the entire network. You’ll use ip subnet-zero and RIPv2 as the routing protocol. (RIPv2 supports VLSM networks, RIPv1 does not—both of them will be discussed in Chapter 6.) Figure 3.10 shows the network diagram and the IP address of the RouterA S0/0 interface.

192.168.10.112/302 hostsNetwork H

Lab_D

Lab_A

F0/0

F0/0

192.168.10.8/29

Lab_E

Lab_B

F0/0

F0/0

192.168.10.32/27192.168.10.104/302 hostsNetwork F192.168.10.16/28192.168.10.64/27

30 hostsNetwork B

20 hostsNetwork C

6 hostsNetwork D

14 hostsNetwork A192.168.10.100/302 hostsNetwork E2 hostsNetwork G192.168.10.108/30

10089c03.fm  Page 142  Thursday, August 30, 2007  12:33 PM




 Variable Length Subnet Masks (VLSMs) 143 FIGURE3.7 A VLSM table, example one

Variable Length Subnet Masks Worksheet

SubnetMaskSubnetsHostsBlock/26/27/28/29/3019222424024825248163264623014626432168404812162024283236404448525660646872768084889296100104108112116120124128132136140144148152156160154158172176180184188192196200204208212216220224228232236240244248252256

Class C Network           192.168.10.0NetworkHostsBlockSubnetMaskABCDEFGH1430206222216323284444/28/27/27/29/30/30/30/30240224224248252252252252

E - 192.168.10.96/30F - 192.168.10.100/30G - 192.168.10.104/30H - 192.168.10.108/30D - 192.168.10.8/29A - 192.168.10.16/28B - 192.168.10.32/27C - 192.168.10.64/27

 

10089c03.fm  Page 143  Thursday, February 28, 2008  7:13 AM




144Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPFIGURE3.8VLSM network example 2From the list of IP addresses on the right of the figure, which IP address will be placed in each router’s FastEthernet 0/0 interface and serial 0/1 of RouterB?To answer this question, first look for clues in Figure 3.10. The first clue is that interface S0/0 on RouterA has IP address 192.168.55.2/30 assigned, which makes for an easy answer. A /30, as you know, is 255.255.255.252, which gives you a block size of 4. Your subnets are 0, 4, 8, etc. Since the known host has an IP address of 2, the only other valid host in the zero subnet is 1, so the third answer down is what you want for the s0/1 interface of RouterB.The next clues are the listed number of hosts for each of the LANs. RouterA needs 7 hosts, a block size of 16 (/28); RouterB needs 90 hosts, a block size of 128 (/25); and RouterC needs 23 hosts, a block size of 32 (/27).Figure 3.11 shows the answers to this question.Once you figured out the block size needed for each LAN, this was actually a pretty simple question—all you need to do is look for the right clues and, of course, know your block sizes.One last example of VLSM design before we move on to summarization. Figure 3.12 shows three routers, all running RIPv2. Which class C addressing scheme would you use to satisfy the needs of this network yet save as much address space as possible?This is a really sweet network, just waiting for you to fill out the chart. There are block sizes of 64, 32, and 16 and two block sizes of 4. This should be a slam dunk for you. Take a look at my answer in Figure 3.13.This is what I did: Starting at subnet 0, I used the block size of 64. (I didn’t have to—I could have started with a block size of 4, but I usually like to start with the largest block size and move to the smallest.) Okay, then I added the block sizes of 32 and 16 and the two block sizes of 4. There’s still a lot of room to add subnets to this network—very cool!

Corp

SF

Fa0/1

Fa0/0

Fa0/0

Fa0/1

Fa0/0

Fa0/1

Fa0/3

Fa0/0

Bldg1

NY

Fa0/2

Fa0/0Net = B10 hostsNet = C12 hosts2 hostsNet = D12 hostsNet = G2 hostsNet = E2 hostsNet = F

30 hostsNet = A60 hostsNet = H14 hostsNet = I60 hostsNet = J8 hostsNet = KA: /27B: /28C: /28D: /30E: /30F: /30G: /28H: /26I: /28J: /26K: /28

10089c03.fm  Page 144  Thursday, August 30, 2007  12:33 PM




 Variable Length Subnet Masks (VLSMs) 145 FIGURE3.9 VLSM table example 2

Variable Length Subnet Masks Worksheet

SubnetMaskSubnetsHostsBlock/26/27/28/29/3019222424024825248163264623014626432168404812162024283236404448525660646872768084889296100104108112116120124128132136140144148152156160154158172176180184188192196200204208212216220224228232236240244248252256

Class C Network           192.168.10.0NetworkHostsBlockSubnetMaskABCDEFGHIJK3010122221260146083216164441664166416320 1624424825220864192128224224240240252252252240192240192240

B - 192.168.10.0/28C - 192.168.10.16/28A - 192.168.10.32/27H - 192.168.10.64/26J - 192.168.10.128/26I - 192.168.10.192/28G - 192.168.10.208/28K - 192.168.10.224/28D - 192.168.10.244/30E - 192.168.10.248/30F - 192.168.10.252/30

 

10089c03.fm  Page 145  Wednesday, February 27, 2008  4:43 PM




146Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPFIGURE3.10VLSM design example 1FIGURE3.11Solution to VLSM design example 1FIGURE3.12VLSM design example 2

RouterA

7 hosts

RouterB

90 hosts

192.168.55.57/27192.168.55.29/28192.168.55.1/30192.168.55.132/25192.168.55.0/30192.168.55.127/26S0/0

RouterC

23 hosts

F0/0: 

S0/1:

F0/0: 

F0/0: 

192.168.55.2/30

RouterA

7 hosts

RouterB

90 hosts

S0/0

RouterC

23 hosts

F0/0:192.168.55.29/28 F0/0:192.168.55.132/25F0/0:192.168.55.57/27S0/1: 192.168.55.1/30192.168.55.2/30

4: Serial 1 5: Serial 2 

60 hosts Net 1 

30 hosts Net 2 

12 hosts Net 3 

10089c03.fm  Page 146  Thursday, August 30, 2007  12:33 PM




Summarization147FIGURE3.13Solution to VLSM design example 2SummarizationSummarization, also called route aggregation, allows routing protocols to advertise many net-works as one address. The purpose of this is to reduce the size of routing tables on routers to save memory, which also shortens the amount of time for IP to parse the routing table and find the path to a remote network.Figure 3.14 shows how a summary address would be used in an internetwork.FIGURE3.14Summary address used in an internetworkSummarization is actually somewhat simple because all you really need to have down are the block sizes that we just used in learning subnetting and VLSM design. For example, if you wanted to summarize the following networks into one network advertisement, you just have to find the block size first; then you can easily find your answer:192.168.16.0 through network 192.168.31.0

1: 192.168.10.0/26 2: 192.168.10.64/27 3: 192.168.10.96/28 4: 192.168.10.112/305: 192.168.10.116/30-chart cut in interest of brevity- 04812162024283236404448525660646872768084889296100104108112116120124128

10.0.0.0/1610.1.0.0/1610.2.0.0/16…10.255.0.0/16

10.0.0.0/8

10089c03.fm  Page 147  Thursday, August 30, 2007  12:33 PM




148Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPWhat’s the block size? There are exactly 16 Class C networks, so this neatly fits into a block size of 16.Okay, now that you know the block size, you can find the network address and mask used to summarize these networks into one advertisement. The network address used to advertise the summary address is always the first network address in the block—in this example, 192.168.16.0. To figure out a summary mask, in this same example, what mask is used to get a block size of 16? Yes, 240 is correct. This 240 would be placed in the third octet—the octet where we are summarizing. So, the mask would be 255.255.240.0.

You’ll learn how to apply these summary addresses to a router in Chapter 7.Here’s another example:Networks 172.16.32.0 through 172.16.50.0This is not as clean as the previous example because there are two possible answers, and here’s why: Since you’re starting at network 32, your options for block sizes are 4, 8, 16, 32, 64, etc., and block sizes of 16 and 32 could work as this summary address. Answer #1: If you used a block size of 16, then the network address is 172.16.32.0 with a mask of 255.255.240.0 (240 provides a block of 16). However, this only summarizes from 32 to 47, which means that networks 48 through 50 would be advertised as single networks. This is probably the best answer, but that depends on your network design. Let’s look at the next answer. Answer #2: If you used a block size of 32, then your summary address would still be 172.16.32.0, but the mask would be 255.255.224.0 (224 provides a block of 32). The possible problem with this answer is that it will summarize networks 32 to 63 and we only have networks 32 to 50. No worries if you’re planning on adding networks 51 to 63 later into the same network, but you could have serious problems in your internetwork if some-how networks 51 to 63 were to show up and be advertised from somewhere else in your network! This is the reason why answer number one is the safest answer.Let’s take a look at another example, but let’s look at it from a host’s perspective.Your summary address is 192.168.144.0/20—what’s the range of host addresses that would be forwarded according to this summary? The /20 provides a summary address of 192.168.144.0 and mask of 255.255.240.0.The third octet has a block size of 16, and starting at summary address 144, the next block of 16 is 160, so our network summary range is 144 to 159 in the third octet (again, you must be able to count in 16s!).A router that has this summary address in the routing table will forward any packet with destination IP addresses of 192.168.144.1 through 192.168.159.254.Only two more summarization examples, then we’ll move on to troubleshooting.In Figure 3.15, the Ethernet networks connected to router R1 are being summarized to R2 as 192.168.144.0/20. Which range of IP addresses will R2 forward to R1 according to this summary?

10089c03.fm  Page 148  Thursday, August 30, 2007  12:33 PM




Summarization149FIGURE3.15Summarization example 1No worries—this is really an easier question than it looks. The question actually has the summary address listed: 192.168.144.0/20. You already know that /20 is 255.255.240.0, which means you’ve got a block size of 16 in the third octet. Starting at 144 (this is also right there in the question), the next block size of 16 is 160, so you can’t go above 159 in the third octet. The IP addresses that will be forwarded are 192.168.144.1 through 192.168.159.255. (Yes, the broadcast address is forwarded.)Okay, last one. In Figure 3.16, there are five networks connected to router R1. What’s the best summary address to R2?FIGURE3.16Summarization example 2I’m going to be honest—this is a much harder question than the one in Figure 3.15. You’re going to have to look pretty hard to see the answer. The first thing to do with this is to write down all the networks and see if you can find anything in common with all six:

The Ethernet networks connected to router R1 are being summarized to R2 as 192.168.144.0/20. Which IP addresses will R2 forward to R1 according to this summary?  192.168.144.0/20 

R1R2

172.1.4.128/25 What is the best summary to R2?172.1.4.0/25 172.1.6.0/24 172.1.5.0/24 172.1.7.0/24 

R1R2

10089c03.fm  Page 149  Thursday, August 30, 2007  12:33 PM




 150 Chapter3  Subnetting, VLSMs, and Troubleshooting TCP/IP  172.1.4.128/25  172.1.7.0/24  172.1.6.0/24  172.1.5.0/24  172.1.4.0/25Do you see an octet that looks interesting to you? I do. It’s the third octet. 4, 5, 6, 7, and yes, it’s a block size of 4. So you can summarize 172.1.4.0 using a mask of 255.255.252.0, which means you will use a block size of 4 in the third octet. The IP addresses forwarded with this summary are 172.1.4.1 through 172.1.7.255.Now to summarize this summarization section: Basically, if you’ve nailed down your block sizes, then finding and applying summary addresses and masks is actually fairly easy. But you’re going to get bogged down pretty quickly if you don’t know what a /20 is or if you can’t count by 16s! Troubleshooting IP Addressing Troubleshooting IP addressing is obviously an important skill because running into trouble somewhere along the way is pretty much a sure thing, and it’s going to happen to you. No—I’m not a pessimist; I’m just keeping it real. Because of this nasty fact, it will be great when you can save the day because you can both figure out (diagnose) the problem and fix it on an IP network whether you’re at work or at home!So this is where I’m going to show you the “Cisco way” of troubleshooting IP addressing. Let’s use Figure 3.17 as an example of your basic IP trouble—poor Sally can’t log in to the Win-dows server. Do you deal with this by calling the Microsoft team to tell them their server is a pile of junk and causing all your problems? Probably not such a great idea—let’s first double-check our network instead. FIGURE3.17 Basic IP troubleshooting

Sally172.16.10.2Server172.16.20.2

E0172.16.10.1

 

10089c03.fm  Page 150  Wednesday, February 27, 2008  4:53 PM




Troubleshooting IP Addressing151Okay let’s get started by going over the troubleshooting steps that Cisco follows. They’re pretty simple, but important nonetheless. Pretend you’re at a customer host and they’re com-plaining that they can’t communicate to a server that just happens to be on a remote network. Here are the four troubleshooting steps Cisco recommends:1.Open a DOS window and ping 127.0.0.1. This is the diagnostic, or loopback, address, and if you get a successful ping, your IP stack is considered to be initialized. If it fails, then you have an IP stack failure and need to reinstall TCP/IP on the host.C:\>ping 127.0.0.1Pinging 127.0.0.1 with 32 bytes of data:Reply from 127.0.0.1: bytes=32 time<1ms TTL=128Reply from 127.0.0.1: bytes=32 time<1ms TTL=128Reply from 127.0.0.1: bytes=32 time<1ms TTL=128Reply from 127.0.0.1: bytes=32 time<1ms TTL=128Ping statistics for 127.0.0.1:    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),Approximate round trip times in milli-seconds:

    Minimum = 0ms, Maximum = 0ms, Average = 0ms2.From the DOS window, ping the IP address of the local host. If that’s successful, your net-work interface card (NIC) is functioning. If it fails, there is a problem with the NIC. Suc-cess here doesn’t mean that a cable is plugged into the NIC, only that the IP protocol stack on the host can communicate to the NIC (via the LAN driver).C:\>ping 172.16.10.2Pinging 172.16.10.2 with 32 bytes of data:Reply from 172.16.10.2: bytes=32 time<1ms TTL=128Reply from 172.16.10.2: bytes=32 time<1ms TTL=128Reply from 172.16.10.2: bytes=32 time<1ms TTL=128Reply from 172.16.10.2: bytes=32 time<1ms TTL=128Ping statistics for 172.16.10.2:    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),Approximate round trip times in milli-seconds:

    Minimum = 0ms, Maximum = 0ms, Average = 0ms3.From the DOS window, ping the default gateway (router). If the ping works, it means that the NIC is plugged into the network and can communicate on the local network. If it fails, you have a local physical network problem that could be anywhere from the NIC to the router.C:\>ping 172.16.10.1Pinging 172.16.10.1 with 32 bytes of data:Reply from 172.16.10.1: bytes=32 time<1ms TTL=128Reply from 172.16.10.1: bytes=32 time<1ms TTL=128Reply from 172.16.10.1: bytes=32 time<1ms TTL=128Reply from 172.16.10.1: bytes=32 time<1ms TTL=128

10089c03.fm  Page 151  Thursday, August 30, 2007  12:33 PM




152Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPPing statistics for 172.16.10.1:    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),Approximate round trip times in milli-seconds:

    Minimum = 0ms, Maximum = 0ms, Average = 0ms4.If steps 1 through 3 were successful, try to ping the remote server. If that works, then you know that you have IP communication between the local host and the remote server. You also know that the remote physical network is working.C:\>ping 172.16.20.2Pinging 172.16.20.2 with 32 bytes of data:Reply from 172.16.20.2: bytes=32 time<1ms TTL=128Reply from 172.16.20.2: bytes=32 time<1ms TTL=128Reply from 172.16.20.2: bytes=32 time<1ms TTL=128Reply from 172.16.20.2: bytes=32 time<1ms TTL=128Ping statistics for 172.16.20.2:    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),Approximate round trip times in milli-seconds:

    Minimum = 0ms, Maximum = 0ms, Average = 0msIf the user still can’t communicate with the server after steps 1 through 4 are successful, you probably have some type of name resolution problem and need to check your Domain Name Sys-tem (DNS) settings. But if the ping to the remote server fails, then you know you have some type of remote physical network problem and need to go to the server and work through steps 1 through 3 until you find the snag.Before we move on to determining IP address problems and how to fix them, I just want to mention some basic DOS commands that you can use to help troubleshoot your network from both a PC and a Cisco router (the commands might do the same thing, but they are imple-mented differently).Packet InterNet Groper (ping)Uses ICMP echo request and replies to test if a node IP stack is initialized and alive on the network.tracerouteDisplays the list of routers on a path to a network destination by using TTL time-outs and ICMP error messages. This command will not work from a DOS prompt.tracertSame command as traceroute, but it’s a Microsoft Windows command and will not work on a Cisco router.arp -aDisplays IP-to-MAC-address mappings on a Windows PC.show ip arpSame command as arp -a, but displays the ARP table on a Cisco router. Like the commands traceroute and tracert, they are not interchangeable through DOS and Cisco.ipconfig /allUsed only from a DOS prompt, shows you the PC network configuration.Once you’ve gone through all these steps and used the appropriate DOS commands, if nec-essary, what do you do if you find a problem? How do you go about fixing an IP address con-figuration error? Let’s move on and discuss how to determine the IP address problems and how to fix them.

10089c03.fm  Page 152  Thursday, August 30, 2007  12:33 PM




Troubleshooting IP Addressing153Determining IP Address ProblemsIt’s common for a host, router, or other network device to be configured with the wrong IP address, subnet mask, or default gateway. Because this happens way too often, I’m going to teach you how to both determine and fix IP address configuration errors.Once you’ve worked through the four basic steps of troubleshooting and determined there’s a problem, you obviously then need to find and fix it. It really helps to draw out the network and IP addressing scheme. If it’s already done, consider yourself lucky and go buy a lottery ticket, because although it should be done, it rarely is. And if it is, it’s usually outdated or inaccurate anyway. Typ-ically it is not done, and you’ll probably just have to bite the bullet and start from scratch.

I’ll show you how to draw out your network using CDP in Chapter 5, “Managing the Cisco IOS.”Once you have your network accurately drawn out, including the IP addressing scheme, you need to verify each host’s IP address, mask, and default gateway address to determine the problem. (I’m assuming that you don’t have a physical problem or that if you did, you’ve already fixed it.)Let’s check out the example illustrated in Figure 3.18. A user in the sales department calls and tells you that she can’t get to ServerA in the marketing department. You ask her if she can get to ServerB in the marketing department, but she doesn’t know because she doesn’t have rights to log on to that server. What do you do?FIGURE3.18IP address problem 1

1900

Lab_AF0/27F0/26

F0/0S0/0192.168.1.97/27192.168.1.100/27S0/0DCES0/1DCE

192.168.1.62

2950

Lab_BF0/3F0/2F0/1

F0/0192.168.1.95192.168.1.33Default gateway:192.168.1 62ServerA192.168.1.66Default gateway:192.168.1.95ServerB192.168.1.65Default gateway192.168.1.95MarketingSales

10089c03.fm  Page 153  Thursday, August 30, 2007  12:33 PM




154Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPYou ask the client to go through the four troubleshooting steps that you learned about in the preceding section. Steps 1 through 3 work, but step 4 fails. By looking at the figure, can you determine the problem? Look for clues in the network drawing. First, the WAN link between the Lab_A router and the Lab_B router shows the mask as a /27. You should already know that this mask is 255.255.255.224 and then determine that all networks are using this mask. The net-work address is 192.168.1.0. What are our valid subnets and hosts? 256 – 224 = 32, so this makes our subnets 32, 64, 96, 128, etc. So, by looking at the figure, you can see that subnet 32 is being used by the sales department, the WAN link is using subnet 96, and the marketing department is using subnet 64.Now you’ve got to determine what the valid host ranges are for each subnet. From what you learned at the beginning of this chapter, you should now be able to easily determine the subnet address, broadcast addresses, and valid host ranges. The valid hosts for the Sales LAN are 33 through 62—the broadcast address is 63 because the next subnet is 64, right? For the Marketing LAN, the valid hosts are 65 through 94 (broadcast 95), and for the WAN link, 97 through 126 (broadcast 127). By looking at the figure, you can determine that the default gate-way on the Lab_B router is incorrect. That address is the broadcast address of the 64 subnet, so there’s no way it could be a valid host.Did you get all that? Maybe we should try another one, just to make sure. Figure 3.19 shows a network problem. A user in the Sales LAN can’t get to ServerB. You have the user run through the four basic troubleshooting steps and find that the host can communicate to the local network but not to the remote network. Find and define the IP addressing problem.FIGURE3.19IP address problem 2

1900

Lab_AF0/27F0/26

F0/0S0/0192.168.1.41/29192.168.1.46/29S0/0DCES0/1DCE

192.168.1.30

2950

Lab_BF0/3F0/2F0/1

F0/0192.168.1.81192.168.1.25Default gateway:192.168.1.30ServerA192.168.1.86Default gateway:192.168.1.81ServerB192.168.1.87Default gateway:192.168.1.81MarketingSales

10089c03.fm  Page 154  Thursday, August 30, 2007  12:33 PM




Troubleshooting IP Addressing155If you use the same steps used to solve the last problem, you can see first that the WAN link again provides the subnet mask to use— /29, or 255.255.255.248. You need to determine what the valid subnets, broadcast addresses, and valid host ranges are to solve this problem.The 248 mask is a block size of 8 (256 – 248 = 8), so the subnets both start and increment in multiples of 8. By looking at the figure, you see that the Sales LAN is in the 24 subnet, the WAN is in the 40 subnet, and the Marketing LAN is in the 80 subnet. Can you see the problem yet? The valid host range for the Sales LAN is 25–30, and the configuration appears correct. The valid host range for the WAN link is 41–46, and this also appears correct. The valid host range for the 80 subnet is 81–86, with a broadcast address of 87 because the next subnet is 88. ServerB has been configured with the broadcast address of the subnet.Okay, now that you can figure out misconfigured IP addresses on hosts, what do you do if a host doesn’t have an IP address and you need to assign one? What you need to do is look at other hosts on the LAN and figure out the network, mask, and default gateway. Let’s take a look at a couple of examples of how to find and apply valid IP addresses to hosts.You need to assign a server and router IP addresses on a LAN. The subnet assigned on that segment is 192.168.20.24/29, and the router needs to be assigned the first usable address and the server the last valid host ID. What are the IP address, mask, and default gateway assigned to the server?To answer this, you must know that a /29 is a 255.255.255.248 mask, which provides a block size of 8. The subnet is known as 24, the next subnet in a block of 8 is 32, so the broad-cast address of the 24 subnet is 31, which makes the valid host range 25–30.Server IP address: 192.168.20.30Server mask: 255.255.255.248Default gateway: 192.168.20.25 (router’s IP address)As another example, let’s take a look at Figure 3.20 and solve this problem.FIGURE3.20Find the valid host #1.

RouterA

HostA

E0: 192.168.10.33/27

10089c03.fm  Page 155  Thursday, August 30, 2007  12:33 PM




156Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPLook at the router’s IP address on Ethernet0. What IP address, subnet mask, and valid host range could be assigned to the host?The IP address of the router’s Ethernet0 is 192.168.10.33/27. As you already know, a /27 is a 224 mask with a block size of 32. The router’s interface is in the 32 subnet. The next subnet is 64, so that makes the broadcast address of the 32 subnet 63 and the valid host range 33–62.Host IP address: 192.168.10.34–62 (any address in the range except for 33, which is assigned to the router)Mask: 255.255.255.224Default gateway: 192.168.10.33Figure 3.21 shows two routers with Ethernet configurations already assigned. What are the host addresses and subnet masks of hosts A and B?FIGURE3.21Find the valid host #2RouterA has an IP address of 192.168.10.65/26 and RouterB has an IP address of 192.168.10.33/28. What are the host configurations? RouterA Ethernet0 is in the 192.168.10.64 subnet and RouterB Ethernet0 is in the 192.168.10.32 network.Host A IP address: 192.168.10.66–126Host A mask: 255.255.255.192Host A default gateway: 192.168.10.65Host B IP address: 192.168.10.34–46Host B mask: 255.255.255.240Host B default gateway: 192.168.10.33Just a couple more examples and then this chapter is history. Hang in there!Figure 3.22 shows two routers; you need to configure the S0/0 interface on RouterA. The network assigned to the serial link is 172.16.17.0/22. What IP address can be assigned?First, you must know that a /22 CIDR is 255.255.252.0, which makes a block size of 4 in the third octet. Since 17 is listed, the available range is 16.1 through 19.254; so, for example, the IP address S0/0 could be 172.16.18.255 since that’s within the range.

RouterB

HostB

E0: 192.168.10.33/28

RouterA

HostA

E0: 192.168.10.65/26

10089c03.fm  Page 156  Thursday, August 30, 2007  12:33 PM




Summary157Okay, last one! You have one Class C network ID and you need to provide one usable sub-net per city while allowing enough usable host addresses for each city specified in Figure 3.23. What is your mask?FIGURE3.22Find the valid host address #3Actually, this is probably the easiest thing you’ve done all day! I count 5 subnets needed and the Wyoming office needs 16 users (always look for the network that needs the most hosts). What block size is needed for the Wyoming office? 32. (Remember, you cannot use a block size of 16 because you always have to subtract 2!) What mask provides you with a block size of 32? 224. Bingo! This provides 8 subnets, each with 30 hosts.You’re done, the diva has sung, the chicken has crossed the road…whew! Okay, take a good break (but skip the shot and the beer for now), then come back and go through the written labs and review questions.FIGURE3.23Find the valid subnet mask.SummaryDid you read Chapters 2 and 3 and understand everything on the first pass? If so, that is fantastic—congratulations! The thing is, you probably got lost a couple of times—and as I told you, that’s what usually happens, so don’t stress. Don’t feel bad if you have to read each chapter more than once, or even 10 times, before you’re truly good to go.This chapter provided you with an important understanding of IP subnetting. After reading this chapter, you should be able to subnet IP addresses in your head. You should also know how to design and implement simple VLSM networks.

RouterB

RouterA

S0/0S0/0

172.16.17.0/22

Corporate7 users

Wy.16 users

N.Y.7 users

S.F.13 users

L.A.15 users

10089c03.fm  Page 157  Thursday, August 30, 2007  12:33 PM




158Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPYou should also understand the Cisco troubleshooting methods. You must remember the four steps that Cisco recommends you take when trying to narrow down exactly where a net-work/IP addressing problem is and then know how to proceed systematically in order to fix it. In addition, you should be able to find valid IP addresses and subnet masks by looking at a network diagram.Exam EssentialsRemember the steps to subnet in your head.Understand how IP addressing and subnetting work. First, determine your block size by using the 256-subnet mask math. Then count your subnets and determine the broadcast address of each subnet—it is always the number right before the next subnet. Your valid hosts are the numbers between the subnet address and the broadcast address.Understand the various block sizes.This is an important part of understanding IP address-ing and subnetting. The valid block sizes are always 4, 8, 16, 32, 64, 128, etc. You can deter-mine your block size by using the 256-subnet mask math.Remember the four diagnostic steps.The four simple steps that Cisco recommends for trouble–shooting are ping the loopback address, ping the NIC, ping the default gateway, and ping the remote device.You must be able to find and fix an IP addressing problem.Once you go through the four troubleshooting steps that Cisco recommends, you must be able to determine the IP addressing problem by drawing out the network and finding the valid and invalid hosts addressed in your network.Understand the troubleshooting tools that you can use from your host and a Cisco routerping 127.0.0.1 tests your local IP stack. tracert is a Windows DOS command to track the path a packet takes through an internetwork to a destination. Cisco routers use the command traceroute, or just trace for short. Don’t confuse the Windows and Cisco commands. Although they produce the same output, they don’t work from the same prompts. ipconfig /all will display your PC network configuration from a DOS prompt, and arp -a (again from a DOS prompt) will display IP-to-MAC-address mapping on a Windows PC.Written Labs 3In this section, you’ll complete the following labs to make sure you’ve got the information and concepts contained within them fully dialed in: Lab 3.1: Written Subnet Practice #1 Lab 3.2: Written Subnet Practice #2 Lab 3.3: Written Subnet Practice #3(The answers to the written labs can be found following the answers to the review questions for this chapter.)

10089c03.fm  Page 158  Thursday, August 30, 2007  12:33 PM




Written Labs 3159Written Lab 3.1: Written Subnet Practice #1Write the subnet, broadcast address, and valid host range for question 1 through question 6:1.192.168.100.25/302.192.168.100.37/283.192.168.100.66/274. 192.168.100.17/295.192.168.100.99/266. 192.168.100.99/257.You have a Class B network and need 29 subnets. What is your mask?8.What is the broadcast address of 192.168.192.10/29?9.How many hosts are available with a Class C /29 mask?10. What is the subnet for host ID 10.16.3.65/23?Written Lab 3.2: Written Subnet Practice Given a Class B network and the net bits identified (CIDR), complete the following table to identify the subnet mask and the number of host addresses possible for each mask.Classful AddressSubnet MaskNumber of Hosts per Subnet (2x – 2)/16  /17  /18  /19  /20  /21  /22  /23  /24  /25  /26  /27  /28  /29  /30  

10089c03.fm  Page 159  Thursday, August 30, 2007  12:33 PM




160Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPWritten Lab 3.3: Written Subnet Practice(The answers to the written labs can be found following the answers to the review questions for this chapter.)Decimal IP AddressAddress ClassNumber of Subnet and Host BitsNumber of Subnets (2x)Number of Hosts (2x – 2)10.25.66.154/23    172.31.254.12/24    192.168.20.123/28    63.24.89.21/18    128.1.1.254/20    208.100.54.209/30    

10089c03.fm  Page 160  Thursday, August 30, 2007  12:33 PM




Review Questions161Review Questions

The following questions are designed to test your understanding of this chapter’s material. For more information on how to get additional ques-tions, please see this book’s Introduction.1.What is the maximum number of IP addresses that can be assigned to hosts on a local subnet that uses the 255.255.255.224 subnet mask?A.14B.15C.16D.30E.31F.622.You have a network that needs 29 subnets while maximizing the number of host addresses available on each subnet. How many bits must you borrow from the host field to provide the correct subnet mask?A.2B.3C.4D.5E.6F.73.What is the subnetwork address for a host with the IP address 200.10.5.68/28?A.200.10.5.56B.200.10.5.32C.200.10.5.64D.200.10.5.04.The network address of 172.16.0.0/19 provides how many subnets and hosts?A.7 subnets, 30 hosts eachB.7 subnets, 2,046 hosts eachC.7 subnets, 8,190 hosts eachD.8 subnets, 30 hosts eachE.8 subnets, 2,046 hosts eachF.8 subnets, 8,190 hosts each

10089c03.fm  Page 161  Thursday, August 30, 2007  12:33 PM




162Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IP5.Which two statements describe the IP address 10.16.3.65/23? (Choose two.)A.The subnet address is 10.16.3.0 255.255.254.0.B.The lowest host address in the subnet is 10.16.2.1 255.255.254.0.C.The last valid host address in the subnet is 10.16.2.254 255.255.254.0.D.The broadcast address of the subnet is 10.16.3.255 255.255.254.0.E.The network is not subnetted.6.If a host on a network has the address 172.16.45.14/30, what is the subnetwork this host belongs to?A.172.16.45.0B.172.16.45.4C.172.16.45.8D.172.16.45.12E.172.16.45.167.On a VLSM network, which mask should you use on point-to-point WAN links in order to reduce the waste of IP addresses?A./27B./28C./29D./30E./318.What is the subnetwork number of a host with an IP address of 172.16.66.0/21?A.172.16.36.0B.172.16.48.0C.172.16.64.0D.172.16.0.09.You have an interface on a router with the IP address of 192.168.192.10/29. Including the router interface, how many hosts can have IP addresses on the LAN attached to the router interface?A.6B.8C.30D.62E.126

10089c03.fm  Page 162  Thursday, August 30, 2007  12:33 PM




Review Questions16310.You need to configure a server that is on the subnet 192.168.19.24/29. The router has the first available host address. Which of the following should you assign to the server?A.192.168.19.0 255.255.255.0B.192.168.19.33 255.255.255.240C.192.168.19.26 255.255.255.248D.192.168.19.31 255.255.255.248E.192.168.19.34 255.255.255.24011.You have an interface on a router with the IP address of 192.168.192.10/29. What is the broadcast address the hosts will use on this LAN?A.192.168.192.15B.192.168.192.31C.192.168.192.63D.192.168.192.127E.192.168.192.25512.You need to subnet a network that has 5 subnets, each with at least 16 hosts. Which classful subnet mask would you use?A.255.255.255.192B.255.255.255.224C.255.255.255.240D.255.255.255.24813.A network administrator is connecting hosts A and B directly through their Ethernet interfaces, as shown in the illustration. Ping attempts between the hosts are unsuccessful. What can be done to provide connectivity between the hosts? (Choose two.)A.A crossover cable should be used in place of the straight-through cable.B.A rollover cable should be used in place of the straight-through cable.C.The subnet masks should be set to 255.255.255.192.D.A default gateway needs to be set on each host.E.The subnet masks should be set to 255.255.255.0.

IP Address: 192.168.1.20 Mask 255.255.255.240 

IP Address: 192.168.1.201 Mask 255.255.255.240 

Straight-through Cable

10089c03.fm  Page 163  Thursday, August 30, 2007  12:33 PM




164Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IP14.If an Ethernet port on a router were assigned an IP address of 172.16.112.1/25, what would be the valid subnet address of this host?A.172.16.112.0B.172.16.0.0C.172.16.96.0D.172.16.255.0E.172.16.128.015.Using the following illustration, what would be the IP address of E0 if you were using the eighth subnet? The network ID is 192.168.10.0/28 and you need to use the last available IP address in the range. The zero subnet should not be considered valid for this question.A.192.168.10.142B.192.168.10.66C.192.168.100.254D.192.168.10.143E.192.168.10.12616.Using the illustration from the previous question, what would be the IP address of S0 if you were using the first subnet? The network ID is 192.168.10.0/28 and you need to use the last available IP address in the range. Again, the zero subnet should not be considered valid for this question.A.192.168.10.24B.192.168.10.62C.192.168.10.30D.192.168.10.127

RouterS0E0

10089c03.fm  Page 164  Thursday, August 30, 2007  12:33 PM




Review Questions16517.Which configuration command must be in effect to allow the use of 8 subnets if the Class C subnet mask is 255.255.255.224?A.Router(config)#ip classlessB.Router(config)#ip version 6C.Router(config)#no ip classfulD.Router(config)#ip unnumberedE.Router(config)#ip subnet-zeroF.Router(config)#ip all-nets18.You have a network with a subnet of 172.16.17.0/22. Which is the valid host address?A.172.16.17.1 255.255.255.252B.172.16.0.1 255.255.240.0C.172.16.20.1 255.255.254.0D.172.16.16.1 255.255.255.240E.172.16.18.255 255.255.252.0F.172.16.0.1 255.255.255.019.Your router has the following IP address on Ethernet0: 172.16.2.1/23. Which of the following can be valid host IDs on the LAN interface attached to the router? (Choose two.)A.172.16.0.5B.172.16.1.100C.172.16.1.198D.172.16.2.255E.172.16.3.0F.172.16.3.25520.To test the IP stack on your local host, which IP address would you ping?A.127.0.0.0B.1.0.0.127C.127.0.0.1D.127.0.0.255E.255.255.255.255

10089c03.fm  Page 165  Thursday, August 30, 2007  12:33 PM




166Chapter3 Subnetting, VLSMs, and Troubleshooting TCP/IPAnswers to Review Questions1.D. A /27 (255.255.255.224) is 3 bits on and 5 bits off. This provides 8 subnets, each with 30 hosts. Does it matter if this mask is used with a Class A, B, or C network address? Not at all. The number of host bits would never change.2.D. A 240 mask is 4 subnet bits and provides 16 subnets, each with 14 hosts. We need more sub-nets, so let’s add subnet bits. One more subnet bit would be a 248 mask. This provides 5 subnet bits (32 subnets) with 3 host bits (6 hosts per subnet). This is the best answer.3.C. This is a pretty simple question. A /28 is 255.255.255.240, which means that our block size is 16 in the fourth octet. 0, 16, 32, 48, 64, 80, etc. The host is in the 64 subnet.4.F. A CIDR address of /19 is 255.255.224.0. This is a Class B address, so that is only 3 subnet bits, but it provides 13 host bits, or 8 subnets, each with 8,190 hosts.5.B, D. The mask 255.255.254.0 (/23) used with a Class A address means that there are 15 subnet bits and 9 host bits. The block size in the third octet is 2 (256 – 254). So this makes the subnets in the interesting octet 0, 2, 4, 6, etc., all the way to 254. The host 10.16.3.65 is in the 2.0 subnet. The next subnet is 4.0, so the broadcast address for the 2.0 subnet is 3.255. The valid host addresses are 2.1 through 3.254.6.D. A /30, regardless of the class of address, has a 252 in the fourth octet. This means we have a block size of 4 and our subnets are 0, 4, 8, 12, 16, etc. Address 14 is obviously in the 12 subnet.7.D. A point-to-point link uses only two hosts. A /30, or 255.255.255.252, mask provides two hosts per subnet.8.C. A /21 is 255.255.248.0, which means we have a block size of 8 in the third octet, so we just count by 8 until we reach 66. The subnet in this question is 64.0. The next subnet is 72.0, so the broadcast address of the 64 subnet is 71.255.9.A. A /29 (255.255.255.248), regardless of the class of address, has only 3 host bits. Six hosts is the maximum number of hosts on this LAN, including the router interface.10.C. A /29 is 255.255.255.248, which is a block size of 8 in the fourth octet. The subnets are 0, 8, 16, 24, 32, 40, etc. 192.168.19.24 is the 24 subnet, and since 32 is the next subnet, the broadcast address for the 24 subnet is 31. 192.168.19.26 is the only correct answer.11.A. A /29 (255.255.255.248) has a block size of 8 in the fourth octet. This means the subnets are 0, 8, 16, 24, etc. 10 is in the 8 subnet. The next subnet is 16, so 15 is the broadcast address.12.B. You need 5 subnets, each with at least 16 hosts. The mask 255.255.255.240 provides 16 subnets with 14 hosts—this will not work. The mask 255.255.255.224 provides 8 subnets, each with 30 hosts. This is the best answer.13.A, E. First, if you have two hosts directly connected, as shown in the graphic, then you need a crossover cable. A straight-through cable won’t work. Second, the hosts have different masks, which puts them in different subnets. The easy solution is just to set both masks to 255.255.255.0 (/24).

10089c03.fm  Page 166  Thursday, August 30, 2007  12:33 PM




Answers to Review Questions16714.A. A /25 mask is 255.255.255.128. Used with a Class B network, the third and fourth octets are used for subnetting with a total of 9 subnet bits, 8 bits in the third octet and 1 bit in the fourth octet. Since there is only 1 bit in the fourth octet, the bit is either off or on—which is a value of 0 or 128. The host in the question is in the 0 subnet, which has a broadcast address of 127 since 128 is the next subnet.15.A. A /28 is a 255.255.255.240 mask. Let’s count to the ninth subnet (we need to find the broadcast address of the eighth subnet, so we need to count to the ninth subnet). Starting at 16 (remember, the question stated that we will not use subnet zero, so we start at 16, not 0), 16, 32, 48, 64, 80, 96, 112, 128, 144. The eighth subnet is 128 and the next subnet is 144, so our broadcast address of the 128 subnet is 143. This makes the host range 129–142. 142 is the last valid host.16.C. A /28 is a 255.255.255.240 mask. The first subnet is 16 (remember that the question stated not to use subnet zero) and the next subnet is 32, so our broadcast address is 31. This makes our host range 17–30. 30 is the last valid host.17.E. A Class C subnet mask of 255.255.255.224 is 3 bits on and 5 bits off (11100000) and pro-vides 8 subnets, each with 30 hosts. However, if the command ip subnet-zero is not used, then only 6 subnets would be available for use.18.E. A Class B network ID with a /22 mask is 255.255.252.0, with a block size of 4 in the third octet. The network address in the question is in subnet 172.16.16.0 with a broadcast address of 172.16.19.255. Only option E even has the correct subnet mask listed, and 172.16.18.255 is a valid host.19.D, E. The router’s IP address on the E0 interface is 172.16.2.1/23, which is 255.255.254.0. This makes the third octet a block size of 2. The router’s interface is in the 2.0 subnet, and the broadcast address is 3.255 because the next subnet is 4.0. The valid host range is 2.1 through 3.254. The router is using the first valid host address in the range.20.C. To test the local stack on your host, ping the loopback interface of 127.0.0.1.

10089c03.fm  Page 167  Thursday, August 30, 2007  12:33 PM




 168 Chapter3  Subnetting, VLSMs, and Troubleshooting TCP/IP Answers to Written Lab 3.1 1. 192.168.100.25/30. A /30 is 255.255.255.252. The valid subnet is 192.168.100.24, broadcast is 192.168.100.27, and valid hosts are 192.168.100.25 and 26. 2. 192.168.100.37/28. A /28 is 255.255.255.240. The fourth octet is a block size of 16. Just count by 16s until you pass 37. 0, 16, 32, 48. The host is in the 32 subnet, with a broad-cast address of 47. Valid hosts 33–46. 3. 192.168.100.66/27. A /27 is 255.255.255.224. The fourth octet is a block size of 32. Count by 32s until you pass the host address of 66. 0, 32, 64, 96. The host is in the 64 subnet, broadcast address of 95. Valid host range of 65-94. 4. 192.168.100.17/29. A /29 is 255.255.255.248. The fourth octet is a block size of 8. 0, 8, 16, 24. The host is in the 16 subnet, broadcast of 23. Valid hosts 17–22. 5. 192.168.100.99/26. A /26 is 255.255.255.192. The fourth octet has a block size of 64. 0, 64, 128. The host is in the 64 subnet, broadcast of 127. Valid hosts 65–126. 6. 192.168.100.99/25. A /25 is 255.255.255.128. The fourth octet is a block size of 128. 0, 128. The host is in the 0 subnet, broadcast of 127. Valid hosts 1–126. 7. A default Class B is 255.255.0.0. A Class B 255.255.255.0 mask is 256 subnets, each with 254 hosts. We need fewer subnets. If we used 255.255.240.0, this provides 16 subnets. Let’s add one more subnet bit. 255.255.248.0. This is 5 bits of subnetting, which provides 32 subnets. This is our best answer, a /21. 8. A /29 is 255.255.255.248. This is a block size of 8 in the fourth octet. 0, 8, 16. The host is in the 8 subnet, broadcast is 15. 9. A /29 is 255.255.255.248, which is 5 subnet bits and 3 host bits. This is only 6 hosts per subnet. 10. A /23 is 255.255.254.0. The third octet is a block size of 2. 0, 2, 4. The host is in the 16.2.0 subnet; the broadcast address is 16.3.255.

 

10089c03.fm  Page 168  Wednesday, February 27, 2008  4:40 PM




Answers to Written Lab 3.3169Answers to Written Lab 3.2Answers to Written Lab 3.3Classful AddressSubnet MaskNumber of Hosts per Subnet (2h – 2)/16255.255.0.065,534/17255.255.128.032,766/18255.255.192.016,382/19255.255.224.08,190/20255.255.240.04,094/21255.255.248.02,046/22255.255.252.01,022/23255.255.254.0510/24255.255.255.0254/25255.255.255.128126/26255.255.255.19262/27255.255.255.22430/28255.255.255.24014/29255.255.255.2486/30255.255.255.2522Decimal IP AddressAddress ClassNumber of Subnet and Host BitsNumber of Subnets (2x)Number of Hosts        (2x – 2)10.25.66.154/23A15/932768510172.31.254.12/24B8/8256254192.168.20.123/28C4/4161463.24.89.21/18A10/141,02416,384128.1.1.254/20B4/12164094208.100.54.209/30C6/2642

10089c03.fm  Page 169  Thursday, August 30, 2007  12:33 PM




10089c03.fm  Page 170  Thursday, August 30, 2007  12:33 PM




 

Chapter 4 Cisco’s Internetworking Operating System (IOS) and Security Device Manager (SDM)

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Implement an IP addressing scheme and IP Services to meet network requirements in a medium-size Enterprise branch office network  Configure, verify and troubleshoot DHCP and DNS operation on a router (including: CLI/SDM)   Configure, verify, and troubleshoot basic router operation and routing on Cisco devices  Describe the operation of Cisco routers (including: router bootup process, POST, router components)   Access and utilize the router to set basic parameters (including: CLI/SDM)   Connect, configure, and verify operation status of a device interface   Verify device configuration and network connectivity using ping, traceroute, telnet, SSH or other utilities   Verify network connectivity (including: using ping, traceroute, and telnet or SSH)   Troubleshoot routing issues   Verify router hardware and software operation using SHOW & DEBUG commands 

 

10089c04.fm  Page 171  Thursday, August 30, 2007  12:36 PM




 The time has come to introduce you to the Cisco Internetwork Operating System (IOS). The IOS is what runs Cisco routers as well as Cisco’s switches, and it’s what allows you to configure the devices as well.So that’s what you’re going to learn about in this chapter. I’m going to show you how to configure a Cisco IOS router using the Cisco IOS command-line interface (CLI). When you become proficient with this interface, you’ll be able to configure hostnames, banners, pass-words, and more, as well as troubleshoot using the Cisco IOS. From there, we’ll take a peek at Cisco’s Security Device Manager (SDM) and you’ll find out how to set up an HTTPS session to a router to provide the same types of configurations. The SDM really becomes a much more powerful tool in later chapters because it makes configuring access lists, VPNs, and IPSec a snap, but first, you need to learn the basics of Cisco’s IOS.I’m also going to get you up to speed on the vital basics of router configurations and com-mand verifications. Here’s a list of the subjects we’ll be covering in this chapter:  Understanding and configuring the Cisco Internetwork Operating System (IOS)  Connecting to a router  Bringing up a router  Logging into a router  Understanding the router prompts  Understanding the CLI prompts  Performing editing and help features  Gathering basic routing information  Setting administrative functions  Setting hostnames  Setting banners  Setting passwords  Setting interface descriptions  Performing interface configurations  Viewing, saving, and erasing configurations  Verifying routing configurationsAnd just as it was with preceding chapters, the fundamentals that you’ll learn in this chapter are foundational building blocks that really need to be in place before you go on to the next chapters in the book.

 

10089c04.fm  Page 172  Thursday, August 30, 2007  12:36 PM




 The IOS User Interface 173

 For up-to-the-minute updates for this chapter, please see  www.lammle.com   and/or  www.sybex.com . The IOS User Interface The  Cisco Internetwork Operating System (IOS)  is the kernel of Cisco routers and most switches. In case you didn’t know, a kernel is the basic, indispensable part of an operating system that allocates resources and manages things such as low-level hardware interfaces and security.In the following sections, I’ll show you the Cisco IOS and how to configure a Cisco router using the command-line interface (CLI). We’ll use Cisco’s SDM toward the end of the chapter.

 I’m going to save Cisco switch configurations for Chapter 8, “LAN Switching  and Spanning Tree Protocol (STP).” Cisco Router IOS The Cisco IOS is a proprietary kernel that provides routing, switching, internetworking, and tele-communications features. The first IOS was written by William Yeager in 1986, and it enabled net-worked applications. It runs on most Cisco routers as well as an ever-increasing number of Cisco Catalyst switches, like the Catalyst 2950/2960 and 3550/3560 series switches.These are some important things that the Cisco router IOS software is responsible for:  Carrying network protocols and functions  Connecting high-speed traffic between devices  Adding security to control access and stop unauthorized network use  Providing scalability for ease of network growth and redundancy  Supplying network reliability for connecting to network resourcesYou can access the Cisco IOS through the console port of a router, from a modem into the auxiliary (or Aux) port, or even through Telnet. Access to the IOS command line is called an  EXEC session . Connecting to a Cisco Router You can connect to a Cisco router to configure it, verify its configuration, and check statistics. There are different ways to do this, but most often, the first place you would connect to is the console port. The  console port  is usually an RJ-45 (8-pin modular) connection located at the back of the router—by default, there may or may not be a password set. The new ISR routers use cisco as the username and cisco as the password by default.

 

10089c04.fm  Page 173  Thursday, August 30, 2007  12:36 PM




 174 Chapter4  Cisco’s IOS and Security Device Manager

 See Chapter 1, “Internetworking,” for an explanation of how to configure a PC  to connect to a router console port. You can also connect to a Cisco router through an  auxiliary port —which is really the same thing as a console port, so it follows that you can use it as one. But an auxiliary port also allows you to configure modem commands so that a modem can be connected to the router. This is a cool feature—it lets you dial up a remote router and attach to the auxiliary port if the router is down and you need to configure it  out-of-band  (meaning out of the network).The third way to connect to a Cisco router is in-band, through the program  Telnet . ( In-band  means configuring the router through the network, the opposite of “out-of-band.”)Telnet is a terminal emulation program that acts as though it’s a dumb terminal. You can use Telnet to connect to any active interface on a router, such as an Ethernet or serial port.Figure 4.1 shows an illustration of a Cisco 2600 series modular router, which is a cut above routers populating the 2500 series because it has a faster processor and can handle many more interfaces. Both the 2500 and 2600 series routers are end of life (EOL), and you can only buy them used. However, many 2600 series routers are still found in production, so it’s important to understand them. Pay close attention to all the different kinds of interfaces and connections. FIGURE4.1 A Cisco 2600 router The 2600 series router can have multiple serial interfaces, which can be used for connecting a T1 or Frame Relay using a serial V.35 WAN connection. Multiple Ethernet or FastEthernet ports can be used on the router, depending on the model. This router also has one console and one auxiliary connection via RJ-45 connectors.Another router I want to talk about is the 2800 series (shown in Figure 4.2). This router has replaced the 2600 series router series and is referred to as an Integrated Services Router (ISR). It gets its name because many of the services, like security, are built into it. It’s a modular device like the 2600, but it’s much faster and a lot more sleek—it’s elegantly designed to sup-port a broad new range of interface options. FIGURE4.2 A Cisco 2800 router

Cisco 2610 router

ETHERNET 0/0LINKACTLOCKBACKCONSOLEAUX

CISCO 2610100–240VAC1.2–0.6AConsole port (RJ-45)Ethernet 0/010BaseT port (RJ-45)Auxiliary port (RJ-45)

 

10089c04.fm  Page 174  Thursday, August 30, 2007  12:36 PM




 The IOS User Interface 175 I mentioned that security is built in—the 2800 has the Security Device Manager (SDM) pre-installed. The SDM is a Web-based device-management tool for Cisco routers that can help you configure a router via a web console. I’ll cover that later in this chapter. You need to keep in mind that for the most part, you get some serious bang for your buck with the 2800—unless you start adding a lot of interfaces to it. You’ve got to pony up for each one of those little beau-ties, and things can really start to add up—fast!There are a couple of other series of routers that are less expensive then the 2800 series: the 1800 and 800 series. You may want to look into these routers if you’re looking for a less-expensive alternative to the 2800 but still want to run the same 12.4 IOS and the latest SDM.Figure 4.3 shows an 1841 router that holds most of the same interfaces as the 2800, but it’s smaller and less expensive. The real reason you would opt for a 2800 instead of an 1800 series router comes down to the more advanced interfaces you can run on the 2800—things like the wireless controller and switching modules. FIGURE4.3 A Cisco 1841 router As a heads up, I’m going to be using all new 2800, 1800, and 800 series routers throughout this book to demonstrate examples of router configurations. But understand that you can use the 2600, and even 2500 routers to practice routing principles.

 You can find more information about all Cisco routers at  www.cisco.com/en/ US/products/hw/routers/index.html . Bringing Up a Router When you first bring up a Cisco router, it will run a power-on self-test (POST). If it passes, it will then look for and load the Cisco IOS from flash memory—if an IOS file is present. (Just in case you don’t know, flash memory is electronically erasable programmable read-only memory—an EEPROM.) After that, the IOS loads and looks for a valid configuration—the startup-config—that’s stored in nonvolatile RAM, or NVRAM.The following messages appear when you first boot or reload a router (I am using my 2811 router):

 System Bootstrap, Version 12.4(13r)T, RELEASE SOFTWARE (fc1)Technical Support: http://www.cisco.com/techsupportCopyright (c) 2006 by cisco Systems, Inc.

 

10089c04.fm  Page 175  Thursday, August 30, 2007  12:36 PM




 176 Chapter4  Cisco’s IOS and Security Device Manager Initializing memory for ECCc2811 platform with 262144 Kbytes of main memoryMain memory is configured to 64 bit mode with ECC enabledUpgrade ROMMON initializedprogram load complete, entry point: 0x8000f000, size: 0xcb80

 program load complete, entry point: 0x8000f000, size: 0xcb80 This is the first part of the router boot process output. It’s information about the bootstrap program that first runs the POST. It then tells the router how to load, which by default is to find the IOS in flash memory. It also lists the amount of RAM in the router.The next part shows us that the IOS is being decompressed into RAM:

 program load complete, entry point: 0x8000f000, size: 0x14b45f8Self decompressing the image :   ####################################################################

    ############################################ [OK] The pound signs are telling us that the IOS is being loaded into RAM. After it is decom-pressed into RAM, the IOS is loaded and starts running the router, as shown below. Notice that the IOS version is stated as advanced security version 12.4(12):

 [some output cut]Cisco IOS Software, 2800 Software (C2800NM-ADVSECURITYK9-M), Version   12.4(12), RELEASE SOFTWARE (fc1)Technical Support: http://www.cisco.com/techsupportCopyright (c) 1986-2006 by Cisco Systems, Inc.Compiled Fri 17-Nov-06 12:02 by prod_rel_team

 Image text-base: 0x40093160, data-base: 0x41AA0000 A sweet new feature of the new ISR routers is that the IOS name is no longer cryptic. The filename actually tells you what the IOS can do, as in Advanced Security. Once the IOS is loaded, the information learned from the POST will be displayed next, as you can see here:

 [some output cut]Cisco 2811 (revision 49.46) with 249856K/12288K bytes of memory.Processor board ID FTX1049A1AB2 FastEthernet interfaces4 Serial(sync/async) interfaces1 Virtual Private Network (VPN) ModuleDRAM configuration is 64 bits wide with parity enabled.239K bytes of non-volatile configuration memory.

 62720K bytes of ATA CompactFlash (Read/Write) There are two FastEthernet interfaces, four serial interfaces, plus a VPN module. The amount of RAM, NVRAM, and flash are also displayed. The above router output shows us that there’s 256MB of RAM, 239K of NVARM, and 64MB of flash.

 

10089c04.fm  Page 176  Thursday, August 30, 2007  12:36 PM




 The IOS User Interface 177 When the IOS is loaded and up and running, a preconfiguration (called startup-config) will be copied from NVRAM into RAM. The copy of this file will be placed in RAM and called running-config.

 My 1841 and 871W routers boot exactly the same as the 2811 router. The 1841 and 871W do show less memory and different interfaces, but other than that, they have the same bootup procedure and the same preconfig- ured startup-config file. Bringing Up a Non-ISR Router (a 2600) As you’re about to see, the boot cycle is about the same for non-ISR routers as for the ISR routers. The following messages appear when you first boot or reload a 2600 router:

 System Bootstrap, Version 11.3(2)XA4, RELEASE SOFTWARE (fc1)Copyright (c) 1999 by cisco Systems, Inc.TAC:Home:SW:IOS:Specials for info

 C2600 platform with 65536 Kbytes of main memory The next part shows us that the IOS is being decompressed into RAM:

 program load complete, entry point:0x80008000, size:0x43b7fcSelf decompressing the image :#################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################

 ## [OK] So far, everything is pretty much the same. Notice below that the IOS version is stated as version 12.3(20):

 Cisco Internetwork Operating System SoftwareIOS (tm) C2600 Software (C2600-IK9O3S3-M), Version 12.3(20), RELEASE   SOFTWARE (fc2)Technical Support: http://www.cisco.com/techsupportCopyright (c) 1986-2006 by cisco Systems, Inc.Compiled Tue 08-Aug-06 20:50 by kesnyder

 Image text-base: 0x80008098, data-base: 0x81A0E7A8

 

10089c04.fm  Page 177  Thursday, August 30, 2007  12:36 PM




 178 Chapter4  Cisco’s IOS and Security Device Manager Just as with the 2800 series, once the IOS is loaded, the information learned from the POST will be displayed:

 cisco 2610 (MPC860) processor (revision 0x202) with 61440K/4096K bytes   of memory.Processor board ID JAD03348593 (1529298102)M860 processor: part number 0, mask 49Bridging software.X.25 software, Version 3.0.0.1 Ethernet/IEEE 802.3 interface(s)1 Serial network interface(s)2 Serial(sync/async) network interface(s)32K bytes of non-volatile configuration memory.

 16384K bytes of processor board System flash (Read/Write) Okay—finally what we see here is one Ethernet interface and three serial interfaces. The amount of RAM and flash is also displayed, and the above router output shows there are 64MB of RAM and 16MB of flash.And as I mentioned, when the IOS is loaded and up and running, a valid configuration called the startup-config will be loaded from NVRAM. But here’s where it differs from the default bootup of the ISR routers—if there isn’t a configuration in NVRAM, the router will broadcast looking for a valid one on a TFTP host. (This can only happen if the router senses carrier detect, or CD, on any interface.) If the broadcast fails, it will then go into what is called  setup mode —a step-by-step process to help you configure the router. So you need to remember that if you plug any interface of your router into your network and then boot your router, you may have to wait a couple minutes while the router searches for the configuration.

 You can have your ISR routers perform this boot cycle by erasing the startup-config and reloading the router. This will give you a clean router with no  default configuration. I’ll show you how to do this a little later in the chapter.  You can also enter setup mode at any time from the command line by typing the command  setup  from something called privileged mode, which I’ll get to in a minute. Setup mode covers only some global commands and is generally just unhelpful. Here is an example:

 Would you like to enter the initial configuration dialog? [yes/no]: yAt any point you may enter a question mark ‘?’ for help.Use ctrl-c to abort configuration dialog at any prompt.Default settings are in square brackets ‘[]’.Basic management setup configures only enough connectivityfor management of the system, extended setup will ask you

10089c04.fm  Page 178  Thursday, August 30, 2007  12:36 PM




Command-Line Interface (CLI)179to configure each interface on the systemWould you like to enter basic management setup? [yes/no]: yConfiguring global parameters:  Enter host name [Router]:Ctrl+C

Configuration aborted, no changes made.

You can exit setup mode at any time by pressing Ctrl+C.I highly recommend going through setup mode once, then never again. You should always use the CLI or SDM.Command-Line Interface (CLI)I sometimes refer to the CLI as “Cash Line Interface” because if you can create advanced configurations on Cisco routers and switches using the CLI, then you’ll get the cash!To use the CLI, press Enter after the router finishes booting up. After you do that, the router will respond with messages that tell you all about the status of each and every one of its interfaces and then display a banner and ask you to log in. Here’s an example:

[some output cut]*Feb 28 16:42:00.967: %VPN_HW-6-INFO_LOC: Crypto engine: onboard 0   State changed to: Initialized*Feb 28 16:42:00.971: %VPN_HW-6-INFO_LOC: Crypto engine: onboard 0   State changed to: Enabled*Feb 28 16:42:01.471: %LINK-3-UPDOWN: Interface FastEthernet0/0,   changed state to up*Feb 28 16:42:01.471: %LINK-3-UPDOWN: Interface FastEthernet0/1,   changed state to up*Feb 28 16:42:01.471: %LINK-3-UPDOWN: Interface Serial0/0/0, changed   state to down*Feb 28 16:42:01.471: %LINK-3-UPDOWN: Interface Serial0/0/1, changed   state to down*Feb 28 16:42:01.471: %LINK-3-UPDOWN: Interface Serial0/1/0, changed   state to down*Feb 28 16:42:01.471: %LINK-3-UPDOWN: Interface Serial0/2/0, changed   state to down

10089c04.fm  Page 179  Thursday, August 30, 2007  12:36 PM




180Chapter4 Cisco’s IOS and Security Device Manager[some output cut]-----------------------------------------------------------------------Cisco Router and Security Device Manager (SDM) is installed on this   device. This feature requires the one-time use of the username   “cisco” with the password “cisco”. The default username and password   have a privilege level of 15.Please change these publicly known initial credentials using SDM or the   IOS CLI. Here are the Cisco IOS commands.username <myuser>  privilege 15 secret 0 <mypassword>no username ciscoReplace <myuser> and <mypassword> with the username and password you   want to use.For more information about SDM please follow the instructions in the  QUICK START GUIDE for your router or go to http://www.cisco.com/go/sdm-----------------------------------------------------------------------User Access VerificationUsername: ciscoPassword: cisco [this won’t show on your screen]

yourname#From here, just log in using cisco/cisco as the username/password and you will be in privilege mode—something I’ll talk about next.Now, the reason there’s a configuration already installed on the router is so that you can connect with the SDM via HTTPS without having to configure the router. Again, I’ll be going through the preconfigured startup-config a bit later in this chapter.Entering the CLI from a Non-ISR RouterAfter the interface status messages appear and you press Enter, the Router> prompt will appear. This is called user exec mode (user mode), and it’s mostly used to view statistics, but it’s also a stepping stone to logging in to privileged mode.You can only view and change the configuration of a Cisco router in privileged exec mode (privileged mode), which you can enter with the enable command.Here’s how:

Router>enable

Router#You now end up with a Router# prompt, which indicates that you’re in privileged mode, where you can both view and change the router’s configuration. You can go back from privileged mode into user mode by using the disable command, as seen here:

Router#disable

Router>

10089c04.fm  Page 180  Thursday, August 30, 2007  12:36 PM




Command-Line Interface (CLI)181At this point, you can type logout from either mode to exit the console:

Router>logoutRouter con0 is now available

Press RETURN to get started.

Remember, by deleting the default configuration of an ISR router and reloading, you’ll get to these same prompts, and you won’t be prompted for a username and password.In the following sections, I am going to show you how to perform some basic administra-tive configurations.Overview of Router ModesTo configure from a CLI, you can make global changes to the router by typing configure terminal (or config t for short), which puts you in global configuration mode and changes what’s known as the running-config. A global command (a command run from global config) is set only once and affects the entire router.You can type config from the privileged-mode prompt and then just press Enter to take the default of terminal, as seen here:

yourname#configConfiguring from terminal, memory, or network [terminal]? [press enter]Enter configuration commands, one per line.  End with CNTL/Z.

yourname(config)#At this point, you make changes that affect the router as a whole (globally), hence the term global configuration mode. To change the running-config—the current configuration running in dynamic RAM (DRAM)—you use the configure terminal command, as I just demonstrated.To change the startup-config—the configuration stored in NVRAM—you use the configure memory command (or config mem for short), which merges the startup-config file into the running-config file in RAM. If you want to change a router configuration stored on a TFTP host (which is covered in Chapter 5, “Managing the Cisco IOS”), you use the configure network command (or config net for short), which also merges the file with the running-config file in RAM.The configure terminal, configure memory, and configure network commands are all used to configure information into RAM on a router; however, typically only the configure terminal command is used. It is possible, though, that the commands config mem and config net can be useful if you screw up your running-config file and don’t want to reboot your router.

10089c04.fm  Page 181  Thursday, August 30, 2007  12:36 PM




182Chapter4 Cisco’s IOS and Security Device ManagerHere are some of the other options under the configure command:

yourname(config)#exit or press cntl-zyourname#config ?  confirm            Confirm replacement of running-config witha new config file  memory             Configure from NV memory  network            Configure from a TFTP network host  overwrite-network  Overwrite NV memory from TFTP network host  replace            Replace the running-config with a new config file  terminal           Configure from the terminal

  <cr>As you can see, Cisco has added a few more commands in the 12.4 IOS. We’ll go through these commands in Chapter 5.CLI PromptsIt’s really important that you understand the different prompts you can find when configuring a router. Knowing these well will help you navigate and recognize where you are at any time within configuration mode. In this section, I’m going to demonstrate the prompts that are used on a Cisco router and discuss the various terms used. (Always check your prompts before making any changes to a router’s configuration!)I’m not going into every different command prompt offered, because doing that would be reaching beyond the scope of this book. Instead, I’m going to describe all the different prompts you’ll see throughout this chapter and the rest of the book. These command prompts really are the ones you’ll use most in real life anyway; plus, they’re the ones you’ll need to know for the exam.

Don’t freak! It’s not important that you understand what each of these com-mand prompts accomplishes yet because I’m going to completely fill you in on all of them really soon. So right now, just relax and focus on becoming familiar with the different prompts available and all will be well!InterfacesTo make changes to an interface, you use the interface command from global configu-ration mode:

yourname(config)#interface ?  Async              Async interface  BVI                Bridge-Group Virtual Interface  CDMA-Ix            CDMA Ix interface  CTunnel            CTunnel interface

10089c04.fm  Page 182  Thursday, August 30, 2007  12:36 PM




Command-Line Interface (CLI)183  Dialer             Dialer interface  FastEthernet       FastEthernet IEEE 802.3  Group-Async        Async Group interface  Lex                Lex interface  Loopback           Loopback interface  MFR                Multilink Frame Relay bundle interface  Multilink          Multilink-group interface  Null               Null interface  Port-channel       Ethernet Channel of interfaces  Serial             Serial  Tunnel             Tunnel interface  Vif                PGM Multicast Host interface  Virtual-PPP        Virtual PPP interface  Virtual-Template   Virtual Template interface  Virtual-TokenRing  Virtual TokenRing  range              interface range commandyourname(config)#interface fastEthernet 0/0

yourname(config-if)#Did you notice that the prompt changed to yourname(config-if)#? This tells you that you’re in interface configuration mode. And wouldn’t it be nice if the prompt also gave you an indication of what interface you were configuring? Well, at least for now we’ll have to live without the prompt information, because it doesn’t. One thing is for sure: You really have to pay attention when configuring a router!SubinterfacesSubinterfaces allow you to create logical interfaces within the router. The prompt then changes to yourname(config-subif)#:

yourname(config-if)#interface f0/0.1

yourname(config-subif)#

You can read more about subinterfaces in Chapter 9, “Virtual LANs” and Chapter 14, “Wide Area Networks,” but don’t skip ahead just yet!Line CommandsTo configure user-mode passwords, use the line command. The prompt then becomes yourname(config-line)#:

yourname#config tEnter configuration commands, one per line.  End with CNTL/Z.

10089c04.fm  Page 183  Thursday, August 30, 2007  12:36 PM




184Chapter4 Cisco’s IOS and Security Device Manageryourname(config)#line ?  <0-337>  First Line number  aux      Auxiliary line  console  Primary terminal line  tty      Terminal controller  vty      Virtual terminal  x/y      Slot/Port for Modems

  x/y/z    Slot/Subslot/Port for ModemsThe line console 0 command is known as a major command (also called a global com-mand), and any command typed from the (config-line) prompt is known as a subcommand.Routing Protocol ConfigurationsTo configure routing protocols such as RIP and EIGRP, you’ll use the prompt yourname(config-router#):

yourname#config tEnter configuration commands, one per line.  End with CNTL/Z.yourname(config)#router ripyourname(config-router)#version 2

yourname(config-router)#Defining Router TermsTable 4.1 defines some of the terms we’ve used so far.TABLE4.1Router TermsModeDefinitionUser EXEC modeLimited to basic monitoring commandsPrivileged EXEC modeProvides access to all other router commandsGlobal configuration modeCommands that affect the entire systemSpecific configuration modesCommands that affect interfaces/processes onlySetup modeInteractive configuration dialog

10089c04.fm  Page 184  Thursday, August 30, 2007  12:36 PM




Command-Line Interface (CLI)185Editing and Help FeaturesYou can use the Cisco advanced editing features to help you configure your router. If you type in a question mark (?) at any prompt, you’ll be given a list of all the commands available from that prompt:

yourname#?Exec commands:  access-enable    Create a temporary Access-List entry  access-profile   Apply user-profile to interface  access-template  Create a temporary Access-List entry  archive          manage archive files  auto             Exec level Automation  bfe              For manual emergency modes setting  calendar         Manage the hardware calendar  cd               Change current directory  clear            Reset functions  clock            Manage the system clock  cns              CNS agents  configure        Enter configuration mode  connect          Open a terminal connection  copy             Copy from one file to another  crypto           Encryption related commands.  ct-isdn          Run an ISDN component test command  debug            Debugging functions (see also ‘undebug’)  delete           Delete a file  dir              List files on a filesystem  disable          Turn off privileged commands  disconnect       Disconnect an existing network connection

 --More--Plus, at this point you can press the spacebar to get another page of information, or you can press Enter to go one command at a time. You can also press Q (or any other key, for that matter) to quit and return to the prompt.Here’s a shortcut: To find commands that start with a certain letter, use the letter and the question mark with no space between them:

yourname#c?calendar  cd         clear    clockcns       configure  connect  copycrypto    ct-isdn  

yourname#c

10089c04.fm  Page 185  Thursday, August 30, 2007  12:36 PM




186Chapter4 Cisco’s IOS and Security Device ManagerBy typing c?, we received a response listing all the commands that start with c. Also notice that the yourname#c prompt reappears after the list of commands is displayed. This can be helpful when you have long commands and need the next possible command. It would be pretty lame if you had to retype the entire command every time you used a question mark!To find the next command in a string, type the first command and then a question mark:

yourname#clock ?  read-calendar    Read the hardware calendar into the clock  set              Set the time and date  update-calendar  Update the hardware calendar from the clockyourname#clock set ?  hh:mm:ss  Current Timeyourname#clock set 11:15:11 ?  <1-31>  Day of the month  MONTH   Month of the yearyourname#clock set 11:15:11 25 aug ?  <1993-2035>  Yearyourname#clock set 11:15:11 25 aug 2007 ?  <cr>yourname#clock set 11:15:11 25 aug 2007*Aug 25 11:15:11.000: %SYS-6-CLOCKUPDATE: System clock hasbeen updated from 18:52:53 UTC Wed Feb 28 2007 to 11:15:11

UTC Sat Aug 25 2007, configured from console by cisco on console.By typing the clock ? command, you’ll get a list of the next possible parameters and what they do. Notice that you should just keep typing a command, a space, and then a question mark until <cr> (carriage return) is your only option.If you’re typing commands and receive

yourname#clock set 11:15:11

% Incomplete command.you’ll know that the command string isn’t done yet. Just press the up arrow key to redisplay the last command entered, and then continue with the command by using your question mark.And if you receive the error

yourname(config)#access-list 110 permit host 1.1.1.1                                      ^

% Invalid input detected at ‘^’ marker.you’ve entered a command incorrectly. See that little caret—the ^? It’s a very helpful tool that marks the exact point where you blew it and entered the command incorrectly. Here’s another example of when you’ll see the caret:

yourname#sh serial 0/0/0              ^

% Invalid input detected at ‘^’ marker.

10089c04.fm  Page 186  Thursday, August 30, 2007  12:36 PM




Command-Line Interface (CLI)187This command looks right, but be careful! The problem is that the full command is show interface serial 0/0/0.Now if you receive the error

yourname#sh ru

% Ambiguous command:  “sh ru”it means there are multiple commands that begin with the string you entered and it’s not unique. Use the question mark to find the command you need:

yourname#sh ru?

rudpv1  running-configAs you can see, there are two commands that start with show ru.Table 4.2 lists the enhanced editing commands available on a Cisco router.TABLE4.2Enhanced Editing CommandsCommandMeaningCtrl+AMoves your cursor to the beginning of the lineCtrl+EMoves your cursor to the end of the lineEsc+BMoves back one wordCtrl+BMoves back one characterCtrl+FMoves forward one characterEsc+FMoves forward one wordCtrl+DDeletes a single characterBackspaceDeletes a single characterCtrl+RRedisplays a lineCtrl+UErases a lineCtrl+WErases a wordCtrl+ZEnds configuration mode and returns to EXECTabFinishes typing a command for you

10089c04.fm  Page 187  Thursday, August 30, 2007  12:36 PM




188Chapter4 Cisco’s IOS and Security Device ManagerAnother cool editing feature I want to show you is the automatic scrolling of long lines. In the following example, the command typed had reached the right margin and automatically moved 11 spaces to the left (the dollar sign [$] indicates that the line has been scrolled to the left):

yourname#config tEnter configuration commands, one per line. End with CNTL/Z.

yourname(config)#$110 permit host 171.10.10.10 0.0.0.0 eq 23You can review the router-command history with the commands shown in Table 4.3.The following example demonstrates the show history command and how to change the history size, as well as how to verify it with the show terminal command. First, use the show history command to see the last 20 commands that were entered on the router:

yourname#show history en sh history show terminal sh cdp neig sh ver sh flash sh int fa0 sh history sh int s0/0

 sh int s0/1Now use the show terminal command to verify the terminal history size:

yourname#show terminalLine 0, Location: ““, Type: ““TABLE4.3 Router-Command HistoryCommandMeaningCtrl+P or up arrowShows last command enteredCtrl+N or down arrowShows previous commands enteredshow historyShows last 10 commands entered by defaultshow terminalShows terminal configurations and history buffer sizeterminal history sizeChanges buffer size (max 256)

10089c04.fm  Page 188  Thursday, August 30, 2007  12:36 PM




Command-Line Interface (CLI)189[output cut]Modem type is unknown.Session limit is not set.Time since activation: 00:21:41Editing is enabled.History is enabled, history size is 20.DNS resolution in show commands is enabledFull user help is disabledAllowed input transports are none.Allowed output transports are pad telnet rlogin lapb-ta mop v120 ssh.Preferred transport is telnet.No output characters are padded

No special data dispatching charactersThe terminal history size command, used from privileged mode, can change the size of the history buffer:

yourname#terminal history size ? <0-256> Size of history buffer

yourname#terminal history size 25You verify the change with the show terminal command:

yourname#show terminalLine 0, Location: ““, Type: ““[output cut]Editing is enabled.History is enabled, history size is 25.Full user help is disabledAllowed transports are lat pad v120 telnet mop rlogin  nasi. Preferred is lat.No output characters are paddedNo special data dispatching characters

Group codes:  0Gathering Basic Routing InformationThe show version command will provide basic configuration for the system hardware as well as the software version and the boot images. Here’s an example:

yourname#show versionCisco IOS Software, 2800 Software (C2800NM-ADVSECURITYK9-M), Version   12.4(12), RELEASE SOFTWARE (fc1)

10089c04.fm  Page 189  Thursday, August 30, 2007  12:36 PM




190Chapter4 Cisco’s IOS and Security Device ManagerTechnical Support: http://www.cisco.com/techsupportCopyright (c) 1986-2006 by Cisco Systems, Inc.

Compiled Fri 17-Nov-06 12:02 by prod_rel_teamThe preceding section of output describes the Cisco IOS running on the router. The follow-ing section describes the read-only memory (ROM) used, which is used to boot the router and holds the POST:

ROM: System Bootstrap, Version 12.4(13r)T, RELEASE SOFTWARE (fc1)The next section shows how long the router has been running, how it was restarted (if you see a system restarted by bus error, that is a very bad thing), the location from which the Cisco IOS was loaded, and the IOS name. Flash is the default:

yourname uptime is 2 hours, 30 minutesSystem returned to ROM by power-onSystem restarted at 09:04:07 UTC Sat Aug 25 2007

System image file is “flash:c2800nm-advsecurityk9-mz.124-12.bin”This next section displays the processor, the amount of DRAM and flash memory, and the interfaces the POST found on the router:

[some output cut]Cisco 2811 (revision 53.50) with 249856K/12288K bytes of memory.Processor board ID FTX1049A1AB2 FastEthernet interfaces4 Serial(sync/async) interfaces1 Virtual Private Network (VPN) ModuleDRAM configuration is 64 bits wide with parity enabled.239K bytes of non-volatile configuration memory.

When Do You Use the Cisco Editing Features?A couple of editing features are used quite often and some not so much, if at all. Understand that Cisco didn’t make these up; these are just old Unix commands. However, Ctrl+A is really helpful to negate a command.For example, if you were to put in a long command and then decide you didn’t want to use that command in your configuration after all, or if it didn’t work, then you could just press your up arrow key to show the last command entered, press Ctrl+A, type no and then a space, press Enter—and poof! The command is negated. This doesn’t work on every command, but it works on a lot of them.

10089c04.fm  Page 190  Thursday, August 30, 2007  12:36 PM




Router and Switch Administrative Configurations19162720K bytes of ATA CompactFlash (Read/Write)

Configuration register is 0x2102The configuration register value is listed last—it’s something I’ll cover in Chapter 5.In addition, the show interfaces and show ip interface brief commands are very useful in verifying and troubleshooting a router as well as network issues. These commands are covered later in this chapter. Don’t miss it!Router and Switch Administrative ConfigurationsEven though this section isn’t critical to making a router or switch work on a network, it’s still really important; in it, I’m going to lead you through configuring commands that will help you administer your network.The administrative functions that you can configure on a router and switch are as follows: Hostnames Banners Passwords Interface descriptionsRemember, none of these will make your routers or switches work better or faster, but trust me, your life will be a whole lot better if you just take the time to set these configurations on each of your network devices. That’s because doing this makes troubleshooting and maintain-ing your network sooooo much easier—seriously! In this next section, I’ll be demonstrating commands on a Cisco router, but these commands are exactly the same on a Cisco switch.HostnamesYou can set the identity of the router with the hostname command. This is only locally sig-nificant, which means that it has no bearing on how the router performs name lookups or how the router works on the internetwork. However, I’ll use the hostname in Chapter 14 for authentication purposes when I discuss PPP.Here’s an example:

yourname#config tEnter configuration commands, one per line. End with  CNTL/Z.yourname(config)#hostname ToddTodd(config)#hostname AtlantaAtlanta(config)#hostname Todd

Todd(config)#

10089c04.fm  Page 191  Thursday, August 30, 2007  12:36 PM




192Chapter4 Cisco’s IOS and Security Device ManagerEven though it’s pretty tempting to configure the hostname after your own name, it’s defi-nitely a better idea to name the router something pertinent to the location. This is because giving it a hostname that’s somehow relevant to where the device actually lives will make finding it a whole lot easier. And it also helps you confirm that you are, indeed, configuring the right device. For this chapter, we’ll leave it at Todd for now.BannersA banner is more than just a little cool—one very good reason for having a banner is to give any and all who dare attempt to telnet or dial into your internetwork a little security notice. And you can create a banner to give anyone who shows up on the router exactly the informa-tion you want them to have.Make sure you’re familiar with these four available banner types: exec process creation banner, incoming terminal line banner, login banner, and message of the day banner (all illus-trated in the following code):

Todd(config)#banner ?  LINE            c banner-text c, where ‘c’ is a delimiting character  exec            Set EXEC process creation banner  incoming        Set incoming terminal line banner  login           Set login banner  motd            Set Message of the Day banner  prompt-timeout  Set Message for login authentication timeout

  slip-ppp        Set Message for SLIP/PPPMessage of the day (MOTD) is the most extensively used banner. It gives a message to every person dialing into or connecting to the router via Telnet or an auxiliary port, or even through a console port as seen here:

Todd(config)#banner motd ?LINE c banner-text c, where ‘c’ is a delimiting characterTodd(config)#banner motd #Enter TEXT message. End with the character ‘#’.$ Acme.com network, then you must disconnect immediately.#Todd(config)#^ZTodd#00:25:12: %SYS-5-CONFIG_I: Configured from console by  consoleTodd#exitRouter con0 is now available

10089c04.fm  Page 192  Thursday, August 30, 2007  12:36 PM




Router and Switch Administrative Configurations193Press RETURN to get started.If you are not authorized to be in Acme.com network, then youmust disconnect immediately.

Todd#The preceding MOTD banner essentially tells anyone connecting to the router to get lost if they’re not on the guest list! The part to understand is the delimiting character—the thing that’s used to tell the router when the message is done. You can use any character you want for it, but (I hope this is obvious) you can’t use the delimiting character in the message itself. Also, once the message is complete, press Enter, then the delimiting character, and then Enter again. It’ll still work if you don’t do that, but if you have more than one banner, they’ll be com-bined as one message and put on a single line.For example, you can set a banner on one line as shown:

Todd(config)#banner motd x Unauthorized access prohibited! xThis example will work just fine, but if you add another MOTD banner message, they would end up on a single line.Here are some details of the other banners I mentioned:Exec bannerYou can configure a line-activation (exec) banner to be displayed when an EXEC process (such as a line activation or incoming connection to a VTY line) is created. By simply starting a user exec session through a console port, you’ll activate the exec banner.Incoming bannerYou can configure a banner to be displayed on terminals connected to reverse Telnet lines. This banner is useful for providing instructions to users who use reverse Telnet.Login bannerYou can configure a login banner to be displayed on all connected terminals. This banner is displayed after the MOTD banner but before the login prompts. The login ban-ner can’t be disabled on a per-line basis, so to globally disable it, you’ve got to delete it with the no banner login command.Here is an example of a login banner:

!banner login ^C-----------------------------------------------------------------Cisco Router and Security Device Manager (SDM) is installed on this device.This feature requires the one-time use of the username “cisco”with the password “cisco”. The default username and password have a privilege level of 15.Please change these publicly known initial credentials using SDM or the IOS CLI.Here are the Cisco IOS commands.username <myuser>  privilege 15 secret 0 <mypassword>no username cisco

10089c04.fm  Page 193  Thursday, August 30, 2007  12:36 PM




194Chapter4 Cisco’s IOS and Security Device ManagerReplace <myuser> and <mypassword> with the username and password you want to use.For more information about SDM please follow the instructions in the QUICK STARTGUIDE for your router or go to http://www.cisco.com/go/sdm-----------------------------------------------------------------^C

!The above login banner should look pretty familiar—it’s the banner that Cisco has in its default configuration for its ISR routers. This banner is displayed before the login prompts but after the MOTD banner.Setting PasswordsFive passwords are used to secure your Cisco routers: console, auxiliary, telnet (VTY), enable password, and enable secret. The enable secret and enable password are used to set the pass-word that’s used to secure privileged mode. This will prompt a user for a password when the enable command is used. The other three are used to configure a password when user mode is accessed through the console port, through the auxiliary port, or via Telnet.Let’s take a look at each of these now.Enable PasswordsYou set the enable passwords from global configuration mode like this:

Todd(config)#enable ? last-resort Define enable action if no TACACS servers             respond password    Assign the privileged level password secret      Assign the privileged level secret

 use-tacacs  Use TACACS to check enable passwordsThe following points describe the enable password parameters:last-resortAllows you to still enter the router if you set up authentication through a TACACS server and it’s not available. But it isn’t used if the TACACS server is working.passwordSets the enable password on older, pre-10.3 systems, and isn’t ever used if an enable secret is set.secretThis is the newer, encrypted password that overrides the enable password if it’s set.use-tacacsThis tells the router to authenticate through a TACACS server. It’s conve-nient if you have anywhere from a dozen to multitudes of routers because, well, would you like to face the fun task of changing the password on all those routers? If you’re sane, no, you wouldn’t. So instead, just go through the TACACS server and you only have to change the password once—yeah!

10089c04.fm  Page 194  Thursday, August 30, 2007  12:36 PM




Router and Switch Administrative Configurations195Here’s an example of setting the enable passwords:

Todd(config)#enable secret toddTodd(config)#enable password toddThe enable password you have chosen is the same as your  enable secret. This is not recommended. Re-enter the

  enable password.If you try to set the enable secret and enable passwords the same, the router will give you a nice, polite warning to change the second password. If you don’t have older legacy routers, don’t even bother to use the enable password.User-mode passwords are assigned by using the line command:

Todd(config)#line ?  <0-337>  First Line number  aux      Auxiliary line  console  Primary terminal line  tty      Terminal controller  vty      Virtual terminal  x/y      Slot/Port for Modems

  x/y/z    Slot/Subslot/Port for ModemsHere are the lines to be concerned with:auxSets the user-mode password for the auxiliary port. It’s usually used for attaching a modem to the router, but it can be used as a console as well.consoleSets a console user-mode password.vtySets a Telnet password on the router. If this password isn’t set, then Telnet can’t be used by default.To configure the user-mode passwords, you configure the line you want and use either the login or no login command to tell the router to prompt for authentication. The next sec-tions will provide a line-by-line example of the configuration of each line configurationAuxiliary PasswordTo configure the auxiliary password, go into global configuration mode and type line aux ?. You can see here that you only get a choice of 0–0 (that’s because there’s only one port):

Todd#config tEnter configuration commands, one per line.  End with CNTL/Z.Todd(config)#line aux ?  <0-0>  First Line numberTodd(config)#line aux 0Todd(config-line)#login

10089c04.fm  Page 195  Thursday, August 30, 2007  12:36 PM




196Chapter4 Cisco’s IOS and Security Device Manager% Login disabled on line 1, until ‘password’ is setTodd(config-line)#password aux

Todd(config-line)#loginIt’s important to remember the login command or the auxiliary port won’t prompt for authentication.Cisco has begun this process of not letting you set the login command before a password is set on a line because if you set the login command under a line and then don’t set a pass-word, the line won’t be usable. And it will prompt for a password that doesn’t exist. So this is a good thing—a feature, not a hassle!

Definitely remember that although Cisco has this new “password feature” on its routers starting in its newer IOS (12.2 and above), it’s not in all its IOSes.Console PasswordTo set the console password, use the line console 0 command. But look at what happened when I tried to type line console 0 ? from the (config-line)# prompt—I received an error. You can still type line console 0 and it will accept it, but the help screens just don’t work from that prompt. Type exit to get back one level and you’ll find that your help screens now work. This is a “feature.” Really.Here’s the example:

Todd(config-line)#line console ?% Unrecognized commandTodd(config-line)#exit       Todd(config)#line console ?  <0-0>  First Line numberTodd(config-line)#password console

Todd(config-line)#loginSince there’s only one console port, I can only choose line console 0. You can set all your line passwords to the same password, but for security reasons, I’d recommend that you make them different.There are a few other important commands to know for the console port.For one, the exec-timeout 0 0 command sets the time-out for the console EXEC session to zero, which basically means to never time out. The default time-out is 10 minutes. (If you’re feeling mischievous, try this on people at work: Set it to 0 1. That will make the console time out in 1 second! And to fix it, you have to continually press the down arrow key while chang-ing the time-out time with your free hand!)logging synchronous is a very cool command, and it should be a default command, but it’s not. It stops annoying console messages from popping up and disrupting the input you’re trying to type. The messages still pop up, but you are returned to your router prompt without your input interrupted. This makes your input messages oh-so-much easier to read.

10089c04.fm  Page 196  Thursday, August 30, 2007  12:36 PM




Router and Switch Administrative Configurations197Here’s an example of how to configure both commands:

Todd(config-line)#line con 0Todd(config-line)#exec-timeout ?  <0-35791>  Timeout in minutesTodd(config-line)#exec-timeout 0 ?  <0-2147483>  Timeout in seconds  <cr>Todd(config-line)#exec-timeout 0 0

Todd(config-line)#logging synchronous

You can set the console to go from never timing out (0 0) to timing out in 35,791 minutes and 2,147,483 seconds. The default is 10 minutes.Telnet PasswordTo set the user-mode password for Telnet access into the router, use the line vty command. Routers that aren’t running the Enterprise edition of the Cisco IOS default to five VTY lines, 0 through 4. But if you have the Enterprise edition, you’ll have significantly more. The best way to find out how many lines you have is to use that question mark:

Todd(config-line)#line vty 0 ?% Unrecognized commandTodd(config-line)#exit     Todd(config)#line vty 0 ?  <1-1180>  Last Line number  <cr>Todd(config)#line vty 0 1180Todd(config-line)#password telnet

Todd(config-line)#loginRemember, you cannot get help from your (config-line)# prompt. You must go back to privilege mode in order to use the question mark (?).

You may or may not have to set the login command before the password on the VTY lines—it depends on the IOS version. The result is the same either way.So what will happen if you try to telnet into a router that doesn’t have a VTY password set? You’ll receive an error stating that the connection is refused because, well, the password isn’t set. So, if you telnet into a router and receive the message

Todd#telnet SFRouterTrying SFRouter (10.0.0.1)…Open

10089c04.fm  Page 197  Thursday, August 30, 2007  12:36 PM




198Chapter4 Cisco’s IOS and Security Device ManagerPassword required, but none set[Connection to SFRouter closed by foreign host]

Todd#then the remote router (SFRouter in this example) does not have the VTY (Telnet) password set. But you can get around this and tell the router to allow Telnet connections without a pass-word by using the no login command:

SFRouter(config-line)#line vty 0 4

SFRouter(config-line)#no login

I do not recommend using the no login command to allow Telnet connec-tions without a password unless you are in a testing or classroom environ-ment! In a production network, you should always set your VTY password.After your routers are configured with an IP address, you can use the Telnet program to configure and check your routers instead of having to use a console cable. You can use the Tel-net program by typing telnet from any command prompt (DOS or Cisco). Anything Telnet is covered more thoroughly in Chapter 5.Setting Up Secure Shell (SSH)Instead of Telnet, you can use Secure Shell, which creates a more secure session than the Telnet application that uses an unencrypted data stream. Secure Shell (SSH) uses encrypted keys to send data so that your username and password are not sent in the clear.Here are the steps to setting up SSH:1.Set your hostname:

Router(config)#hostname Todd2.Set the domain name (both the hostname and domain name are required for the encryp-tion keys to be generated):

Todd(config)#ip domain-name Lammle.com3.Generate the encryption keys for securing the session:Todd(config)#crypto key generate rsa general-keys modulus ?  <360-2048>  size of the key modulus [360-2048]Todd(config)#crypto key generate rsa general-keys modulus 1024The name for the keys will be: Todd.Lammle.com% The key modulus size is 1024 bits% Generating 1024 bit RSA keys, keys will be non-exportable...[OK]

*June 24 19:25:30.035: %SSH-5-ENABLED: SSH 1.99 has been enabled

10089c04.fm  Page 198  Thursday, August 30, 2007  12:36 PM




Router and Switch Administrative Configurations1994.Set the max idle timer for a SSH session:Todd(config)#ip ssh time-out ?  <1-120>  SSH time-out interval (secs)

Todd(config)#ip ssh time-out 605.Set the max failed attempts for an SSH connection:Todd(config)#ip ssh authentication-retries ?  <0-5>  Number of authentication retries

Todd(config)#ip ssh authentication-retries 26.Connect to the vty lines of the router:

Todd(config)#line vty 0 11807.Last, configure SSH and then Telnet as access protocols:

Todd(config-line)#transport input ssh telnetIf you do not use the keyword telnet at the end of the command string, then only SSH will work on the router. I am not suggesting you use either way, but just understand that SSH is more secure than Telnet.Encrypting Your PasswordsBecause only the enable secret password is encrypted by default, you’ll need to manually configure the user-mode and enable passwords for encryption.Notice that you can see all the passwords except the enable secret when performing a show running-config on a router:

Todd#sh running-configBuilding configuration...[output cut]!enable secret 5 $1$2R.r$DcRaVo0yBnUJBf7dbG9XE0enable password todd![output cut]!line con 0 exec-timeout 0 0 password console logging synchronous login  line aux 0 password aux

10089c04.fm  Page 199  Thursday, August 30, 2007  12:36 PM




200Chapter4 Cisco’s IOS and Security Device Manager loginline vty 0 4 access-class 23 in privilege level 15 password telnet login transport input telnet sshline vty 5 15 access-class 23 in privilege level 15 password telnet login transport input telnet sshline vty 16 1180 password telnet login!

endTo manually encrypt your passwords, use the service password-encryption command. Here’s an example of how to do it:

Todd#config tEnter configuration commands, one per line.  End with CNTL/Z.Todd(config)#service password-encryptionTodd(config)#exitTodd#sh runBuilding configuration...[output cut]!enable secret 5 $1$2R.r$DcRaVo0yBnUJBf7dbG9XE0enable password 7 131118160F![output cut]!line con 0 exec-timeout 0 0 password 7 0605002F5F41051C logging synchronous login  line aux 0

10089c04.fm  Page 200  Thursday, August 30, 2007  12:36 PM




Router and Switch Administrative Configurations201 password 7 03054E13 loginline vty 0 4 access-class 23 in privilege level 15 password 7 01070308550E12 login transport input telnet sshline vty 5 15 access-class 23 in privilege level 15 password 7 01070308550E12 login transport input telnet sshline vty 16 1180 password 7 120D001B1C0E18 login!endTodd#config tTodd(config)#no service password-encryptionTodd(config)#^Z

Todd#There you have it! The passwords will now be encrypted. You just encrypt the passwords, perform a show run, and then turn off the command. You can see that the enable password and the line passwords are all encrypted.But before I get into showing you all about setting descriptions on your routers, let’s talk about encrypting passwords a bit more. As I said, if you set your passwords and then turn on the service password-encryption command, you have to perform a show running-config before you turn off the encryption service or your passwords won’t be encrypted. You don’t have to turn off the encryption service at all; you’d only do that if your router is running low on pro-cesses. And if you turn on the service before you set your passwords, then you don’t even have to view them to get them encrypted.DescriptionsSetting descriptions on an interface is helpful to the administrator and, as with the hostname, only locally significant. The description command is a helpful one because you can, for instance, use it to keep track of circuit numbers.

10089c04.fm  Page 201  Thursday, August 30, 2007  12:36 PM




202Chapter4 Cisco’s IOS and Security Device ManagerHere’s an example:

Todd#config tTodd(config)#int s0/0/0Todd(config-if)#description Wan to SF circuit number 6fdda12345678Todd(config-if)#int fa0/0Todd(config-if)#description Sales VLANTodd(config-if)#^Z

Todd#You can view the description of an interface with either the show running-config com-mand or the show interface command:

Todd#sh run[output cut]!interface FastEthernet0/0 description Sales VLAN ip address 10.10.10.1 255.255.255.248 duplex auto speed auto!interface Serial0/0/0 description Wan to SF circuit number 6fdda 12345678 no ip address shutdown![output cut]Todd#sh int f0/0FastEthernet0/0 is up, line protocol is down  Hardware is MV96340 Ethernet, address is 001a.2f55.c9e8 (bia 001a.2f55.c9e8)  Description: Sales VLAN [output cut]Todd#sh int s0/0/0Serial0/0/0 is administratively down, line protocol is down  Hardware is GT96K Serial

  Description: Wan to SF circuit number 6fdda12345678

10089c04.fm  Page 202  Thursday, August 30, 2007  12:36 PM




Router and Switch Administrative Configurations203Doing the do CommandBeginning with IOS version 12.3, Cisco has finally added a command to the IOS that allows you to view the configuration and statistics from within configuration mode. (In the examples I gave you in the previous section, all show commands were run from privileged mode.)In fact, with a pre-12.3 router, you’d get the following error if you tried to view the con-figuration from global-config:

Router(config)#sh run                ^

% Invalid input detected at ‘^’ marker.Compare that to the output I get from entering that same command on my router that’s running the 12.4 IOS:

Enter configuration commands, one per line.  End with CNTL/Z.Todd(config)#do show runBuilding configuration...Current configuration : 3276 bytes![output cut]        Todd(config)#do sh int f0/0FastEthernet0/0 is up, line protocol is down

description: A Helpful CommandBob, a senior network administrator at Acme Corporation in San Francisco, has over 50 WAN links to various branches throughout the U.S. and Canada. Whenever an interface goes down, Bob spends a lot of time trying to figure out the circuit number as well as the phone number of the provider of the WAN link.The interface description command would be very helpful to Bob because he can use this command on his LAN links to discern exactly where every router interface is connected. And Bob would benefit tremendously by adding circuit numbers to each and every WAN interface, along with the phone number of the responsible provider.So by spending the few hours it would take to add this information to each and every router interface, Bob can save a huge amount of precious time when his WAN links go down—and you know they will!

10089c04.fm  Page 203  Thursday, August 30, 2007  12:36 PM




 204 Chapter4  Cisco’s IOS and Security Device Manager   Hardware is MV96340 Ethernet, address is 001a.2f55.c9e8 (bia    001a.2f55.c9e8)  Description: Sales VLAN

 [output cut] So basically, you can pretty much run any command from any configuration prompt now—cool, huh? Going back to the example of encrypting our passwords, the  do  command would definitely have gotten the party started sooner—so, my friends, this is a very, very good thing indeed! Router Interfaces Interface configuration is one of the most important router configurations because without interfaces, a router is pretty much a completely useless object. Plus, interface configurations must be totally precise to enable communication with other devices. Network layer addresses, media type, bandwidth, and other administrator commands are all used to configure an interface.Different routers use different methods to choose the interfaces used on them. For instance, the following command shows a Cisco 2522 router with 10 serial interfaces, labeled 0 through 9:

 Router(config)# int serial ?

  <0-9> Serial interface number Now it’s time to choose the interface you want to configure. Once you do that, you will be in interface configuration for that specific interface. The following command would be used to choose serial port 5, for example:

 Router(config)# int serial 5

 Router(config-if)# The 2522 router has one Ethernet 10BaseT port, and typing  interface ethernet 0  can configure that interface, as seen here:

 Router(config)# int ethernet ?  <0-0> Ethernet interface numberRouter(config)# int ethernet 0

 Router(config-if)# As I showed you above, the 2500 router is a fixed-configuration router. This means that when you buy that model, you’re stuck with that physical configuration—a huge reason why I don’t use them much. I certainly never would use them in a production setting anymore.To configure an interface, we always used the  interface   type number  sequence, but with the 2600 and 2800 series routers (actually, any ISR router for that matter), there’s a physical 

 

10089c04.fm  Page 204  Friday, November 7, 2008  10:45 PM




Router Interfaces205slot in the router, with a port number on the module plugged into that slot. So on a modular router, the configuration would be interface type slot/port, as seen here:

Router(config)#int fastethernet ? <0-1> FastEthernet interface numberRouter(config)#int fastethernet 0% Incomplete command.Router(config)#int fastethernet 0?/Router(config)#int fastethernet 0/?

 <0-1> FastEthernet interface numberMake note of the fact that you can’t just type int fastethernet 0. You must type the full command: type slot/port, or int fastethernet 0/0 (or int fa 0/0).For the ISR series, it’s basically the same, only you get even more options. For example, the built-in FastEthernet interfaces work with the same configuration we used with the 2600 series:

Todd(config)#int fastEthernet 0/?  <0-1>  FastEthernet interface numberTodd(config)#int fastEthernet 0/0

Todd(config-if)#But the rest of the modules are different—they use three numbers instead of two. The first 0 is the router itself, and then you choose the slot, and then the port. Here’s an example of a serial interface on my 2811:

Todd(config)#interface serial ?  <0-2>  Serial interface numberTodd(config)#interface serial 0/0/?  <0-1>  Serial interface numberTodd(config)#interface serial 0/0/0

Todd(config-if)#This can look a little dicey, I know, but I promise it’s really not that hard! It helps to remem-ber that you should always view a running-config output first so you know what interfaces you have to deal with. Here’s my 2801 output:

Todd(config-if)#do show runBuilding configuration...[output cut]!interface FastEthernet0/0 no ip address shutdown

10089c04.fm  Page 205  Thursday, August 30, 2007  12:36 PM




206Chapter4 Cisco’s IOS and Security Device Manager duplex auto speed auto!interface FastEthernet0/1 no ip address shutdown duplex auto speed auto!interface Serial0/0/0 no ip address shutdown no fair-queue!interface Serial0/0/1 no ip address shutdown!interface Serial0/1/0 no ip address shutdown!interface Serial0/2/0 no ip address shutdown clock rate 2000000!

 [output cut]For the sake of brevity, I didn’t include my complete running-config, but I’ve displayed all you need. You can see the two built-in FastEthernet interfaces, the two serial interface in slot 0 (0/0/0 and 0/0/1), the serial interface in slot 1 (0/1/0), and the serial interface in slot 2 (0/2/0). Once you see the interfaces like this, it makes it a lot easier for you to understand how the modules are inserted into the router.Just understand that if you type interface e0 on a 2500, interface fastethernet 0/0 on a 2600, or interface serial 0/1/0 on a 2800, all you’re doing is choosing an interface to con-figure, and basically, they’re all configured the same way after that.I’m going to continue with our router interface discussion in the next sections, and I’ll include how to bring up the interface and set an IP address on a router interface.Bringing Up an InterfaceYou can disable an interface with the interface command shutdown and enable it with the no shutdown command.

10089c04.fm  Page 206  Thursday, August 30, 2007  12:36 PM




Router Interfaces207If an interface is shut down, it’ll display administratively down when you use the show interfaces command (sh int for short):

Todd#sh int f0/1FastEthernet0/1 is administratively down, line protocol is down

[output cut]Another way to check an interface’s status is via the show running-config command. All interfaces are shut down by default. You can bring up the interface with the no shutdown command (no shut for short):

Todd#config tTodd(config)#int f0/1Todd(config-if)#no shutdownTodd(config-if)#*Feb 28 22:45:08.455: %LINK-3-UPDOWN: Interface FastEthernet0/1,     changed state to upTodd(config-if)#do show int f0/1FastEthernet0/1 is up, line protocol is up

[output cut]Configuring an IP Address on an InterfaceEven though you don’t have to use IP on your routers, it’s most often what people actually do use. To configure IP addresses on an interface, use the ip address command from interface configuration mode:

Todd(config)#int f0/1

Todd(config-if)#ip address 172.16.10.2 255.255.255.0Don’t forget to enable the interface with the no shutdown command. Remember to look at the command show interface int to see if the interface is administratively shut down or not. show running-config will also give you this information.

The ip address address mask command starts the IP processing on the interface.If you want to add a second subnet address to an interface, you have to use the secondary parameter. If you type another IP address and press Enter, it will replace the existing IP address and mask. This is definitely a most excellent feature of the Cisco IOS.So let’s try it. To add a secondary IP address, just use the secondary parameter:

Todd(config-if)#ip address 172.16.20.2 255.255.255.0 ?         secondary  Make this IP address a secondary address  <cr>

10089c04.fm  Page 207  Thursday, August 30, 2007  12:36 PM




208Chapter4 Cisco’s IOS and Security Device ManagerTodd(config-if)#ip address 172.16.20.2 255.255.255.0 secondaryTodd(config-if)#^ZTodd(config-if)#do sh runBuilding configuration...[output cut]interface FastEthernet0/1 ip address 172.16.20.2 255.255.255.0 secondary ip address 172.16.10.2 255.255.255.0 duplex auto speed auto

!I really wouldn’t recommend having multiple IP addresses on an interface because it’s ugly and inefficient, but I showed you anyway just in case you someday find yourself dealing with an MIS manager who’s in love with really bad network design and makes you administer it! And who knows? Maybe someone will ask you about it someday and you’ll get to seem really smart because you know this.Using the PipeNo, not that pipe. I mean the output modifier. (Although with some of the router configura-tions I’ve seen in my career, sometimes I wonder!) This pipe ( | ) allows us to wade through all the configurations or other long outputs and get straight to our goods fast. Here’s an example:

Todd#sh run | ?                                append    Append redirected output to URL (URLs supporting append operation            only)  begin     Begin with the line that matches  exclude   Exclude lines that match  include   Include lines that match  redirect  Redirect output to URL  section   Filter a section of output  tee       Copy output to URLTodd#sh run | begin interfaceinterface FastEthernet0/0 description Sales VLAN ip address 10.10.10.1 255.255.255.248 duplex auto speed auto!interface FastEthernet0/1

10089c04.fm  Page 208  Thursday, August 30, 2007  12:36 PM




Router Interfaces209 ip address 172.16.20.2 255.255.255.0 secondary ip address 172.16.10.2 255.255.255.0 duplex auto speed auto!interface Serial0/0/0 description Wan to SF circuit number 6fdda 12345678 no ip address

!So basically, the pipe symbol (output modifier) is what you need to help you get where you want to go light years faster than mucking around in a router’s entire configuration. I use it a lot when I am looking at a large routing table to find out whether a certain route is in the routing table. Here’s an example:

Todd#sh ip route | include 192.168.3.32R       192.168.3.32 [120/2] via 10.10.10.8, 00:00:25, FastEthernet0/0

Todd#First, you need to know that this routing table had over 100 entries, so without my trusty pipe, I’d probably still be looking through that output! It’s a powerfully efficient tool that saves you major time and effort by quickly finding a line in a configuration—or as the pre-ceding example shows, a single route in a huge routing table.Give yourself a little time to play around with the pipe command; get the hang of it, and you’ll be seriously high on your newfound ability to quickly parse through router output.Serial Interface CommandsWait! Before you just jump in and configure a serial interface, you need some key information—like knowing that the interface will usually be attached to a CSU/DSU type of device that provides clocking for the line to the router, as I’ve shown in Figure 4.4.FIGURE4.4A typical WAN connection

DTE 

DTE Clocking is typically provided by DCE network to routers. In nonproduction environments, a DCE network is not alwa

y

s present. 

DCE 

10089c04.fm  Page 209  Thursday, August 30, 2007  12:36 PM




210Chapter4 Cisco’s IOS and Security Device ManagerHere you can see that the serial interface is used to connect to a DCE network via a CSU/DSU that provides the clocking to the router interface. But if you have a back-to-back configuration (for example, one that’s used in a lab environment like I’ve shown you in Figure 4.5), one end—the data communication equipment (DCE) end of the cable—must provide clocking!FIGURE4.5Providing clocking on a nonproduction networkBy default, Cisco routers are all data terminal equipment (DTE) devices, which means that you must configure an interface to provide clocking if you need it to act like a DCE device. Again, you would not provide clocking on a production T1 connection, for example, because you would have a CSU/DSU connected to your serial interface, as Figure 4.4 shows.You configure a DCE serial interface with the clock rate command:

Todd#config tEnter configuration commands, one per line.  End with CNTL/Z.Todd(config)#int s0/0/0Todd(config-if)#clock rate ?        Speed (bits per second)  1200  2400  4800  9600  14400  19200  28800  32000  38400  48000  56000  57600

DTE DCE DCE side determined by cable.Add clocking to DCE side only.Set clock rate if needed.Todd#config tTodd(config)#interface serial 0 Todd(config-if)#clock rate 64000show controllers will show the cable connection type.

10089c04.fm  Page 210  Thursday, August 30, 2007  12:36 PM




Router Interfaces211  64000  72000  115200  125000  128000  148000  192000  250000  256000  384000  500000  512000  768000  800000  1000000  2000000  4000000  5300000  8000000  <300-8000000>    Choose clockrate from list above

Todd(config-if)#clock rate 1000000The clock rate command is set in bits per second. Besides looking at the cable end to check for a label of DCE or DTE, you can see if a router’s serial interface has a DCE cable con-nected with the show controllers int command:

Todd#sh controllers s0/0/0Interface Serial0/0/0Hardware is GT96K

DTE V.35idb at 0x4342FCB0, driver data structure at 0x434373D4Here is an example of an output that shows a DCE connection:

Todd#sh controllers s0/2/0Interface Serial0/2/0Hardware is GT96K

DCE V.35, clock rate 1000000The next command you need to get acquainted with is the bandwidth command. Every Cisco router ships with a default serial link bandwidth of T1 (1.544Mbps). But this has noth-ing to do with how data is transferred over a link. The bandwidth of a serial link is used by 

10089c04.fm  Page 211  Thursday, August 30, 2007  12:36 PM




212Chapter4 Cisco’s IOS and Security Device Managerrouting protocols such as EIGRP and OSPF to calculate the best cost (path) to a remote net-work. So if you’re using RIP routing, the bandwidth setting of a serial link is irrelevant since RIP uses only hop count to determine that. If you’re rereading this part thinking, “Huh—what? Routing protocols? Metrics?”—don’t freak! I’m going over all that soon in Chapter 6, “IP Routing.”Here’s an example of using the bandwidth command:

Todd#config tTodd(config)#int s0/0/0Todd(config-if)#bandwidth ?  <1-10000000>  Bandwidth in kilobits  inherit       Specify that bandwidth is inherited  receive       Specify receive-side bandwidth

Todd(config-if)#bandwidth 1000Did you notice that, unlike the clock rate command, the bandwidth command is configured in kilobits?

After going through all these configuration examples regarding the clock rate command, understand that the new ISR routers automatically detect DCE connections and set the clock rate to 2000000. However, you still need to understand the clock rate command, even though the new routers set it for you automatically!Viewing, Saving, and Erasing ConfigurationsIf you run through setup mode, you’ll be asked if you want to use the configuration you just created. If you say yes, then it will copy the configuration running in DRAM (known as the running-config) into NVRAM and name the file startup-config. Hopefully, you always will use the CLI or SDM and not setup mode.You can manually save the file from DRAM to NVRAM by using the copy running-config startup-config command (you can use the shortcut copy run start also):

Todd#copy running-config startup-configDestination filename [startup-config]? [press enter]Building configuration...[OK]Todd#

Building configuration...

10089c04.fm  Page 212  Thursday, August 30, 2007  12:36 PM




Viewing, Saving, and Erasing Configurations213When you see a question with an answer in [], it means that if you just press Enter, you’re choosing the default answer.Also, when the command asked for the destination filename, the default answer was startup-config. The reason it asks is because you can copy the configuration pretty much anywhere you want. Take a look:

Todd#copy running-config ?  archive:        Copy to archive: file system  flash:          Copy to flash: file system  ftp:            Copy to ftp: file system  http:           Copy to http: file system  https:          Copy to https: file system  ips-sdf         Update (merge with) IPS signature configuration  null:           Copy to null: file system  nvram:          Copy to nvram: file system  rcp:            Copy to rcp: file system  running-config  Update (merge with) current system configuration  scp:            Copy to scp: file system  startup-config  Copy to startup configuration  syslog:         Copy to syslog: file system  system:         Copy to system: file system  tftp:           Copy to tftp: file system  xmodem:         Copy to xmodem: file system

  ymodem:         Copy to ymodem: file systemWe’ll take a closer look at how and where to copy files in Chapter 5.You can view the files by typing show running-config or show startup-config from privileged mode. The sh run command, which is a shortcut for show running-config, tells us that we are viewing the current configuration:

Todd#show running-config   Building configuration...Current configuration : 3343 bytes!version 12.4

[output cut]The sh start command—one of the shortcuts for the show startup-config command—shows us the configuration that will be used the next time the router is reloaded. It also tells us how much NVRAM is being used to store the startup-config file. Here’s an example:

Todd#show startup-configUsing 1978 out of 245752 bytes

10089c04.fm  Page 213  Thursday, August 30, 2007  12:36 PM




 214 Chapter4  Cisco’s IOS and Security Device Manager !version 12.4

 [output cut] Deleting the Configuration and Reloading the Router You can delete the startup-config file by using the  erase startup-config  command:

 Todd# erase startup-config Erasing the nvram filesystem will remove all configuration files!    Continue? [confirm][ enter ][OK]Erase of nvram: completeTodd#*Feb 28 23:51:21.179: %SYS-7-NV_BLOCK_INIT: Initialized the geometry of nvramTodd# sh startup-config startup-config is not presentTodd# reload Proceed with reload? [confirm]System configuration has been modified.

     Save? [yes/no]:  n If you reload or power down and up the router after using the  erase startup-config  command, you’ll be offered setup mode because there’s no configuration saved in NVRAM. You can press Ctrl+C to exit setup mode at any time (the  reload  command can only be used from privileged mode).At this point, you shouldn’t use setup mode to configure your router. So just say no to setup mode, because it’s there to help people who don’t know how to use the Cash Line Interface (CLI), and this no longer applies to you. Be strong—you can do it! Verifying Your Configuration Obviously,  show running-config  would be the best way to verify your configuration and  show startup-config  would be the best way to verify the configuration that’ll be used the next time the router is reloaded—right?Well, once you take a look at the running-config, if all appears well, you can verify your con-figuration with utilities such as Ping and Traceroute. Ping is Packet Internet Groper, a program that uses ICMP echo requests and replies. (ICMP is discussed in Chapter 2, “Introduction to TCP/IP.”) Ping sends a packet to a remote host, and if that host responds, you know that it is alive. But you don’t know if it’s alive and also  well —just because you can ping a Microsoft server does not mean you can log in! Even so, Ping is an awesome starting point for troubleshooting an internetwork.

 

10089c04.fm  Page 214  Friday, November 7, 2008  10:47 PM




Viewing, Saving, and Erasing Configurations215Did you know that you can ping with different protocols? You can, and you can test this by typing ping ? at either the router user-mode or privileged-mode prompt:

Router#ping ?  WORD       Ping destination address or hostname  appletalk  Appletalk echo  clns       CLNS echo  decnet     DECnet echo  ip         IP echo  ipv6       IPv6 echo  ipx        Novell/IPX echo  srb        srb echo  tag        Tag encapsulated IP echo

  <cr>If you want to find a neighbor’s Network layer address, either you need to go to the router or switch itself or you can type show cdp entry * protocol to get the Network layer addresses you need for pinging.

Cisco Discovery Protocol (CDP) is covered in Chapter 5.Traceroute uses ICMP with IP time to live (TTL) time-outs to track the path a packet takes through an internetwork, in contrast to Ping, which just finds the host and responds. And Traceroute can also be used with multiple protocols.

Router#traceroute ?  WORD       Trace route to destination address or hostname  appletalk  AppleTalk Trace  clns       ISO CLNS Trace  ip         IP Trace  ipv6       IPv6 Trace  ipx        IPX Trace

  <cr>Telnet, FTP, and HTTP are really the best tools because they use IP at the Network layer and TCP at the Transport layer to create a session with a remote host. If you can telnet, ftp, or http into a device, your IP connectivity just has to be good.

Router#telnet ? WORD IP address or hostname of a remote system

 <cr>

10089c04.fm  Page 215  Thursday, August 30, 2007  12:36 PM




216Chapter4 Cisco’s IOS and Security Device ManagerFrom the router prompt, you just type a hostname or IP address and it will assume you want to telnet—you don’t need to type the actual command, telnet.In the following sections, I am going to show you how to verify the interface statistics.Verifying with the show interface CommandAnother way to verify your configuration is by typing show interface commands, the first of which is show interface ?. That will reveal all the available interfaces to configure.

The show interfaces command displays the configurable parameters and statistics of all interfaces on a router.This command is very useful for verifying and troubleshooting router and network issues.The following output is from my freshly erased and rebooted 2811 router:

Router#sh int ?  Async              Async interface  BVI                Bridge-Group Virtual Interface  CDMA-Ix            CDMA Ix interface  CTunnel            CTunnel interface  Dialer             Dialer interface  FastEthernet       FastEthernet IEEE 802.3  Loopback           Loopback interface  MFR                Multilink Frame Relay bundle interface  Multilink          Multilink-group interface  Null               Null interface  Port-channel       Ethernet Channel of interfaces  Serial             Serial  Tunnel             Tunnel interface  Vif                PGM Multicast Host interface  Virtual-PPP        Virtual PPP interface  Virtual-Template   Virtual Template interface  Virtual-TokenRing  Virtual TokenRing  accounting         Show interface accounting  counters           Show interface counters  crb                Show interface routing/bridging info  dampening          Show interface dampening info  description        Show interface description  etherchannel       Show interface etherchannel information  irb                Show interface routing/bridging info  mac-accounting     Show interface MAC accounting info  mpls-exp           Show interface MPLS experimental accounting info

10089c04.fm  Page 216  Thursday, August 30, 2007  12:36 PM




Viewing, Saving, and Erasing Configurations217  precedence         Show interface precedence accounting info  pruning            Show interface trunk VTP pruning information  rate-limit         Show interface rate-limit info  stats              Show interface packets & octets, in & out, by switching                     path  status             Show interface line status  summary            Show interface summary  switching          Show interface switching  switchport         Show interface switchport information  trunk              Show interface trunk information  |                  Output modifiers

  <cr>The only “real” physical interfaces are FastEthernet, Serial, and Async; the rest are all log-ical interfaces or commands to verify with.The next command is show interface fastethernet 0/0. It reveals to us the hardware address, logical address, and encapsulation method as well as statistics on collisions, as seen here:

Router#sh int f0/0FastEthernet0/0 is up, line protocol is up  Hardware is MV96340 Ethernet, address is 001a.2f55.c9e8 (bia 001a.2f55.c9e8)  Internet address is 192.168.1.33/27MTU 1500 bytes, BW 100000 Kbit, DLY 100 usec,     reliability 255/255, txload 1/255, rxload 1/255  Encapsulation ARPA, loopback not set  Keepalive set (10 sec)  Auto-duplex, Auto Speed, 100BaseTX/FX  ARP type: ARPA, ARP Timeout 04:00:00  Last input never, output 00:02:07, output hang never  Last clearing of “show interface” counters never  Input queue: 0/75/0/0 (size/max/drops/flushes); Total output drops: 0  Queueing strategy: fifo  Output queue: 0/40 (size/max)  5 minute input rate 0 bits/sec, 0 packets/sec  5 minute output rate 0 bits/sec, 0 packets/sec     0 packets input, 0 bytes     Received 0 broadcasts, 0 runts, 0 giants, 0 throttles     0 input errors, 0 CRC, 0 frame, 0 overrun, 0 ignored     0 watchdog     0 input packets with dribble condition detected     16 packets output, 960 bytes, 0 underruns     0 output errors, 0 collisions, 0 interface resets

10089c04.fm  Page 217  Thursday, August 30, 2007  12:36 PM




218Chapter4 Cisco’s IOS and Security Device Manager     0 babbles, 0 late collision, 0 deferred     0 lost carrier, 0 no carrier     0 output buffer failures, 0 output buffers swapped out

Router# As you probably guessed, we’re going to discuss the important statistics from this output, but first, for fun (this is all fun, right?), I’ve got to ask you, What subnet is the FastEthernet 0/0 a member of and what’s the broadcast address and valid host range?And, my friend, you really have to be able to nail these things Nascar fast! Just in case you didn’t, the address is 192.168.1.33/27. And I’ve gotta be honest—if you don’t know what a /27 is at this point, you’ll need a miracle to pass the exam. (A /27 is 255.255.255.224.) The fourth octet is a block size of 32. The subnets are 0, 32, 64, …; the FastEthernet interface is in the 32 subnet; the broadcast address is 63; and the valid hosts are 33–62.

If you struggled with any of this, please save yourself from certain doom and get yourself back into Chapter 3, “Subnetting, Variable Length Subnet Masks (VLSMs), and Troubleshooting TCP/IP,” now! Read and reread it until you’ve got it dialed in!The preceding interface is working and looks to be in good shape. The show interfaces command will show you if you are receiving errors on the interface, and it will show you the maximum transmission units (MTUs), bandwidth (BW), reliability (255/255 means perfect!), and load (1/255 means no load).Continuing to use the previous output, what is the bandwidth of the interface? Well, other than the easy giveaway of the interface being called a “FastEthernet” interface, we can see the bandwidth is 100000Kbit, which is 100,000,000 (Kbit means to add three zeros), which is 100Mbits per second, or FastEthernet. Gigabit would be 1000000Kbits per second.The most important statistic of the show interface command is the output of the line and data-link protocol status. If the output reveals that FastEthernet 0/0 is up and the line protocol is up, then the interface is up and running:

Router#sh int fa0/0

FastEthernet0/0 is up, line protocol is upThe first parameter refers to the Physical layer, and it’s up when it receives carrier detect. The second parameter refers to the Data Link layer, and it looks for keepalives from the con-necting end. (Keepalives are used between devices to make sure connectivity has not dropped.)Here’s an example of where the problem usually is found—on serial interfaces:

Router#sh int s0/0/0

Serial0/0 is up, line protocol is downIf you see that the line is up but the protocol is down, as shown above, you’re experiencing a clocking (keepalive) or framing problem—possibly an encapsulation mismatch. Check the keepalives on both ends to make sure that they match; that the clock rate is set, if needed, and 

10089c04.fm  Page 218  Thursday, August 30, 2007  12:36 PM




Viewing, Saving, and Erasing Configurations219that the encapsulation type is the same on both ends. The output above would be considered a Data Link layer problem.If you discover that both the line interface and the protocol are down, it’s a cable or inter-face problem. The following output would be considered a Physical layer problem:

Router#sh int s0/0/0

Serial0/0 is down, line protocol is downIf one end is administratively shut down (as shown next), the remote end would present as down and down:

Router#sh int s0/0/0

Serial0/0 is administratively down, line protocol is downTo enable the interface, use the command no shutdown from interface configuration mode.The next show interface serial 0/0/0 command demonstrates the serial line and the maximum transmission unit (MTU)—1,500 bytes by default. It also shows the default band-width (BW) on all Cisco serial links: 1.544Kbps. This is used to determine the bandwidth of the line for routing protocols such as EIGRP and OSPF. Another important configuration to notice is the keepalive, which is 10 seconds by default. Each router sends a keepalive message to its neighbor every 10 seconds, and if both routers aren’t configured for the same keepalive time, it won’t work.

Router#sh int s0/0/0Serial0/0 is up, line protocol is up Hardware is HD64570 MTU 1500 bytes, BW 1544 Kbit, DLY 20000 usec,   reliability 255/255, txload 1/255, rxload 1/255 Encapsulation HDLC, loopback not set, keepalive set  (10 sec) Last input never, output never, output hang never Last clearing of “show interface” counters never Queueing strategy: fifo Output queue 0/40, 0 drops; input queue 0/75, 0 drops 5 minute input rate 0 bits/sec, 0 packets/sec 5 minute output rate 0 bits/sec, 0 packets/sec   0 packets input, 0 bytes, 0 no buffer   Received 0 broadcasts, 0 runts, 0 giants, 0 throttles   0 input errors, 0 CRC, 0 frame, 0 overrun, 0 ignored,   0 abort   0 packets output, 0 bytes, 0 underruns   0 output errors, 0 collisions, 16 interface resets   0 output buffer failures, 0 output buffers swapped out   0 carrier transitions

   DCD=down DSR=down DTR=down RTS=down CTS=down

10089c04.fm  Page 219  Thursday, August 30, 2007  12:36 PM




220Chapter4 Cisco’s IOS and Security Device ManagerYou can clear the counters on the interface by typing the command clear counters:

Router#clear counters ?  Async              Async interface  BVI                Bridge-Group Virtual Interface  CTunnel            CTunnel interface  Dialer             Dialer interface  FastEthernet       FastEthernet IEEE 802.3  Group-Async        Async Group interface  Line               Terminal line  Loopback           Loopback interface  MFR                Multilink Frame Relay bundle interface  Multilink          Multilink-group interface  Null               Null interface  Serial             Serial  Tunnel             Tunnel interface  Vif                PGM Multicast Host interface  Virtual-Template   Virtual Template interface  Virtual-TokenRing  Virtual TokenRing  <cr>Router#clear counters s0/0/0Clear “show interface” counters on this interface  [confirm][enter]Router#00:17:35: %CLEAR-5-COUNTERS: Clear counter on interface  Serial0/0/0 by console

Router#Verifying with the show ip interface CommandThe show ip interface command will provide you with information regarding the layer 3 configurations of a router’s interfaces:

Router#sh ip interfaceFastEthernet0/0 is up, line protocol is up  Internet address is 1.1.1.1/24  Broadcast address is 255.255.255.255  Address determined by setup command  MTU is 1500 bytes  Helper address is not set  Directed broadcast forwarding is disabled  Outgoing access list is not set

10089c04.fm  Page 220  Thursday, August 30, 2007  12:36 PM




Viewing, Saving, and Erasing Configurations221  Inbound  access list is not set  Proxy ARP is enabled  Security level is default  Split horizon is enabled

[output cut]The status of the interface, the IP address and mask, information on whether an access list is set on the interface, and basic IP information are included in this output.Using the show ip interface brief CommandThe show ip interface brief command is probably one of the most helpful commands that you can ever use on a Cisco router. This command provides a quick overview of the router’s interfaces, including the logical address and status:

Router#sh ip int briefInterface         IP-Address      OK? Method Status  ProtocolFastEthernet0/0    unassigned      YES unset  up       up  FastEthernet0/1    unassigned      YES unset  up       up  Serial0/0/0        unassigned      YES unset  up       down  Serial0/0/1        unassigned      YES unset  administratively down down  Serial0/1/0        unassigned      YES unset  administratively down down  

Serial0/2/0        unassigned      YES unset  administratively down down  Remember, administratively down means that you need to type no shutdown under the interface. Notice that Serial0/0/0 is up/down, which means that the physical layer is good and carrier detect is sensed but no keepalives are being received from the remote end. In a nonpro-duction network, like the one I am working with, the clock rate isn’t set.Verifying with the show protocols CommandThe show protocols command is a really helpful command you’d use in order to quickly see the status of layers 1 and 2 of each interface as well as the IP addresses used.Here’s a look at one of my production routers:

Router#sh protocolsGlobal values:  Internet Protocol routing is enabledEthernet0/0 is administratively down, line protocol is downSerial0/0 is up, line protocol is up  Internet address is 100.30.31.5/24Serial0/1 is administratively down, line protocol is downSerial0/2 is up, line protocol is up  Internet address is 100.50.31.2/24Loopback0 is up, line protocol is up

  Internet address is 100.20.31.1/24

10089c04.fm  Page 221  Thursday, August 30, 2007  12:36 PM




222Chapter4 Cisco’s IOS and Security Device ManagerUsing the show controllers CommandThe show controllers command displays information about the physical interface itself. It’ll also give you the type of serial cable plugged into a serial port. Usually, this will only be a DTE cable that plugs into a type of data service unit (DSU).

Router#sh controllers serial 0/0HD unit 0, idb = 0x1229E4, driver structure at 0x127E70buffer size 1524 HD unit 0, V.35 DTE cablecpb = 0xE2, eda = 0x4140, cda = 0x4000Router#sh controllers serial 0/1HD unit 1, idb = 0x12C174, driver structure at 0x131600buffer size 1524 HD unit 1, V.35 DCE cable

cpb = 0xE3, eda = 0x2940, cda = 0x2800Notice that serial 0/0 has a DTE cable, whereas the serial 0/1 connection has a DCE cable. Serial 0/1 would have to provide clocking with the clock rate command. Serial 0/0 would get its clocking from the DSU.Let’s look at this command again. In Figure 4.5, see the DTE/DCE cable between the two routers? Know that you will not see this in production networks!FIGURE4.6The show controllers CommandRouter R1 has a DTE connection—the default for all Cisco routers. Routers R1 and R2 can’t communicate. Check out the output of the show controllers s0/0 command here:

R1#sh controllers serial 0/0HD unit 0, idb = 0x1229E4, driver structure at 0x127E70buffer size 1524 HD unit 0, V.35 DCE cable

cpb = 0xE2, eda = 0x4140, cda = 0x4000The show controllers s0/0 command shows that the interface is a V.35 DCE cable. This means that R1 needs to provide clocking of the line to router R2. Basically, the interface has the wrong label on the cable on the R1 router’s serial interface. But if you add clocking on the R1 router’s serial interface, the network should come right up.Let’s check out another issue, shown in Figure 4.6, that you can solve by using the show controllers command. Again, routers R1 and R2 can’t communicate.

R1 R2 DTE DCE 

10089c04.fm  Page 222  Thursday, August 30, 2007  12:36 PM




Cisco’s Security Device Manager (SDM)223FIGURE4.7The show controllers command used with the show ip interface commandHere’s the output of R1’s show controllers s0/0 command and show ip interface s0/0:

R1#sh controllers s0/0HD unit 0, idb = 0x1229E4, driver structure at 0x127E70buffer size 1524 HD unit 0,DTE V.35 clocks stoppedcpb = 0xE2, eda = 0x4140, cda = 0x4000R1#sh ip interface s0/0Serial0/0 is up, line protocol is down  Internet address is 192.168.10.2/24

  Broadcast address is 255.255.255.255If you use the show controllers command and the show ip interface command, you’ll see that router R1 isn’t receiving clocking of the line. This network is a nonproduction net-work, so no CSU/DSU is connected to provide clocking of the line. This means the DCE end of the cable will be providing the clock rate—in this case, the R2 router. The show ip interface indicates that the interface is up but the protocol is down, which means that no keepalives are being received from the far end. In this example, the likely culprit is the result of bad cable, or no clocking.Cisco’s Security Device Manager (SDM)So finally, here it is… the part about Cisco’s SDM. This beauty is used to help you configure a router from an HTTP or HTTPS interface. Actually, to be real, Cisco did produce something like this in the past, but honestly, it just didn’t work all that well. But this one does—really! And by the way, SDM is available on Cisco router models from Cisco 830 Series to 7301. Plus, it’s preinstalled on all new 850, 870, 1800, 2800, and 3800 series routers. It’s all good!

For the Cisco PIX product, use the Pix Device Manager (PDM), not the SDM.But there is a catch. Although the SDM is a really great tool, it’s best if you pull it out just for advanced configurations—not the super small, simple configurations like those we used in this chapter. Let me explain. Let’s say you wanted to set up an advanced access list, VPN with IPSec, and intrusion protection on your router. If so, then SDM is your baby. You don’t even have to know what IPSec means to configure it and make it work.

R1 R2 S0/0 S0

/

0 

10089c04.fm  Page 223  Thursday, August 30, 2007  12:36 PM




224Chapter4 Cisco’s IOS and Security Device ManagerBut again, there’s both good news and bad news with this. Of course, we can now handle advanced configurations much more easily, but at the same time, it’s not so good because, well, literally anyone can do the same thing!What I’m going to do is show you how to log in using SDM; set your hostname, banner, and enable secret password; and assign a DHCP pool on a router and an IP address to an inter-face. And if it all goes well, after this section you’ll find that SDM will turn out to be much more of a good thing because it will prove to be so much more helpful to you when you’re reading the chapters on the IOS, NAT, wireless technologies, and security than the way it used to be when we were all mired down in basic IOS router configuration. I’ll use it a little in every chapter both to show you the ease you’ll gain and to explain its complexity. But I promise to truly expose the limitations of SDM as well.Honestly, I could write a whole book about SDM, but I don’t need to because Cisco did it already. To find more details about SDM, go to www.cisco.com/go/sdm. Plus, a new router will typically come with a CD that walks you through physically connecting your router and configuring it step by step (overkill in my opinion), but you truly don’t need this CD to connect to your router and use SDM.All you need is a supported ISR router (1800/2800, etc.) and you can download the latest version of SDM complete with instructions for installing it on your computer and your router from the following location: www.cisco.com/pcgi-bin/tablebuild.pl/sdm.

To download this software, you need a Cisco Connection Online (CCO) login. However, this is free and only takes a minute or less to set up. Go to www.cisco.com and click on Register in the top-right corner. Fill in the short form, add the username and password you want to use, and you can now download the SDM and the SDM demo.From this site, you can not only install the SDM on your computer to help the SDM pages load faster when connecting to your router, you can also enable the use of the Cisco SDM demo itself.

I highly recommend downloading and running through the SDM demo if you don’t have an ISR router to play with. First, install SDM on your computer, and then download the demo at www.cisco.com/pcgi-bin/tablebuild.pl/sdm-tool-demo. Read the install instructions, or run through Hands-on Lab 4.6 at the end of this chapter to get you set up and going strong with SDM.You need to know this though—to set up your host to log in using the SDM, you have to make sure your router is configured first. In the preceding section, I had deleted my configu-rations and reloaded the router, so I had to start from scratch. But doing that really isn’t all that hard. You can just choose a LAN interface of the router and then connect a host directly to the router using a crossover cable—not so bad!

Would you like to enter the initial configuration dialog? [yes/no]: nPress RETURN to get started!

10089c04.fm  Page 224  Thursday, August 30, 2007  12:36 PM




Cisco’s Security Device Manager (SDM)225Router>enRouter#config tEnter configuration commands, one per line.  End with CNTL/Z.Router(config)#int f0/0Router(config-if)#ip address 1.1.1.1 255.255.255.0Router(config-if)#no shutRouter(config-if)#do ping 1.1.1.2Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 1.1.1.2, timeout is 2 seconds:!!!!!

Success rate is 100 percent (5/5), round-trip min/avg/max = 1/1/4 msSo what’s this all about? In the preceding configuration, I configured the FastEthernet interface with an IP address and enabled it with the no shutdown command. I then tested my connection by pinging my directly connected host from the router prompt. (This would be a minimum con-figuration and will allow you to connect via SDM.) From here, you just open a browser, with pop-ups enabled, type http://1.1.1.1, and follow the easy prompts once connected.It’s different if you want to set the router to use HTTPS, which allows you access into priv-ileged mode upon connection (meaning we’re setting the router back to the original default configuration). And you need to add a few more commands.First, enable the HTTP/HTTPS server (your router won’t support HTTPS if it doesn’t have the advanced services IOS):

Router(config)#ip http serverRouter(config)#ip http secure-server% Generating 1024 bit RSA keys, keys will be non-exportable...[OK]

Router(config)#ip http authentication localSecond, create a user account using privilege level 15 (the highest level):

Router(config)#username cisco privilege ?  <0-15>  User privilege levelRouter(config)#username cisco privilege 15 password ?  0     Specifies an UNENCRYPTED password will follow  7     Specifies a HIDDEN password will follow  LINE  The UNENCRYPTED (cleartext) user password

Router(config)#username cisco privilege 15 password 0 ciscoLast, configure the console, SSH, and Telnet to provide local login authentication at privilege level access:

Router(config)#line console 0Router(config-line)#login local

10089c04.fm  Page 225  Thursday, August 30, 2007  12:36 PM




226Chapter4 Cisco’s IOS and Security Device ManagerRouter(config-line)#exitRouter(config)#line vty 0 ?  <1-1180>  Last Line number  <cr>Router(config)#line vty 0 1180Router(config-line)#privilege level 15Router(config-line)#login localRouter(config-line)#transport input telnetRouter(config-line)#transport input telnet ssh

Router(config-line)#^ZVoilà! Now let’s connect to my 2811 using HTTPS!Okay—as soon as I connected via https://1.1.1.1, I received a security alert message.I was then prompted to log in with the username/password I created. SDM started loading and told me to hold on, which means it needs a bit of time to load another window. Do not close this window.

10089c04.fm  Page 226  Thursday, August 30, 2007  12:36 PM




 Cisco’s Security Device Manager (SDM) 227 The certificate that I created with the  ip http secure-server  command was loaded onto the router. I chose to click on Always Trust Content from this Publisher and then clicked Yes.Of course, the certificate would not match any site name, so I then had to verify that I wanted to run it.I then had to log in again and then wait for SDM to load, during which time the router had me change my default username and password.

 

10089c04.fm  Page 227  Wednesday, October 29, 2008  2:50 PM




228Chapter4 Cisco’s IOS and Security Device ManagerFinally—yes! I am connected to SDM!Clicking the Configure button at the top of the page, I chose to go step by step through interface configuration by first choosing the type of interface I wanted to configure and then clicking the Create New Connection button at the bottom of the page. This opens the LAN or WAN wizard, depending on which interface you choose to configure. (We’ll configure our router interfaces with the Interface Wizard in Chapter 6.)

10089c04.fm  Page 228  Thursday, August 30, 2007  12:36 PM




Cisco’s Security Device Manager (SDM)229By clicking the Edit Interface/Connection tab, you get to see your interface status.That’s not all—just double-click on an interface to edit it. (You can only do this after you’ve gone through the LAN or WAN wizard and configured the interface.)

10089c04.fm  Page 229  Thursday, August 30, 2007  12:36 PM




230Chapter4 Cisco’s IOS and Security Device ManagerLook down at the bottom left portion of the wizard page and click the Additional Tasks button. From there, just click the Router Properties icon.Here, you can set the hostname, MOTD banner, and enable secret password. Last, I clicked on the DHCP folder, then the DHCP pool icon. I then clicked Add and created a DHCP pool on my router.

10089c04.fm  Page 230  Thursday, August 30, 2007  12:36 PM




Cisco’s Security Device Manager (SDM)231Now, let’s take a look at the configuration on the router:

Todd#sh runBuilding configuration...[output cut]hostname Todd!ip domain name lammle.com[output cut]ip dhcp excluded-address 172.16.10.1ip dhcp excluded-address 172.16.10.11 172.16.10.254!      ip dhcp pool Todd’s_LAN   import all   network 172.16.10.0 255.255.255.0!crypto pki trustpoint TP-self-signed-2645776477 enrollment selfsigned subject-name cn=IOS-Self-Signed-Certificate-2645776477 revocation-check none rsakeypair TP-self-signed-2645776477!crypto pki certificate chain TP-self-signed-2645776477 certificate self-signed 01  3082023E 308201A7 A0030201 02020101 300D0609 2A864886 F70D0101   04050030 31312F30 2D060355 04031326 494F532D 53656C66 2D536967   6E65642D 43657274 69666963 6174652D 32363435 37373634 3737301E   170D3037 30333031 3139313 33335A17 0D323030 31303130 30303030   305A3031 312F302D 06035504 03132649 4F532D53 656C662D 5369676E   65642D43 65727469 66696361 74652D32 36343537 37363437 3730819F   300D0609 2A864886 F70D0101 01050003 818D0030 81890281 8100BB24   [output cut]  quitusername todd privilege 15 secret 5 $1$nvgs$QRNCWKJ7rfmtNNkD2xvGq/[output cut]!line con 0 login localline aux 0line vty 0 4 privilege level 15

10089c04.fm  Page 231  Thursday, August 30, 2007  12:36 PM




232Chapter4 Cisco’s IOS and Security Device Manager login local transport input telnet sshline vty 5 1180 privilege level 15 login local transport input telnet ssh

!So what we can see here is that the router created the hostname, DHCP pool, and certifi-cate. We went through a lot of setup because we were using HTTPS—it’s so much easier and unencumbered with less setup to just use HTTP. But remember, what we used was an ISR default configuration to connect using SDM. We’ll continue to use SDM throughout this book, but I seriously encourage you to get SDM for yourself and start getting familiar with it!SummaryThis was a fun chapter! I showed you a lot about the Cisco IOS and I really hope you gained a lot of insight into the Cisco router world. This chapter started off by explaining the Cisco Internetwork Operating System (IOS) and how you can use the IOS to run and configure Cisco routers. You learned how to bring a router up and what setup mode does. Oh, and by the way, since you can now basically configure Cisco routers, you should never use setup mode, right?After I discussed how to connect to a router with a console and LAN connection, I covered the Cisco help features and how to use the CLI to find commands and command parameters. In addition, I discussed some basic show commands to help you verify your configurations.Administrative functions on a router help you administer your network and verify that you are configuring the correct device. Setting router passwords is one of the most important con-figurations you can perform on your routers. I showed you the five passwords to set. In addition, I used the hostname, interface description, and banners to help you administer your router.Last, I showed you how to configure the router so you can connect using the Secure Device Manager and configure your router. Again, it’s a whole lot easier to use the CLI to configure basic router functions, but I’ll show you how SDM can really help with the more advanced configurations coming up soon.Well, that concludes your introduction to the Cisco IOS. And, as usual, it’s super-important for you to have the basics that we went over in this chapter before you move on to the follow-ing chapters.

10089c04.fm  Page 232  Thursday, August 30, 2007  12:36 PM




Exam Essentials233Exam EssentialsUnderstand what happens (and the sequence in which it happens) when you power on a router.When you first bring up a Cisco router, it will run a power-on self-test (POST), and if that passes, it will look for and load the Cisco IOS from flash memory, if a file is present. The IOS then proceeds to load and looks for a valid configuration in NVRAM called the startup-config. If no file is present in NVRAM, the router will go into setup mode.Remember what setup mode provides.Setup mode is automatically started if a router boots and no startup-config is in NVRAM. You can also bring up setup mode by typing setup from privileged mode. Setup provides a minimum amount of configuration in an easy format for someone who does not understand how to configure a Cisco router from the command line.Understand the difference between user mode and privileged mode.User mode provides a command-line interface with very few available commands by default. User mode does not allow the configuration to be viewed or changed. Privileged mode allows a user to both view and change the configuration of a router. You can enter privileged mode by typing the com-mand enable and entering the enable password or enable secret password, if set.Remember what the command show version provides.The show version command will provide basic configuration for the system hardware as well as the software version, the names and sources of configuration files, the config-register setting, and the boot images.Remember how to set the hostname of a router.The command sequence to set the hostname of a router is as follows:

enableconfig t

hostname ToddRemember the difference between the enable password and enable secret password.Both of these passwords are used to gain access into privileged mode. However, the enable secret password is newer and is always encrypted by default. Also, if you set the enable password and then set the enable secret, only the enable secret will be used.Remember how to set the enable secret on a router.To set the enable secret, you use the command enable secret. Do not use enable secret password password or you will set your password to password password. Here is an example:

enableconfig t

enable secret toddRemember how to set the console password on a router.To set the console password, use the following sequence:

enableconfig t

10089c04.fm  Page 233  Thursday, August 30, 2007  12:36 PM




234Chapter4 Cisco’s IOS and Security Device Managerline console 0login

password toddRemember how to set the Telnet password on a router.To set the Telnet password, the sequence is as follows:

enableconfig tline vty 0 4password todd

loginUnderstand how to troubleshoot a serial link problem.If you type show interface serial 0 and see down, line protocol is down, this will be considered a Physical layer problem. If you see it as up, line protocol is down, then you have a Data Link layer problem.Understand how to verify your router with the show interfaces commandIf you type show interfaces, you can view the statistics for the interfaces on the router, verify whether the interfaces are shut down, and see the IP address of each interface.Written Lab 4Write out the command or commands for the following questions:1.What command is used to set a serial interface to provide clocking to another router at 64Kb?2.If you telnet into a router and get the response connection refused, password not set, what would you do on the destination router to stop receiving this message and not be prompted for a password?3.If you type show inter et 0 and notice the port is administratively down, what would you do?4.If you wanted to delete the configuration stored in NVRAM, what would you type?5.If you wanted to set a user-mode password for the console port, what would you type?6.If you wanted to set the enable secret password to cisco, what would you type?7.If you wanted to see if a serial interface needed to provide clocking, what command would you use?8.What command would you use to see the terminal history size?9.You want to reinitialize the router and totally replace the running-config with the current startup-config. What command will you use?10.How would you set the name of a router to Chicago?(The answers to Written Lab 4 can be found following the answers to the review questions for this chapter.)

10089c04.fm  Page 234  Thursday, August 30, 2007  12:36 PM




Hands-on Labs235Hands-on LabsIn this section, you will perform commands on a Cisco router that will help you understand what you learned in this chapter.You’ll need at least one Cisco router—two would be better, three would be outstanding. The hands-on labs in this section are included for use with real Cisco routers. If you are using software from RouterSim.com or Sybex, please use the hands-on labs found in those pro-grams. It doesn’t matter what series type router you use with these labs (i.e., 2500, 2600, 800, 1800, or 2800).This chapter includes the following six labs:Lab 4.1: Logging into a RouterLab 4.2: Using the Help and Editing FeaturesLab 4.3: Saving a Router ConfigurationLab 4.4: Setting Your PasswordsLab 4.5: Setting the Hostname, Descriptions, IP Address, and Clock RateLab 4.6: Installing SDM on Your ComputerHands-on Lab 4.1: Logging into a Router1.Press Enter to connect to your router. This will put you into user mode.2.At the Router> prompt, type a question mark (?).3.Notice the –more– at the bottom of the screen.4.Press the Enter key to view the commands line by line. Press the spacebar to view the com-mands a full screen at a time. You can type q at any time to quit.5.Type enable or en and press Enter. This will put you into privileged mode where you can change and view the router configuration.6.At the Router# prompt, type a question mark (?). Notice how many options are available to you in privileged mode.7.Type q to quit.8.Type config and press Enter.9.Press Enter to configure your router using your terminal.10.At the Router(config)# prompt, type a question mark (?), then q to quit, or hit the spacebar to view the commands.11.Type interface e0 or int e0 (or even int fa0/0) and press Enter. This will allow you to configure interface Ethernet 0.12.At the Router(config-if)# prompt, type a question mark (?).13.Type int s0 (int s0/0) or interface s0 (same as the interface serial 0 com-mand) and press Enter. This will allow you to configure interface serial 0. Notice that you can go from interface to interface easily.

10089c04.fm  Page 235  Thursday, August 30, 2007  12:36 PM




236Chapter4 Cisco’s IOS and Security Device Manager14.Type encapsulation ?.15.Type exit. Notice how this brings you back one level.16.Press Ctrl+Z. Notice how this brings you out of configuration mode and places you back into privileged mode.17.Type disable. This will put you into user mode.18.Type exit, which will log you out of the router.Hands-on Lab 4.2: Using the Help and Editing Features1.Log into the router and go to privileged mode by typing en or enable.2.Type a question mark (?).3.Type cl? and then press Enter. Notice that you can see all the commands that start with cl.4.Type clock ? and press Enter.

Notice the difference between steps 3 and 4. Step 3 has you type letters with no space and a question mark, which will give you all the commands that start with cl. Step 4 has you type a command, space, and question mark. By doing this, you will see the next available parameter.5.Set the router’s clock by typing clock ? and, following the help screens, setting the router’s time and date.6.Type clock ?.7.Type clock set ?.8.Type clock set 10:30:30 ?.9.Type clock set 10:30:30 14 March ?.10.Type clock set 10:30:30 14 March 2002.11.Press Enter.12.Type show clock to see the time and date.13.From privileged mode, type show access-list 10. Don’t press Enter.14.Press Ctrl+A. This takes you to the beginning of the line.15.Press Ctrl+E. This should take you back to the end of the line.16.Press Ctrl+A, then Ctrl+F. This should move you forward one character.17.Press Ctrl+B, which will move you back one character.18.Press Enter, then press Ctrl+P. This will repeat the last command.19.Press the up arrow key on your keyboard. This will also repeat the last command.20.Type sh history. This shows you the last 10 commands entered.

10089c04.fm  Page 236  Thursday, August 30, 2007  12:36 PM




Hands-on Labs23721.Type terminal history size ?. This changes the history entry size. The ? is the number of allowed lines.22.Type show terminal to gather terminal statistics and history size.23.Type terminal no editing. This turns off advanced editing. Repeat steps 14 through 18 to see that the shortcut editing keys have no effect until you type terminal editing.24.Type terminal editing and press Enter to re-enable advanced editing.25.Type sh run, then press your Tab key. This will finish typing the command for you.26.Type sh start, then press your Tab key. This will finish typing the command for you.Hands-on Lab 4.3: Saving a Router Configuration1.Log into the router and go into privileged mode by typing en or enable, then press Enter.2.To see the configuration stored in NVRAM, type sh start and press Tab and Enter, or type show startup-config and press Enter. However, if no configuration has been saved, you will get an error message.3.To save a configuration to NVRAM, which is known as startup-config, you can do one of the following: Type copy run start and press Enter. Type copy running, press Tab, type start, press Tab, and press Enter. Type copy running-config startup-config and press Enter.4.Type sh start, press Tab, then press Enter.5.Type sh run, press Tab, then press Enter.6.Type erase start, press Tab, then press Enter.7.Type sh start, press Tab, then press Enter. You should get an error message.8.Type reload, then press Enter. Acknowledge the reload by pressing Enter. Wait for the router to reload.9.Say no to entering setup mode, or just press Ctrl+C.Hands-on Lab 4.4: Setting Your Passwords1.Log into the router and go into privileged mode by typing en or enable.2.Type config t and press Enter.3.Type enable ?.4.Set your enable secret password by typing enable secret password (the third word should be your own personalized password) and pressing Enter. Do not add the param-eter password after the parameter secret (this would make your password the word password). An example would be enable secret todd.

10089c04.fm  Page 237  Thursday, August 30, 2007  12:36 PM




238Chapter4 Cisco’s IOS and Security Device Manager5.Now let’s see what happens when you log all the way out of the router and then log in. Log out by pressing Ctrl+Z, and then type exit and press Enter. Go to privileged mode. Before you are allowed to enter privileged mode, you will be asked for a password. If you successfully enter the secret password, you can proceed.6.Remove the secret password. Go to privileged mode, type config t, and press Enter. Type no enable secret and press Enter. Log out and then log back in again; now you should not be asked for a password.7.One more password used to enter privileged mode is called the enable password. It is an older, less secure password and is not used if an enable secret password is set. Here is an example of how to set it:config t

enable password todd18.Notice that the enable secret and enable passwords are different. They cannot be the same.9.Type config t to be at the right level to set your console and auxiliary passwords, then type line ?.10.Notice that the parameters for the line commands are auxiliary, vty, and console. You will set all three.11.To set the Telnet or VTY password, type line vty 0 4 and then press Enter. The 0 4 is the range of the five available virtual lines used to connect with Telnet. If you have an enterprise IOS, the number of lines may vary. Use the question mark to determine the last line number available on your router.12.The next command is used to set the authentication on or off. Type login and press Enter to prompt for a user-mode password when telnetting into the router. You will not be able to telnet into a router if the password is not set.

You can use the no login command to disable the user-mode password prompt when using Telnet.13.One more command you need to set for your VTY password is password. Type password password to set the password. (password is your password.)14.Here is an example of how to set the VTY password:config tline vty 0 4login

password todd15.Set your auxiliary password by first typing line auxiliary 0 or line aux 0.16.Type login.17.Type password password.

10089c04.fm  Page 238  Thursday, August 30, 2007  12:36 PM




Hands-on Labs23918.Set your console password by first typing line console 0 or line con 0.19.Type login.20.Type password password. Here is an example of the last two commands:config tline con 0loginpassword todd1line aux 0login

password todd21.You can add the Exec-timeout 0 0 command to the console 0 line. This will stop the console from timing out and logging you out. The command will now look like this:config tline con 0loginpassword todd2

exec-timeout 0 022.Set the console prompt to not overwrite the command you’re typing with console mes-sages by using the command logging synchronous.config tline con 0

logging synchronousHands-on Lab 4.5: Setting the Hostname, Descriptions, IP Address, and Clock Rate1.Log into the router and go into privileged mode by typing en or enable.2.Set your hostname on your router by using the hostname command. Notice that it is one word. Here is an example of setting your hostname:Router#config tRouter(config)#hostname RouterA

RouterA(config)#Notice that the hostname of the router changed as soon as you pressed Enter.3.Set a banner that the network administrators will see by using the banner command.4.Type config t, then banner ?.5.Notice that you can set four different banners. For this lab we are only interested in the login and message of the day (MOTD) banners.

10089c04.fm  Page 239  Thursday, August 30, 2007  12:36 PM




240Chapter4 Cisco’s IOS and Security Device Manager6.Set your MOTD banner, which will be displayed when a console, auxiliary, or Telnet con-nection is made to the router, by typingconfig tbanner motd #This is an motd banner

#7.The preceding example used a # sign as a delimiting character. This tells the router when the message is done. You cannot use the delimiting character in the message itself.8.You can remove the MOTD banner by typingconfig t

no banner motd9.Set the login banner by typingconfig tbanner login #This is a login banner

#10.The login banner will display immediately after the MOTD but before the user-mode password prompt. Remember that you set your user-mode passwords by setting the con-sole, auxiliary, and VTY line passwords.11.You can remove the login banner by typingconfig t

no banner login12.You can add an IP address to an interface with the ip address command. You need to get into interface configuration mode first; here is an example of how you do that:config tint e0 (you can use int Ethernet 0 too)ip address 1.1.1.1 255.255.0.0

no shutdownNotice that the IP address (1.1.1.1) and subnet mask (255.255.0.0) are configured on one line. The no shutdown (or no shut for short) command is used to enable the interface. All interfaces are shut down by default.13.You can add identification to an interface by using the description command. This is useful for adding information about the connection. Only administrators see this, not users. Here is an example:config tint s0ip address 1.1.1.2 255.255.0.0no shut

description Wan link to Miami

10089c04.fm  Page 240  Thursday, August 30, 2007  12:36 PM




Hands-on Labs24114.You can add the bandwidth of a serial link as well as the clock rate when simulating a DCE WAN link. Here is an example:config tint s0bandwidth 64

clock rate 64000Hands-on Lab 4.6: Installing SDM on Your ComputerThis lab will have you download and install the full SDM program on your computer and then add the demo program inside the program. The links included in this lab were current as of this writing, but they could change at any time.1.Download and install the latest SDM program from Cisco at the following location: www.cisco.com/pcgi-bin/tablebuild.pl/sdm.2.After the SDM program is installed on your computer, download the SDM demo at www.cisco.com/pcgi-bin/tablebuild.pl/sdm-tool-demo.3.Unzip the demo program into a folder of your choice.4.Copy the dataFile.zip file to C:\.5.Make sure you disable your pop-up blockers on your browser.6.When SDM is installed on a PC , Internet Explorer may display HTML source code when you attempt to launch SDM. To fix this problem, choose Tools    Internet Options    Advanced. Then scroll to the Security section, check Allow Active Content to Run in Files on My Computer, and click Apply. Then relaunch SDM.7.Click the Cisco SDM icon on your desktop to launch the SDM demo version. Enter the loopback address 127.0.0.1 and click the Launch button to start the SDM demo applica-tion. You can choose to use HTTPS if you want, and if you do, accept any security cer-tificate notification messages that pop up.8.The SDM demo will tell you there is debugging running on the router and to disable it because of possible performance degrading, but leave debugging running on the “simu-lation.” You’re not degrading any router performance.9.Run through all the tabs: Configure Interfaces, Routing Protocols, Create a DHCP Pool, and so on—everything you can find. Spend some time getting to know SDM.10.Understand that not all features are supported on the demo, but it is better than not hav-ing SDM and the demo actually works pretty well.11.If you want to practice using certificates, use the ca.cer and router.cer certificates to dem-onstrate importing certificate authority (CA) and router certificates. You could get these certificates from SDM_demo_tool.zip.

10089c04.fm  Page 241  Thursday, August 30, 2007  12:36 PM




242Chapter4 Cisco’s IOS and Security Device ManagerReview Questions

The following questions are designed to test your understanding of this chapter’s material. For more information on how to get additional ques-tions, please see this book’s Introduction.1.You type show running-config and get this output:[output cut]Line console 0      Exec-timeout 1 44      Password 7098C0BQR      Login

[output cut]What do the two numbers following the exec-timeout command mean?A.If no command has been typed in 44 seconds, the console connection will be closed.B.If no router activity has been detected in 1 hour and 44 minutes, the console will be locked out.C.If no commands have been typed in 1 minute and 44 seconds, the console connection will be closed.D.If you’re connected to the router by a Telnet connection, input must be detected within 1 minute and 44 seconds or the connection will be closed.2.You need to find the broadcast address used on a LAN on your router. What command will you type into the router from user mode to find the broadcast address?A.show running-configB.show startup-configC.show interfacesD.show protocols3.You want to totally reinitialize the router and replace the current running-config with the cur-rent startup-config. What command will you use?A.replace run startB.copy run startC.copy start runD.reload

10089c04.fm  Page 242  Thursday, August 30, 2007  12:36 PM




Review Questions2434.Which command will show you whether a DTE or a DCE cable is plugged into serial 0?A.sh int s0B.sh int serial 0C.show controllers s 0D.show serial 0 controllers5.What keystroke will terminate setup mode?A.Ctrl+ZB.Ctrl+^C.Ctrl+CD.Ctrl+Shift+^6.You set the console password, but when you display the configuration, the password doesn’t show up; it looks like this:[output cut]Line console 0      Exec-timeout 1 44      Password 7098C0BQR      Login

[output cut]What cause the password to be stored like this?A.encrypt passwordB.service password-encryptionC.service-password-encryptionD.exec-timeout 1 447.Which of the following commands will configure all the default VTY ports on a router?A.Router#line vty 0 4B.Router(config)#line vty 0 4C.Router(config-if)#line console 0D.Router(config)#line vty all8.Which of the following commands sets the secret password to Cisco?A.enable secret password CiscoB.enable secret ciscoC.enable secret CiscoD.enable password Cisco

10089c04.fm  Page 243  Thursday, August 30, 2007  12:36 PM




244Chapter4 Cisco’s IOS and Security Device Manager9.If you wanted administrators to see a message when logging into the router, which command would you use?A.message banner motdB.banner message motdC.banner motdD.message motd10.How many simultaneous Telnet sessions does a Cisco router support by default?A.1B.2C.3D.4E.5F.611.What command do you type to save the configuration stored in RAM to NVRAM?A.Router(config)#copy current to startingB.Router#copy starting to runningC.Router(config)#copy running-config startup-configD.Router#copy run startup12.You try to telnet into SFRouter from router Corp and receive this message:Corp#telnet SFRouterTrying SFRouter (10.0.0.1)…OpenPassword required, but none set[Connection to SFRouter closed by foreign host]

Corp#Which of the following sequences will address this problem correctly?A.Corp(config)#line console 0B.SFRemote(config)#line console 0C.Corp(config)#line vty 0 4D.SFRemote(config)#line vty 0 413.Which command will delete the contents of NVRAM on a router?A.delete NVRAMB.delete startup-configC.erase NVRAMD.erase start

10089c04.fm  Page 244  Thursday, August 30, 2007  12:36 PM




Review Questions24514.What is the problem with an interface if you type show interface serial 0 and receive the following message?

Serial0 is administratively down, line protocol is downA.The keepalives are different times.B.The administrator has the interface shut down.C.The administrator is pinging from the interface.D.No cable is attached.15.Which of the following commands displays the configurable parameters and statistics of all interfaces on a router?A.show running-configB.show startup-configC.show interfacesD.show versions16.If you delete the contents of NVRAM and reboot the router, what mode will you be in?A.Privileged modeB.Global modeC.Setup modeD.NVRAM loaded mode17.You type the following command into the router and receive the following output:Router#show serial 0/0            ^

% Invalid input detected at ‘^’ marker.Why was this error message displayed?A.You need to be in privileged mode.B.You cannot have a space between serial and 0/0.C.The router does not have a serial0/0 interface.D.Part of the command is missing.18.You type Router#sh ru and receive an % ambiguous command error. Why did you receive this message?A.The command requires additional options or parameters.B.There is more than one show command that starts with the letters ru.C.There is no show command that starts with ru.D.The command is being executed from the wrong router mode.

10089c04.fm  Page 245  Thursday, August 30, 2007  12:36 PM




246Chapter4 Cisco’s IOS and Security Device Manager19.Which of the following commands will display the current IP addressing and the layer 1 and 2 status of an interface? (Choose three.)A.show versionB.show protocolsC.show interfacesD.show controllersE.show ip interfaceF.show running-config20.What layer of the OSI model would you assume the problem is in if you type show interface serial 1 and receive the following message?

Serial1 is down, line protocol is downA.Physical layerB.Data Link layerC.Network layerD.None; it is a router problem.

10089c04.fm  Page 246  Thursday, August 30, 2007  12:36 PM




Answers to Review Questions247Answers to Review Questions1.C. The exec-timeout command is set in minutes and seconds.2.C. The command show ip protocols will actually show you the broadcast address for each interface—too bad it isn’t a possible answer. Your best answer is show interfaces, which will provide the IP address and mask for each interface. You can then determine the mask from the vast subnetting knowledge you gained in Chapter 3.3.D. You may have picked option C, which isn’t a bad answer. Remember, though, it doesn’t replace the configuration, it appends it. To completely replace the running-config with the startup-config, you must reload the router.4.C. The show controllers serial 0 command will show you whether either a DTE or DCE cable is connected to the interface. If it is a DCE connection, you need to add clocking with the clock rate command.5.C. You can exit setup mode at any time by using the keystroke Ctrl+C.6.B. The command service password-encryption, from global configuration mode, will encrypt the passwords.7.B. From global configuration mode, use the line vty 0 4 command to set all five default VTY lines.8.C. The enable secret password is case sensitive, so the second option is wrong. To set the enable secret password, use the enable secret password command from global configuration mode.9.C. The typical banner is a message of the day (MOTD) and is set by using the global configu-ration mode command banner motd.10.E. Cisco routers, if they do not have the Enterprise edition of the IOS, will default to five simul-taneous Telnet sessions.11.D. To copy the running-config to NVRAM so that it will be used if the router is restarted, use the copy running-config startup-config command (copy run start for short).12.D. To allow a VTY (Telnet) session into your router, you must set the VTY password. Option C is wrong because it is setting the password on the wrong router. Notice that the answers have you set the login command before you set the password. Remember, Cisco may have you set the password before the login command.13.D. The erase startup-config command erases the contents of NVRAM and will put you in setup mode if the router is restarted.14.B. If an interface is shut down, the show interface command will show the interface as administratively shut down. (It is possible that no cable is attached, but you can’t tell that from this message.)

10089c04.fm  Page 247  Thursday, August 30, 2007  12:36 PM




248Chapter4 Cisco’s IOS and Security Device Manager15.C. With the show interfaces command, you can view the configurable parameters, get sta-tistics for the interfaces on the router, verify if the interfaces are shut down, and see the IP address of each interface.16.C. If you delete the startup-config and reload the router, the router will automatically enter setup mode. You can also type setup from privileged mode at any time.17.D. You can view the interface statistics from user mode, but the command is show interface serial 0/0.18.B. The % ambiguous command error means that there is more then one possible command that starts with ru. Use a question mark to find the correct command.19.B, C, E. The commands show protocols, show interfaces, and show ip interface will show you the layer 1 and 2 status and the IP addresses of your router’s interfaces.20.A. If you see that a serial interface and the protocol are both down, then you have a Physical layer problem. If you see serial1 is up, line protocol is down, then you are not receiving (Data Link) keepalives from the remote end.

10089c04.fm  Page 248  Thursday, August 30, 2007  12:36 PM




Answers to Written Lab 4249Answers to Written Lab 41.clock rate 640002.config t, line vty 0 4, no login3.config t, int e0, no shut4.erase startup-config5.config t, line console 0, login, password todd6.config t, enable secret cisco7.show controllers int8.show terminal9.Router#reload10.config t, hostname Chicago

10089c04.fm  Page 249  Thursday, August 30, 2007  12:36 PM




10089c04.fm  Page 250  Thursday, August 30, 2007  12:36 PM




 

Chapter 5 Managing a Cisco Internetwork

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Configure, verify, and troubleshoot basic router operation and routing on Cisco devices  Manage IOS configuration files (including: save, edit, upgrade, restore)   Manage Cisco IOS  Verify network connectivity (including: using ping, traceroute, and telnet or SSH) 

 

10089.book  Page 251  Monday, July 23, 2007  3:17 PM




 Here in Chapter 5, I’m going to show you how to manage Cisco routers on an internetwork. The Internetwork Operat-ing System (IOS) and configuration files reside in different locations in a Cisco device, so it’s really important to understand both where these files are located and how they work.You’ll be learning about the main components of a router, the router boot sequence, and the configuration register, including how to use the configuration register for password recovery. After that, you’ll find out how to manage routers by using the  copy  command with a TFTP host when using the Cisco IOS File System (IFS) and the Cisco SDM.We’ll wrap up the chapter with an exploration of the Cisco Discovery Protocol, and you’ll learn how to resolve hostnames and some important Cisco IOS troubleshooting techniques.

 For up-to-the-minute updates for this chapter, please see  www.lammle.com   and/or  www.sybex.com . The Internal Components of a Cisco Router To configure and troubleshoot a Cisco internetwork, you need to know the major components of Cisco routers and understand what each one does. Table 5.1 describes the major Cisco router components. TABLE5.1 Cisco Router Components  ComponentDescription BootstrapStored in the microcode of the ROM, the bootstrap is used to bring a router up during initialization. It will boot the router and then load the IOS.POST (power-on self-test)Stored in the microcode of the ROM, the POST is used to check the basic functionality of the router hardware and determines which interfaces are present.ROM monitorStored in the microcode of the ROM, the ROM monitor is used for manufacturing, testing, and troubleshooting.

 

10089.book  Page 252  Monday, July 23, 2007  3:17 PM




 The Router Boot Sequence 253 The Router Boot Sequence When a router boots up, it performs a series of steps, called the  boot sequence , to test the hard-ware and load the necessary software. The boot sequence consists of the following steps: 1. The router performs a POST. The POST tests the hardware to verify that all components of the device are operational and present. For example, the POST checks for the different interfaces on the router. The POST is stored in and run from  ROM (read-only memory) . 2. The bootstrap then looks for and loads the Cisco IOS software. The bootstrap is a pro-gram in ROM that is used to execute programs. The bootstrap program is responsible for finding where each IOS program is located and then loading the file. By default, the IOS software is loaded from flash memory in all Cisco routers.

 The default order of an IOS loading from a router is Flash, TFTP server,  then ROM.Mini-IOSCalled the RXBOOT or bootloader by Cisco, the mini-IOS is a small IOS in ROM that can be used to bring up an interface and load a Cisco IOS into flash memory. The mini-IOS can also perform a few other maintenance operations.RAM (random access memory)Used to hold packet buffers, ARP cache, routing tables, and also the software and data structures that allow the router to function. Running-config is stored in RAM, and most routers expand the IOS from flash into RAM upon boot.ROM (read-only memory)Used to start and maintain the router. Holds the POST and the bootstrap program, as well as the mini-IOS.Flash memoryStores the Cisco IOS by default. Flash memory is not erased when the router is reloaded. It is EEPROM (electronically erasable pro-grammable read-only memory) created by Intel.NVRAM (nonvolatile RAM)Used to hold the router and switch configuration. NVRAM is not erased when the router or switch is reloaded. Does not store an IOS. The configuration register is stored in NVRAM.Configuration registerUsed to control how the router boots up. This value can be found as the last line of the  show version  command output and by default is set to 0x2102, which tells the router to load the IOS from flash memory as well as to load the configuration from NVRAM. TABLE5.1 Cisco Router Components (continued) ComponentDescription

 

10089.book  Page 253  Monday, July 23, 2007  3:17 PM




 254 Chapter5  Managing a Cisco Internetwork 3. The IOS software looks for a valid configuration file stored in NVRAM. This file is called startup-config and is only there if an administrator copies the running-config file into NVRAM. (As you already know, the new ISR routers have a small startup-config file preloaded.) 4. If a startup-config file is in NVRAM, the router will copy this file and place it in RAM and call the file running-config. The router will use this file to run the router. The router should now be operational. If a startup-config file is not in NVRAM, the router will broadcast out any interface that detects carrier detect (CD) for a TFTP host looking for a configuration, and when that fails (typically it will fail—most people won’t even realize the router has attempted this process), it will start the setup mode configuration process. Managing Configuration Register All Cisco routers have a 16-bit software register that’s written into NVRAM. By default, the  configuration register  is set to load the Cisco IOS from  flash memory  and to look for and load the startup-config file from NVRAM. In the following sections, I am going to discuss the configuration register settings and how to use these settings to provide password recovery on your routers. Understanding the Configuration Register Bits The 16 bits (2 bytes) of the configuration register are read from 15 to 0, from left to right. The default configuration setting on Cisco routers is 0x2102. This means that bits 13, 8, and 1 are on, as shown in Table 5.2. Notice that each set of 4 bits (called a nibble) is read in binary with a value of 8, 4, 2, 1.

 Add the prefix  0x  to the configuration register address. The  0x  means that the  digits that follow are in hexadecimal. Table 5.3 lists the software configuration bit meanings. Notice that bit 6 can be used to ignore the NVRAM contents. This bit is used for password recovery—something I’ll go over with you soon in the section “Recovering Passwords” later in this chapter. TABLE5.2 The Configuration Register Bit Numbers Configuration Register  2    1   0  2  Bit number1514131211109876543210Binary0010000100000010

 

10089.book  Page 254  Monday, July 23, 2007  3:17 PM




 Managing Configuration Register 255

 Remember that in hex, the scheme is 0–9 and A–F (A = 10, B = 11, C = 12, D = 13, E = 14, and F = 15). This means that a 210F setting for the configuration register  is actually 210(15), or 1111 in binary. The boot field, which consists of bits 0–3 in the configuration register, controls the router boot sequence. Table 5.4 describes the boot field bits. TABLE5.3 Software Configuration Meanings BitHexDescription 0–30x0000–0x000FBoot field (see Table 5.4).60x0040Ignore NVRAM contents.70x0080OEM bit enabled.80x101Break disabled.100x0400IP broadcast with all zeros.5, 11–120x0800–0x1000Console line speed.130x2000Boot default ROM software if network boot fails.140x4000IP broadcasts do not have net numbers.150x8000Enable diagnostic messages and ignore NVRAM contents. TABLE5.4 The Boot Field (Configuration Register Bits 00–03) Boot FieldMeaningUse 00ROM monitor modeTo boot to ROM monitor mode, set the configuration reg-ister to 2100. You must manually boot the router with the  b  command. The router will show the  rommon>  prompt.01Boot image from ROMTo boot an IOS image stored in ROM, set the configura-tion register to 2101. The router will show the  Router(boot)>  prompt.02–FSpecifies a default boot file nameAny value from 2102 through 210F tells the router to use the boot commands specified in NVRAM.

 

10089.book  Page 255  Monday, July 23, 2007  3:17 PM




 256 Chapter5  Managing a Cisco Internetwork Checking the Current Configuration Register Value You can see the current value of the configuration register by using the  show version  command ( sh version  or  show ver  for short), as demonstrated here:

 Router# sh version Cisco IOS Software, 2800 Software (C2800NM-ADVSECURITYK9-M), Version    12.4(12), RELEASE SOFTWARE (fc1)[output cut]

 Configuration register is 0x2102 The last information given from this command is the value of the configuration register. In this example, the value is 0x2102—the default setting. The configuration register setting of 0x2102 tells the router to look in NVRAM for the boot sequence.Notice that the  show version  command also provides the IOS version, and in the preced-ing example, it shows the IOS version as 12.4(12).

 The  show version  command will display system hardware configuration  information, software version, and the names of the boot images on a router. Changing the Configuration Register You can change the configuration register value to modify how the router boots and runs. These are the main reasons you would want to change the configuration register:  To force the system into the ROM monitor mode  To select a boot source and default boot filename  To enable or disable the  Break  function  To control broadcast addresses  To set the console terminal baud rate  To load operating software from ROM  To enable booting from a Trivial File Transfer Protocol (TFTP) server

 Before you change the configuration register, make sure you know the current configuration register value. Use the  show version  command to  get this information. You can change the configuration register by using the  config-register  command. Here’s an example. The following commands tell the router to boot a small IOS from ROM and then show the current configuration register value:

 Router(config)# config-register 0x2101 Router(config)# ^Z

 

10089.book  Page 256  Monday, July 23, 2007  3:17 PM




 Managing Configuration Register 257 Router# sh ver [output cut]Configuration register is 0x2102 (will be 0x2101 at next

   reload) Notice that the  show version  command displays the current configuration register value and also what that value will be when the router reboots. Any change to the configuration reg-ister won’t take effect until the router is reloaded. The 0x2101 will load the IOS from ROM the next time the router is rebooted. You may see it listed as 0x101—that’s basically the same thing, and it can be written either way.Here is our router after setting the configuration register to 0x2101 and reloading:

 Router(boot)# sh ver Cisco IOS Software, 2800 Software (C2800NM-ADVSECURITYK9-M), Version    12.4(12), RELEASE SOFTWARE (fc1)[output cut]ROM: System Bootstrap, Version 12.4(13r)T, RELEASE SOFTWARE (fc1)Router uptime is 3 minutesSystem returned to ROM by power-onSystem image file is “flash:c2800nm-advsecurityk9-mz.124-12.bin”[output cut]

 Configuration register is 0x2101 At this point, if you typed  show flash , you’d still see the IOS in flash memory ready to go. But we told our router to load from ROM, which is why the hostname shows up with (boot).

 Router(boot)# sh flash -#- --length-- -----date/time------ path1     21710744 Jan 2 2007 22:41:14 +00:00 c2800nm-advsecurityk9-mz.124-12.bin2         1823 Dec 5 2006 14:46:26 +00:00 sdmconfig-2811.cfg3      4734464 Dec 5 2006 14:47:12 +00:00 sdm.tar4       833024 Dec 5 2006 14:47:38 +00:00 es.tar5      1052160 Dec 5 2006 14:48:10 +00:00 common.tar6         1038 Dec 5 2006 14:48:32 +00:00 home.shtml7       102400 Dec 5 2006 14:48:54 +00:00 home.tar8       491213 Dec 5 2006 14:49:22 +00:00 128MB.sdf9      1684577 Dec 5 2006 14:50:04 +00:00 securedesktop-ios-3.1.1.27-k9.pkg10      398305 Dec 5 2006 14:50:34 +00:00 sslclient-win-1.1.0.154.pkg

 32989184 bytes available (31027200 bytes used)

 

10089.book  Page 257  Monday, July 23, 2007  3:17 PM




 258 Chapter5  Managing a Cisco Internetwork So even though we have our full IOS in flash, we changed the default loading of the router’s software by changing the configuration register. If you want to set the configuration register back to the default, just type this:

 Router(boot)# config t Router(boot)(config)# config-register 0x2102 Router(boot)(config)# ^Z

 Router(boot)# reload In the next section, I’ll show you how to load the router into ROM monitor mode so you can perform password recovery. Recovering Passwords If you’re locked out of a router because you forgot the password, you can change the con-figuration register to help you get back on your feet. As I said earlier, bit 6 in the config-uration register is used to tell the router whether to use the contents of NVRAM to load a router configuration.The default configuration register value is 0x2102, meaning that bit 6 is off. With the default setting, the router will look for and load a router configuration stored in NVRAM (startup-config). To recover a password, you need to turn on bit 6. Doing this will tell the router to ignore the NVRAM contents. The configuration register value to turn on bit 6 is 0x2142.Here are the main steps to password recovery: 1. Boot the router and interrupt the boot sequence by performing a break, which will take the router into ROM monitor mode. 2. Change the configuration register to turn on bit 6 (with the value 0x2142). 3. Reload the router. 4. Enter privileged mode. 5. Copy the startup-config file to running-config. 6. Change the password. 7. Reset the configuration register to the default value. 8. Save the router configuration. 9. Reload the router (optional).I’m going to cover these steps in more detail in the following sections. I’ll also show you the commands to restore access to ISR, 2600, and even 2500 series routers. (You can still use 2500s (kinda) and you never know when you might need this information!)As I said, you can enter ROM monitor mode by pressing Ctrl+Break during router bootup. But if the IOS is corrupt or missing, if there’s no network connectivity available to find a TFTP host, or if the mini-IOS from ROM doesn’t load (meaning the default router fallback failed), the router will enter ROM monitor mode by default.

 

10089c05.fm  Page 258  Friday, November 7, 2008  10:48 PM




Managing Configuration Register259Interrupting the Router Boot SequenceYour first step is to boot the router and perform a break. This is usually done by pressing the Ctrl+Break key combination when using HyperTerminal (personally, I use SecureCRT) while the router first reboots.After you’ve performed a break, you should see something like this for a 2600 series router (it is pretty much the same output for the ISR series):

System Bootstrap, Version 11.3(2)XA4, RELEASE SOFTWARE (fc1)Copyright (c) 1999 by cisco Systems, Inc.TAC:Home:SW:IOS:Specials for infoPC = 0xfff0a530, Vector = 0x500, SP = 0x680127b0C2600 platform with 32768 Kbytes of main memoryPC = 0xfff0a530, Vector = 0x500, SP = 0x80004374monitor: command “boot” aborted due to user interrupt

rommon 1 >Notice the line monitor: command “boot” aborted due to user interrupt. At this point, you will be at the rommon 1> prompt, which is called ROM monitor mode.Changing the Configuration RegisterAs I explained earlier, you can change the configuration register by using the config-register command. To turn on bit 6, use the configuration register value 0x2142.

Remember that if you change the configuration register to 0x2142, the startup-config will be bypassed and the router will load into setup mode.Cisco ISR/2600 Series CommandsTo change the bit value on a Cisco ISR/2600 series router, you just enter the command at the rommon 1> prompt:

rommon 1 >confreg 0x2142You must reset or power cycle for new config to take effect

rommon 2 >resetCisco 2500 Series CommandsTo change the configuration register on a 2500 series router, type o after creating a break sequence on the router. This brings up a menu of configuration register option settings. To change the configuration register, enter the command o/r, followed by the new register value. Here’s an example of turning on bit 6 on a 2501 router:

System Bootstrap, Version 11.0(10c), SOFTWARECopyright (c) 1986-1996 by cisco Systems

10089.book  Page 259  Monday, July 23, 2007  3:17 PM




260Chapter5 Managing a Cisco Internetwork2500 processor with 14336 Kbytes of main memoryAbort at 0x1098FEC (PC)>oConfiguration register = 0x2102 at last bootBit#    Configuration register option settings:15      Diagnostic mode disabled14      IP broadcasts do not have network numbers13      Boot default ROM software if network boot fails12-11   Console speed is 9600 baud10      IP broadcasts with ones08      Break disabled07      OEM disabled06      Ignore configuration disabled03-00   Boot file is cisco2-2500 (or ‘boot system’ command)

>o/r 0x2142Notice that the last entry in the router output is 03-00. This tells the router what the IOS boot file is. By default, the router will use the first file found in the flash memory, so if you want to boot a different filename, you can either change the configuration register or use the boot system flash:ios_name command. (I’ll show you the boot system command in a minute.)Reloading the Router and Entering Privileged ModeAt this point, you need to reset the router like this: From the ISR/2600 series router, type I (for initialize) or reset. From the 2500 series router, type I.The router will reload and ask if you want to use setup mode (because no startup-config is used). Answer no to entering setup mode, press Enter to go into user mode, and then type enable to go into privileged mode.Viewing and Changing the ConfigurationNow you’re past the point where you would need to enter the user-mode and privileged-mode passwords in a router. Copy the startup-config file to the running-config file:

copy startup-config running-configOr use the shortcut:

copy start runThe configuration is now running in random access memory (RAM), and you’re in privileged mode, meaning that you can now view and change the configuration. But you can’t view the enable secret setting for the password since it is encrypted. To change the password, do this:

config t

enable secret todd

10089.book  Page 260  Monday, July 23, 2007  3:17 PM




Managing Configuration Register261Resetting the Configuration Register and Reloading the RouterAfter you’re finished changing passwords, set the configuration register back to the default value with the config-register command:

config t

config-register 0x2102Finally, save the new configuration with a copy running-config startup-config and reload the router.

If you save your configuration and reload the router and it comes up in setup mode, the configuration register setting is probably incorrect.Boot System CommandsDid you know that you can configure your router to boot another IOS if the flash is corrupted? Well, you can. In fact, you just might want all your routers to boot from a TFTP host each time anyway because that way, you’ll never have to upgrade each router individually. This may be a smooth way to go because it allows you to just change one file on a TFTP host to perform an upgrade.There are some boot commands you can play with that will help you manage the way your router boots the Cisco IOS—but remember, we’re talking about the router’s IOS here, not the router’s configuration!

Router>enRouter#config tEnter configuration commands, one per line.  End with CNTL/Z.Router(config)#boot ?  bootstrap  Bootstrap image file  config     Configuration file  host       Router-specific config file  network    Network-wide config file

  system     System image fileThe boot command truly gives you a wealth of options, but first, I’ll show you the typical settings that Cisco recommends. So let’s get started—the boot system command will allow you to tell the router which file to boot from flash memory. Remember that the router, by default, boots the first file found in flash. You can change that with the following commands:

Router(config)#boot system ?  WORD   TFTP filename or URL  flash  Boot from flash memory

10089.book  Page 261  Monday, July 23, 2007  3:17 PM




262Chapter5 Managing a Cisco Internetwork  ftp    Boot from a server via ftp  mop    Boot from a Decnet MOP server  rcp    Boot from a server via rcp  rom    Boot from rom  tftp   Boot from a tftp server

Router(config)#boot system flash c2800nm-advsecurityk9-mz.124-12.binThe above command configures the router to boot the IOS listed in it. This is a helpful com-mand for when you load a new IOS into flash and want to test it, or even when you want to totally change which IOS is loading by default.The next command is considered a fall-back routine, but as I said, you can make it a per-manent way to have your routers boot from a TFTP host. Personally, I wouldn’t necessarily recommend doing this (single point of failure); I’m just showing you it’s possible:

Router(config)#boot system tftp ?  WORD  System image filenameRouter(config)#boot system tftp c2800nm-advsecurityk9-mz.124-12.bin ?  Hostname or A.B.C.D  Address from which to download the file  <cr>Router(config)#boot system tftp c2800nm-advsecurityk9-mz.124-12.bin 1.1.1.2

Router(config)#As your last recommended fall-back option—the one to go to if the IOS in flash doesn’t load and the TFTP host does not produce the IOS—load the mini-IOS from ROM like this:

Router(config)#boot system romRouter(config)#do show run | include boot systemboot system flash c2800nm-advsecurityk9-mz.124-12.binboot system tftp c2800nm-advsecurityk9-mz.124-12.bin 1.1.1.2boot system rom

Router(config)#To sum this up, we now have Cisco’s suggested IOS backup routine configured on our router: flash, TFTP host, ROM.Backing Up and Restoring the Cisco IOSBefore you upgrade or restore a Cisco IOS, you really should copy the existing file to a TFTP host as a backup just in case the new image crashes and burns.And you can use any TFTP host to accomplish this. By default, the flash memory in a router is used to store the Cisco IOS. In the following sections, I’ll describe how to check the amount of flash memory, how to copy the Cisco IOS from flash memory to a TFTP host, and how to copy the IOS from a TFTP host to flash memory.

10089.book  Page 262  Monday, July 23, 2007  3:17 PM




Backing Up and Restoring the Cisco IOS263

You’ll learn how to use the Cisco IFS and SDM to manage your IOS files after first learning how to manage them with a TFTP host.But before you back up an IOS image to a network server on your intranet, you’ve got to do these three things: Make sure you can access the network server. Ensure that the network server has adequate space for the code image. Verify the file naming and path requirement.And if you have a laptop or workstation’s Ethernet port directly connected to a router’s Ethernet interface, as shown in Figure 5.1, you need to verify the following before attempting to copy the image to or from the router: TFTP server software must be running on the administrator’s workstation. The Ethernet connection between the router and the workstation must be made with a crossover cable. The workstation must be on the same subnet as the router’s Ethernet interface. The copy flash tftp command must be supplied the IP address of the workstation if you are copying from the router flash. And if you’re copying “into” flash, you need to verify that there’s enough room in flash memory to accommodate the file to be copied.FIGURE5.1Copying an IOS from a workstation to a routerVerifying Flash MemoryBefore you attempt to upgrade the Cisco IOS on your router with a new IOS file, it’s a good idea to verify that your flash memory has enough room to hold the new image. You verify the amount of flash memory and the file or files being stored in flash memory by using the show flash command (sh flash for short):

Router#sh flash-#- --length-- -----date/time------ path1     21710744 Jan 2 2007 22:41:14 +00:00 c2800nm-advsecurityk9-mz.124-12.bin[output cut]

32989184 bytes available (31027200 bytes used)The ISR router above has 64MB of RAM, and roughly half of the memory is in use.

ConsoleE0

10089.book  Page 263  Monday, July 23, 2007  3:17 PM




264Chapter5 Managing a Cisco Internetwork

The show flash command will display the amount of memory consumed by the current IOS image as well as tell you if there’s enough room available to hold both current and new images. You should know that if there’s not enough room for both the old and new image you want to load, the old image will be erased!The amount of flash is actually easier to tally using the show version command on the ISR routers:

Router#show version[output cut]Cisco 2811 (revision 49.46) with 249856K/12288K bytes of memory.Processor board ID FTX1049A1AB2 FastEthernet interfaces4 Serial(sync/async) interfaces1 Virtual Private Network (VPN) ModuleDRAM configuration is 64 bits wide with parity enabled.239K bytes of non-volatile configuration memory.

62720K bytes of ATA CompactFlash (Read/Write)You can see that the amount of flash shows up on the last line. By averaging up, we get the amount of flash to 64MB.Notice that the filename in this example is c2800nm-advsecurityk9-mz.124-12.bin. The main difference in the output of the show flash and show version commands is that the show flash command displays all files in flash and the show version command shows the actual name of the file that the router is using to run the router.Backing Up the Cisco IOSTo back up the Cisco IOS to a TFTP server, you use the copy flash tftp command. It’s a straightforward command that requires only the source filename and the IP address of the TFTP server.The key to success in this backup routine is to make sure you’ve got good, solid connectivity to the TFTP server. Check this by pinging the TFTP device from the router console prompt like this:

Router#ping 1.1.1.2Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 1.1.1.2, timeout  is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip min/avg/max

  = 4/4/8 ms

10089.book  Page 264  Monday, July 23, 2007  3:17 PM




Backing Up and Restoring the Cisco IOS265

The Packet Internet Groper (Ping) utility is used to test network connectivity, and I use it in some of the examples in this chapter. I’ll be talking about it in more detail in the section “Checking Network Connectivity and Troubleshoot-ing” later in the chapter.After you ping the TFTP server to make sure that IP is working, you can use the copy flash tftp command to copy the IOS to the TFTP server as shown next:

Router#copy flash tftpSource filename []?c2800nm-advsecurityk9-mz.124-12.binAddress or name of remote host []?1.1.1.2Destination filename [c2800nm-advsecurityk9-mz.124-12.bin]?[enter]!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!21710744 bytes copied in 60.724 secs (357532 bytes/sec)

Router#Just copy the IOS filename from either the show flash or show version command and then paste it when prompted for the source filename.In the preceding example, the contents of flash memory were copied successfully to the TFTP server. The address of the remote host is the IP address of the TFTP host, and the source filename is the file in flash memory.

The copy flash tftp command won’t prompt you for the location of any file or ask you where to put the file. TFTP is just a “grab it and place it” program in this situation. This means that the TFTP server must have a default direc-tory specified or it won’t work!Restoring or Upgrading the Cisco Router IOSWhat happens if you need to restore the Cisco IOS to flash memory to replace an original file that has been damaged or if you want to upgrade the IOS? You can download the file from a TFTP server to flash memory by using the copy tftp flash command. This command requires the IP address of the TFTP host and the name of the file you want to download.But before you begin, make sure the file you want to place in flash memory is in the default TFTP directory on your host. When you issue the command, TFTP won’t ask you where the file is, so if the file you want to use isn’t in the default directory of the TFTP host, this just won’t work.

Router#copy tftp flashAddress or name of remote host []?1.1.1.2Source filename []?c2800nm-advsecurityk9-mz.124-12.bin

10089.book  Page 265  Monday, July 23, 2007  3:17 PM




266Chapter5 Managing a Cisco InternetworkDestination filename [c2800nm-advsecurityk9-mz.124-12.bin]?[enter]%Warning:There is a file already existing with this nameDo you want to over write? [confirm][enter]Accessing tftp://1.1.1.2/c2800nm-advsecurityk9-mz.124-12.bin...Loading c2800nm-advsecurityk9-mz.124-12.bin from 1.1.1.2 (via   FastEthernet0/0): !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![OK - 21710744 bytes]21710744 bytes copied in 82.880 secs (261954 bytes/sec)

Router#In the above example, I copied the same file into flash memory, so it asked me if I wanted to overwrite it. Remember that we are “playing” with files in flash memory. If I had just cor-rupted my file by overwriting it, I won’t know until I reboot the router. Be careful with this command! If the file is corrupted, you’ll need to do an IOS restore from ROM monitor mode.If you are loading a new file and you don’t have enough room in flash memory to store both the new and existing copies, the router will ask to erase the contents of flash memory before writing the new file into flash memory.

A Cisco router can become a TFTP server host for a router system image that’s run in flash memory. The global configuration command is tftp-server flash: ios_name.Using the Cisco IOS File System (Cisco IFS)Cisco has created a file system called Cisco IFS that allows you to work with files and direc-tories just as you would from a Windows DOS prompt. The commands you use are dir, copy, more, delete, erase or format, cd and pwd, and mkdir and rmdir.Working with IFS gives you the ability to view and classify all files—even those on remote servers. And you definitely want to find out if an image on one of your remote servers is valid before you copy it, right? You also need to know how big it is—size matters here! It’s also a really good idea to take a look at the remote server’s configuration and make sure it’s all good before loading that file on your router.It’s very cool that IFS makes the file system user interface universal—it’s not platform specific anymore. You now get to use the same syntax for all your commands on all of your routers, no matter the platform!Sound too good to be true? Well, it kind of is because you’ll find out that support for all commands on each file system and platform just isn’t there. But it’s really no big deal since various file systems differ in the actions they perform; the commands that aren’t relevant to a particular file system are the very ones that aren’t supported. Be assured that any file system or platform will fully support all the commands you need to manage it.

10089.book  Page 266  Monday, July 23, 2007  3:17 PM




Backing Up and Restoring the Cisco IOS267Another cool IFS feature is that it cuts down on all those obligatory prompts for a lot of the commands. If you want to enter a command, all you have to do is type all the necessary info straight into the command line—no more jumping through hoops of prompts! So, if you want to copy a file to an FTP server, all you’d do is first indicate where the desired source file is on your router, pinpoint where the destination file is on the FTP server, determine the username and password you’re going to use when you want to connect to that server, and type it all in on one line—sleek! And for those of you resistant to change, you can still have the router prompt you for all the information it needs and enjoy entering a more elegantly minimized version of the command than you did before.But even in spite of all this, your router might still prompt you—even if you did everything right in your command line. It comes down to how you’ve got the file prompt command configured and which command you’re trying to use. But no worries—if that happens, the default value will be entered right there in the command, and all you have to do is hit Enter to verify the correct values.IFS also lets you explore various directories and inventory files in any directory you want. Plus, you can make subdirectories in flash memory or on a card, but you only get to do that if you’re working on one of the more recent platforms.And get this—the new file system interface uses URLs to determine the whereabouts of a file. So just as they pinpoint places on the Web, URLs now indicate where files are on your Cisco router, or even on a remote file server! You just type URLs right into your commands to identify where the file or directory is. It’s really that easy—to copy a file from one place to another, you simply enter the copy source-url destination-url command—sweet! IFS URLs are a tad different than what you’re used to though, and there’s an array of formats to use that vary depending on where, exactly, the file is that you’re after.We’re going to use Cisco IFS commands pretty much the same way that we used the copy command in the IOS section earlier: For backing up the IOS For upgrading the IOS For viewing text filesOkay—with all that down, let’s take a look at the common IFS commands available to us for managing the IOS. I’ll get into configuration files soon, but for now I’m going to get you started with going over the basics used to manage the new Cisco IOS.dirSame as with Windows, this command lets you view files in a directory. Type dir, hit Enter, and by default you get the contents of the flash:/ directory output.copyThis is one popular command, often used to upgrade, restore, or back up an IOS. But as I said, when you use it, it’s really important to focus on the details—what you’re copying, where it’s coming from, and where it’s going to land.moreSame as with Unix, this will give you a text file and let you look at it on a card. You can use it to check out your configuration file or your backup configuration file. I’ll go over it more when we get into actual configuration.show fileThis command will give you the skinny on a specified file or file system, but it’s kind of obscure because people don’t use it a lot.

10089.book  Page 267  Monday, July 23, 2007  3:17 PM




268Chapter5 Managing a Cisco InternetworkdeleteThree guesses—yep, it deletes stuff. But with some types of routers, not as well as you’d think. That’s because even though it whacks the file, it doesn’t always free up the space it was using. To actually get the space back, you have to use something called the squeeze command too.erase/formatUse these with care—make sure that when you’re copying files, you say no to the dialog that asks you if you want to erase the file system! The type of memory you’re using determines if you can nix the flash drive or not.cd/pwdSame as with Unix and DOS, cd is the command you use to change directories. Use the pwd command to print (show) the working directory.mkdir/rmdirUse these commands on certain routers and switches to create and delete directories—the mkdir command for creation and the rmdir command for deletion. Use the cd and pwd commands to change into these directories.Using the Cisco IFS to Upgrade an IOSLet’s take a look at some of these Cisco IFS commands on my ISR router (1841 series) with a hostname of R1.We’ll start with the pwd command to verify our default directory and then use the dir command to verify the contents of the default directory (flash:/):

R1#pwdflash:R1#dirDirectory of flash:/    1  -rw-    13937472  Dec 20 2006 19:58:18 +00:00  c1841-ipbase-   mz.124-1c.bin    2  -rw-        1821  Dec 20 2006 20:11:24 +00:00  sdmconfig-18xx.cfg    3  -rw-     4734464  Dec 20 2006 20:12:00 +00:00  sdm.tar    4  -rw-      833024  Dec 20 2006 20:12:24 +00:00  es.tar    5  -rw-     1052160  Dec 20 2006 20:12:50 +00:00  common.tar    6  -rw-        1038  Dec 20 2006 20:13:10 +00:00  home.shtml    7  -rw-      102400  Dec 20 2006 20:13:30 +00:00  home.tar    8  -rw-      491213  Dec 20 2006 20:13:56 +00:00  128MB.sdf    9  -rw-     1684577  Dec 20 2006 20:14:34 +00:00  securedesktop-   ios-3.1.1.27-k9.pkg   10  -rw-      398305  Dec 20 2006 20:15:04 +00:00  sslclient-win-   1.1.0.154.pkg

32071680 bytes total (8818688 bytes free)What we can see here is that we have the basic IP IOS (c1841-ipbase-mz.124-1c.bin). Looks like we need to upgrade our 1841. You’ve just got to love how Cisco puts the IOS type 

10089.book  Page 268  Monday, July 23, 2007  3:17 PM




Backing Up and Restoring the Cisco IOS269in the filename now! First, let’s check the size of the file that’s in flash with the show file com-mand (show flash would also work):

R1#show file info flash:c1841-ipbase-mz.124-1c.binflash:c1841-ipbase-mz.124-1c.bin:  type is image (elf) []  file size is 13937472 bytes, run size is 14103140 bytes

  Runnable image, entry point 0x8000F000, run from ramWith a file that size, the existing IOS will have to be erased before we can add our new IOS file (c1841-advipservicesk9-mz.124-12.bin), which is over 21MB. We’ll use the delete command, but remember, we can play with any file in flash memory and nothing serious will happen until we reboot—that is, if we made a mistake. So obviously, and as I pointed out earlier, we need to be majorly careful here!

R1#delete flash:c1841-ipbase-mz.124-1c.binDelete filename [c1841-ipbase-mz.124-1c.bin]?[enter]Delete flash:c1841-ipbase-mz.124-1c.bin? [confirm][enter]R1#sh flash-#- --length-- -----date/time------ path1         1821 Dec 20 2006 20:11:24 +00:00 sdmconfig-18xx.cfg2      4734464 Dec 20 2006 20:12:00 +00:00 sdm.tar3       833024 Dec 20 2006 20:12:24 +00:00 es.tar4      1052160 Dec 20 2006 20:12:50 +00:00 common.tar5         1038 Dec 20 2006 20:13:10 +00:00 home.shtml6       102400 Dec 20 2006 20:13:30 +00:00 home.tar7       491213 Dec 20 2006 20:13:56 +00:00 128MB.sdf8      1684577 Dec 20 2006 20:14:34 +00:00 securedesktop-ios-3.1.1.27-k9.pkg9       398305 Dec 20 2006 20:15:04 +00:00 sslclient-win-1.1.0.154.pkg22757376 bytes available (9314304 bytes used)R1#sh file info flash:c1841-ipbase-mz.124-1c.bin%Error opening flash:c1841-ipbase-mz.124-1c.bin (File not found)

R1#So with the above commands, I deleted the existing file and then verified the deletion by using both the show flash and show file commands. Let’s add the new file with the copy command, but again, I’m going to make sure I’m careful because this doesn’t make it safer than the first method I showed you earlier:

R1#copy tftp://1.1.1.2//c1841-advipservicesk9-mz.124-12.bin/ flash:/    c1841-advipservicesk9-mz.124-12.bin   Source filename [/c1841-advipservicesk9-mz.124-12.bin/]?[enter]Destination filename [c1841-advipservicesk9-mz.124-12.bin]?[enter]Loading /c1841-advipservicesk9-mz.124-12.bin/ from 1.1.1.2 (via

10089.book  Page 269  Monday, July 23, 2007  3:17 PM




270Chapter5 Managing a Cisco Internetwork    FastEthernet0/0): !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![output cut]!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![OK - 22103052 bytes]22103052 bytes copied in 72.008 secs (306953 bytes/sec)R1#sh flash-#- --length-- -----date/time------ path1         1821 Dec 20 2006 20:11:24 +00:00 sdmconfig-18xx.cfg2      4734464 Dec 20 2006 20:12:00 +00:00 sdm.tar3       833024 Dec 20 2006 20:12:24 +00:00 es.tar4      1052160 Dec 20 2006 20:12:50 +00:00 common.tar5         1038 Dec 20 2006 20:13:10 +00:00 home.shtml6       102400 Dec 20 2006 20:13:30 +00:00 home.tar7       491213 Dec 20 2006 20:13:56 +00:00 128MB.sdf8      1684577 Dec 20 2006 20:14:34 +00:00 securedesktop-ios-3.1.1.27-k9.pkg9       398305 Dec 20 2006 20:15:04 +00:00 sslclient-win-1.1.0.154.pkg10    22103052 Mar 10 2007 19:40:50 +00:00 c1841-advipservicesk9-mz.124-12.bin651264 bytes available (31420416 bytes used)

R1#We can check the file information as well with the show file command:

R1#sh file information flash:c1841-advipservicesk9-mz.124-12.binflash:c1841-advipservicesk9-mz.124-12.bin:  type is image (elf) []  file size is 22103052 bytes, run size is 22268736 bytes

  Runnable image, entry point 0x8000F000, run from ramRemember that the IOS is expanded into RAM when the router boots, so the new IOS will not run until you reload the router. So now let’s take a look at how to use the Cisco SDM to upgrade a router’s IOS.

I really recommend that you play with the Cisco IFS commands on a router just to get a good feel for them because, as I’ve said, they can definitely give you some grief at first!Using the SDM to Manage the Flash MemoryI was going to call this section “Using SDM to Upgrade/Restore/Back Up the IOS on Your Router,” but the SDM allows management of all the files in flash memory (as well as NVRAM), not just the IOS. It can be an easier method of flash file management, but for what it’s worth, you won’t find it safer to do it this way. It is a way to manage your files though. Let’s check it out.

10089.book  Page 270  Monday, July 23, 2007  3:17 PM




Backing Up and Restoring the Cisco IOS271

I mention “safer methods” a lot in this chapter. Clearly, I’ve caused myself some serious pain not being careful enough when working in flash memory! I cannot tell you enough—pay attention when messing around with flash memory!I’m going to connect to my other 1841 router (named R3) and upgrade that IOS using the SDM. Let’s connect and see what’s in flash. Looking at the first screen, we can see that IP is the only feature available and that Firewall, VPN, IPS, and NAC are “X’d” out. Let’s fix that!The next screen shows how to open the file management for flash. Chose File   File Management.

10089.book  Page 271  Monday, July 23, 2007  3:17 PM




272Chapter5 Managing a Cisco InternetworkAt this point, the screen shows all the files in flash, and we can see that we have the “ipbase” IOS.Click Load file from PC on the top of the screen to add a new file. When I tried to load the new IOS, I received the message shown in the next screen shot.I clicked OK, tried to delete the existing file, and then received this message.

10089.book  Page 272  Monday, July 23, 2007  3:17 PM




Backing Up and Restoring the Cisco IOS273I chose yes, and then looked at the File Management window again to verify that the file was deleted.I then chose Load File from PC again and the file started uploading into flash memory.Woo-hoo—finally, success!

10089.book  Page 273  Monday, July 23, 2007  3:17 PM




274Chapter5 Managing a Cisco InternetworkAfter rebooting, we can see that IP, Firewall, VPN, IPS, and NAC are all available with this new IOS!

One of the brilliant features of the ISR routers is that they use the physical flash cards that are accessible from the front or back of any router. You can pull these flash cards out, put them in an appropriate slot in your PC, and the card will show up as a drive. You can then add, change, and delete files. Just put the flash card back in your router and power up—instant upgrade. Nice!Backing Up and Restoring the Cisco ConfigurationAny changes that you make to the router configuration are stored in the running-config file. And if you don’t enter a copy run start command after you make a change to running-con-fig, that change will go poof if the router reboots or gets powered down. So you probably want to make another backup of the configuration information just in case the router or switch completely dies on you. Even if your machine is healthy and happy, it’s good to have for reference and documentation reasons.In the following sections, I’ll describe how to copy the configuration of a router to a TFTP server and how to restore that configuration.

10089.book  Page 274  Monday, July 23, 2007  3:17 PM




Backing Up and Restoring the Cisco Configuration275Backing Up the Cisco Router ConfigurationTo copy the router’s configuration from a router to a TFTP server, you can use either the copy running-config tftp or the copy startup-config tftp command. Either one will back up the router configuration that’s currently running in DRAM or that’s stored in NVRAM.Verifying the Current ConfigurationTo verify the configuration in DRAM, use the show running-config command (sh run for short) like this:

Router#show running-configBuilding configuration...Current configuration : 776 bytes!

version 12.4The current configuration information indicates that the router is running version 12.4 of the IOS.Verifying the Stored ConfigurationNext, you should check the configuration stored in NVRAM. To see this, use the show startup-config command (sh start for short) like this:

Router#show startup-configUsing 776 out of 245752 bytes!

version 12.4The second line shows you how much room your backup configuration is using. Here, we can see that NVRAM is 239KB (again, memory is easier to see with the show version com-mand when you’re using an ISR router) and that only 776 bytes of it are used.If you’re not sure that the files are the same and the running-config file is what you want to use, then use the copy running-config startup-config. This will help you verify that both files are in fact the same. I’ll go through this with you in the next section.Copying the Current Configuration to NVRAMBy copying running-config to NVRAM as a backup, as shown in the following output, you’re assured that your running-config will always be reloaded if the router gets rebooted. In the new IOS version 12.0, you’re prompted for the filename you want to use:

Router#copy running-config startup-configDestination filename [startup-config]?[enter]Building configuration...[OK]

Router#

10089.book  Page 275  Monday, July 23, 2007  3:17 PM




276Chapter5 Managing a Cisco InternetworkThe reason the filename prompt appears is that there are now so many options you can use when using the copy command:

Router#copy running-config ?  archive:        Copy to archive: file system  flash:          Copy to flash: file system  ftp:            Copy to ftp: file system  http:           Copy to http: file system  https:          Copy to https: file system  ips-sdf         Update (merge with) IPS signature configuration  null:           Copy to null: file system  nvram:          Copy to nvram: file system  rcp:            Copy to rcp: file system  running-config  Update (merge with) current system configuration  scp:            Copy to scp: file system  startup-config  Copy to startup configuration  syslog:         Copy to syslog: file system  system:         Copy to system: file system  tftp:           Copy to tftp: file system  xmodem:         Copy to xmodem: file system

  ymodem:         Copy to ymodem: file systemWe’ll go over the copy command again in a minute.Copying the Configuration to a TFTP ServerOnce the file is copied to NVRAM, you can make a second backup to a TFTP server by using the copy running-config tftp command (copy run tftp for short), like this:

Router#copy running-config tftpAddress or name of remote host []?1.1.1.2Destination filename [router-confg]?todd-confg!!776 bytes copied in 0.800 secs (970 bytes/sec)

Router#In the preceding example, I named the file todd-confg because I had not set a hostname for the router. If you have a hostname already configured, the command will automatically use the hostname plus the extension -confg as the name of the file.Restoring the Cisco Router ConfigurationIf you’ve changed your router’s running-config file and want to restore the configuration to the version in the startup-config file, the easiest way to do this is to use the copy startup-config 

10089.book  Page 276  Monday, July 23, 2007  3:17 PM




Backing Up and Restoring the Cisco Configuration277running-config command (copy start run for short). You can also use the older Cisco command config mem to restore a configuration. Of course, this will work only if you copied running-config into NVRAM before making any changes!If you did copy the router’s configuration to a TFTP server as a second backup, you can restore the configuration using the copy tftp running-config command (copy tftp run for short) or the copy tftp startup-config command (copy tftp start for short), as shown here (the old command that provides this function is config net):

Router#copy tftp running-configAddress or name of remote host []?1.1.1.2Source filename []?todd-confgDestination filename[running-config]?[enter]Accessing tftp://1.1.1.2/todd-confg...Loading todd-confg from 1.1.1.2 (via FastEthernet0/0): ![OK - 776 bytes]776 bytes copied in 9.212 secs (84 bytes/sec)Router#*Mar  7 17:53:34.071: %SYS-5-CONFIG_I: Configured from    tftp://1.1.1.2/todd-confg by console

Router#The configuration file is an ASCII text file, meaning that before you copy the configuration stored on a TFTP server back to a router, you can make changes to the file with any text editor. Last, notice that the command was changed to a URL of tftp://1.1.1.2/todd-config. This is the Cisco IOS File System (IFS)—as discussed earlier—and we’ll use that to back up and restore our configuration in a minute.

It is important to remember that when you copy or merge a configuration from a TFTP server to a router’s RAM, the interfaces are shut down by default and you must manually go and enable each interface with the no shutdown command.Erasing the ConfigurationTo delete the startup-config file on a Cisco router, use the command erase startup-config, like this:

Router#erase startup-configErasing the nvram filesystem will remove all configuration files!    Continue? [confirm][enter][OK]Erase of nvram: complete

10089.book  Page 277  Monday, July 23, 2007  3:17 PM




278Chapter5 Managing a Cisco Internetwork*Mar  7 17:56:20.407: %SYS-7-NV_BLOCK_INIT: Initialized the geometry of nvramRouter#reloadSystem configuration has been modified. Save? [yes/no]:nProceed with reload? [confirm][enter] *Mar  7 17:56:31.059: %SYS-5-RELOAD: Reload requested by console.

    Reload Reason: Reload Command.This command deletes the contents of NVRAM on the router. If you type reload at privileged mode and say no to saving changes, the router will reload and come up into setup mode.Using the Cisco IOS File System to Manage Your Router’s Configuration (Cisco IFS)Using the old, faithful copy command is still useful and I recommend it. However, you still need to know about the Cisco IFS. The first thing we’ll do is use the show file command to see the contents of NVRAM and RAM:

R3#show file information nvram:startup-confignvram:startup-config:  type is configR3#cd nvram:R3#pwdnvram:/R3#dirDirectory of nvram:/  190  -rw-         830                    <no date>  startup-config  191  ----           5                    <no date>  private-config  192  -rw-         830                    <no date>  underlying-config    1  -rw-           0                    <no date>  ifIndex-table

196600 bytes total (194689 bytes free)There really are no other commands that will actually show us the contents of NVRAM. However, I am not sure how helpful it is to see them either. Let’s look at the contents of RAM:

R3#cd system:R3#pwdsystem:/R3#dir ?  /all             List all files  /recursive       List files recursively  all-filesystems  List files on all filesystems  archive:         Directory or file name

10089.book  Page 278  Monday, July 23, 2007  3:17 PM




 Backing Up and Restoring the Cisco Configuration 279   cns:             Directory or file name  flash:           Directory or file name  null:            Directory or file name  nvram:           Directory or file name  system:          Directory or file name  xmodem:          Directory or file name  ymodem:          Directory or file name  <cr>R3# dir Directory of system:/    3  dr-x           0                    <no date>  lib   33  dr-x           0                    <no date>  memory    1  -rw-         750                    <no date>  running-config

     2  dr-x           0                    <no date>  vfiles Again, not too exciting. Let’s use the  copy  command with the Cisco IFS to copy a file from a TFTP host to RAM. First, let’s try the old command  config net  that was used for the last 10 years or so to accomplish this same feat:

 R3# config net Host or network configuration file [host]? [enter] This command has been replaced by the command:         ‘copy <url> system:/running-config’

 Address or name of remote host [255.255.255.255]? Although the command tells us that the command has been replaced with the new URL command, the old command will still work. Let’s try it with the Cisco IFS:

 R3# copy tftp://1.1.1.2/todd-confg system://running-config Destination filename [running-config]? [enter] Accessing tftp://1.1.1.2/todd-confg...Loading todd-confg from 1.1.1.2    (via FastEthernet0/0): ![OK - 776 bytes][OK]776 bytes copied in 13.816 secs (56 bytes/sec)R3#*Mar 10 22:12:59.819: %SYS-5-CONFIG_I:

 Configured from tftp://1.1.1.2/todd-confg by console I guess we can say that this was easier than using the  copy tftp run  command—Cisco says it is, so who am I to argue? Maybe it just takes some getting used to. Let’s see if we can make this a bit easier by connecting to our router through HTTP or HTTPS and using the SDM to manage our configuration files.

 

10089c05.fm  Page 279  Friday, November 7, 2008  10:49 PM




280Chapter5 Managing a Cisco InternetworkUsing the SDM to Back Up/Restore and Edit the Router’s ConfigurationHonestly, there really isn’t anything special about how the SDM handles configurations on a router. Basically, if you were to telnet to a router, perform a show run, and copy this output to a text file on your PC, you’ve just performed (more or less) what SDM and its configuration management tools can do. But this is still a less confusing way to manage files than doing so through the Cisco IFS.Why? Well, what’s easier about using the SDM rather than the copy command we covered earlier in this chapter is that no TFTP host is needed. By using the SDM, you can http or https to a router and keep all files local on your PC instead of having to configure a TFTP host. As I mentioned in Chapter 4, SDM is best used for advanced configurations like security, IPS, QOS, and NAT. As you can tell by now, I am a command-line interface (CLI) guy. What can I say? Old habits die hard!Let’s take a quick look at how the SDM can back up and restore your configuration from your host. From the main menu, choose File   Write to Startup Config to back up your con-figuration to NVRAM.

10089.book  Page 280  Monday, July 23, 2007  3:17 PM




Backing Up and Restoring the Cisco Configuration281Then choose File   Save Running Config to PC.One last option for managing your files is to use the Configuration Management screens under Additional Tasks.

10089.book  Page 281  Monday, July 23, 2007  3:17 PM




282Chapter5 Managing a Cisco InternetworkThe Config Editor allows you to change the running-config, but before it will let you do that, you have to agree that you can screw up your router’s configuration and that this is okay with you!It’s best to click the Save Running Configuration button. You can then choose to import the file from RAM or from your PC.

10089.book  Page 282  Monday, July 23, 2007  3:17 PM




Using Cisco Discovery Protocol (CDP)283Last, from Configuration Management, you can choose Reset to Factory Default. This will place the HTTPS management back on the router.As you can see, there are many different ways to screw up, umm, I mean, change the files in flash, NVRAM, and even RAM. Practice on the SDM demo I showed you in Chapter 4, or try to borrow a friend’s router. (You don’t want to practice these commands on your own router, do you?)Using Cisco Discovery Protocol (CDP)Cisco Discovery Protocol (CDP) is a proprietary protocol designed by Cisco to help adminis-trators collect information about both locally attached and remote devices. By using CDP, you can gather hardware and protocol information about neighbor devices, which is useful info for troubleshooting and documenting the network.In the following sections, I am going to discuss the CDP timer and CDP commands used to verify your network.Getting CDP Timers and Holdtime InformationThe show cdp command (sh cdp for short) gives you information about two CDP global parameters that can be configured on Cisco devices: CDP timer is how often CDP packets are transmitted out all active interfaces. CDP holdtime is the amount of time that the device will hold packets received from neighbor devices.

10089.book  Page 283  Monday, July 23, 2007  3:17 PM




284Chapter5 Managing a Cisco InternetworkBoth Cisco routers and Cisco switches use the same parameters.

For this section and the rest of this chapter, my 2811 will have a hostname of Corp, and it will have four serial connections to ISR routers named R1, R2, and R3 (there are two connections to R1) and one FastEthernet connection to a 1242 access point with a hostname of just ap.The output on the Corp router looks like this:

Corp#sh cdpGlobal CDP information:        Sending CDP packets every 60 seconds        Sending a holdtime value of 180 seconds

        Sending CDPv2 advertisements is  enabledUse the global commands cdp holdtime and cdp timer to configure the CDP holdtime and timer on a router:

Corp(config)#cdp ?  advertise-v2      CDP sends version-2 advertisements  holdtime          Specify the holdtime (in sec) to be sent in packets  log               Log messages generated by CDP  run               Enable CDP  source-interface  Insert the interface’s IP in all CDP packets  timer             Specify rate (in sec) at which CDP packets are sent  runCorp(config)#cdp holdtime ?  <10-255>  Length  of time  (in sec) that receiver must keep this packetCorp(config)#cdp timer ?

  <5-254>  Rate at which CDP packets are sent (in  sec)You can turn off CDP completely with the no cdp run command from the global configu-ration mode of a router. To turn CDP off or on for an interface, use the no cdp enable and cdp enable commands. Be patient—I’ll work through these with you in a second.Gathering Neighbor InformationThe show cdp neighbor command (sh cdp nei for short) delivers information about directly connected devices. It’s important to remember that CDP packets aren’t passed through a Cisco switch and that you only see what’s directly attached. So this means that if your router is con-nected to a switch, you won’t see any of the devices hooked up to that switch.The following output shows the show cdp neighbor command used on my ISR router:

Corp#sh cdp neighborsCapability Codes: R - Router, T - Trans Bridge, B - Source Route Bridge

10089.book  Page 284  Monday, July 23, 2007  3:17 PM




 Using Cisco Discovery Protocol (CDP) 285                   S - Switch, H - Host, I - IGMP, r - RepeaterDevice ID    Local Intrfce   Holdtme    Capability  Platform  Port IDap         Fas 0/1            165         T I       AIR-AP124 Fas 0R2         Ser 0/1/0          140        R S I      2801      Ser 0/2/0R3         Ser 0/0/1          157        R S I      1841      Ser 0/0/1R1         Ser 0/2/0          154        R S I      1841      Ser 0/0/1R1         Ser 0/0/0          154        R S I      1841      Ser 0/0/0

 Corp# Okay, we are directly connected with a console cable to the Corp ISR router, and the router is directly connected to four devices. We have two connections to the R1 router. The device ID shows the configured hostname of the connected device, the local interface is our interface, and the port ID is the remote devices’ directly connected interface. All you get to view are directly connected devices.Table 5.5 summarizes the information displayed by the  show cdp neighbor  command for each device.

 It is imperative that you can look at the output of a  show cdp neighbors  com-mand and decipher the neighbor’s device (capability, i.e., router or switch), model number (platform), your port connecting to that device (local inter- face), and the port of the neighbor connecting to you (port ID). TABLE5.5 Output of the  show cdp neighbor  Command FieldDescription Device ID The hostname of the device directly connected. Local Interface The port or interface on which you are receiving the CDP packet. Holdtime The amount of time the router will hold the information before discarding it if no more CDP packets are received. Capability The capability of the neighbor, such as the router, switch, or repeater. The capability codes are listed at the top of the command output. Platform The type of Cisco device directly connected. In the previous output, the Corp router is directly connected to a 1242 Access point, a Cisco 2801 and two 1841 routers. Port ID The neighbor device’s port or interface on which the CDP packets are multicast.

 

10089c05.fm  Page 285  Wednesday, February 27, 2008  4:56 PM




286Chapter5 Managing a Cisco InternetworkAnother command that’ll deliver the goods on neighbor information is the show cdp neighbors detail command (show cdp nei de for short). This command can be run on both routers and switches, and it displays detailed information about each device connected to the device you’re running the command on. Check out this router output for an example:

Corp#sh cdp neighbors detail-------------------------Device ID: apEntry address(es): 10.1.1.2Platform: cisco AIR-AP1242AG-A-K9   ,  Capabilities: Trans-Bridge IGMPInterface: FastEthernet0/1,  Port ID (outgoing port): FastEthernet0Holdtime : 122 secVersion :Cisco IOS Software, C1240 Software (C1240-K9W7-M), Version 12.3(8)JEA,    RELEASE SOFTWARE (fc2)Technical Support: http://www.cisco.com/techsupportCopyright (c) 1986-2006 by Cisco Systems, Inc.Compiled Wed 23-Aug-06 16:45 by kellythwadvertisement version: 2Duplex: fullPower drawn: 15.000 Watts-------------------------Device ID: R2Entry address(es):  IP address: 10.4.4.2Platform: Cisco 2801,  Capabilities: Router Switch IGMPInterface: Serial0/1/0,  Port ID (outgoing port): Serial0/2/0Holdtime : 135 secVersion :Cisco IOS Software, 2801 Software (C2801-ADVENTERPRISEK9-M),    Experimental Version 12.4(20050525:193634) [jezhao-ani 145]Copyright (c) 1986-2005 by Cisco Systems, Inc.Compiled Fri 27-May-05 23:53 by jezhaoadvertisement version: 2VTP Management Domain: ‘‘-------------------------Device ID: R3

10089.book  Page 286  Monday, July 23, 2007  3:17 PM




Using Cisco Discovery Protocol (CDP)287Entry address(es):  IP address: 10.5.5.1Platform: Cisco 1841,  Capabilities: Router Switch IGMPInterface: Serial0/0/1,  Port ID (outgoing port): Serial0/0/1Holdtime : 152 secVersion :Cisco IOS Software, 1841 Software (C1841-IPBASE-M), Version 12.4(1c),    RELEASE SOFTWARE (fc1)Technical Support: http://www.cisco.com/techsupportCopyright (c) 1986-2005 by Cisco Systems, Inc.Compiled Tue 25-Oct-05 17:10 by evmilleradvertisement version: 2VTP Management Domain: ‘‘-------------------------[output cut]

Corp#What are we being shown here? First, we’re given the hostname and IP address of all directly connected devices. In addition to the same information displayed by the show cdp neighbor command (see Table 5.5), the show cdp neighbor detail command gives us the IOS version of the neighbor device.

Remember that you can see the IP address of only directly connected devices.The show cdp entry * command displays the same information as the show cdp neighbors detail command. Here’s an example of the router output using the show cdp entry * command:

Corp#sh cdp entry *-------------------------Device ID: apEntry address(es):Platform: cisco AIR-AP1242AG-A-K9   ,  Capabilities: Trans-Bridge IGMPInterface: FastEthernet0/1,  Port ID (outgoing port): FastEthernet0Holdtime : 160 secVersion :Cisco IOS Software, C1240 Software (C1240-K9W7-M), Version 12.3(8)JEA,    RELEASE SOFTWARE (fc2)

10089.book  Page 287  Monday, July 23, 2007  3:17 PM




288Chapter5 Managing a Cisco InternetworkTechnical Support: http://www.cisco.com/techsupportCopyright (c) 1986-2006 by Cisco Systems, Inc.Compiled Wed 23-Aug-06 16:45 by kellythwadvertisement version: 2Duplex: fullPower drawn: 15.000 Watts-------------------------Device ID: R2Entry address(es):  IP address: 10.4.4.2Platform: Cisco 2801,  Capabilities: Router Switch IGMP --More—

[output cut]There isn’t any difference between the show cdp neighbors detail and show cdp entry * commands. However, the sh cdp entry * command has two options that the show cdp neighbors detail command does not:

Corp#sh cdp entry * ?  protocol  Protocol information  version   Version information  |         Output modifiers  <cr>Corp#show cdp entry * protocolsProtocol information for ap :  IP address: 10.1.1.2Protocol information for R2 :  IP address: 10.4.4.2Protocol information for R3 :  IP address: 10.5.5.1Protocol information for R1 :  IP address: 10.3.3.2Protocol information for R1 :

  IP address: 10.2.2.2The preceding output of the show cdp entry * protocols command can show you just the IP addresses of each directly connected neighbor. The show cdp entry * version will show you only the IOS version of your directly connected neighbors:

Corp#show cdp entry * versionVersion information for ap :  Cisco IOS Software, C1240 Software (C1240-K9W7-M), Version   12.3(8)JEA, RELEASE SOFTWARE (fc2)

10089.book  Page 288  Monday, July 23, 2007  3:17 PM




Using Cisco Discovery Protocol (CDP)289Technical Support: http://www.cisco.com/techsupportCopyright (c) 1986-2006 by Cisco Systems, Inc.Compiled Wed 23-Aug-06 16:45 by kellythwVersion information for R2 :  Cisco IOS Software, 2801 Software (C2801-ADVENTERPRISEK9-M),   Experimental Version 12.4(20050525:193634) [jezhao-ani 145]Copyright (c) 1986-2005 by Cisco Systems, Inc.Compiled Fri 27-May-05 23:53 by jezhaoVersion information for R3 :  Cisco IOS Software, 1841 Software (C1841-IPBASE-M), Version 12.4(1c),   RELEASE SOFTWARE (fc1)Technical Support: http://www.cisco.com/techsupportCopyright (c) 1986-2005 by Cisco Systems, Inc.Compiled Tue 25-Oct-05 17:10 by evmiller --More—

[output cut]Although the show cdp neighbors detail and show cdp entry commands are very similar, the show cdp entry command allows you to display only one line of output for each directly connected neighbor, whereas the show cdp neighbor detail command does not. Next, let’s look at the show cdp traffic command.Gathering Interface Traffic InformationThe show cdp traffic command displays information about interface traffic, including the number of CDP packets sent and received and the errors with CDP.The following output shows the show cdp traffic command used on the Corp router:

Corp#sh cdp trafficCDP counters :        Total packets output: 911, Input: 524        Hdr syntax: 0, Chksum error: 0, Encaps failed: 2        No memory: 0, Invalid packet: 0, Fragmented: 0        CDP version 1 advertisements output: 0, Input: 0        CDP version 2 advertisements output: 911, Input: 524

Corp#This is not really the most important information you can gather from a router, but it does show how many CDP packets are sent and received on a device.

10089.book  Page 289  Monday, July 23, 2007  3:17 PM




290Chapter5 Managing a Cisco InternetworkGathering Port and Interface InformationThe show cdp interface command gives you the CDP status on router interfaces or switch ports.As I said earlier, you can turn off CDP completely on a router by using the no cdp run com-mand. But remember that you can also turn off CDP on a per-interface basis with the no cdp enable command. You enable a port with the cdp enable command. All ports and interfaces default to cdp enable.On a router, the show cdp interface command displays information about each interface using CDP, including the encapsulation on the line, the timer, and the holdtime for each inter-face. Here’s an example of this command’s output on the ISR router:

Corp#sh cdp interfaceFastEthernet0/0 is administratively down, line protocol is down  Encapsulation ARPA  Sending CDP packets every 60 seconds  Holdtime is 180 secondsFastEthernet0/1 is up, line protocol is up  Encapsulation ARPA  Sending CDP packets every 60 seconds  Holdtime is 180 secondsSerial0/0/0 is up, line protocol is up  Encapsulation HDLC  Sending CDP packets every 60 seconds  Holdtime is 180 secondsSerial0/0/1 is up, line protocol is up  Encapsulation HDLC  Sending CDP packets every 60 seconds  Holdtime is 180 secondsSerial0/1/0 is up, line protocol is up  Encapsulation HDLC  Sending CDP packets every 60 seconds  Holdtime is 180 secondsSerial0/2/0 is up, line protocol is up  Encapsulation HDLC  Sending CDP packets every 60 seconds

  Holdtime is 180 secondsThe above output is nice because it always tells you the interface’s status. To turn off CDP on one interface on a router, use the no cdp enable command from interface configuration mode:

Corp#config tCorp(config)#int s0/0/0Corp(config-if)#no cdp enable

10089.book  Page 290  Monday, July 23, 2007  3:17 PM




Using Cisco Discovery Protocol (CDP)291Corp(config-if)#do show cdp interface    FastEthernet0/0 is administratively down, line protocol is down  Encapsulation ARPA  Sending CDP packets every 60 seconds  Holdtime is 180 secondsFastEthernet0/1 is up, line protocol is up  Encapsulation ARPA  Sending CDP packets every 60 seconds  Holdtime is 180 secondsSerial0/0/1 is up, line protocol is up  Encapsulation HDLC  Sending CDP packets every 60 seconds  Holdtime is 180 secondsSerial0/1/0 is up, line protocol is up  Encapsulation HDLC  Sending CDP packets every 60 seconds  Holdtime is 180 secondsSerial0/2/0 is up, line protocol is up  Encapsulation HDLC  Sending CDP packets every 60 seconds  Holdtime is 180 seconds

Corp(config-if)#Notice that serial 0/0/0 isn’t listed in the router output. To get that output, you’d have to per-form a cdp enable on serial 0/0/0. It would then show up in the output:

Corp(config-if)#cdp enableCorp(config-if)#^Z

Corp#

CDP Can Save Lives!Karen has just been hired as a senior network consultant at a large hospital in Dallas, Texas. She is expected to be able to take care of any problem that comes up. No stress here—she only has to worry about people possibly not getting the right health care if the network goes down. Talk about a potential life-or-death situation!Karen starts her job happily. Soon, of course, the network has some problems. She asks one of the junior administrators for a network map so she can troubleshoot the network. This per-son tells her that the old senior administrator (who just got fired) had them with him and now no one can find them—ouch!

10089.book  Page 291  Monday, July 23, 2007  3:17 PM




292Chapter5 Managing a Cisco InternetworkDocumenting a Network Topology Using CDPAs the title of this section implies, I’m now going to show you how to document a sample net-work by using CDP. You’ll learn to determine the appropriate router types, interface types, and IP addresses of various interfaces using only CDP commands and the show running-config command. And you can only console into the Lab_A router to document the network. You’ll have to assign any remote routers the next IP address in each range. Figure 5.2 is what you’ll use to complete the documentation.FIGURE5.2Documenting a network topology using CDP

Doctors are calling every couple of minutes because they can’t get the necessary information they need to take care of their patients. What should she do?CDP to the rescue! Thank God this hospital has all Cisco routers and switches and that CDP is enabled by default on all Cisco devices. Also, luckily, the disgruntled administrator who just got fired didn’t turn off CDP on any devices before he left.All Karen has to do now is to use the show cdp neighbor detail command to find all the infor-mation she needs about each device to help draw out the hospital network and save lives!The only snag for you nailing this in your own network is if you don’t know the passwords of all those devices. Your only hope then is to somehow find out the access passwords or to perform password recovery on them.So, use CDP— you never know when you may end up saving someone’s life.This is a true story.

IP address Fa0/0 S1 Fa0/1 .1 .1 .1 .1 

S0/0 S0/1

Lab_A 

Router 

Int 

IP address 

Router 

IP address 

Router 

Int Int 

IP address 

Router 

Int 

10089.book  Page 292  Monday, July 23, 2007  3:17 PM




Using Cisco Discovery Protocol (CDP)293In this output, you can see that you have a router with four interfaces: two FastEthernet and two serial. First, determine the IP addresses of each interface by using the show running-config command:

Lab_A#sh running-configBuilding configuration...Current configuration : 960 bytes!version 12.2service timestamps debug uptimeservice timestamps log uptimeno service password-encryption!hostname Lab_A!ip subnet-zero!!interface FastEthernet0/0 ip address 192.168.21.1 255.255.255.0 duplex auto!interface FastEthernet0/1 ip address 192.168.18.1 255.255.255.0 duplex auto!interface Serial0/0ip address 192.168.23.1 255.255.255.0!interface Serial0/1ip address 192.168.28.1 255.255.255.0!ip classless!line con 0line aux 0line vty 0 4!

end

10089.book  Page 293  Monday, July 23, 2007  3:17 PM




294Chapter5 Managing a Cisco InternetworkWith this step completed, you can now write down the IP addresses of the Lab_A router’s four interfaces. Next, you need to determine the type of device on the other end of each of these interfaces. It’s easy to do this—just use the show cdp neighbors command:

  Lab_A#sh cdp neighbors  Capability Codes: R - Router, T - Trans Bridge, B - Source Route Bridge  S - Switch, H - Host, I - IGMP, r - Repeater  Device ID   Local Intrfce     Holdtme    Capability Platform  Port ID  Lab_B        Fas 0/0            178          R        2501     E0  Lab_C        Fas 0/1            137          R        2621     Fa0/0  Lab_D        Ser 0/0            178          R        2514     S1  Lab_E        Ser 0/1            137          R        2620     S0/1

  Lab_A#You’ve got a good deal of information now! By using both the show running-config and show cdp neighbors commands, you know about all the IP addresses of the Lab_A router plus the types of routers connected to each of the Lab_A router’s links and all the interfaces of the remote routers.And by using all the information gathered from show running-config and show cdp neighbors, we can now create the topology in Figure 5.3.If we needed to, we could’ve also used the show cdp neighbors detail command to view the neighbor’s IP addresses. But since we know the IP addresses of each link on the Lab_A router, we already know what the next available IP address is going to be.FIGURE5.3Network topology documented

192.168.21 .2/24 192.168.18 .2/24 2501 192.168.23 .2/24 2514 E0 Fa0/0 Fa0/0 2621 192.168.28.2/24 S0/1 S1 2620 Fa0/1 .1 .1 .1 .1 

S0/0 S0/1 

Lab_A 

10089.book  Page 294  Monday, July 23, 2007  3:17 PM




Using Telnet295Using TelnetTelnet, part of the TCP/IP protocol suite, is a virtual terminal protocol that allows you to make connections to remote devices, gather information, and run programs.After your routers and switches are configured, you can use the Telnet program to recon-figure and/or check up on your routers and switches without using a console cable. You run the Telnet program by typing telnet from any command prompt (DOS or Cisco). You need to have VTY passwords set on the routers for this to work.Remember, you can’t use CDP to gather information about routers and switches that aren’t directly connected to your device. But you can use the Telnet application to connect to your neighbor devices and then run CDP on those remote devices to get information on them.You can issue the telnet command from any router prompt like this:

Corp#telnet 10.2.2.2Trying 10.2.2.2 ... OpenPassword required, but none set[Connection to 10.2.2.2 closed by foreign host]

Corp#As you can see, I didn’t set my passwords—how embarrassing! Remember that the VTY ports on a router are configured as login, meaning that we have to either set the VTY passwords or use the no login command. (You can review setting passwords in Chapter 4, “Cisco’s Inter-networking Operating System (IOS) and Security Device Manager (SDM),” if you need to.)

If you find you can’t telnet into a device, it could be that the password on the remote device hasn’t been set. It’s also possible that an access control list is filtering the Telnet session.On a Cisco router, you don’t need to use the telnet command; you can just type in an IP address from a command prompt and the router will assume that you want to telnet to the device. Here’s how that looks using just the IP address:

Corp#10.2.2.2     Trying 10.2.2.2 ... OpenPassword required, but none set[Connection to 10.2.2.2 closed by foreign host]

Corp#

10089.book  Page 295  Monday, July 23, 2007  3:17 PM




296Chapter5 Managing a Cisco InternetworkAt this point, it would be a great idea to set those VTY passwords on the router I want to telnet into. Here’s what I did on the remote router named R1:

R1#config tEnter configuration commands, one per line.  End with CNTL/Z.R1(config)#line vty 0 ?  <1-807>  Last Line number  <cr>R1(config)#line vty 0 807R1(config-line)#password telnetR1(config-line)#login

R1(config-line)#^ZNow let’s try this again. Here I’m connecting to the router from the Corp ISR console:

Corp#10.2.2.2Trying 10.2.2.2 ... OpenUser Access VerificationPassword:

R1>Remember that the VTY password is the user-mode password, not the enable-mode password. Watch what happens when I try to go into privileged mode after telnetting into router R1:

R1>en% No password set

R1>It is basically saying, “No way!” This is a really good security feature because you don’t want anyone telnetting into your device and being able to just type the enable command to get into privileged mode. You’ve got to set your enable-mode password or enable secret pass-word to use Telnet to configure remote devices!

When you telnet into a remote device, you will not see console messages by default. For example, you will not see debugging output. To allow con-sole messages to be sent to your Telnet session, use the terminal monitor command.In the following examples, I am going to show you how to telnet into multiple devices simultaneously and then show you how to use hostnames instead of IP addresses.

10089.book  Page 296  Monday, July 23, 2007  3:17 PM




Using Telnet297Telnetting into Multiple Devices SimultaneouslyIf you telnet to a router or switch, you can end the connection by typing exit at any time. But what if you want to keep your connection to a remote device but still come back to your orig-inal router console? To do that, you can press the Ctrl+Shift+6 key combination, release it, and then press X.Here’s an example of connecting to multiple devices from my Corp router console:

Corp#10.2.2.2Trying 10.2.2.2 ... OpenUser Access VerificationPassword:R1>Ctrl+Shift+6

Corp#In this example, I telnetted to the R1 router and then typed the password to enter user mode. I next pressed Ctrl+Shift+6, then X (but you can’t see that because it doesn’t show on the screen output). Notice that my command prompt is now back at the Corp router.Let’s run through some verification commands..Checking Telnet ConnectionsTo see the connections made from your router to a remote device, use the show sessions command:

Corp#sh sessionsConn Host                Address             Byte  Idle Conn Name   1 10.2.2.2            10.2.2.2               0     0 10.2.2.2*  2 10.1.1.2            10.1.1.2               0     0 10.1.1.2

Corp#See that asterisk (*) next to connection 2? It means that session 2 was your last session. You can return to your last session by pressing Enter twice. You can also return to any session by typing the number of the connection and pressing Enter.Checking Telnet UsersYou can list all active consoles and VTY ports in use on your router with the show users command:

Corp#sh users    Line       User       Host(s)              Idle       Location*  0 con 0                10.1.1.2             00:00:01

                          10.2.2.2             00:01:06

10089.book  Page 297  Monday, July 23, 2007  3:17 PM




298Chapter5 Managing a Cisco InternetworkIn the command’s output, con represents the local console. In this example, the console is con-nected to two remote IP addresses, or in other words, two devices. In the next example, I typed sh users on the ap device that the Corp router had telnetted into and is connected to via line 1:

Corp#sh sessionsConn Host                Address             Byte  Idle Conn Name   1 10.1.1.2            10.1.1.2               0     0 10.1.1.2*  2 10.2.2.2            10.2.2.2               0     0 10.2.2.2Corp#1[Resuming connection 1 to 10.1.1.2 ... ]ap>sh users    Line       User       Host(s)              Idle       Location*  1 vty 0                idle                 00:00:00 10.1.1.1

ap>This output shows that the console is active and that VTY port 1 is being used. The asterisk represents the current terminal session from which the show user command was entered.Closing Telnet SessionsYou can end Telnet sessions a few different ways—typing exit or disconnect is probably the easiest and quickest.To end a session from a remote device, use the exit command:

ap>exit[Connection to 10.1.1.2 closed by foreign host]

Corp#Since the ap device was my last session, I just pressed Enter twice to return to that session.To end a session from a local device, use the disconnect command:

Corp#sh sessionConn Host                Address             Byte  Idle Conn Name   2 10.2.2.2            10.2.2.2               0     0 10.2.2.2Corp#disconnect ?  <0-0>  The number of an active network connection  qdm    Disconnect QDM web-based clients  ssh    Disconnect an active SSH connectionCorp#disconnect 2Closing connection to 10.2.2.2 [confirm][enter]

Corp#In this example, I used the session number 2 because that was the connection to the R1 router that I wanted to end. As I showed, you can use the show sessions command to see the connection number.

10089.book  Page 298  Monday, July 23, 2007  3:17 PM




Using Telnet299If you want to end a session of a device attached to your local device through Telnet, you should first check to see if any devices are telnetted into your router. To get that information, use the show users command like this:

R1#sh users    Line       User       Host(s)              Idle     Location*  0 con 0                idle                 00:00:00 

  vty 194                 idle                 00:00:21 10.2.2.1This output shows that VTY has IP address 10.2.2.1 connected. That’s the Corp router. Also notice that the Corp router connected to line 194—remember, you cannot choose which line you connect to! This is why we set the same password on all lines.To clear the connection, use the clear line # command:

R1#clear line 194[confirm][enter] [OK]R1#sh users    Line       User       Host(s)              Idle       Location

*  0 con 0                idle                 00:00:00 This output confirms that the line has been cleared.Using SDM to Telnet into Your RouterThere is not too much to tell you regarding Telnet services when using SDM. You don’t get a menu or options, just a pop-up DOS screen that telnets into the router you are already con-nected to via HTTP or HTTPS.Click the Tools menu, and then choose Telnet.

10089.book  Page 299  Monday, July 23, 2007  3:17 PM




300Chapter5 Managing a Cisco InternetworkOnce you choose Telnet, a DOS screen pops up and you will be in user mode (once you enter the telnet password, of course).It would be nice if you had other options with Telnet when using SDM, but that is not to be at this time.Resolving HostnamesTo use a hostname rather than an IP address to connect to a remote device, the device that you are using to make the connection must be able to translate the hostname to an IP address.There are two ways to resolve hostnames to IP addresses: building a host table on each router or building a Domain Name System (DNS) server, which is similar to a dynamic host table.Building a Host TableA host table provides name resolution only on the router that it was built upon. The command to build a host table on a router is as follows:

ip host host_name tcp_port_number ip_addressThe default is TCP port number 23, but you can create a session using Telnet with a different TCP port number if you want. You can also assign up to eight IP addresses to a hostname.Here’s an example of configuring a host table on the Corp router with two entries to resolve the names for the R1 router and the ap device:

Corp#config tCorp(config)#ip host R1 ?  <0-65535>   Default telnet port number  A.B.C.D     Host IP address  additional  Append addresses  mx          Configure a MX record  ns          Configure an NS record  srv         Configure a SRV record

10089.book  Page 300  Monday, July 23, 2007  3:17 PM




Resolving Hostnames301Corp(config)#ip host R1 10.2.2.2 ?  A.B.C.D  Host IP address  <cr>Corp(config)#ip host R1 10.2.2.2

Corp(config)#ip host ap 10.1.1.2Notice in the above router configuration that I can just keep adding IP addresses to refer-ence a host, one after another, up to eight IP address. And to see the newly built host table, just use the show hosts command:

Corp(config)#do show hostsDefault domain is not setName/address lookup uses domain serviceName servers are 255.255.255.255Codes: UN - unknown, EX - expired, OK - OK, ?? - revalidate       temp - temporary, perm - permanent       NA - Not Applicable None - Not definedHost                      Port  Flags      Age Type   Address(es)ap                        None  (perm, OK)  0   IP    10.1.1.2R1                        None  (perm, OK)  0   IP    10.2.2.2Corp(config)#^Z

Corp#You can see the two hostnames plus their associated IP addresses in the preceding router output. The perm in the Flags column means that the entry is manually configured. If it said temp, it would be an entry that was resolved by DNS.

The show hosts command provides information on temporary DNS entries and permanent name-to-address mappings created using the ip host command.To verify that the host table resolves names, try typing the hostnames at a router prompt. Remember that if you don’t specify the command, the router assumes you want to telnet.In the following example, I’ll use the hostnames to telnet into the remote devices and press Ctrl+Shift+6 and then X to return to the main console of the Corp router:

Corp#r1Trying R1 (10.2.2.2)... OpenUser Access VerificationPassword:R1>Ctrl+Shift+6

10089.book  Page 301  Monday, July 23, 2007  3:17 PM




302Chapter5 Managing a Cisco InternetworkCorp#apTrying ap (10.1.1.2)... OpenUser Access VerificationPassword:ap>Ctrl+Shift+6

Corp#I successfully used entries in the host table to create a session to two devices and used the names to telnet into both devices. Names in the host table are not case sensitive.Notice that the entries in the following show sessions output now display the hostnames and IP addresses instead of just the IP addresses:

Corp#sh sessionsConn Host                Address             Byte  Idle Conn Name   1 r1                  10.2.2.2               0     1 r1*  2 ap                  10.1.1.2               0     0 ap

Corp#If you want to remove a hostname from the table, just use the no ip host command like this:

RouterA(config)#no ip host R1The problem with the host table method is that you would need to create a host table on each router to be able to resolve names. And if you have a whole bunch of routers and want to resolve names, using DNS is a much better choice!Using DNS to Resolve NamesIf you have a lot of devices and don’t want to create a host table in each device, you can use a DNS server to resolve hostnames.Any time a Cisco device receives a command it doesn’t understand, it will try to resolve it through DNS by default. Watch what happens when I type the special command todd at a Cisco router prompt:

Corp#toddTranslating “todd”...domain server (255.255.255.255)Translating “todd”...domain server (255.255.255.255)Translating “todd”...domain server (255.255.255.255)% Unknown command or computer name, or unable to find  computer address

Corp#

10089.book  Page 302  Monday, July 23, 2007  3:17 PM




Resolving Hostnames303It doesn’t know my name or what command I am trying to type, so it tries to resolve this through DNS. This is really annoying for two reasons: first, because it doesn’t know my name <grin>, and second, because I need to hang out and wait for the name lookup to time out. You can get around this and prevent a time-consuming DNS lookup by using the no ip domain-lookup command on your router from global configuration mode.If you have a DNS server on your network, you need to add a few commands to make DNS name resolution work: The first command is ip domain-lookup, which is turned on by default. It needs to be entered only if you previously turned it off (with the no ip domain-lookup command). The command can be used without the hyphen as well (ip domain lookup). The second command is ip name-server. This sets the IP address of the DNS server. You can enter the IP addresses of up to six servers. The last command is ip domain-name. Although this command is optional, it really should be set. It appends the domain name to the hostname you type in. Since DNS uses a fully qualified domain name (FQDN) system, you must have a full DNS name, in the form domain.com.Here’s an example of using these three commands:

Corp#config tCorp(config)#ip domain-lookupCorp(config)#ip name-server ?  A.B.C.D  Domain server IP address (maximum of 6)Corp(config)#ip name-server 192.168.0.70Corp(config)#ip domain-name lammle.comCorp(config)#^Z

Corp#After the DNS configurations are set, you can test the DNS server by using a hostname to ping or telnet a device like this:

Corp#ping R1Translating “R1”...domain server (192.168.0.70) [OK]Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 10.2.2.2, timeout is  2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip min/avg/max

  = 28/31/32 msNotice that the router uses the DNS server to resolve the name.

10089.book  Page 303  Monday, July 23, 2007  3:17 PM




 304 Chapter5  Managing a Cisco Internetwork After a name is resolved using DNS, use the  show hosts  command to see that the device cached this information in the host table:

 Corp# sh hosts Default domain is lammle.comName/address lookup uses domain serviceName servers are 192.168.0.70Host                  Flags      Age Type   Address(es)R1                    (temp, OK)  0   IP    10.2.2.2ap                    (perm, OK)  0   IP    10.1.1.2

 Corp# The entry that was resolved is shown as  temp , but the ap device is still  perm , meaning that it’s a static entry. Notice that the hostname is a full domain name. If I hadn’t used the  ip domain-name lammle.com  command, I would have needed to type in  ping r1.lammle.com , which is a pain.

 Should You Use a Host Table or a DNS Server? Karen has finally finished drawing out her network by using CDP and the doctors are much hap-pier. However, Karen is having a difficult time administering the network because she has to look at the network drawing to find an IP address every time she needs to telnet to a remote router.Karen was thinking about putting host tables on each router, but with literally hundreds of routers, this is a daunting task.Most networks have a DNS server now anyway, so adding a hundred or so hostnames into it would be an easy way to go—certainly easier than adding these hostnames to each and every router! She can just add the three commands on each router and blammo—she’s resolving names.Using a DNS server makes it easy to update any old entries too—remember, even one little change and off she goes to each and every router to manually update its table if she’s using static host tables.Keep in mind that this has nothing to do with name resolution on the network and nothing to do with what a host on the network is trying to accomplish. This is only used when you’re trying to resolve names from the router console.

 

10089c05.fm  Page 304  Friday, November 7, 2008  10:50 PM




Checking Network Connectivity and Troubleshooting305Checking Network Connectivity and TroubleshootingYou can use the ping and traceroute commands to test connectivity to remote devices, and both of them can be used with many protocols, not just IP. But don’t forget that the show ip route command is a good troubleshooting command for verifying your routing table and the show interfaces command will show you the status of each interface.I’m not going to get into the show interfaces commands here because we’ve already been over that in Chapter 4. But I am going to go over both the debug command and the show processes command you need to troubleshoot a router.Using the ping CommandSo far, you’ve seen many examples of pinging devices to test IP connectivity and name reso-lution using the DNS server. To see all the different protocols that you can use with the Ping program, type ping ?:

Corp#ping ?  WORD  Ping destination address or hostname  clns  CLNS echo  ip    IP echo  srb   srb echo  tag   Tag encapsulated IP echo

  <cr>The ping output displays the minimum, average, and maximum times it takes for a ping packet to find a specified system and return. Here’s an example:

Corp#ping R1Translating “R1”...domain server (192.168.0.70)[OK]Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 10.2.2.2, timeout  is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip min/avg/max  = 1/2/4 ms

Corp#You can see that the DNS server was used to resolve the name, and the device was pinged in 1 ms (milliseconds), an average of 2 ms, and up to 4 ms.

10089.book  Page 305  Monday, July 23, 2007  3:17 PM




306Chapter5 Managing a Cisco Internetwork

The ping command can be used in user and privileged mode but not config-uration mode.Pinging with SDMUnlike with the Telnet option in SDM, we at least have a screen we can use to choose an option or two.Once you choose Tools  Ping, the Ping screen appears.From here you can choose the source interface to ping from, which is a nice option. Enter your destination and then click Ping.

10089.book  Page 306  Monday, July 23, 2007  3:17 PM




Checking Network Connectivity and Troubleshooting307Using the traceroute CommandTraceroute (the traceroute command, or trace for short) shows the path a packet takes to get to a remote device. It uses time to live (TTL) time-outs and ICMP error messages to outline the path a packet takes through an internetwork to arrive at remote host.Trace (the trace command), which can be used from either user mode or privileged mode, allows you to figure out which router in the path to an unreachable network host should be examined more closely for the cause of the network’s failure.To see the protocols that you can use with the traceroute command, type traceroute ?:

Corp#traceroute ?  WORD       Trace route to destination address or hostname  appletalk  AppleTalk Trace  clns       ISO CLNS Trace  ip         IP Trace  ipv6       IPv6 Trace  ipx        IPX Trace

  <cr>The trace command shows the hop or hops that a packet traverses on its way to a remote device. Here’s an example:

Corp#traceroute r1Type escape sequence to abort.Tracing the route to R1 (10.2.2.2)  1 R1 (10.2.2.2) 4 msec *  0 msec

Corp#You can see that the packet went through only one hop to find the destination.

Do not get confused! You can’t use the tracert command—it’s a Windows command. For a router, use the traceroute command!Here’s an example of using tracert from a Windows DOS prompt (notice the command tracert!):

C:\>tracert www.whitehouse.govTracing route to a1289.g.akamai.net [69.8.201.107]over a maximum of 30 hops:

10089.book  Page 307  Monday, July 23, 2007  3:17 PM




308Chapter5 Managing a Cisco Internetwork  1     *        *        *     Request timed out.  2    53 ms    61 ms    53 ms  hlrn-dsl-gw15-207.hlrn.qwest.net         [207.225.112.207]  3    53 ms    55 ms    54 ms  hlrn-agw1.inet.qwest.net [71.217.188.113]  4    54 ms    53 ms    54 ms  hlr-core-01.inet.qwest.net [205.171.253.97]  5    54 ms    53 ms    54 ms  apa-cntr-01.inet.qwest.net [205.171.253.26]  6    54 ms    53 ms    53 ms  63.150.160.34  7    54 ms    54 ms    53 ms  www.whitehouse.gov [69.8.201.107]

Trace complete.Okay, let’s move on now and talk about how to troubleshoot your network using the debug command.DebuggingDebug is a troubleshooting command that’s available from the privileged exec mode of Cisco IOS. It’s used to display information about various router operations and the related traffic generated or received by the router, plus any error messages.It’s a useful and informative tool, but you really need to understand some important facts about its use. Debug is regarded as a very high-priority task because it can consume a huge amount of resources and the router is forced to process-switch the packets being debugged. So you don’t just use debug as a monitoring tool—it’s meant to be used for a short period of time and only as a troubleshooting tool. By using it, you can really find out some truly significant facts about both working and faulty software and/or hardware components.Because debugging output takes priority over other network traffic, and because the debug all command generates more output than any other debug command, it can severely diminish the router’s performance—even render it unusable. So in virtually all cases, it’s best to use more-specific debug commands.As you can see from the following output, you can’t enable debugging from user mode, only privileged mode:

Corp>debug ?% Unrecognized commandCorp>en Corp#debug ?  aaa                     AAA Authentication, Authorization and Accounting  access-expression       Boolean access expression  adjacency               adjacency  all                     Enable all debugging

[output cut]

10089.book  Page 308  Monday, July 23, 2007  3:17 PM




Checking Network Connectivity and Troubleshooting309If you’ve got the freedom to pretty much take out a router and you really want to have some fun with debugging, use the debug all command:

Corp#debug allThis may severely impact network performance. Continue? (yes/[no]):yesAll possible debugging has been turned on2d20h: SNMP: HC Timer 824AE5CC fired2d20h: SNMP: HC Timer 824AE5CC rearmed, delay = 200002d20h: Serial0/0: HDLC myseq 4, mineseen 0, yourseen 0, line down2d20h:2d20h: Rudpv1 Sent: Pkts 0,  Data Bytes 0,  Data Pkts 02d20h: Rudpv1 Rcvd: Pkts 0,  Data Bytes 0,  Data Pkts 02d20h: Rudpv1 Discarded: 0,  Retransmitted 02d20h:2d20h: RIP-TIMER: periodic timer expired2d20h: Serial0/0: HDLC myseq 5, mineseen 0, yourseen 0, line down2d20h: Serial0/0: attempting to restart2d20h: PowerQUICC(0/0): DCD is up.2d20h: is_up: 0 state: 4 sub state: 1 line: 02d20h:2d20h: Rudpv1 Sent: Pkts 0,  Data Bytes 0,  Data Pkts 02d20h: Rudpv1 Rcvd: Pkts 0,  Data Bytes 0,  Data Pkts 02d20h: Rudpv1 Discarded: 0,  Retransmitted 02d20h: un allAll possible debugging has been turned off

Corp#To disable debugging on a router, just use the command no in front of the debug command:

Corp#no debug allBut I typically just use the undebug all command since it is so easy when using the shortcut:

Corp#un allRemember that instead of using the debug all command, it’s almost always better to use specific commands—and only for short periods of time. Here’s an example of deploying debug ip rip that will show you RIP updates being sent and received on a router:

Corp#debug ip ripRIP protocol debugging is on

10089.book  Page 309  Monday, July 23, 2007  3:17 PM




310Chapter5 Managing a Cisco InternetworkCorp#1w4d: RIP: sending v2 update to 224.0.0.9 via Serial0/0 (192.168.12.1)1w4d: RIP: build update entries1w4d:   10.10.10.0/24 via 0.0.0.0, metric 2, tag 01w4d:   171.16.125.0/24 via 0.0.0.0, metric 3, tag 01w4d:   172.16.12.0/24 via 0.0.0.0, metric 1, tag 01w4d:   172.16.125.0/24 via 0.0.0.0, metric 3, tag 01w4d: RIP: sending v2 update to 224.0.0.9 via Serial0/2 (172.16.12.1)1w4d: RIP: build update entries1w4d:   192.168.12.0/24 via 0.0.0.0, metric 1, tag 01w4d:   192.168.22.0/24 via 0.0.0.0, metric 2, tag 01w4d: RIP: received v2 update from 192.168.12.2 on Serial0/01w4d:    192.168.22.0/24 via 0.0.0.0 in 1 hops

Corp#un allI’m sure you can see that the debug command is one powerful command. And because of this, I’m also sure you realize that before you use any of the debugging commands, you should make sure you check the utilization of your router. This is important because in most cases, you don’t want to negatively impact the device’s ability to process the packets through on your internetwork. You can determine a specific router’s utilization information by using the show processes command.

Remember, when you telnet into a remote device, you will not see console messages by default! For example, you will not see debugging output. To allow console messages to be sent to your Telnet session, use the terminal monitor command.Using the show processes CommandAs mentioned in the previous section, you’ve really got to be careful when using the debug command on your devices. If your router’s CPU utilization is consistently at 50 percent or more, it’s probably not a good idea to type in the debug all command unless you want to see what a router looks like when it crashes!So what other approaches can you use? Well, the show processes (or show processes cpu) is a good tool for determining a given router’s CPU utilization. Plus, it’ll give you a list of active processes along with their corresponding process ID, priority, scheduler test (status), CPU time used, number of times invoked, and so on. Lots of great stuff! Plus, this command is super handy when you want to evaluate your router’s performance and CPU utilization—for instance, when you find yourself otherwise tempted to reach for the debug command.Okay—what do you see in the output below? The first line shows the CPU utilization out-put for the last 5 seconds, 1 minute, and 5 minutes. The output provides 2%/0% in front of 

10089.book  Page 310  Monday, July 23, 2007  3:17 PM




Exam Essentials311the CPU utilization for the last 5 seconds. The first number equals the total utilization and the second one delimits the utilization due to interrupt routines:

Corp#sh processesCPU utilization for five seconds: 2%/0%; one minute: 0%; five minutes: 0% PID QTy PC Runtime (ms)    Invoked   uSecs    Stacks TTY Process   1 Cwe 8034470C    0        1       0 5804/6000   0 Chunk Manager   2 Csp 80369A88    4       1856       2 2616/3000 0 Load Meter      3 M*         0    112     14    800010656/12000  0 Exec            5 Lst 8034FD9C  268246    52101 5148 5768/6000   0 Check heaps     6 Cwe 80355E5C  20         3    6666 5704/6000   0 Pool Manager    7 Mst 802AC3C4   0         2       0 5580/6000   0 Timers

[output cut]So basically, the output from the show processes command shows that our router is happily able to process debugging commands without being overloaded.SummaryIn this chapter, you learned how Cisco routers are configured and how to manage those configurations.This chapter covered the internal components of a router, which included ROM, RAM, NVRAM, and flash.In addition, I covered what happens when a router boots and which files are loaded. The configuration register tells the router how to boot and where to find files, and you learned how to change and verify the configuration register settings for password recovery purposes.Next, you learned how to back up and restore a Cisco IOS image, as well as how to back up and restore the configuration of a Cisco router. I showed you how to manage these files using the CLI, IFS, and SDM.Then you learned how to use CDP and Telnet to gather information about remote devices. Finally, the chapter covered how to resolve hostnames and use the ping and trace commands to test network connectivity, as well as how to use the debug and show processes commands.Exam EssentialsRemember the various configuration register commands and settings.The 0x2102 setting is the default on all Cisco routers and tells the router to look in NVRAM for the boot sequence. 0x2101 tells the router to boot from ROM, and 0x2142 tells the router to not load the startup-config in NVRAM to provide password recovery.

10089.book  Page 311  Monday, July 23, 2007  3:17 PM




312Chapter5 Managing a Cisco InternetworkRemember how to back up an IOS image.By using the privileged-mode command copy flash tftp, you can back up a file from flash memory to a TFTP (network) server.Remember how to restore or upgrade an IOS image.By using the privileged-mode com-mand copy tftp flash, you can restore or upgrade a file from a TFTP (network) server to flash memory.Remember what you must complete before you back up an IOS image to a network server.Make sure that you can access the network server, ensure that the network server has adequate space for the code image, and verify the file naming and path requirement.Remember how to save the configuration of a router.There are a couple of ways to do this, but the most common, as well as most tested, method is copy running-config startup-config.Remember how to erase the configuration of a router.Type the privileged-mode command erase startup-config and reload the router.Understand when to use CDP.Cisco Discovery Protocol can be used to help you document as well as troubleshoot your network.Remember what the output from the show cdp neighbors command shows.The show cdp neighbors command provides the following information: device ID, local interface, holdtime, capability, platform, and port ID (remote interface).Understand how to telnet into a router and keep your connection but return to your originating console.If you telnet to a router or switch, you can end the connection by typing exit at any time. However, if you want to keep your connection to a remote device but still come back to your original router console, you can press the Ctrl+Shift+6 key combination, release it, and then press X.Remember the command to verify your Telnet sessions.The command show sessions will provide you with information about all the sessions your router has with other routers.Remember how to build a static host table on a router.By using the global configuration command ip host host_name ip_address, you can build a static host table on your router. You can apply multiple IP addresses against the same host entry.Remember how to verify your host table on a router.You can verify the host table with the show hosts command.Understand when to use the ping command.Packet Internet Groper (Ping) uses ICMP echo request and ICMP echo replies to verify an active IP address on a network.Remember how to ping a valid host ID.You can ping an IP address from a router’s user mode or privileged mode but not from configuration mode. You must ping a valid address, such as 1.1.1.1.

10089.book  Page 312  Monday, July 23, 2007  3:17 PM




Hands-on Labs313Written Lab 5Write the answers to the following questions:1.What is the command to copy a Cisco IOS to a TFTP server?2.What is the command to copy a Cisco startup-config file to a TFTP server?3.What is the command to copy the startup-config file to DRAM?4.What is an older command that you can use to copy the startup-config file to DRAM?5.What command can you use to see the neighbor router’s IP address from your router prompt?6.What command can you use to see the hostname, local interface, platform, and remote port of a neighbor router?7.What keystrokes can you use to telnet into multiple devices simultaneously?8.What command will show you your active Telnet connections to neighbor and remote devices?9.What command can you use to upgrade a Cisco IOS?10.What command can you use to merge a backup configuration with the configuration in RAM?(The answers to Written Lab 5 can be found following the answers to the review questions for this chapter.)Hands-on LabsTo complete the labs in this section, you need at least one router (three would be best) and at least one PC running as a TFTP server. Remember that the labs listed here were created for use with real routers.

You can always run the first two labs by using the SDM demo (mostly) and skip the TFTP host sections of these labs; however, you need to know how to use both methods.Here is a list of the labs in this chapter:Lab 5.1: Backing Up Your Router IOSLab 5.2: Upgrading or Restoring Your Router IOSLab 5.3: Backing Up the Router ConfigurationLab 5.4: Using the Cisco Discovery Protocol (CDP)Lab 5.5: Using TelnetLab 5.6: Resolving Hostnames

10089.book  Page 313  Monday, July 23, 2007  3:17 PM




314Chapter5 Managing a Cisco InternetworkHands-on Lab 5.1: Backing Up Your Router IOS1.Log into your router and go into privileged mode by typing en or enable.2.Make sure you can connect to the TFTP server that is on your network by pinging the IP address from the router console.3.Type show flash to see the contents of flash memory.4.Type show version at the router privileged-mode prompt to get the name of the IOS cur-rently running on the router. If there is only one file in flash memory, the show flash and show version commands show the same file. Remember that the show version com-mand shows you the file that is currently running and the show flash command shows you all of the files in flash memory.5.Once you know you have good Ethernet connectivity to the TFTP server and you also know the IOS filename, back up your IOS by typing copy flash tftp. This command tells the router to copy the contents of flash memory (this is where the IOS is stored by default) to a TFTP server.6.Enter the IP address of the TFTP server and the source IOS filename. The file is now copied and stored in the TFTP server’s default directory.Hands-on Lab 5.2: Upgrading or Restoring Your Router IOS1.Log into your router and go into privileged mode by typing en or enable.2.Make sure you can connect to the TFTP server by pinging the IP address of the server from the router console.3.Once you know you have good Ethernet connectivity to the TFTP server, issue the copy tftp flash command.4.Confirm that the router is not functioning during the restore or upgrade by following the prompts provided on the router console.5.Enter the IP address of the TFTP server.6.Enter the name of the IOS filename you want to restore or upgrade.7.Confirm that you understand that the contents of flash memory will be erased.8.Watch in amazement as your IOS is deleted out of flash memory and your new IOS is cop-ied to flash memory.If the file that was in flash memory is deleted but the new version wasn’t copied to flash memory, the router will boot from ROM monitor mode. You’ll need to figure out why the copy operation did not take place.Hands-on Lab 5.3: Backing Up the Router Configuration1.Log into your router and go into privileged mode by typing en or enable.2.Ping the TFTP server to make sure you have IP connectivity.3.From RouterB, type copy run tftp.

10089.book  Page 314  Monday, July 23, 2007  3:17 PM




Hands-on Labs3154.Type the IP address of the TFTP server (for example, 172.16.30.2) and press Enter.5.The router will prompt you for a filename. The hostname of the router is followed by the suffix -confg (yes, I spelled that correctly). You can use any name you want.

Name of configuration file to write [RouterB-confg]?Press Enter to accept the default name.

Write file RouterB-confg on host 172.16.30.2? [confirm]Press Enter.Hands-on Lab 5.4: Using the Cisco Discovery Protocol (CDP)1.Log into your router and go into privileged mode by typing en or enable.2.From the router, type sh cdp and press Enter. You should see that CDP packets are being sent out to all active interfaces every 60 seconds and the holdtime is 180 seconds (these are the defaults).3.To change the CDP update frequency to 90 seconds, type cdp timer 90 in global con-figuration mode.RouterC#config tEnter configuration commands, one per line.  End with  CNTL/Z.RouterC(config)#cdp timer ?  <5-900>  Rate at which CDP packets are sent (in sec)

RouterC(config)#cdp timer 904.Verify that your CDP timer frequency has changed by using the command show cdp in privileged mode.RouterC#sh cdpGlobal CDP information:Sending CDP packets every 90 seconds

Sending a holdtime value of 180 seconds5.Now use CDP to gather information about neighbor routers. You can get the list of avail-able commands by typing sh cdp ?.RouterC#sh cdp ?  entry     Information for specific neighbor entry  interface CDP interface status and configuration  neighbors CDP neighbor entries  traffic   CDP statistics

  <cr>

10089.book  Page 315  Monday, July 23, 2007  3:17 PM




316Chapter5 Managing a Cisco Internetwork6.Type sh cdp int to see the interface information plus the default encapsulation used by the interface. It also shows the CDP timer information.7.Type sh cdp entry * to see the CDP information received from all devices.8.Type show cdp neighbors to gather information about all connected neighbors. (You should know the specific information output by this command.)9.Type show cdp neighbors detail. Notice that it produces the same output as show cdp entry *.Hands-on Lab 5.5: Using Telnet1.Log into your router and go into privileged mode by typing en or enable.2.From RouterA, telnet into your remote router by typing telnet ip_address from the command prompt.3.Type in RouterB’s IP address from RouterA’s command prompt. Notice that the router automatically tries to telnet to the IP address you specified. You can use the telnet com-mand or just type in the IP address.4.From RouterB, press Ctrl+Shift+6 and then X to return to RouterA’s command prompt. Now telnet into your third router, RouterC. Press Ctrl+Shift+6 and then X to return to RouterA.5.From RouterA, type show sessions. Notice your two sessions. You can press the num-ber displayed to the left of the session and press Enter twice to return to that session. The asterisk shows the default session. You can press Enter twice to return to that session.6.Go to the session for your RouterB. Type show users. This shows the console connection and the remote connection. You can use the disconnect command to clear the session or just type exit from the prompt to close your session with RouterB.7.Go to the RouterC’s console port by typing show sessions on the first router and using the connection number to return to RouterC. Type show user and notice the connection to your first router, RouterA.8.Type clear line to disconnect the Telnet session.Hands-on Lab 5.6: Resolving Hostnames1.Log into your router and go into privileged mode by typing en or enable.2.From RouterA, type todd and press Enter at the command prompt. Notice the error you receive and the delay. The router is trying to resolve the hostname to an IP address by looking for a DNS server. You can turn this feature off by using the no ip domain-lookup command from global configuration mode.3.To build a host table, you use the ip host command. From RouterA, add a host table entry for RouterB and RouterC by entering the following commands:ip host routerb ip_address

ip host routerc ip_address

10089.book  Page 316  Monday, July 23, 2007  3:17 PM




Hands-on Labs317Here is an example:ip host routerb 172.16.20.2

ip host routerc 172.16.40.24.Test your host table by typing ping routerb from the command prompt (not the config prompt).RouterA#ping routerbType escape sequence to abort.Sending 5, 100-byte ICMP Echos to 172.16.20.2, timeout  is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip

  min/avg/max = 4/4/4 ms5.Test your host table by typing ping routerc.RouterA#ping routercType escape sequence to abort.Sending 5, 100-byte ICMP Echos to 172.16.40.2, timeout  is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip

  min/avg/max = 4/6/8 ms6.Keep your session to RouterB open, and then return to RouterA by pressing Ctrl+Shift+6, then X.7.Telnet to RouterC by typing routerc at the command prompt.8.Return to RouterA and keep the session to RouterC open by pressing Ctrl+Shift+6, then X.9.View the host table by typing show hosts and pressing Enter.Default domain is not setName/address lookup uses domain serviceName servers are 255.255.255.255Host                 Flags      Age Type   Address(es)routerb             (perm, OK)  0   IP    172.16.20.2

routerc             (perm, OK)  0   IP    172.16.40.2

10089.book  Page 317  Monday, July 23, 2007  3:17 PM




318Chapter5 Managing a Cisco InternetworkReview Questions

The following questions are designed to test your understanding of this chapter’s material. For more information on how to get additional questions, please see this book’s Introduction.1.What does the command o/r 0x2142 provide?A.It is used to restart the router.B.It is used to bypass the configuration in NVRAM.C.It is used to enter ROM Monitor mode.D.It is used to view the lost password.2.Which command will copy the IOS to a backup host on your network?A.transfer IOS to 172.16.10.1B.copy run startC.copy tftp flashD.copy start tftpE.copy flash tftp3.You are troubleshooting a connectivity problem in your corporate network and want to isolate the problem. You suspect that a router on the route to an unreachable network is at fault. What IOS user exec command should you issue?A.Router>pingB.Router>traceC.Router>show ip routeD.Router>show interfaceE.Router>show cdp neighbors4.You copy a configuration from a network host to a router’s RAM. The configuration looks correct, yet it is not working at all. What could the problem be?A.You copied the wrong configuration into RAM.B.You copied the configuration into flash memory instead.C.The copy did not override the shutdown command in running-config.D.The IOS became corrupted after the copy command was initiated.

10089.book  Page 318  Monday, July 23, 2007  3:17 PM




Review Questions3195.A network administrator wants to upgrade the IOS of a router without removing the image currently installed. What command will display the amount of memory consumed by the cur-rent IOS image and indicate whether there is enough room available to hold both the current and new images?A.show versionB.show flashC.show memoryD.show buffersE.show running-config6.The corporate office sends you a new router to connect, but upon connecting the console cable, you see that there is already a configuration on the router. What should be done before a new configuration is entered in the router?A.RAM should be erased and the router restarted.B.Flash should be erased and the router restarted.C.NVRAM should be erased and the router restarted.D.The new configuration should be entered and saved.7.Which command loads a new version of the Cisco IOS into a router?A.copy flash ftpB.copy ftp flashC.copy flash tftpD.copy tftp flash8.Which command will show you the IOS version running on your router?A.sh IOSB.sh flashC.sh versionD.sh running-config9.What should the configuration register value be after you successfully complete the password recovery procedure and return the router to normal operation?A.0x2100B.0x2101C.0x2102D.0x2142

10089.book  Page 319  Monday, July 23, 2007  3:17 PM




320Chapter5 Managing a Cisco Internetwork10.You save the configuration on a router with the copy running-config startup-config command and reboot the router. The router, however, comes up with a blank configuration. What can the problem be?A.You didn’t boot the router with the correct command.B.NVRAM is corrupted.C.The configuration register setting is incorrect.D.The newly upgraded IOS is not compatible with the hardware of the router.E.The configuration you save is not compatible with the hardware.11.If you want to have more than one Telnet session open at the same time, what keystroke com-bination would you use?A.Tab+spacebarB.Ctrl+X, then 6C.Ctrl+Shift+X, then 6D.Ctrl+Shift+6, then X12.You are unsuccessful in telnetting into a remote device. What could the problem be? (Choose two.)A.IP addresses are incorrect.B.Access control list is filtering Telnet.C.There is a defective serial cable.D.The VTY password is missing.13.What information is displayed by the show hosts command? (Choose two.)A.Temporary DNS entriesB.The names of the routers created using the hostname commandC.The IP addresses of workstations allowed to access the routerD.Permanent name-to-address mappings created using the ip host commandE.The length of time a host has been connected to the router via Telnet14.Which three commands can be used to check LAN connectivity problems on a router? (Choose three.)A.show interfacesB.show ip routeC.tracertD.pingE.dns lookups

10089.book  Page 320  Monday, July 23, 2007  3:17 PM




Review Questions32115.You telnet to a router and make your necessary changes; now you want to end the Telnet session. What command do you type in?A.closeB.disableC.disconnectD.exit16.You telnet into a remote device and type debug ip rip, but no output from the debug command is seen. What could the problem be?A.You must type the show ip rip command first.B.IP addressing on the network is incorrect.C.You must use the terminal monitor command.D.Debug output is sent only to the console.17.Which command displays the configuration register setting?A.show ip routeB.show boot versionC.show versionD.show flash18.You need to gather the IP address of a remote switch that is located in Hawaii. What can you do to find the address?A.Fly to Hawaii, console into the switch, then relax and have a drink with an umbrella in it.B.Issue the show ip route command on the router connected to the switch.C.Issue the show cdp neighbor command on the router connected to the switch.D.Issue the show ip arp command on the router connected to the switch.E.Issue the show cdp neighbors detail command on the router connected to the switch.19.You have your laptop directly connected into a router’s Ethernet port. Which of the following are among the requirements for the copy flash tftp command to be successful? (Choose three.)A.TFTP server software must be running on the router.B.TFTP server software must be running on your laptop.C.The Ethernet cable connecting the laptop directly into the router’s Ethernet port must be a straight-through cable.D.The laptop must be on the same subnet as the router’s Ethernet interface.E.The copy flash tftp command must be supplied the IP address of the laptop.F.There must be enough room in the flash memory of the router to accommodate the file to be copied.

10089.book  Page 321  Monday, July 23, 2007  3:17 PM




322Chapter5 Managing a Cisco Internetwork20.The configuration register setting of 0x2102 provides what function to a router?A.Tells the router to boot into ROM monitor modeB.Provides password recoveryC.Tells the router to look in NVRAM for the boot sequenceD.Boots the IOS from a TFTP serverE.Boots an IOS image stored in ROM

10089.book  Page 322  Monday, July 23, 2007  3:17 PM




Answers to Review Questions323Answers to Review Questions1.B. The default configuration setting is 0x2102, which tells the router to load the IOS from flash and the configuration from NVRAM. 0x2142 tells the router to bypass the configuration in NVRAM so that you can perform password recovery.2.E. To copy the IOS to a backup host, which is stored in flash memory by default, use the copy flash tftp command.3.B. The command traceroute (trace for short), which can be issued from user mode or priv-ileged mode, is used to find the path a packet takes through an internetwork and will also show you where the packet stops because of an error on a router.4.C. Since the configuration looks correct, you probably didn’t screw up the copy job. However, when you perform a copy from a network host to a router, the interfaces are automatically shut down and need to be manually enabled with the no shutdown command.5.B. The show flash command will provide you with the current IOS name and size and the size of flash memory.6.C. Before you start to configure the router, you should erase the NVRAM with the erase startup-config command and then reload the router using the reload command.7.D. The command copy tftp flash will allow you to copy a new IOS into flash memory on your router.8.C. The best answer is show version, which shows you the IOS file running currently on your router. The show flash command shows you the contents of flash memory, not which file is running.9.C. All Cisco routers have a default configuration register setting of 0x2102, which tells the router to load the IOS from flash memory and the configuration from NVRAM.10.C. If you save a configuration and reload the router and it comes up either in setup mode or as a blank configuration, chances are you have the configuration register setting incorrect.11.D. To keep open one or more Telnet sessions, use the Ctrl+Shift+6 and then X keystroke combination.12.B, D. The best answers, the ones you need to remember, are that either an access control list is filtering the Telnet session or the VTY password is not set on the remote device.13.A, D. The show hosts command provides information on temporary DNS entries and permanent name-to-address mappings created using the ip host command.14.A, B, D. The tracert command is a Windows command and will not work on a router! A router uses the traceroute command.15.D. Since the question never mentioned anything about a suspended session, you can assume that the Telnet session is still open, and you would just type exit to close the session.

10089.book  Page 323  Monday, July 23, 2007  3:17 PM




324Chapter5 Managing a Cisco Internetwork16.C. To see console messages through your Telnet session, you must enter the terminal monitor command.17.C. The show version command provides you with the current configuration register setting.18.E. Although answer A is certainly the “best” answer, unfortunately answer E will work just fine and your boss would probably prefer you to use the show cdp neighbors detail command.19.B, D, E. Before you back up an IOS image to a laptop directly connected to a router’s Ethernet port, make sure the TFTP server software is running on your laptop, that the Ethernet cable is a “crossover,” and that the laptop is in the same subnet as the router’s Ethernet port, and then you can use the copy flash tftp command from your laptop.20.C. The default configuration setting of 0x2102 tells the router to look in NVRAM for the boot sequence.

10089.book  Page 324  Monday, July 23, 2007  3:17 PM




Answers to Written Lab 5325Answers to Written Lab 51.copy flash tftp2.copy start tftp3.copy start run4.config mem5.show cdp neighbor detail or show cdp entry *6.show cdp neighbor7.Ctrl+Shift+6, then X8.show sessions9.copy tftp flash10.Either copy tftp run or copy start run

10089.book  Page 325  Monday, July 23, 2007  3:17 PM




10089.book  Page 326  Monday, July 23, 2007  3:17 PM




 

Chapter 6 IP Routing

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Describe how a network works   Determine the path between two hosts across a network   Configure, verify, and troubleshoot basic router operation and routing on Cisco devices  Describe basic routing concepts (including: packet forwarding, router lookup process)   Configure, verify, and troubleshoot RIPv2   Access and utilize the router to set basic parameters (including: CLI/SDM)   Connect, configure, and verify operation status of a device interface   Verify device configuration and network connectivity using ping, traceroute, telnet, SSH or other utilities   Perform and verify routing configuration tasks for a static or default route given specific routing requirements   Compare and contrast methods of routing and routing protocols   Verify network connectivity (including: using ping, traceroute, and telnet or SSH)   Troubleshoot routing issues   Verify router hardware and software operation using SHOW & DEBUG commands    Implement basic router security 

 

10089c06.fm  Page 327  Friday, November 7, 2008  10:52 PM




 In this chapter, I’m going to discuss the IP routing process. This is an important subject to understand since it pertains to all routers and configurations that use IP. IP routing is the process of moving packets from one network to another network using routers. And as before, by routers I mean Cisco routers, of course!But before you read this chapter, you must understand the difference between a routing protocol and a routed protocol. A  routing protocol  is used by routers to dynamically find all the networks in the internetwork and to ensure that all routers have the same routing table. Basically, a routing protocol determines the path of a packet through an internetwork. Examples of routing protocols are RIP, RIPv2, EIGRP, and OSPF.Once all routers know about all networks, a  routed protocol  can be used to send user data (packets) through the established enterprise. Routed protocols are assigned to an interface and determine the method of packet delivery. Examples of routed protocols are IP and IPv6.I’m pretty sure that I don’t have to tell you that this is definitely important stuff to know. You most likely understand that from what I’ve said so far. IP routing is basically what Cisco routers do, and they do it very well. Again, this chapter is dealing with truly fundamental material—these are things you must know if you want to understand the objectives covered in this book!In this chapter, I’m going to show you how to configure and verify IP routing with Cisco routers. I’ll be covering the following:  Routing basics  The IP routing process  Static routing  Default routing  Dynamic routingIn Chapter 7, “Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF),” I’ll be moving into more advanced, dynamic routing with EIGRP and OSPF. But first, you’ve really got to nail down the basics of how packets actually move through an internetwork, so let’s get started!

 For up-to-the minute updates for this chapter, please see  www.lammle.com   and/or  www.sybex.com .

 

10089c06.fm  Page 328  Thursday, August 30, 2007  12:42 PM




 Routing Basics 329 Routing Basics Once you create an internetwork by connecting your WANs and LANs to a router, you’ll need to configure logical network addresses, such as IP addresses, to all hosts on the internetwork so that they can communicate across that internetwork.The term  routing  is used for taking a packet from one device and sending it through the network to another device on a different network. Routers don’t really care about hosts—they only care about networks and the best path to each network. The logical network address of the destination host is used to get packets to a network through a routed net-work, and then the hardware address of the host is used to deliver the packet from a router to the correct destination host.If your network has no routers, then it should be apparent that you are not routing. Routers route traffic to all the networks in your internetwork. To be able to route packets, a router must know, at a minimum, the following:  Destination address  Neighbor routers from which it can learn about remote networks  Possible routes to all remote networks  The best route to each remote network  How to maintain and verify routing informationThe router learns about remote networks from neighbor routers or from an administrator. The router then builds a routing table (a map of the internetwork) that describes how to find the remote networks. If a network is directly connected, then the router already knows how to get to it.If a network isn’t directly connected to the router, the router must use one of two ways to learn how to get to the remote network: static routing, meaning that someone must hand-type all network locations into the routing table, or something called dynamic routing.In  dynamic routing , a protocol on one router communicates with the same protocol running on neighbor routers. The routers then update each other about all the networks they know about and place this information into the routing table. If a change occurs in the network, the dynamic routing protocols automatically inform all routers about the event. If  static routing  is used, the administrator is responsible for updating all changes by hand into all routers. Typically, in a large network, a combination of both dynamic and static routing is used.Before we jump into the IP routing process, let’s take a look at a simple example that dem-onstrates how a router uses the routing table to route packets out of an interface. We’ll be going into a more detailed study of the process in the next section.Figure 6.1 shows a simple two-router network. Lab_A has one serial interface and three LAN interfaces.Looking at Figure 6.1, can you see which interface Lab_A will use to forward an IP data-gram to a host with an IP address of 10.10.10.10?

 

10089c06.fm  Page 329  Thursday, August 30, 2007  12:42 PM




 330 Chapter6  IP Routing FIGURE6.1 A simple routing example By using the command  show ip route , we can see the routing table (map of the internet-work) that Lab_A uses to make forwarding decisions:

 Lab_A# sh ip route [output cut]Gateway of last resort is not setC      10.10.10.0/24 is directly connected, FastEthernet0/0C      10.10.20.0/24 is directly connected, FastEthernet0/1C      10.10.30.0/24 is directly connected, FastEthernet0/2

 C      10.10.40.0/24 is directly connected, Serial 0/0 The  C  in the routing table output means that the networks listed are “directly connected,” and until we add a routing protocol—something like RIP, EIGRP, etc.—to the routers in our inter-network (or use static routes), we’ll have only directly connected networks in our routing table.So let’s get back to the original question: By looking at the figure and the output of the routing table, can you tell what IP will do with a received packet that has a destination IP address of 10.10.10.10? The router will packet-switch the packet to interface FastEthernet 0/0, and this interface will frame the packet and then send it out on the network segment.Because we can, let’s do another example: Based on the output of the next routing table, which interface will a packet with a destination address of 10.10.10.14 be forwarded from?

 Lab_A# sh ip route [output cut]Gateway of last resort is not setC      10.10.10.16/28 is directly connected, FastEthernet0/0

S0/010.10.40.1/24Fa0/110.10.20.1/24Fa0/010.10.10.1/24Fa0/210.10.30.1/24Lab_A

 

10089c06.fm  Page 330  Thursday, August 30, 2007  12:42 PM




 The IP Routing Process 331 C      10.10.10.8/29 is directly connected, FastEthernet0/1C      10.10.10.4/30 is directly connected, FastEthernet0/2

 C      10.10.10.0/30 is directly connected, Serial 0/0 First, you can see that the network is subnetted and each interface has a different mask. And I have to tell you—you just can’t answer this question if you can’t subnet! 10.10.10.14 would be a host in the 10.10.10.8/29 subnet connected to the FastEthernet0/1 interface. Don’t freak out if you don’t get it. Just go back and reread Chapter 3 if you’re struggling, and this should make perfect sense to you afterward.For everyone who’s ready to move on, let’s get into this process in more detail. The IP Routing Process The IP routing process is fairly simple and doesn’t change, regardless of the size of your net-work. For an example, we’ll use Figure 6.2 to describe step-by-step what happens when Host_A wants to communicate with Host_B on a different network. FIGURE6.2 IP routing example using two hosts and one router In this example, a user on Host_A pings Host_B’s IP address. Routing doesn’t get simpler than this, but it still involves a lot of steps. Let’s work through them: 1. Internet Control Message Protocol (ICMP) creates an echo request payload (which is just the alphabet in the data field). 2. ICMP hands that payload to Internet Protocol (IP), which then creates a packet. At a min-imum, this packet contains an IP source address, an IP destination address, and a Protocol field with 01h. (Remember that Cisco likes to use  0x  in front of hex characters, so this could look like 0x01.) All of that tells the receiving host whom it should hand the payload to when the destination is reached—in this example, ICMP. 3. Once the packet is created, IP determines whether the destination IP address is on the local network or a remote one. 4. Since IP determines that this is a remote request, the packet needs to be sent to the default gateway so the packet can be routed to the remote network. The Registry in Windows is parsed to find the configured default gateway. 5. The default gateway of host 172.16.10.2 (Host_A) is configured to 172.16.10.1. For this packet to be sent to the default gateway, the hardware address of the router’s interface 

Lab_AHost_A172.16.10.2

Host_B172.16.20.2E0172.16.10.1E1172.16.20.1

 

10089c06.fm  Page 331  Thursday, August 30, 2007  12:42 PM




 332 Chapter6  IP Routing Ethernet 0 (configured with the IP address of 172.16.10.1) must be known. Why? So the packet can be handed down to the Data Link layer, framed, and sent to the router’s inter-face that’s connected to the 172.16.10.0 network. Because hosts only communicate via hardware addresses on the local LAN, it’s important to recognize that for Host_A to com-municate to Host_B, it has to send packets to the Media Access Control (MAC) address of the default gateway on the local network.

 MAC addresses are always local on the LAN and never go through and past  a router. 6. Next, the Address Resolution Protocol (ARP) cache of the host is checked to see if the IP address of the default gateway has already been resolved to a hardware address:  If it has, the packet is then free to be handed to the Data Link layer for framing. (The hardware destination address is also handed down with that packet.) To view the ARP cache on your host, use the following command: C:\> arp -a Interface: 172.16.10.2 --- 0x3  Internet Address      Physical Address      Type

   172.16.10.1          00-15-05-06-31-b0     dynamic  If the hardware address isn’t already in the ARP cache of the host, an ARP broadcast is sent out onto the local network to search for the hardware address of 172.16.10.1. The router responds to the request and provides the hardware address of Ethernet 0, and the host caches this address. 7. Once the packet and destination hardware address are handed to the Data Link layer, the LAN driver is used to provide media access via the type of LAN being used (in this example, Ethernet). A frame is then generated, encapsulating the packet with control information. Within that frame are the hardware destination and source addresses plus, in this case, an Ether-Type field that describes the Network layer protocol that handed the packet to the Data Link layer—in this instance, IP. At the end of the frame is something called a Frame Check Sequence (FCS) field that houses the result of the cyclic redundancy check (CRC). The frame would look something like what I’ve detailed in Figure 6.3. It contains Host_A’s hardware (MAC) address and the destination hardware address of the default gateway. It does not include the remote host’s MAC address—remember that! FIGURE6.3 Frame used from Host_A to the Lab_A router when Host_B is pinged

Destination MAC (routers E0 MAC address) Source MAC (Host_A MAC address) Ether-Type field FCS (CRC) Packet 

 

10089c06.fm  Page 332  Thursday, August 30, 2007  12:42 PM




 The IP Routing Process 333 8. Once the frame is completed, it’s handed down to the Physical layer to be put on the physical medium (in this example, twisted-pair wire) one bit at a time. 9. Every device in the collision domain receives these bits and builds the frame. They each run a CRC and check the answer in the FCS field. If the answers don’t match, the frame is discarded.  If the CRC matches, then the hardware destination address is checked to see if it matches too (which, in this example, is the router’s interface Ethernet 0).  If it’s a match, then the Ether-Type field is checked to find the protocol used at the Network layer. 10. The packet is pulled from the frame, and what is left of the frame is discarded. The packet is handed to the protocol listed in the Ether-Type field—it’s given to IP. 11. IP receives the packet and checks the IP destination address. Since the packet’s destination address doesn’t match any of the addresses configured on the receiving router itself, the router will look up the destination IP network address in its routing table. 12. The routing table must have an entry for the network 172.16.20.0 or the packet will be discarded immediately and an ICMP message will be sent back to the originating device with a destination network unreachable message. 13. If the router does find an entry for the destination network in its table, the packet is switched to the exit interface—in this example, interface Ethernet 1. The output below displays the Lab_A router’s routing table. The  C  means “directly connected.” No routing protocols are needed in this network since all networks (all two of them) are directly connected. Lab_A> sh ip route Codes:C - connected,S - static,I - IGRP,R - RIP,M - mobile,B –   BGP, D - EIGRP,EX - EIGRP external,O - OSPF,IA - OSPF inter   area, N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external   type 2, E1 - OSPF external type 1, E2 - OSPF external type 2,   E – EGP,i - IS-IS, L1 - IS-IS level-1, L2 - IS-IS level-2, ia   - IS-IS intearea * - candidate default, U - per-user static   route, o – ODR P - periodic downloaded static routeGateway of last resort is not set     172.16.0.0/24 is subnetted, 2 subnetsC       172.16.10.0 is directly connected, Ethernet0

 C       172.16.20.0 is directly connected, Ethernet1 14. The router packet-switches the packet to the Ethernet 1 buffer.

 

10089c06.fm  Page 333  Thursday, August 30, 2007  12:42 PM




 334 Chapter6  IP Routing 15.The Ethernet 1 buffer needs to know the hardware address of the destination host and first checks the ARP cache. If the hardware address of Host_B has already been resolved and is in the router’s ARP cache, then the packet and the hardware address are handed down to the Data Link layer to be framed. Let’s take a look at the ARP cache on the Lab_A router by using the show ip arp command:    Lab_A#sh ip arp    Protocol  Address     Age(min) Hardware Addr  Type   Interface    Internet  172.16.20.1   -     00d0.58ad.05f4  ARPA   Ethernet0    Internet  172.16.20.2   3     0030.9492.a5dd  ARPA   Ethernet0    Internet  172.16.10.1   -     00d0.58ad.06aa  ARPA   Ethernet0

    Internet  172.16.10.2  12     0030.9492.a4ac  ARPA   Ethernet0The dash (-) means that this is the physical interface on the router. From the output above, we can see that the router knows the 172.16.10.2 (Host_A) and 172.16.20.2 (Host_B) hardware addresses. Cisco routers will keep an entry in the ARP table for 4 hours. If the hardware address has not already been resolved, the router sends an ARP request out E1 looking for the hardware address of 172.16.20.2. Host_B responds with its hardware address, and the packet and destination hardware address are both sent to the Data Link layer for framing.16.The Data Link layer creates a frame with the destination and source hardware address, Ether-Type field, and FCS field at the end. The frame is handed to the Physical layer to be sent out on the physical medium one bit at a time.17.Host_B receives the frame and immediately runs a CRC. If the result matches what’s in the FCS field, the hardware destination address is then checked. If the host finds a match, the Ether-Type field is then checked to determine the protocol that the packet should be handed to at the Network layer—IP in this example.18.At the Network layer, IP receives the packet and checks the IP destination address. Since there’s finally a match made, the Protocol field is checked to find out whom the payload should be given to.19.The payload is handed to ICMP, which understands that this is an echo request. ICMP responds to this by immediately discarding the packet and generating a new payload as an echo reply.20.A packet is then created including the source and destination addresses, Protocol field, and payload. The destination device is now Host_A.21.IP then checks to see whether the destination IP address is a device on the local LAN or on a remote network. Since the destination device is on a remote network, the packet needs to be sent to the default gateway.22.The default gateway IP address is found in the Registry of the Windows device, and the ARP cache is checked to see if the hardware address has already been resolved from an IP address.23.Once the hardware address of the default gateway is found, the packet and destination hardware addresses are handed down to the Data Link layer for framing.

10089c06.fm  Page 334  Thursday, August 30, 2007  12:42 PM




The IP Routing Process33524.The Data Link layer frames the packet of information and includes the following in the header: The destination and source hardware addresses The Ether-Type field with 0x0800 (IP) in it The FCS field with the CRC result in tow25.The frame is now handed down to the Physical layer to be sent out over the network medium one bit at a time.26.The router’s Ethernet 1 interface receives the bits and builds a frame. The CRC is run, and the FCS field is checked to make sure the answers match.27.Once the CRC is found to be okay, the hardware destination address is checked. Since the router’s interface is a match, the packet is pulled from the frame and the Ether-Type field is checked to see what protocol at the Network layer the packet should be delivered to.28.The protocol is determined to be IP, so it gets the packet. IP runs a CRC check on the IP header first and then checks the destination IP address.

IP does not run a complete CRC as the Data Link layer does—it only checks the header for errors.Since the IP destination address doesn’t match any of the router’s interfaces, the routing table is checked to see whether it has a route to 172.16.10.0. If it doesn’t have a route over to the destination network, the packet will be discarded immediately. (This is the source point of confusion for a lot of administrators—when a ping fails, most people think the packet never reached the destination host. But as we see here, that’s not always the case. All it takes is for just one of the remote routers to be lacking a route back to the originating host’s network and—poof!—the packet is dropped on the return trip, not on its way to the host.)

Just a quick note to mention that when (if) the packet is lost on the way back to the originating host, you will typically see a “request timed out” message because it is an unknown error. If the error occurs because of a known issue, such as if a route is not in the routing table on the way to the destination device, you will see a destination unreachable message. This should help you determine if the problem occurred on the way to the destination or on the way back.29.In this case, the router does know how to get to network 172.16.10.0—the exit interface is Ethernet 0—so the packet is switched to interface Ethernet 0.30.The router checks the ARP cache to determine whether the hardware address for 172.16.10.2 has already been resolved.

10089c06.fm  Page 335  Thursday, August 30, 2007  12:42 PM




336Chapter6 IP Routing31.Since the hardware address to 172.16.10.2 is already cached from the originating trip to Host_B, the hardware address and packet are handed to the Data Link layer.32.The Data Link layer builds a frame with the destination hardware address and source hardware address and then puts IP in the Ether-Type field. A CRC is run on the frame and the result is placed in the FCS field.33.The frame is then handed to the Physical layer to be sent out onto the local network one bit at a time.34.The destination host receives the frame, runs a CRC, checks the destination hardware address, and looks in the Ether-Type field to find out whom to hand the packet to.35.IP is the designated receiver, and after the packet is handed to IP at the Network layer, it checks the protocol field for further direction. IP finds instructions to give the payload to ICMP, and ICMP determines the packet to be an ICMP echo reply.36.ICMP acknowledges that it has received the reply by sending an exclamation point (!) to the user interface. ICMP then attempts to send four more echo requests to the destination host.You’ve just experienced Todd’s 36 easy steps to understanding IP routing. The key point to understand here is that if you had a much larger network, the process would be the same. In a really big internetwork, the packet just goes through more hops before it finds the desti-nation host.It’s super-important to remember that when Host_A sends a packet to Host_B, the desti-nation hardware address used is the default gateway’s Ethernet interface. Why? Because frames can’t be placed on remote networks—only local networks. So packets destined for remote networks must go through the default gateway.Let’s take a look at Host_A’s ARP cache now:

C:\ >arp -aInterface: 172.16.10.2 --- 0x3  Internet Address      Physical Address      Type  172.16.10.1           00-15-05-06-31-b0     dynamic

  172.16.20.1           00-15-05-06-31-b0     dynamicDid you notice that the hardware (MAC) address that Host_A uses to get to Host_B is the Lab_A E0 interface? Hardware addresses are always local, and they never pass a router’s inter-face. Understanding this process is as important as air to you, so carve this into your memory!Testing Your IP Routing UnderstandingI really want to make sure you understand IP routing because it’s super-important. So I’m going to use this section to test your understanding of the IP routing process by having you look at a couple of figures and answer some very basic IP routing questions.Figure 6.4 shows a LAN connected to RouterA, which is, in turn, connected via a WAN link to RouterB. RouterB has a LAN connected with an HTTP server attached.

10089c06.fm  Page 336  Thursday, August 30, 2007  12:42 PM




The IP Routing Process337FIGURE6.4IP routing example 1The critical information you need to glean from this figure is exactly how IP routing will occur in this example. Okay—we’ll cheat a bit. I’ll give you the answer, but then you should go back over the figure and see if you can answer example 2 without looking at my answers.1.The destination address of a frame, from HostA, will be the MAC address of the F0/0 interface of the RouterA router.2.The destination address of a packet will be the IP address of the network interface card (NIC) of the HTTP server.3.The destination port number in the segment header will have a value of 80.That example was a pretty simple one, and it was also very to the point. One thing to remember is that if multiple hosts are communicating to the server using HTTP, they must all use a different source port number. That is how the server keeps the data separated at the Transport layer.Let’s mix it up a little and add another internetworking device into the network and then see if you can find the answers. Figure 6.5 shows a network with only one router but two switches.FIGURE6.5IP routing example 2

Fa0/0

Fa0/0

S0/0RouterARouterB

HTTP ServerHostAS0/0

Fa0/0

RouterAHostA

Fa0/1

HTTPS Server

10089c06.fm  Page 337  Thursday, August 30, 2007  12:42 PM




338Chapter6 IP RoutingWhat you want to understand about the IP routing process here is what happens when HostA sends data to the HTTPS server:1.The destination address of a frame, from HostA, will be the MAC address of the F0/0 interface of the RouterA router.2.The destination address of a packet will be the IP address of the network interface card (NIC) of the HTTPS server.3.The destination port number in the segment header will have a value of 443.Notice that the switches weren’t used as either a default gateway or another destination. That’s because switches have nothing to do with routing. I wonder how many of you chose the switch as the default gateway (destination) MAC address for HostA? If you did, don’t feel bad—just take another look with that fact in mind. It’s very important to remember that the destination MAC address will always be the router’s interface—if your packets are destined for outside the LAN, as they were in these last two examples.Before we move into some of the more advanced aspects of IP routing, let’s discuss ICMP in more detail, as well as how ICMP is used in an internetwork. Take a look at the network shown in Figure 6.6. Ask yourself what will happen if the LAN interface of Lab_C goes down.FIGURE6.6ICMP error exampleLab_C will use ICMP to inform Host A that Host B can’t be reached, and it will do this by sending an ICMP destination unreachable message. Lots of people think that the Lab_A router would be sending this message, but they would be wrong because the router that sends the message is the one with that interface that’s down.Let’s look at another problem: Look at the output of a corporate router’s routing table:

Corp#sh ip route[output cut]R    192.168.215.0 [120/2] via 192.168.20.2, 00:00:23, Serial0/0R    192.168.115.0 [120/1] via 192.168.20.2, 00:00:23, Serial0/0R    192.168.30.0 [120/1] via 192.168.20.2, 00:00:23, Serial0/0C    192.168.20.0 is directly connected, Serial0/0

C    192.168.214.0 is directly connected, FastEthernet0/0What do we see here? If I were to tell you that the corporate router received an IP packet with a source IP address of 192.168.214.20 and a destination address of 192.168.22.3, what do you think the Corp router will do with this packet?

Lab_A Lab_B Host A 

E0 E0 

Lab_CHost B 

E0 icmp

10089c06.fm  Page 338  Thursday, August 30, 2007  12:42 PM




The IP Routing Process339If you said, “The packet came in on the FastEthernet 0/0 interface, but since the routing table doesn’t show a route to network 192.168.22.0 (or a default route), the router will discard the packet and send an ICMP destination unreachable message back out interface FastEthernet 0/0,” you’re a genius! The reason it does this is because that’s the source LAN where the packet originated from.Now, let’s check out another figure and talk about the frames and packets in detail. Really, we’re not exactly chatting about anything new; I’m just making sure that you totally, com-pletely, fully understand basic IP routing. That’s because this book, and the exam objectives it’s geared toward, are all about IP routing, which means you need to be all over this stuff! We’ll use Figure 6.7 for the next few questions.FIGURE6.7Basic IP routing using MAC and IP addressesReferring to Figure 6.7, here’s a list of all the questions you need the answers to emblazoned in your brain:1.In order to begin communicating with the Sales server, Host 4 sends out an ARP request. How will the devices exhibited in the topology respond to this request?2.Host 4 has received an ARP reply. Host 4 will now build a packet, then place this packet in the frame. What information will be placed in the header of the packet that leaves Host 4 if Host 4 is going to communicate to the Sales server?3.At last, the Lab_A router has received the packet and will send it out Fa0/0 onto the LAN toward the server. What will the frame have in the header as the source and des-tination addresses?4.Host 4 is displaying two web documents from the Sales server in two browser windows at the same time. How did the data find its way to the correct browser windows?I probably should write the following in a teensy font and put them upside down in another part of the book so it would be really hard for you to cheat and peek, but since it’s actually you who’s going to lose out if you peek, here are your answers:1.In order to begin communicating with the server, Host 4 sends out an ARP request. How will the devices exhibited in the topology respond to this request? Since MAC addresses must stay on the local network, the Lab_B router will respond with the MAC address of 

3 

4 

1 

2 

S0/0 S0/0 Fa0/0 Fa0/0 Lab_A Lab_B 

Sales server 

10089c06.fm  Page 339  Thursday, August 30, 2007  12:42 PM




340Chapter6 IP Routingthe Fa0/0 interface and Host 4 will send all frames to the MAC address of the Lab_B Fa0/0 interface when sending packets to the Sales server.2.Host 4 has received an ARP reply. Host 4 will now build a packet, then place this packet in the frame. What information will be placed in the header of the packet that leaves Host 4 if Host 4 is going to communicate to the Sales server? Since we’re now talking about packets, not frames, the source address will be the IP address of Host 4 and the des-tination address will be the IP address of the Sales server.3.Finally, the Lab_A router has received the packet and will send it out Fa0/0 onto the LAN toward the server. What will the frame have in the header as the source and destination addresses? The source MAC address will be the Lab_A router’s Fa0/0 interface, and the destination MAC address will be the Sales server’s MAC address. (All MAC addresses must be local on the LAN.)4.Host 4 is displaying two web documents from the Sales server in two different browser windows at the same time. How did the data find its way to the correct browser windows? TCP port numbers are used to direct the data to the correct application window.Great! But we’re not quite done yet. I’ve got a few more questions for you before you actu-ally get to configure routing in a real network. Ready? Figure 6.8 shows a basic network, and Host 4 needs to get email. Which address will be placed in the destination address field of the frame when it leaves Host 4?FIGURE6.8Testing basic routing knowledgeThe answer is that Host 4 will use the destination MAC address of the Fa0/0 interface of the Lab_B router—which I’m so sure you knew, right? Look at Figure 6.8 again: Host 4 needs to communicate to Host 1. Which OSI layer 3 source address will be placed in the packet header when it reaches Host 1?Hopefully you know this: At layer 3, the source IP address will be Host 4 and the destina-tion address in the packet will be the IP address of Host 1. Of course, the destination MAC address from Host 4 will always be the Fa0/0 address of the Lab_B router, right? And since we have more than one router, we’ll need a routing protocol that communicates between both of them so that traffic can be forwarded in the right direction to reach the network in which Host 1 is attached.

3 

4 

1 

2 

S0/0 S0/0 Fa0/0 Fa0/0 Lab_A Lab_B 

Email server 

10089c06.fm  Page 340  Thursday, August 30, 2007  12:42 PM




 The IP Routing Process 341 Okay—one more question and you’re on your way to being an IP routing genius! Again, using Figure 6.8., Host 4 is transferring a file to the email server connected to the Lab_A router. What would be the layer 2 destination address leaving Host 4? Yes, I’ve asked this question more than once. But not this one: What will be the source MAC address when the frame is received at the email server?Hopefully, you answered that the layer 2 destination address leaving Host 4 will be the MAC address of the Fa0/0 interface of the Lab_B router and that the source layer 2 address that the email server will receive will be the Fa0/0 interface of the Lab_A router.If you did, you’re all set to get the skinny on how IP routing is handled in a larger network. Configuring IP Routing It’s time to get serious and configure a real network! Figure 6.9 shows five routers: Corp, Remote1, Remote2, Remote3, and the 871W (which is a wireless router). Remember that, by default, these routers only know about networks that are directly connected to them. You also want to keep in mind that the 1242 shown in the figure is an access point—not a wireless router like the 871W. Think of the access point as more of a hub than a router. FIGURE6.9 Configuring IP routing

Remote1 Remote2 

Remote3 

871W F0/0 (DCE) 

1242AP 

BVI1 10.1.1.0 F0/1 Corp S0/0/0  S0/0/1  S0/1/0 S0/2/0 

HostA HostB HostC HostD (DCE) (DCE) (DCE) VLAN1 10.1.6.0 10.1.7.0 10.1.8.010.1.9.0 10.1.10.0 10.1.11.0 F0/1 D0/3/0 F0/1 F0/0 F0/0 10.1.2.010.1.2.010.1.4.0 10.1.5.0 S0/0/0  s0/0/1  S0/2/0  S0/0/1  10.1.12.0WHA 

WHB 

WHC 

 

10089c06.fm  Page 341  Friday, November 7, 2008  10:54 PM




342Chapter6 IP RoutingAs you might guess, I’ve got quite a nice collection of routers for us to play with. The Corp router is a 2811 with a Wireless Controller module; something you’ll get to see in Chapter 12. Remote routers 1 and 3 are 1841 ISR routers, and Remote2 is a 2801 with a wireless WIC card and a switch module. I’m simply going to call the group of remote routers R1, R2, and R3. (You can still perform most of the commands I use in this book with older routers, but you need at least a new 800 or 1800 series to run the SDM.)The first step for this project is to correctly configure each router with an IP address on each interface. Table 6.1 shows the IP address scheme I’m going to use to configure the network. After we go over how the network is configured, I’ll cover how to configure IP routing. Each network in the following table has a 24-bit subnet mask (255.255.255.0), which makes the interesting (subnet) octet the third one.TABLE6.1Network Addressing for the IP Network RouterNetwork AddressInterfaceAddressCORP   Corp10.1.1.0F0/110.1.1.1Corp10.1.2.0S0/0/010.1.2.1Corp10.1.3.0S0/0/1(DCE)10.1.3.1Corp10.1.4.0s0/1/010.1.4.1Corp10.1.5.0s0/2/010.1.5.1R1   R110.1.2.0S0/0/0 (DCE)10.1.2.2R110.1.3.0S0/0/110.1.3.2R110.1.6.0F0/010.1.6.1R110.1.7.0F0/110.1.7.1R2   R210.1.4.0S0/2/0 (DCE)10.1.4.2R210.1.8.0D0/3/010.1.8.1R210.1.9.0F0/010.1.9.1

10089c06.fm  Page 342  Thursday, August 30, 2007  12:42 PM




The IP Routing Process343The router configuration is really a pretty straightforward process since you just need to add IP addresses to your interfaces and then perform a no shutdown on those same interfaces. It gets a tad more complex later on, but for right now, let’s configure the IP addresses in the network.Corp ConfigurationWe need to configure five interfaces to configure the Corp router. And configuring the host-names of each router will make identification much easier. While we’re at it, why not set the interface descriptions, banner, and router passwords too? It’s a really good idea to make a habit of configuring these commands on every router.To get started, I performed an erase startup-config on the router and reloaded, so we’ll start in setup mode. I choose no to entering setup mode, which will get us straight to the user-name prompt of the console. I’m going to configure all my routers this way except for R3, which I’ll configure using the SDM just for fun. You can configure your routers either way.Here’s how I did all that:

         --- System Configuration Dialog ---Would you like to enter the initial configuration dialog? [yes/no]: n[output cut]Press RETURN to get started!Router>enR3   R310.1.5.0S0/0/0/ (DCE)10.1.5.2R310.1.10.0F0/010.1.10.1R310.1.11.0F0/110.1.11.1871W   871W10.1.11.0Vlan 110.1.11.2871W10.1.12.0Dot11radio010.1.12.11242 AP   1242 AP10.1.1.0BVI 110.1.1.2TABLE6.1Network Addressing for the IP Network(continued)RouterNetwork AddressInterfaceAddress

10089c06.fm  Page 343  Thursday, August 30, 2007  12:42 PM




344Chapter6 IP RoutingRouter#config tRouter(config)#hostname CorpCorp(config)#enable secret toddCorp(config)#interface fastEthernet 0/1Corp(config-if)#ip address 10.1.1.1 255.255.255.0Corp(config-if)#description Connection to 1242 APCorp(config-if)#no shutdownCorp(config-if)#int s0/0/0Corp(config-if)#ip address 10.1.2.1 255.255.255.0Corp(config-if)#description 1st Connection to R1Corp(config-if)#no shutCorp(config-if)#int s0/0/1Corp(config-if)#ip address 10.1.3.1 255.255.255.0Corp(config-if)#description 2nd Connection to R1Corp(config-if)#no shutCorp(config-if)#int s0/1/0Corp(config-if)#ip address 10.1.4.1 255.255.255.0Corp(config-if)#description Connection to R2Corp(config-if)#no shutCorp(config-if)#int s0/2/0Corp(config-if)#ip address 10.1.5.1 255.255.255.0Corp(config-if)#description Connection to R3Corp(config-if)#no shutCorp(config-if)#line con 0Corp(config-line)#password consoleCorp(config-line)#loginCorp(config-line)#logging synchronousCorp(config-line)#exec-timeout 0 0Corp(config-line)#line aux 0Corp(config-line)#password auxCorp(config-line)#loginCorp(config-line)#exitCorp(config)#line vty 0 ?  <1-1180>  Last Line number  <cr>Corp(config)#line vty 0 1180Corp(config-line)#password telnetCorp(config-line)#loginCorp(config-line)#exitCorp(config)#no ip domain-lookup

10089c06.fm  Page 344  Thursday, August 30, 2007  12:42 PM




The IP Routing Process345Corp(config)#banner motd # This is my Corp 2811 ISR Router #Corp(config-if)#^ZCorp#copy running-config startup-configDestination filename [startup-config]?[enter]Building configuration...[OK]

Corp#

If you have a hard time understanding this configuration process, refer back to Chapter 4, “Cisco’s Internetworking Operating System (IOS) and Security Device Manager (SDM).”To view the IP routing tables created on a Cisco router, use the command show ip route. The command output is shown as follows:

Corp#sh ip routeCodes: C - connected, S - static, R - RIP, M - mobile, B - BGP       D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area       N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2       E1 - OSPF external type 1, E2 - OSPF external type 2       i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS   level-2, ia - IS-IS inter area, * - candidate default, U - per-user   static route, o - ODR, P - periodic downloaded static routeGateway of last resort is not set     10.0.0.0/24 is subnetted, 1 subnetsC       10.1.1.0 is directly connected, FastEthernet0/1

Corp#It’s important to remember that only configured, directly connected networks are going to show up in the routing table. So why is it that I only see the FastEthernet0/1 interface in the routing table? No worries—that’s just because you won’t see the serial interfaces come up until the other side of the serial links is operational. As soon as we configure our R1, R2, and R3 routers, all those interfaces should pop right up.But did you notice the C on the left side of the output of the routing table? When you see that there, it means that the network is directly connected. The codes for each type of connec-tion are listed at the top of the show ip route command, along with their abbreviations.

In the interest of brevity, the codes will be cut in the rest of this chapter.

10089c06.fm  Page 345  Thursday, August 30, 2007  12:42 PM




346Chapter6 IP RoutingThe Corp serial interface 0/0/1 is a DCE connection, which means that we need to add the clock rate command to the interface. Remember that you don’t need to use the clock rate command in production. Even though this is very true, it’s still imperative that you know how/when you can use it and that you understand it really well when studying for your CCNA exam!We can see our clocking with the show controllers command:

Corp#sh controllers s0/0/1Interface Serial0/0/1Hardware is GT96K

DCE V.35, clock rate 2000000One last thing before we get into configuring the Remote routers: Did you notice the clock rate is 2000000 under the s0/0/1 interface of the Corp router? That’s important because if you think back to when we were configuring the Corp router, you’ll recall that I didn’t set the clock rate. The reason I didn’t is because ISR routers will auto-detect a DCE-type cable and auto-matically configure the clock rate—a really sweet feature!R1 ConfigurationNow we’re ready to configure the next router—R1. To make that happen correctly, keep in mind that we have four interfaces to deal with: serial 0/0/0, serial 0/0/1, FastEthernet 0/0, and FastEthernet 0/1. So let’s make sure we don’t forget to add the hostname, passwords, interface descriptions, and banner to the router configuration. As I did with the Corp router, I erased the configuration and reloaded.Here’s the configuration I used:

R1#erase start% Incomplete command.R1#erase startup-configErasing the nvram filesystem will remove all configuration files!   Continue? [confirm][enter][OK]Erase of nvram: completeR1#reloadProceed with reload? [confirm][enter][output cut]%Error opening tftp://255.255.255.255/network-confg (Timed out)%Error opening tftp://255.255.255.255/cisconet.cfg (Timed out)         --- System Configuration Dialog ---Would you like to enter the initial configuration dialog? [yes/no]: n

10089c06.fm  Page 346  Thursday, August 30, 2007  12:42 PM




The IP Routing Process347Before we move on, I really want to discuss the above output with you. First, notice that the new 12.4 ISR routers will no longer take the command erase start. The router has only one command after erase that starts with s, as shown here:

Router#erase s?

startup-config I know, you’d think that the IOS would continue to accept the command, but nope—sorry! The second thing I want to point out is that the output tells us the router is looking for a TFTP host to see if it can download a configuration. When that fails, it goes straight into setup mode. This gives you a great picture of the Cisco router default boot sequence we talked about in Chapter 5.Okay, let’s get back to configuring our router:

Press RETURN to get started!Router>enRouter#config tRouter(config)#hostname R1R1(config)#enable secret toddR1(config)#int s0/0/0R1(config-if)#ip address 10.1.2.2 255.255.255.0R1(config-if)#Description 1st Connection to Corp RouterR1(config-if)#no shutR1(config-if)#int s0/0/1      R1(config-if)#ip address 10.1.3.2 255.255.255.0R1(config-if)#no shutR1(config-if)#description 2nd connection to Corp RouterR1(config-if)#int f0/0R1(config-if)#ip address 10.1.6.1 255.255.255.0R1(config-if)#description Connection to HostAR1(config-if)#no shutR1(config-if)#int f0/1R1(config-if)#ip address 10.1.7.1 255.255.255.0R1(config-if)#description Connection to HostBR1(config-if)#no shutR1(config-if)#line con 0R1(config-line)#password consoleR1(config-line)#loginR1(config-line)#logging synchronousR1(config-line)#exec-timeout 0 0R1(config-line)#line aux 0R1(config-line)#password aux

10089c06.fm  Page 347  Thursday, August 30, 2007  12:42 PM




348Chapter6 IP RoutingR1(config-line)#loginR1(config-line)#exitR1(config)#line vty 0 ?  <1-807>  Last Line number  <cr>R1(config)#line vty 0 807R1(config-line)#password telnetR1(config-line)#loginR1(config-line)#banner motd # This is my R1 ISR Router #R1(config)#no ip domain-lookupR1(config)#exitR1#copy run startDestination filename [startup-config]?[enter]Building configuration...[OK]R1#

Let’s take a look at our configuration of the interfaces.

R1#sh run | begin interface interface FastEthernet0/0 description Connection to HostA ip address 10.1.6.1 255.255.255.0 duplex auto speed auto!interface FastEthernet0/1 description Connection to HostB ip address 10.1.7.1 255.255.255.0 duplex auto speed auto!interface Serial0/0/0 description 1st Connection to Corp Router ip address 10.1.2.2 255.255.255.0!interface Serial0/0/1 description 2nd connection to Corp Router ip address 10.1.3.2 255.255.255.0

!

10089c06.fm  Page 348  Thursday, August 30, 2007  12:42 PM




 The IP Routing Process 349 The  show ip route  command displays the following:

 R1# show ip route      10.0.0.0/24 is subnetted, 4 subnetsC       10.1.3.0 is directly connected, Serial0/0/1C       10.1.2.0 is directly connected, Serial0/0/0C       10.1.7.0 is directly connected, FastEthernet0/1C       10.1.6.0 is directly connected, FastEthernet0/0

 R1# Notice that router R1 knows how to get to networks 10.1.3.0, 10.1.2.0, 10.1.7.0, and 10.1.6.0. We can now ping to the Corp router from R1:

 R1# ping 10.1.2.1 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 10.1.2.1, timeout is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip min/avg/max = 1/2/4 ms

 R1# Now let’s go back to the Corp router and look at the routing table:

 Corp# sh ip route [output cut]     10.0.0.0/24 is subnetted, 4 subnetsC       10.1.3.0 is directly connected, Serial0/0/1C       10.1.2.0 is directly connected, Serial0/0/0C       10.1.1.0 is directly connected, FastEthernet0/1

 Corp# Since the serial links are up—remember, DCE is now detected automatically with ISR routers and the clock rate is automatically added to the interface configuration—we can now see all three. And once we configure R2 and R3, we’ll see two more networks in the routing table of the Corp router. The Corp router can’t see either the 10.1.6.0 or 10.1.7.0 networks because we don’t have any routing configured yet. R2 Configuration To configure R2, we’re going to do pretty much the same thing we did with the other two routers. There are three interfaces: serial 0/2/0, FastEthernet 0/0/0, and Dot11radio 0/3/0 to deal with, and again, we’ll be sure to add the hostname, passwords, interface descriptions, and a banner to the router configuration:

 Router> en Router# config t Router(config)# hostname R2

 

10089c06.fm  Page 349  Friday, November 7, 2008  10:56 PM




350Chapter6 IP RoutingR2(config)#enable secret toddR2(config)#int s0/2/0R2(config-if)#ip address 10.1.4.2 255.255.255.0R2(config-if)#description Connection to Corp ISR RouterR2(config-if)#no shutR2(config-if)#int f0/0R2(config-if)#ip address 10.1.9.1 255.255.255.0R2(config-if)#description Connection to HostCR2(config-if)#no shutR2(config-if)#int dot11radio 0/3/0R2(config-if)#ip address 10.1.8.1 255.255.255.0R2(config-if)#description Admin WLANR2(config-if)#ssid ADMINR2(config-if-ssid)#guest-modeR2(config-if-ssid)#authentication openR2(config-if-ssid)#infrastructure-ssidR2(config-if-ssid)#no shutR2(config-if)#line con 0R2(config-line)#password consoleR2(config-line)#loginR2(config-line)#logging syncR2(config-line)#exec-timeout 0 0R2(config-line)#line aux 0R2(config-line)#password auxR2(config-line)#loginR2(config-line)#exitR2(config)#line vty 0 ?  <1-807>  Last Line number  <cr>R2(config)#line vty 0 807R2(config-line)#password telnetR2(config-line)#loginR2(config-line)#exitR2(config)#banner motd # This is my R2 ISR Router #R2(config)#no ip domain-lookupR2(config)#^ZR2#copy run startDestination filename [startup-config]?[enter]Building configuration...[OK]

R2#

10089c06.fm  Page 350  Thursday, August 30, 2007  12:42 PM




 The IP Routing Process 351 Nice—everything was pretty straightforward except for that wireless interface. It’s true, the wireless interface is really just another interface on a router, and it looks just like that in the routing table as well. But, in order to bring up the wireless interface, more configurations are needed than for a simple FastEthernet interface. So check out the following output, and then I’ll tell you about the special configuration needs for this wireless interface:

 R2(config-if)# int dot11radio0/3/0 R2(config-if)# ip address 10.1.8.1 255.255.255.0 R2(config-if)# description Connection to WLAN R2(config-if)# no shut R2(config-if)# ssid ADMIN R2(config-if-ssid)# guest-mode R2(config-if-ssid)# authentication open R2(config-if-ssid)# infrastructure-ssid

 R2(config-if-ssid)# no shut So, what we see here is that everything is pretty commonplace until we get to the SSID config-uration. This is the Service Set Identifier that creates a wireless network that hosts can connect to. Unlike access points, the interface on the R2 router is actually a routed interface, which is the rea-son why the IP address is placed under the physical interface—typically the IP address would be placed under the management VLAN or Bridge-Group Virtual Interface (BVI).That  guest-mode  line means that the interface will broadcast the SSID so wireless hosts will understand that they can connect to this interface.  Authentication open  means just that…no authentication. (Even so, you still have to type that command in at minimum to make the wireless interface work.) Last, the  infrastructure-ssid  indicates that this inter-face can be used to communicate to other access points, or other devices on the infrastruc-ture—to the actual wired network itself.But wait, we’re not done yet—we still need to configure the DHCP pool for the wireless clients:

 R2# config t R2(config)# ip dhcp pool Admin R2(dhcp-config)# network 10.1.8.0 255.255.255.0 R2(dhcp-config)# default-router 10.1.8.1 R2(dhcp-config)# exit R2(config)# ip dhcp excluded-address 10.1.8.1

 R2(config)# Creating DHCP pools on a router is actually a pretty simple process. To do so, you just create the pool name, add the network/subnet and the default gateway, and exclude any addresses you don’t want handed out (like the default gateway address). And you’d usually add a DNS server as well.The output of the following  show ip route  command displays the directly connected net-works of 10.1.9.0, 8.0, and 4.0, as you can see here:

 R2# sh ip route      10.0.0.0/24 is subnetted, 3 subnets

 

10089c06.fm  Page 351  Friday, November 7, 2008  10:57 PM




352Chapter6 IP RoutingC       10.1.9.0 is directly connected, FastEthernet0/0C       10.1.8.0 is directly connected, Dot11Radio0/3/0C       10.1.4.0 is directly connected, Serial0/2/0

R2#The Corp, R1, and R2 routers now have all their links up. But we still need to configure R3 (the 871W router) and the 1241 AP.

Wireless networks will be discussed in detail in Chapter 12, “Cisco’s Wireless Technologies.”R3 ConfigurationJust as I said, I’m going to use the SDM for the R3 router. My first step is to set an IP address on the F0/0 interface. I used a crossover cable to connect my PC directly to the f0/0 port.Now since I want to set up the router with security, I’ve got to configure the router back to the factory defaults. I can do this via the CLI just as I showed you back in Chapter 4, but it’s actually a whole lot easier to do this using SDM!Using HTTP, I was able to access the R3 router, go to the Configure page, and choose Addi-tional Tasks. Then, I just clicked on Configuration Management and Reset to Factory Default.I clicked the Reset Router button in the bottom-right corner and then configured my PC using the directions shown on the screen in the above screen shot.

10089c06.fm  Page 352  Thursday, August 30, 2007  12:42 PM




The IP Routing Process353Again, using HTTPS, I connected back to SDM using the 10.10.10.1 address that was provided in the directions. SDM had me log in twice with the username cisco as well as a password of cisco. I then had to accept the certificate from the router, and I’m good to go with a secure connection.The first thing the router had me do after SDM was loaded was change the username and password.Then I needed to log in again using my new name and password.After that, I chose Configure and then Interfaces and Connections, which is in the upper-left corner, under Home. Clicking the Serial (PPP, HDLC or Frame Relay) button got me to where I could choose Create New Connection.

10089c06.fm  Page 353  Thursday, August 30, 2007  12:42 PM




354Chapter6 IP RoutingThe Create New Connection button took me to the WAN Wizard.I was then able to choose my interface; then I clicked Next.

10089c06.fm  Page 354  Thursday, August 30, 2007  12:42 PM




The IP Routing Process355I then chose High-Level Data Link Control and clicked Next. (I’ll get into HDLC in Chapter 14.)I was then able to add my IP address and mask.

10089c06.fm  Page 355  Thursday, August 30, 2007  12:42 PM




356Chapter6 IP RoutingIP Unnumbered is truly an interesting configuration because it lets you set up a network connection without using an IP address. Instead, you “borrow” an IP address from another active interface. This comes in pretty handy if you happen to be a bit short on subnets!Anyway, the next screen asked if I wanted to set up static routing and NAT. Again, this is something I’ll get into more later on, so we’re not going to configure it just yet.Moving on, I clicked Next and received a summary of my serial 0/0/1 configuration.

10089c06.fm  Page 356  Thursday, August 30, 2007  12:42 PM




The IP Routing Process357I clicked Finish, and the commands were uploaded to my R3 router. (I’m going to configure both the F0/0 and F0/1 interfaces the same way.)After choosing the FastEthernet 0/1 interface from the same location from where I started to configure the s0/0/1 interface. I chose Create New Connection and was taken to the LAN Wizard.

10089c06.fm  Page 357  Thursday, August 30, 2007  12:42 PM




358Chapter6 IP RoutingThe LAN Wizard allows you to either choose straight routing (which is what we want to do here) or configure 802.1Q trunking, which I’ll discuss in detail in Chapter 9, “Virtual LANs.” I configured the IP address and mask and then clicked Next.What’s cool about the SDM at this point is that it would build a DHCP server for this LAN if I wanted it too. Man, this is easy.

10089c06.fm  Page 358  Thursday, August 30, 2007  12:42 PM




The IP Routing Process359Since I accidentally entered the wrong IP address for F0/1, the only way to change it now is to choose Configure and Edit Interface/Connection in the SDM, or use the CLI.From here, I can double-click on the FastEthernet 0/1 interface and change the IP address.After using the LAN Wizard to set up F0/0, I had to save the configuration and then recon-figure my PC into the right network and reconnect to SDM to verify my configuration.We’re good—R3 is now configured! Even though my console and VTY password get con-figured automatically when I set up the user todd, I still had to choose Configure and then Additional Tasks and then Router Properties to set the hostname and enable secret password.871W ConfigurationAlthough I can configure the 871 router with SDM, I’ll configure it using the CLI. First, I need to erase the default configuration and reload it, as I did with the other routers (except R3).

Router>enRouter#config tRouter(config)#hostname 871W871W(config)#int vlan 1871W(config-if)#ip address 10.1.11.2 255.255.255.0

10089c06.fm  Page 359  Thursday, August 30, 2007  12:42 PM




360Chapter6 IP Routing871W(config-if)#no shut871W(config-if)#int dot11radio 0871W(config-if)#ip address 10.1.12.1 255.255.255.0871W(config-if)#no shut871W(config-if)#ssid R3WLAN871W(config-if-ssid)#guest-mode871W(config-if-ssid)#authentication open871W(config-if-ssid)#infrastructure-ssid871W(config-if-ssid)#line con 0871W(config-line)#password console871W(config-line)#logging sync871W(config-line)#exec-timeout 0 0871W(config-line)#exit871W(config)#line vty 0 ?  <1-4>  Last Line number  <cr>871W(config)#line vty 0 4871W(config-line)#password telnet871W(config-line)#login871W(config-line)#ip dhcp pool R3WLAN871W(dhcp-config)#network 10.1.12.0 255.255.255.0871W(dhcp-config)#default-router 10.1.12.1871W(dhcp-config)#exit871W(config)#ip dhcp excluded-address 10.1.12.1871W(config)#exit871W#copy run startDestination filename [startup-config]?[enter]Building configuration...[OK]

871W#The 871W has a four-port switch, which means that you’ve got to place the IP address under the management VLAN interface. You just can’t get away with simply putting IP addresses on layer 2 switch interfaces.To be totally honest, I think this was a faster configuration than using SDM. But I guess, in production, the SDM with HTTPS would really be a more secure way to administer the router. And as promised, I’ll show you soon (in Chapter 12) why using SDM is the easier way to go when you want to set up wireless security.Let’s take a look at the routing table now:

871W#sh ip route     10.0.0.0/24 is subnetted, 2 subnets

10089c06.fm  Page 360  Thursday, August 30, 2007  12:42 PM




 The IP Routing Process 361 C       10.1.11.0 is directly connected, Vlan1

 C       10.1.12.0 is directly connected, Dot11Radio0 We have both our networks showing directly connected. Let’s configure our last device, and then we’ll start configuring routing. 1242AP Configuration Configuring the 1242AP is a bit different because it’s an access point (again, think hub), not a router. I’ll configure this device from the CLI, but you can use an HTTP interface as well. But you can’t use SDM. The HTTP interface will be easier to use when we start adding security and when we get into some more complex configurations.Check out the output:

 ap> en Password:ap# config t ap(config)# hostname 1242AP 1242AP(config)# enable secret todd 1242AP(config)# int dot11Radio 0 1242AP(config-if)# description CORPWLAN 1242AP(config-if)# no shutdown 1242AP(config-if)# ssid CORPWLAN 1242AP(config-if-ssid)# guest-mode 1242AP(config-if-ssid)# authentication open 1242AP(config-if-ssid)# infrastructure-ssid 1242AP(config-if-ssid)# exit 1242AP(config-if)# exit 1242AP(config)# line con 0 1242AP(config-line)# password console 1242AP(config-line)# login 1242AP(config-line)# logging synchronous 1242AP(config-line)# exec-timeout 0 0 1242AP(config-line)# exit 1242AP(config)# line vty 0 ?   <1-15>  Last Line number  <cr>1242AP(config)# line vty 0 15 1242AP(config-line)# password telnet 1242AP(config-line)# login 1242AP(config-line)# int bvi 1 1242AP(config-if)# ip address 10.1.1.2 255.255.255.0 1242AP(config-if)# no shut 1242AP(config-if)# exit

 

10089c06.fm  Page 361  Friday, November 7, 2008  10:58 PM




362Chapter6 IP Routing1242AP(config)#ip default-gateway 10.1.1.11242AP(config)#ip dhcp pool CORPWLAN1242AP(dhcp-config)#network 10.1.1.0 255.255.255.01242AP(dhcp-config)#default-router 10.1.1.11242AP(dhcp-config)#exit1242AP(config)#ip dhcp excluded-address 10.1.1.11242AP(config)#ip dhcp excluded-address 10.1.1.21242AP(config)#no ip domain-lookup1242AP(config)#^Z1242AP#copy run startDestination filename [startup-config]?[enter]Building configuration...[OK]

1242AP#Even though the SSID configuration is the same as it is for the R2 routed radio interface, notice there’s no IP address under the Dot11radio 0 interface. Why? Because it’s not a routed port, so the IP address is instead placed under the Bridge Virtual Interface (BVI). I also set a default gateway so this device can be managed from outside the LAN.You need to know that just as with a switch, you don’t need to add an IP address to the AP for it to function. I could just as easily have added the DHCP pool to the Corp router for the wireless LAN, not added an IP address or pool to the AP at all, and it still would have worked just the same.Configuring IP Routing in Our NetworkOur network is good to go—right? After all, it’s been correctly configured with IP addressing, administrative functions, and even clocking (automatically on the ISR routers). But how does a router send packets to remote networks when the only way it can send them is by looking at the routing table to find out how to get to the remote networks? Our configured routers only have information about directly connected networks in each routing table. And what happens when a router receives a packet for a network that isn’t listed in the routing table? It doesn’t send a broadcast looking for the remote network—the router just discards it. Period.So we’re not exactly ready to rock after all. But no worries—there are several ways to con-figure the routing tables to include all the networks in our little internetwork so that packets will be forwarded. And what’s best for one network isn’t necessarily what’s best for another. Understanding the different types of routing will really help you come up with the best solu-tion for your specific environment and business requirements.You’ll learn about the following types of routing in the following sections: Static routing Default routing Dynamic routing

10089c06.fm  Page 362  Thursday, August 30, 2007  12:42 PM




Configuring IP Routing in Our Network363I’m going to start off by describing and implementing static routing on our network because if you can implement static routing and make it work, it means you have a solid understanding of the internetwork. So let’s get started.Static RoutingStatic routing occurs when you manually add routes in each router’s routing table. There are pros and cons to static routing, but that’s true for all routing processes.Static routing has the following benefits: There is no overhead on the router CPU, which means you could possibly buy a cheaper router than you would use if you were using dynamic routing. There is no bandwidth usage between routers, which means you could possibly save money on WAN links. It adds security because the administrator can choose to allow routing access to certain networks only.Static routing has the following disadvantages: The administrator must really understand the internetwork and how each router is con-nected in order to configure routes correctly. If a network is added to the internetwork, the administrator has to add a route to it on all routers—by hand. It’s not feasible in large networks because maintaining it would be a full-time job in itself.Okay—that said, here’s the command syntax you use to add a static route to a routing table:

ip route [destination_network] [mask] [next-hop_address or

  exitinterface] [administrative_distance] [permanent]This list describes each command in the string:ip routeThe command used to create the static route.destination_networkThe network you’re placing in the routing table.maskThe subnet mask being used on the network.next-hop_addressThe address of the next-hop router that will receive the packet and for-ward it to the remote network. This is a router interface that’s on a directly connected net-work. You must be able to ping the router interface before you add the route. If you type in the wrong next-hop address or the interface to that router is down, the static route will show up in the router’s configuration but not in the routing table.exitinterfaceUsed in place of the next-hop address if you want, and shows up as a directly connected route.administrative_distanceBy default, static routes have an administrative distance of 1 (or even 0 if you use an exit interface instead of a next-hop address). You can change the default value by adding an administrative weight at the end of the command. I’ll talk a lot more about this subject later in the chapter when we get to the section on dynamic routing.

10089c06.fm  Page 363  Thursday, August 30, 2007  12:42 PM




364Chapter6 IP RoutingpermanentIf the interface is shut down or the router can’t communicate to the next-hop router, the route will automatically be discarded from the routing table. Choosing the permanent option keeps the entry in the routing table no matter what happens.Before we dive into configuring static routes, let’s take a look at a sample static route and see what we can find out about it.

Router(config)#ip route 172.16.3.0 255.255.255.0 192.168.2.4 The ip route command tells us simply that it is a static route. 172.16.3.0 is the remote network we want to send packets to. 255.255.255.0 is the mask of the remote network. 192.168.2.4 is the next hop, or router, we will send packets to.However, suppose the static route looked like this:

Router(config)#ip route 172.16.3.0 255.255.255.0 192.168.2.4 150The 150 at the end changes the default administrative distance (AD) of 1 to 150. No worries—I’ll talk much more about AD when we get into dynamic routing. For now, just remember that the AD is the trustworthiness of a route, where 0 is best and 255 is worst.One more example, then we’ll start configuring:

Router(config)#ip route 172.16.3.0 255.255.255.0 s0/0/0Instead of using a next-hop address, we can use an exit interface that will make the route show up as a directly connected network. Functionally, the next hop and exit interface work exactly the same. To help you understand how static routes work, I’ll demonstrate the con-figuration on the internetwork shown previously in Figure 6.9.CorpEach routing table automatically includes directly connected networks. To be able to route to all networks within the internetwork, the routing table must include information that describes where these other networks are located and how to get to them.The Corp router is connected to five networks. For the Corp router to be able to route to all networks, the following networks have to be configured into its routing table: 10.1.6.0 10.1.7.0 10.1.8.0 10.1.9.0 10.1.10.0 10.1.11.0 10.1.12.0

10089c06.fm  Page 364  Thursday, August 30, 2007  12:42 PM




 Configuring IP Routing in Our Network 365 The following router output shows the static routes on the Corp router and the routing table after the configuration. For the Corp router to find the remote networks, I had to place an entry into the routing table describing the remote network, the remote mask, and where to send the packets. I am going to add a “150” at the end of each line to raise the administrative distance. (When we get to dynamic routing, you’ll see why I did it this way.)

 Corp(config)# ip route 10.1.6.0 255.255.255.0 10.1.2.2 150 Corp(config)# ip route 10.1.6.0 255.255.255.0 10.1.3.2 151 Corp(config)# ip route 10.1.7.0 255.255.255.0 10.1.3.2 150 Corp(config)# ip route 10.1.7.0 255.255.255.0 10.1.2.2 151 Corp(config)# ip route 10.1.8.0 255.255.255.0 10.1.4.2 150 Corp(config)# ip route 10.1.9.0 255.255.255.0 10.1.4.2 150 Corp(config)# ip route 10.1.10.0 255.255.255.0 10.1.5.2 150 Corp(config)# ip route 10.1.11.0 255.255.255.0 10.1.5.2 150 Corp(config)# ip route 10.1.12.0 255.255.255.0 10.1.5.2 150 Corp(config)# do show run | begin ip route ip route 10.1.6.0 255.255.255.0 10.1.2.2 150ip route 10.1.6.0 255.255.255.0 10.1.3.2 151ip route 10.1.7.0 255.255.255.0 10.1.3.2 150ip route 10.1.7.0 255.255.255.0 10.1.2.2 151ip route 10.1.8.0 255.255.255.0 10.1.4.2 150ip route 10.1.9.0 255.255.255.0 10.1.4.2 150ip route 10.1.10.0 255.255.255.0 10.1.5.2 150ip route 10.1.11.0 255.255.255.0 10.1.5.2 150

 ip route 10.1.12.0 255.255.255.0 10.1.5.2 150 For networks 10.1.6.0 and 10.1.7.0, I put in both paths to each network, but I made one link a higher (151) AD. This will be a backup route in case the other link fails. If I made them both the same AD, we would end up with a routing loop. (Static routing can’t handle multiple links to the same destination.) After the router is configured, you can type   show ip route  to see the static routes:

 Corp(config)# do show ip route      10.0.0.0/24 is subnetted, 12 subnetsS       10.1.11.0 [150/0] via 10.1.5.2S       10.1.10.0 [150/0] via 10.1.5.2S       10.1.9.0 [150/0] via 10.1.4.2S       10.1.8.0 [150/0] via 10.1.4.2S       10.1.12.0 [150/0] via 10.1.5.2C       10.1.3.0 is directly connected, Serial0/0/0C       10.1.2.0 is directly connected, Serial0/0/1C       10.1.1.0 is directly connected, FastEthernet0/1

 

10089c06.fm  Page 365  Wednesday, October 29, 2008  2:48 PM




366Chapter6 IP RoutingS       10.1.7.0 [150/0] via 10.1.3.2S       10.1.6.0 [150/0] via 10.1.2.2C       10.1.5.0 is directly connected, Serial0/2/0

C       10.1.4.0 is directly connected, Serial0/1/0The Corp router is configured to route and know about all routes to all networks. I con-figured two routes to each remote network on R1, but the routing table will only show the route with the lower AD. The other link will show up in the routing table only if the link with that lower value it’s currently using fails.I want you to understand that if the routes don’t appear in the routing table, it’s because the router can’t communicate with the next-hop address you’ve configured. You can use the permanent parameter to keep the route in the routing table even if the next-hop device can’t be contacted.The S in the preceding routing table entries means that the network is a static entry. The [1/0] is the administrative distance and metric (something we’ll cover later) to the remote net-work. Here, the next-hop interface is 0, indicating that it’s directly connected.Okay—we’re good. The Corp router now has all the information it needs to communicate with the other remote networks. But keep in mind that if the R1, R2, R3, and 871W routers aren’t configured with all the same information, the packets will simply be discarded. We’ll need to fix this by configuring static routes.

Don’t stress about the 150/151 at the end of the static route configuration. I promise I will discuss the topic really soon in this chapter, not a later one! Be assured that you don’t need to worry about it at this point.R1The R1 router is directly connected to the networks 10.1.2.0, 10.1.3.0, 10.1.6.0, and 10.1.7.0, so we’ve got to configure the following static routes on the R1 router: 10.1.1.0 10.1.4.0 10.1.5.0 10.1.8.0 10.1.9.0 10.1.10.0 10.1.11.0 10.1.12.0Here’s the configuration for the R1 router. Remember, we’ll never create a static route to any network we’re directly connected to, and we can use the next hop of either 10.1.2.1 or 

10089c06.fm  Page 366  Thursday, August 30, 2007  12:42 PM




Configuring IP Routing in Our Network36710.1.3.1 since we have two links between the Corp and R1 routers. I’ll change between next hops so all data doesn’t go down one link. It really doesn’t matter which link I use since I can’t load-balance with static routing. We’ll be able to load-balance when we use dynamic routing like RIP, EIGRP, and OSPF, but for now, the links will just provide a backup route to each net-work. Let’s check out the output:

R1(config)#ip route 10.1.1.0 255.255.255.0 10.1.2.1 150R1(config)#ip route 10.1.1.0 255.255.255.0 10.1.3.1 151R1(config)#ip route 10.1.4.0 255.255.255.0 10.1.2.1 150R1(config)#ip route 10.1.4.0 255.255.255.0 10.1.3.1 151R1(config)#ip route 10.1.5.0 255.255.255.0 10.1.2.1 150R1(config)#ip route 10.1.5.0 255.255.255.0 10.1.3.1 151R1(config)#ip route 10.1.8.0 255.255.255.0 10.1.3.1 150R1(config)#ip route 10.1.8.0 255.255.255.0 10.1.2.1 151R1(config)#ip route 10.1.9.0 255.255.255.0 10.1.3.1 150R1(config)#ip route 10.1.9.0 255.255.255.0 10.1.2.1 151R1(config)#ip route 10.1.10.0 255.255.255.0 10.1.3.1 150R1(config)#ip route 10.1.10.0 255.255.255.0 10.1.2.1 151R1(config)#ip route 10.1.11.0 255.255.255.0 10.1.3.1 150R1(config)#ip route 10.1.11.0 255.255.255.0 10.1.2.1 151R1(config)#ip route 10.1.12.0 255.255.255.0 10.1.3.1 150R1(config)#ip route 10.1.12.0 255.255.255.0 10.1.2.1 151R1(config)#do show run | begin ip routeip route 10.1.1.0 255.255.255.0 10.1.2.1 150ip route 10.1.1.0 255.255.255.0 10.1.3.1 151ip route 10.1.4.0 255.255.255.0 10.1.2.1 150ip route 10.1.4.0 255.255.255.0 10.1.3.1 151ip route 10.1.5.0 255.255.255.0 10.1.2.1 150ip route 10.1.5.0 255.255.255.0 10.1.3.1 151ip route 10.1.8.0 255.255.255.0 10.1.3.1 150ip route 10.1.8.0 255.255.255.0 10.1.2.1 151ip route 10.1.9.0 255.255.255.0 10.1.3.1 150ip route 10.1.9.0 255.255.255.0 10.1.2.1 151ip route 10.1.10.0 255.255.255.0 10.1.3.1 150ip route 10.1.10.0 255.255.255.0 10.1.2.1 151ip route 10.1.11.0 255.255.255.0 10.1.3.1 150ip route 10.1.11.0 255.255.255.0 10.1.2.1 151ip route 10.1.12.0 255.255.255.0 10.1.3.1 150

ip route 10.1.12.0 255.255.255.0 10.1.2.1 151

10089c06.fm  Page 367  Thursday, August 30, 2007  12:42 PM




368Chapter6 IP RoutingThis was a pretty long configuration because I configured two paths to each network. By looking at the routing table, you can see that the R1 router now understands how to find each network:

R1(config)#do show ip route     10.0.0.0/24 is subnetted, 12 subnetsS       10.1.11.0 [150/0] via 10.1.3.1S       10.1.10.0 [150/0] via 10.1.3.1S       10.1.9.0 [150/0] via 10.1.3.1S       10.1.8.0 [150/0] via 10.1.3.1S       10.1.12.0 [150/0] via 10.1.3.1C       10.1.3.0 is directly connected, Serial0/0/1C       10.1.2.0 is directly connected, Serial0/0/0S       10.1.1.0 [150/0] via 10.1.2.1C       10.1.7.0 is directly connected, FastEthernet0/1C       10.1.6.0 is directly connected, FastEthernet0/0S       10.1.5.0 [150/0] via 10.1.2.1

S       10.1.4.0 [150/0] via 10.1.2.1The R1 router now has a complete routing table. As soon as the other routers in the inter-network have all the networks in their routing table, R1 will be able to communicate with all remote networks.

Remember, the route with the higher administrative distance will not show up in the routing table unless the route with the lower administrative distance goes away.R2The R2 router is directly connected to three networks 10.1.4.0, 10.1.8.0, and 10.1.9.0, so these routes need to be added: 10.1.1.0 10.1.2.0 10.1.3.0 10.1.5.0 10.1.6.0 10.1.7.0 10.1.10.0

10089c06.fm  Page 368  Thursday, August 30, 2007  12:42 PM




Configuring IP Routing in Our Network369 10.1.11.0 10.1.12.0Here’s the configuration for the R2 router:

R2(config)#ip route 10.1.1.0 255.255.255.0 10.1.4.1 150R2(config)#ip route 10.1.2.0 255.255.255.0 10.1.4.1 150R2(config)#ip route 10.1.3.0 255.255.255.0 10.1.4.1 150R2(config)#ip route 10.1.5.0 255.255.255.0 10.1.4.1 150R2(config)#ip route 10.1.6.0 255.255.255.0 10.1.4.1 150R2(config)#ip route 10.1.7.0 255.255.255.0 10.1.4.1 150R2(config)#ip route 10.1.10.0 255.255.255.0 10.1.4.1 150R2(config)#ip route 10.1.11.0 255.255.255.0 10.1.4.1 150R2(config)#ip route 10.1.12.0 255.255.255.0 10.1.4.1 150R2(config)#do show run | begin ip routeip route 10.1.1.0 255.255.255.0 10.1.4.1 150ip route 10.1.2.0 255.255.255.0 10.1.4.1 150ip route 10.1.3.0 255.255.255.0 10.1.4.1 150ip route 10.1.5.0 255.255.255.0 10.1.4.1 150ip route 10.1.6.0 255.255.255.0 10.1.4.1 150ip route 10.1.7.0 255.255.255.0 10.1.4.1 150ip route 10.1.10.0 255.255.255.0 10.1.4.1 150ip route 10.1.11.0 255.255.255.0 10.1.4.1 150

ip route 10.1.12.0 255.255.255.0 10.1.4.1 150The following output shows the routing table on the R2 router:

R2(config)#do show ip route     10.0.0.0/24 is subnetted, 12 subnetsS       10.1.11.0 [150/0] via 10.1.4.1S       10.1.10.0 [150/0] via 10.1.4.1C       10.1.9.0 is directly connected, FastEthernet0/0C       10.1.8.0 is directly connected, Dot11Radio0/3/0S       10.1.12.0 [150/0] via 10.1.4.1S       10.1.3.0 [150/0] via 10.1.4.1S       10.1.2.0 [150/0] via 10.1.4.1S       10.1.1.0 [150/0] via 10.1.4.1S       10.1.7.0 [150/0] via 10.1.4.1S       10.1.6.0 [150/0] via 10.1.4.1S       10.1.5.0 [150/0] via 10.1.4.1

C       10.1.4.0 is directly connected, Serial0/2/0

10089c06.fm  Page 369  Thursday, August 30, 2007  12:42 PM




370Chapter6 IP RoutingR2 now shows all 12 networks in the internetwork, so it too can now communicate with all routers and networks (that are configured so far).R3The R3 router is directly connected to networks 10.1.5.0, 10.1.10.0, and 10.1.11.0, but we need to add these routes: 10.1.1.0 10.1.2.0 10.1.3.0 10.1.4.0 10.1.6.0 10.1.7.0 10.1.8.0 10.1.9.0 10.1.12.0As before, I’m going to use SDM to configure the static routing for the R3 router. The configuration is pretty simple, and I can use either the next-hop address or the exit interface. Since I like to type as little as possible, I’m going with the exit interface because it only takes a mouse click.

10089c06.fm  Page 370  Thursday, August 30, 2007  12:42 PM




Configuring IP Routing in Our Network371After all our routes are configured, we can see them in the routing screen.From this screen, it is easy to edit the static routes.Let’s take a look at the configuration and the routing table uploaded to the router from SDM:

R3#show run | begin ip route  ip route 10.1.1.0 255.255.255.0 Serial0/0/1 150 permanentip route 10.1.2.0 255.255.255.0 Serial0/0/1 150 permanentip route 10.1.3.0 255.255.255.0 Serial0/0/1 150 permanentip route 10.1.4.0 255.255.255.0 Serial0/0/1 150 permanentip route 10.1.6.0 255.255.255.0 Serial0/0/1 150 permanentip route 10.1.7.0 255.255.255.0 Serial0/0/1 150 permanentip route 10.1.8.0 255.255.255.0 Serial0/0/1 150 permanentip route 10.1.9.0 255.255.255.0 Serial0/0/1 150 permanentip route 10.1.12.0 255.255.255.0 FastEthernet0/1 150 permanentR3#show ip route     10.0.0.0/24 is subnetted, 12 subnetsC       10.1.11.0 is directly connected, FastEthernet0/1C       10.1.10.0 is directly connected, FastEthernet0/0S       10.1.9.0 is directly connected, Serial0/0/1S       10.1.8.0 is directly connected, Serial0/0/1

10089c06.fm  Page 371  Thursday, August 30, 2007  12:42 PM




372Chapter6 IP RoutingS       10.1.12.0 is directly connected, FastEthernet0/1S       10.1.3.0 is directly connected, Serial0/0/1S       10.1.2.0 is directly connected, Serial0/0/1S       10.1.1.0 is directly connected, Serial0/0/1S       10.1.7.0 is directly connected, Serial0/0/1S       10.1.6.0 is directly connected, Serial0/0/1C       10.1.5.0 is directly connected, Serial0/0/1S       10.1.4.0 is directly connected, Serial0/0/1

R3#Looking at the show ip route command output, you can see that the static routes are listed as directly connected. Strange? Not really, because I used the exit interface instead of the next-hop address, and functionally, there’s no difference. We really don’t need the permanent command because all that will do is ensure that the route stays in the routing table even if the link to that route goes down. I configured the permanent command only because it was easy to do with SDM (just another mouse click). We’re almost there—just one more router to go: the 871W.871WNow for this router, I’m going to configure something called default routing since the 871W is configured as a stub. A stub indicates that the wireless network in this design has only one way out to reach all other networks. I’ll show you the configuration, verify the network in the next section, then I’ll discuss default routing in detail. Here’s the configuration:

871W(config)#ip route 0.0.0.0 0.0.0.0 10.1.11.1871W(config)#ip classless871W(config)#do show ip route     10.0.0.0/24 is subnetted, 2 subnetsC       10.1.11.0 is directly connected, Vlan1C       10.1.12.0 is directly connected, Dot11Radio0S*   0.0.0.0/0 [1/0] via 10.1.11.1

871W(config)#This seems a lot easier, doesn’t it? And it is, but there’s a catch—you can’t do things like this on all routers, only on stub networks. I could’ve used default routing in routers R1 and R2 as well, and I didn’t add the 150 to this default route even though I easily could have. I didn’t do that because it’s really simple to just remove the route when we get to dynamic routing later.So we’re there—we’ve done it! All the routers have the correct routing table, so all rout-ers and hosts should be able to communicate without a hitch—for now. But if you add even one more network or another router to the internetwork, you’ll have to update each and every router’s routing tables by hand—yikes! This isn’t a problem at all if you’ve got a small network, but it’s obviously extremely time-consuming if you’re dealing with a large internetwork!

10089c06.fm  Page 372  Thursday, August 30, 2007  12:42 PM




 Configuring IP Routing in Our Network 373 Verifying Your Configuration We’re not done yet—once all the routers’ routing tables are configured, they need to be verified. The best way to do this, besides using the  show ip route  command, is with the Ping program. I’ll test and verify by pinging from the 871W router.Here’s the output:

 871W# ping 10.1.1.2 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 10.1.1.2, timeout is 2 seconds:!!!!!

 Success rate is 100 percent (5/5), round-trip min/avg/max = 1/2/4 ms From router 871W, a ping to HostA, B, C, and D will also test for good IP connectivity. Here’s the router output:

 871W# ping 10.1.6.2 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 10.1.6.2, timeout is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip min/avg/max = 4/6/12 ms871W# ping 10.1.7.2 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 10.1.7.2, timeout is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip min/avg/max = 4/4/4 ms871W# ping 10.1.9.2 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 10.1.9.2, timeout is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip min/avg/max = 4/4/4 ms871W# ping 10.1.10.2 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 10.1.10.2, timeout is 2 seconds:!!!!!

 Success rate is 100 percent (5/5) Also, we can trace from the 871W router to see the hops the packet takes to get to HostA:

 871W# trace 10.1.6.2 Type escape sequence to abort.Tracing the route to 10.1.6.2  1 10.1.11.1 0 msec 0 msec 0 msec  2 10.1.5.1 4 msec 0 msec 4 msec

 

10089c06.fm  Page 373  Friday, November 7, 2008  10:59 PM




374Chapter6 IP Routing  3 10.1.2.2 0 msec 0 msec 4 msec

  4 10.1.6.2 4 msec 4 msec *Since we can communicate from end to end and to each host without a problem, our static route configuration has been successful!Default RoutingWe use default routing to send packets with a remote destination network not in the routing table to the next-hop router. You should only use default routing on stub networks—those with only one exit path out of the network.In the internetworking example used in the previous section, the only routers that are con-sidered to be in a stub network are R1, R2, and the 871W. If you tried to put a default route on router R3, packets wouldn’t be forwarded to the correct networks because they have more than one interface routing to other routers. You can easily create loops with default routing, so be careful!To configure a default route, you use wildcards in the network address and mask locations of a static route (as I demonstrated in the 871W configuration). In fact, you can just think of a default route as a static route that uses wildcards instead of network and mask information.By using a default route, you can just create one static route entry instead. This sure is easier then typing in all those routes!

871W(config)#ip route 0.0.0.0 0.0.0.0 10.1.11.1871W(config)#ip classless871W(config)#do show ip route  Gateway of last resort is 10.1.11.1 to network 0.0.0.0 10.0.0.0/24 is subnetted, 2 subnetsC       10.1.11.0 is directly connected, Vlan1C       10.1.12.0 is directly connected, Dot11Radio0S*   0.0.0.0/0 [1/0] via 10.1.11.1

871W(config)#If you look at the routing table, you’ll see only the two directly connected networks plus an S*, which indicates that this entry is a candidate for a default route. I could have completed the default route command another way:

871W(config)#ip route 0.0.0.0 0.0.0.0 vlan1What this is telling us is that if you don’t have an entry for a network in the routing table, just forward it out VLAN1 (which will send it out FastEthernet0/0). You can choose the IP address of the next-hop router or the exit interface—either way, it will work the same. Remember, I used this exit interface configuration with the R3 static route configs.Notice also in the routing table that the gateway of last resort is now set. Even so, there’s one more command you must be aware of when using default routes: the ip classless command.

10089c06.fm  Page 374  Thursday, August 30, 2007  12:42 PM




Configuring IP Routing in Our Network375All Cisco routers are classful routers, meaning they expect a default subnet mask on each interface of the router. When a router receives a packet for a destination subnet that’s not in the routing table, it will drop the packet by default. If you’re using default routing, you must use the ip classless command because it is possible that no remote subnets will be in the routing table.Since I have version 12.4 of the IOS on my routers, the ip classless command is on by default. If you’re using default routing and this command isn’t in your configuration, you will need to add it if you have subnetted networks on your routers. The command is shown here:

871W(config)#ip classlessNotice that it’s a global configuration mode command. The interesting part of the ip classless command is that without it, default routing sometimes works but sometimes doesn’t. To be on the safe side, you should always turn on the ip classless command when you use default routing.There’s another command you can use to configure a gateway of last resort—the ip default-network command. Figure 6.10 shows a network that needs to have a gateway of last resort statement configured.FIGURE6.10Configuring a gateway of last resortHere are three commands (all providing the same solution) for adding a gateway of last resort on the gateway router to the ISP.

Gateway(config)#ip route 0.0.0.0 0.0.0.0 217.124.6.1Gateway(config)#ip route 0.0.0.0 0.0.0.0 s0/0

Gateway(config)#ip default-network 217.124.6.0As I said before, all three of these commands would accomplish the goal of setting the gate-way of last resort, but there are some small differences between them. First, the exit interface 

ISP.1217.124.6.0.2Gateway

10089c06.fm  Page 375  Thursday, August 30, 2007  12:42 PM




376Chapter6 IP Routingsolution would be used over the other two solutions because it has an AD of 0. Also, the ip default-network command would advertise the default network when you configure an IGP (like RIP) on the router. This is so other routers in your internetwork will receive this route as a default route automatically.But what happens if you misconfigured a default route? Let’s take a look at the output of a show ip route command and compare that to the network in Figure 6.11 and see if you can find a problem:

Router#sh ip route[output cut]Gateway of last resort is 172.19.22.2 to network 0.0.0.0C      172.17.22.0 is directly connected, FastEthernet0/0C      172.18.22.0  is directly connected, Serial0/0

S*     0.0.0.0/0 [1/0] via 172.19.22.2FIGURE6.11Misconfigured default routeFind anything? You can see by looking at the figure and the directly connected routes in the routing table that the WAN link is on network 172.18.22.0 and that the default route is for-warding all packets to the 172.19.22.0 network. This is just bad—it will never work, so the problem is a misconfigured static (default) route.One last thing before moving on to dynamic routing. If you have the routing table output as shown in the following lines, what happens if the router receives a packet from 10.1.6.100 destined for host 10.1.8.5?

Corp#sh ip route[output cut]Gateway of last resort is 10.1.5.5 to network 0.0.0.0R     10.1.3.0 [120/1] via 10.1.2.2, 00:00:00, Serial 0/0C     10.1.2.0  is directly connected, Serial0/0C     10.1.5.0  is directly connected, Serial0/1C     10.1.6.0  is directly connected, Fastethernet0/0

R*    0.0.0.0/0 [120/0] via 10.1.5.5, 00:00:00 Serial 0/1This is a tad different than what I’ve shown you up until now because the default route is listed as R*, which means it’s a RIP-injected route. This is because someone configured the 

172.18.22.0172.17.22.0172.31.5.0

10089c06.fm  Page 376  Thursday, August 30, 2007  12:42 PM




Dynamic Routing377ip default-network command on a remote router as well as configuring RIP, causing RIP to advertise this route through the internetwork as a default route. Since the destination address is 10.1.8.5 and there is no route to network 10.1.8.0, the router would use the default route and send the packet out serial 0/1.Dynamic RoutingDynamic routing is when protocols are used to find networks and update routing tables on routers. True, this is easier than using static or default routing, but it’ll cost you in terms of router CPU pro-cesses and bandwidth on the network links. A routing protocol defines the set of rules used by a router when it communicates routing information between neighbor routers.The routing protocol I’m going to talk about in this chapter is Routing Information Protocol (RIP) versions 1 and 2, with a bit of Interior Gateway Routing Protocol (IGRP) thrown in.Two types of routing protocols are used in internetworks: interior gateway protocols (IGPs) and exterior gateway protocols (EGPs). IGPs are used to exchange routing information with routers in the same autonomous system (AS). An AS is a collection of networks under a common administrative domain, which basically means that all routers sharing the same routing table information are in the same AS. EGPs are used to communicate between ASes. An example of an EGP is Border Gateway Protocol (BGP), which is beyond the scope of this book.Since routing protocols are so essential to dynamic routing, I’m going to give you the basic information you need to know about them next. Later on in this chapter, we’ll focus on configuration.Routing Protocol BasicsThere are some important things you should know about routing protocols before getting deeper into RIP. Specifically, you need to understand administrative distances, the three dif-ferent kinds of routing protocols, and routing loops. We will look at each of these in more detail in the following sections.Administrative DistancesThe administrative distance (AD) is used to rate the trustworthiness of routing information received on a router from a neighbor router. An administrative distance is an integer from 0 to 255, where 0 is the most trusted and 255 means no traffic will be passed via this route.If a router receives two updates listing the same remote network, the first thing the router checks is the AD. If one of the advertised routes has a lower AD than the other, then the route with the lowest AD will be placed in the routing table.If both advertised routes to the same network have the same AD, then routing protocol metrics (such as hop count or bandwidth of the lines) will be used to find the best path to the remote network. The advertised route with the lowest metric will be placed in the routing table. But if both advertised routes have the same AD as well as the same metrics, then the routing protocol will load-balance to the remote network (which means that it sends packets down each link).

10089c06.fm  Page 377  Thursday, August 30, 2007  12:42 PM




378Chapter6 IP RoutingTable 6.2 shows the default administrative distances that a Cisco router uses to decide which route to take to a remote network.If a network is directly connected, the router will always use the interface connected to the network. If you configure a static route, the router will then believe that route over any other learned routes. You can change the administrative distance of static routes, but by default, they have an AD of 1. In our static route configuration, the AD of each route is set at 150 or 151. This lets us configure routing protocols without having to remove the static routes. They’ll be used as backup routes in case the routing protocol experiences a failure of some type.For example, if you have a static route, a RIP-advertised route, and an IGRP-advertised route listing the same network, then by default, the router will always use the static route unless you change the AD of the static route—which we did.Routing ProtocolsThere are three classes of routing protocols:Distance vectorThe distance-vector protocols find the best path to a remote network by judging distance. Each time a packet goes through a router, that’s called a hop. The route with the least number of hops to the network is determined to be the best route. The vector indicates the direction to the remote network. Both RIP and IGRP are distance-vector rout-ing protocols. They send the entire routing table to directly connected neighbors.Link stateIn link-state protocols, also called shortest-path-first protocols, the routers each create three separate tables. One of these tables keeps track of directly attached TABLE6.2Default Administrative DistancesRoute SourceDefault ADConnected interface0Static route1EIGRP90IGRP100OSPF110RIP120External EIGRP170Unknown255 (this route will never be used)

10089c06.fm  Page 378  Thursday, August 30, 2007  12:42 PM




Distance-Vector Routing Protocols379neighbors, one determines the topology of the entire internetwork, and one is used as the routing table. Link-state routers know more about the internetwork than any distance-vector routing protocol. OSPF is an IP routing protocol that is completely link state. Link-state protocols send updates containing the state of their own links to all other routers on the network.HybridHybrid protocols use aspects of both distance vector and link state—for example, EIGRP.There’s no set way of configuring routing protocols for use with every business. This is something you really have to do on a case-by-case basis. If you understand how the different routing protocols work, you can make good, solid decisions that truly meet the individual needs of any business.Distance-Vector Routing ProtocolsThe distance-vector routing algorithm passes complete routing table contents to neighboring routers, which then combine the received routing table entries with their own routing tables to complete the router’s routing table. This is called routing by rumor, because a router receiving an update from a neighbor router believes the information about remote networks without actu-ally finding out for itself.It’s possible to have a network that has multiple links to the same remote network, and if that’s the case, the administrative distance of each received update is checked first. If the AD is the same, the protocol will have to use other metrics to determine the best path to use to that remote network.RIP uses only hop count to determine the best path to a network. If RIP finds more than one link with the same hop count to the same remote network, it will automatically perform a round-robin load balancing. RIP can perform load balancing for up to six equal-cost links (four by default).However, a problem with this type of routing metric arises when the two links to a remote network are different bandwidths but the same hop count. Figure 6.12, for example, shows two links to remote network 172.16.10.0.FIGURE6.12Pinhole congestion

SOSO

SOSO

Network172.16.10.0

Router B

Router ANetwork172.16.30.0T156KNetwork172.16.20.0

Router D

Router C

10089c06.fm  Page 379  Thursday, August 30, 2007  12:42 PM




380Chapter6 IP RoutingSince network 172.16.30.0 is a T1 link with a bandwidth of 1.544Mbps and network 172.16.20.0 is a 56K link, you’d want the router to choose the T1 over the 56K link, right? But because hop count is the only metric used with RIP routing, the two links would be seen as being of equal cost. This little snag is called pinhole congestion.It’s important to understand what a distance-vector routing protocol does when it starts up. In Figure 6.13, the four routers start off with only their directly connected networks in their routing tables. After a distance-vector routing protocol is started on each router, the routing tables are updated with all route information gathered from neighbor routers.As shown in Figure 6.13, each router has only the directly connected networks in each routing table. Each router sends its complete routing table out to each active interface. The routing table of each router includes the network number, exit interface, and hop count to the network.FIGURE6.13The internetwork with distance-vector routingIn Figure 6.14, the routing tables are complete because they include information about all the networks in the internetwork. They are considered converged. When the routers are con-verging, it is possible that no data will be passed. That’s why fast convergence time is a serious plus. In fact, that’s one of the problems with RIP—its slow convergence time.The routing table in each router keeps information regarding the remote network number, the interface to which the router will send packets to reach that network, and the hop count or metric to the network.Routing LoopsDistance-vector routing protocols keep track of any changes to the internetwork by broadcasting periodic routing updates out all active interfaces. This broadcast includes the complete routing table. This works just fine, but it’s expensive in terms of CPU process and link bandwidth. And if a network outage happens, real problems can occur. Plus, the slow convergence of distance-vector routing protocols can result in inconsistent routing tables and routing loops.

172.16.10.0172.16.20.0172.16.40.0172.16.50.0172.16.30.0

S0

E02501A

F0/02621A

S1

S0

E02501B

E0

S02501C

Routing TableF0/00172.16.10.0

Routing TableE00172.16.10.0S00172.16.20.0

Routing TableS00172.16.20.0

Routing TableS00172.16.40.0E00172.16.30.0S10172.16.40.0E00172.16.50.0

10089c06.fm  Page 380  Thursday, August 30, 2007  12:42 PM




Distance-Vector Routing Protocols381Routing loops can occur because every router isn’t updated simultaneously, or even close to it. Here’s an example—let’s say that the interface to Network 5 in Figure 6.15 fails. All routers know about Network 5 from RouterE. RouterA, in its tables, has a path to Network 5 through RouterB.FIGURE6.14Converged routing tablesFIGURE6.15Routing loop exampleWhen Network 5 fails, RouterE tells RouterC. This causes RouterC to stop routing to Net-work 5 through RouterE. But routers A, B, and D don’t know about Network 5 yet, so they keep sending out update information. RouterC will eventually send out its update and cause B to stop routing to Network 5, but routers A and D are still not updated. To them, it appears that Network 5 is still available through RouterB with a metric of 3.The problem occurs when RouterA sends out its regular 30-second “Hello, I’m still here—these are the links I know about” message, which includes the ability to reach Network 5, and now routers B and D receive the wonderful news that Network 5 can be reached from RouterA, 

172.16.10.0172.16.20.0172.16.40.0172.16.50.0172.16.30.0

S0

E02501A

F0/02621A

S1

S0

E02501B

E0

S02501C

Routing Table

Routing Table

Routing TableS00172.16.20.0

Routing TableE00172.16.30.0S10172.16.40.0S01172.16.10.0S11172.16.50.0F0/00172.16.10.0F0/01172.16.20.0F0/02172.16.30.0F0/02172.16.40.0F0/03172.16.50.0E00172.16.10.0S00172.16.20.0S01172.16.30.0S01172.16.40.0S02172.16.50.0S00172.16.40.0E00172.16.50.0S02172.16.10.0S01172.16.20.0S01172.16.30.0

RouterA

RouterD

RouterB

RouterC

RouterENetwork 3Network 4Network 556KT3

10089c06.fm  Page 381  Thursday, August 30, 2007  12:42 PM




382Chapter6 IP Routingso routers B and D then send out the information that Network 5 is available. Any packet des-tined for Network 5 will go to RouterA, to RouterB, and then back to RouterA. This is a routing loop—how do you stop it?Maximum Hop CountThe routing loop problem just described is called counting to infinity, and it’s caused by gossip (broadcasts) and wrong information being communicated and propagated throughout the internetwork. Without some form of intervention, the hop count increases indefinitely each time a packet passes through a router.One way of solving this problem is to define a maximum hop count. RIP permits a hop count of up to 15, so anything that requires 16 hops is deemed unreachable. In other words, after a loop of 15 hops, Network 5 will be considered down. Thus, the maximum hop count will control how long it takes for a routing table entry to become invalid or questionable.Split HorizonAnother solution to the routing loop problem is called split horizon. This reduces incorrect routing information and routing overhead in a distance-vector network by enforcing the rule that routing information cannot be sent back in the direction from which it was received.In other words, the routing protocol differentiates which interface a network route was learned on, and once this is determined, it won’t advertise the route back out that same inter-face. This would have prevented RouterA from sending the updated information it received from RouterB back to RouterB.Route PoisoningAnother way to avoid problems caused by inconsistent updates and stop network loops is route poisoning. For example, when Network 5 goes down, RouterE initiates route poisoning by advertising Network 5 as 16, or unreachable (sometimes referred to as infinite).This poisoning of the route to Network 5 keeps RouterC from being susceptible to incor-rect updates about the route to Network 5. When RouterC receives a route poisoning from RouterE, it sends an update, called a poison reverse, back to RouterE. This ensures that all routes on the segment have received the poisoned route information.HolddownsA holddown prevents regular update messages from reinstating a route that is going up and down (called flapping). Typically, this happens on a serial link that’s losing connectivity and then coming back up. If there wasn’t a way to stabilize this, the network would never converge and that one flapping interface could bring the entire network down!Holddowns prevent routes from changing too rapidly by allowing time for either the downed route to come back up or the network to stabilize somewhat before changing to the next best route. These also tell routers to restrict, for a specific time period, changes that might affect recently removed routes. This prevents inoperative routes from being prematurely restored to other routers’ tables.

10089c06.fm  Page 382  Thursday, August 30, 2007  12:42 PM




Routing Information Protocol (RIP)383Routing Information Protocol (RIP)Routing Information Protocol (RIP) is a true distance-vector routing protocol. RIP sends the complete routing table out to all active interfaces every 30 seconds. RIP only uses hop count to determine the best way to a remote network, but it has a maximum allowable hop count of 15 by default, meaning that 16 is deemed unreachable. RIP works well in small networks, but it’s inefficient on large networks with slow WAN links or on networks with a large number of routers installed.RIP version 1 uses only classful routing, which means that all devices in the network must use the same subnet mask. This is because RIP version 1 doesn’t send updates with subnet mask information in tow. RIP version 2 provides something called prefix routing and does send subnet mask information with the route updates. This is called classless routing.In the following sections, we will discuss the RIP timers and then RIP configuration.RIP TimersRIP uses four different kinds of timers to regulate its performance:Route update timerSets the interval (typically 30 seconds) between periodic routing updates in which the router sends a complete copy of its routing table out to all neighbors.Route invalid timerDetermines the length of time that must elapse (180 seconds) before a router determines that a route has become invalid. It will come to this conclusion if it hasn’t heard any updates about a particular route for that period. When that happens, the router will send out updates to all its neighbors letting them know that the route is invalid.Holddown timerThis sets the amount of time during which routing information is sup-pressed. Routes will enter into the holddown state when an update packet is received that indi-cated the route is unreachable. This continues either until an update packet is received with a better metric or until the holddown timer expires. The default is 180 seconds.Route flush timerSets the time between a route becoming invalid and its removal from the routing table (240 seconds). Before it’s removed from the table, the router notifies its neigh-bors of that route’s impending demise. The value of the route invalid timer must be less than that of the route flush timer. This gives the router enough time to tell its neighbors about the invalid route before the local routing table is updated.Configuring RIP RoutingTo configure RIP routing, just turn on the protocol with the router rip command and tell the RIP routing protocol which networks to advertise. That’s it. Let’s configure our five-router internetwork (Figure 6.9) with RIP routing.CorpRIP has an administrative distance of 120. Static routes have an administrative distance of 1 by default, and since we currently have static routes configured, the routing tables won’t be 

10089c06.fm  Page 383  Thursday, August 30, 2007  12:42 PM




384Chapter6 IP Routingpropagated with RIP information. However, because I added the 150/151 to the end of each static route, we’re good to go.You can add the RIP routing protocol by using the router rip command and the network command. The network command tells the routing protocol which classful network to advertise.Look at the Corp router configuration and see how easy this is:

Corp#config tCorp(config)#router rip

Corp(config-router)#network 10.0.0.0That’s it. Two or three commands and you’re done—sure makes your job a lot easier than when using static routes, doesn’t it? However, keep in mind the extra router CPU process and bandwidth that you’re consuming. Notice I didn’t type in subnets, only the classful network address (all subnet bits and host bits off!). It is the job of the routing protocol to find the subnets and populate the routing tables. Since we have no router buddies running RIP, we won’t see any RIP routes in the routing table yet.

Remember that RIP uses the classful address when configuring the network address. Because of this, all subnet masks must be the same on all devices in the network (this is called classful routing). To clarify this, let’s say you’re using a Class B network address of 172.16.0.0/24 with subnets 172.16.10.0, 172.16.20.0, and 172.16.30.0. You would only type in the classful network address of 172.16.0.0 and let RIP find the subnets and place them in the routing table.R1Let’s configure our R1 router :

R1#config tR1(config)#router ripR1(config-router)#network 10.0.0.0R1(config-router)#do show ip route     10.0.0.0/24 is subnetted, 12 subnetsS       10.1.11.0 [150/0] via 10.1.3.1S       10.1.10.0 [150/0] via 10.1.3.1S       10.1.9.0 [150/0] via 10.1.3.1S       10.1.8.0 [150/0] via 10.1.3.1S       10.1.12.0 [150/0] via 10.1.3.1C       10.1.3.0 is directly connected, Serial0/0/1C       10.1.2.0 is directly connected, Serial0/0/0R       10.1.1.0 [120/1] via 10.1.3.1, 00:00:04, Serial0/0/1

10089c06.fm  Page 384  Thursday, August 30, 2007  12:42 PM




Routing Information Protocol (RIP)385                 [120/1] via 10.1.2.1, 00:00:04, Serial0/0/0C       10.1.7.0 is directly connected, FastEthernet0/1C       10.1.6.0 is directly connected, FastEthernet0/0R       10.1.5.0 [120/1] via 10.1.3.1, 00:00:04, Serial0/0/1                 [120/1] via 10.1.2.1, 00:00:04, Serial0/0/0R       10.1.4.0 [120/1] via 10.1.3.1, 00:00:09, Serial0/0/1                 [120/1] via 10.1.2.1, 00:00:09, Serial0/0/0

R1(config-router)#That was pretty straightforward. Let’s talk about this routing table. Since we have one RIP buddy out there that we are exchanging routing tables with, we can see the RIP networks coming from the Corp router. (All the other routes still show up as static.) RIP also found both connec-tions to the Corp router and will load-balance between them.R2Let’s configure our R2 router with RIP:

R2#config tR2(config)#router ripR2(config-router)#network 10.0.0.0R2(config-router)#do show ip route     10.0.0.0/24 is subnetted, 12 subnetsS       10.1.11.0 [150/0] via 10.1.4.1S       10.1.10.0 [150/0] via 10.1.4.1C       10.1.9.0 is directly connected, FastEthernet0/0C       10.1.8.0 is directly connected, Dot11Radio0/3/0S       10.1.12.0 [150/0] via 10.1.4.1R       10.1.3.0 [120/1] via 10.1.4.1, 00:00:03, Serial0/2/0R       10.1.2.0 [120/1] via 10.1.4.1, 00:00:03, Serial0/2/0R       10.1.1.0 [120/1] via 10.1.4.1, 00:00:03, Serial0/2/0R       10.1.7.0 [120/2] via 10.1.4.1, 00:00:03, Serial0/2/0R       10.1.6.0 [120/2] via 10.1.4.1, 00:00:03, Serial0/2/0

R       10.1.5.0 [120/1] via 10.1.4.1, 00:00:03, Serial0/2/0The routing table is growing Rs as we add RIP buddies! We can still see that all routes are in the routing table; some are still static routes. Two more routers to go.R3Let’s configure our R3 router with RIP—as usual with R3, we’ll use the SDM.

10089c06.fm  Page 385  Thursday, August 30, 2007  12:42 PM




386Chapter6 IP RoutingFrom the routing screen, I clicked the Edit button to the right of Dynamic Routing. I then was able to configure RIP and the network number and then clicked on the interfaces I didn’t want RIP to be broadcast out. The interfaces that RIP will broadcast out will be unchecked.These are called passive interfaces and we’ll talk about it more in a minute. No reason to broadcast RIP out an interface where no routers will be.

10089c06.fm  Page 386  Thursday, August 30, 2007  12:42 PM




Routing Information Protocol (RIP)387From the SDM screen, we can see that we’re done with R3.871WHere is the last router’s RIP configuration:

871W#config t871W(config)#no ip route 0.0.0.0 0.0.0.0 10.1.11.1871W(config)#router rip871W(config-router)#network 10.0.0.0871W(config-router)#do sh ip route     10.0.0.0/24 is subnetted, 12 subnetsC       10.1.11.0 is directly connected, Vlan1R       10.1.10.0 [120/1] via 10.1.11.1, 00:00:23, Vlan1R       10.1.9.0 [120/3] via 10.1.11.1, 00:00:23, Vlan1R       10.1.8.0 [120/3] via 10.1.11.1, 00:00:23, Vlan1C       10.1.12.0 is directly connected, Dot11Radio0R       10.1.3.0 [120/2] via 10.1.11.1, 00:00:23, Vlan1R       10.1.2.0 [120/2] via 10.1.11.1, 00:00:23, Vlan1R       10.1.1.0 [120/2] via 10.1.11.1, 00:00:23, Vlan1R       10.1.7.0 [120/3] via 10.1.11.1, 00:00:24, Vlan1R       10.1.6.0 [120/3] via 10.1.11.1, 00:00:24, Vlan1R       10.1.5.0 [120/1] via 10.1.11.1, 00:00:24, Vlan1R       10.1.4.0 [120/2] via 10.1.11.1, 00:00:24, Vlan1

871W#Finally, all routes showing in the routing table are RIP injected routes.It’s important to remember administrative distances and why we needed to either remove the static routes before we added RIP routing or set them higher than 120 as we did.By default, directly connected routes have an administrative distance of 0, static routes have an administrative distance of 1, and RIP has an administrative distance of 120. I call RIP the “gossip protocol” because it reminds me of junior high school, where if you hear a rumor (advertised route), it just has to be true without exception. And that pretty much sums up how RIP behaves on an internetwork—rumor mill as protocol!Verifying the RIP Routing TablesEach routing table should now have all directly connected routes as well as RIP-injected routes received from neighboring routers.This output shows us the contents of the Corp routing table:

     10.0.0.0/24 is subnetted, 12 subnetsR       10.1.11.0 [120/1] via 10.1.5.2, 00:00:28, Serial0/2/0R       10.1.10.0 [120/1] via 10.1.5.2, 00:00:28, Serial0/2/0

10089c06.fm  Page 387  Thursday, August 30, 2007  12:42 PM




388Chapter6 IP RoutingR       10.1.9.0 [120/1] via 10.1.4.2, 00:00:26, Serial0/1/0R       10.1.8.0 [120/1] via 10.1.4.2, 00:00:26, Serial0/1/0R       10.1.12.0 [120/2] via 10.1.5.2, 00:00:28, Serial0/2/0C       10.1.3.0 is directly connected, Serial0/0/1C       10.1.2.0 is directly connected, Serial0/0/0C       10.1.1.0 is directly connected, FastEthernet0/1R       10.1.7.0 [120/1] via 10.1.3.2, 00:00:07, Serial0/0/1                 [120/1] via 10.1.2.2, 00:00:10, Serial0/0/0R       10.1.6.0 [120/1] via 10.1.3.2, 00:00:07, Serial0/0/1                 [120/1] via 10.1.2.2, 00:00:10, Serial0/0/0C       10.1.5.0 is directly connected, Serial0/2/0

C       10.1.4.0 is directly connected, Serial0/1/0This output shows us that the routing table has the same entries that it had when we were using static routes—except for that R. The R means that the networks were added dynamically using the RIP routing protocol. The [120/1] is the administrative distance of the route (120) along with the number of hops to that remote network (1). From the Corp router, all networks are one hop away except network 10.1.12.0, which is two hops away.So while yes, it’s true that RIP has worked in our little internetwork, it’s not the solution for every enterprise. That’s because this technique has a maximum hop count of only 15 (16 is deemed unreachable). Plus, it performs full routing-table updates every 30 seconds, which would bring a larger internetwork to a painful crawl pretty quick!There’s one more thing I want to show you about RIP routing tables and the parameters used to advertise remote networks. Notice, as an example, that the following routing table shows [120/15] in the 10.1.3.0 network metric. This means that the administrative distance is 120, the default for RIP, but the hop count is 15. Remember that each time a router sends out an update to a neighbor router, it increments the hop count by one for each route.

R3#sh ip route     10.0.0.0/24 is subnetted, 12 subnetsC       10.1.11.0 is directly connected, FastEthernet0/1C       10.1.10.0 is directly connected, FastEthernet0/0R       10.1.9.0 [120/2] via 10.1.5.1, 00:00:15, Serial0/0/1R       10.1.8.0 [120/2] via 10.1.5.1, 00:00:15, Serial0/0/1R       10.1.12.0 [120/1] via 10.1.11.2, 00:00:00, FastEthernet0/1R       10.1.3.0 [120/15] via 10.1.5.1, 00:00:15, Serial0/0/1R       10.1.2.0 [120/1] via 10.1.5.1, 00:00:15, Serial0/0/1R       10.1.1.0 [120/1] via 10.1.5.1, 00:00:15, Serial0/0/1R       10.1.7.0 [120/2] via 10.1.5.1, 00:00:15, Serial0/0/1R       10.1.6.0 [120/2] via 10.1.5.1, 00:00:15, Serial0/0/1C       10.1.5.0 is directly connected, Serial0/0/1R       10.1.4.0 [120/1] via 10.1.5.1, 00:00:15, Serial0/0/1

R3#

10089c06.fm  Page 388  Thursday, August 30, 2007  12:42 PM




Routing Information Protocol (RIP)389So this [120/15] is really bad because the next router that receives the table from router R3 will just discard the route to network 10.1.3.0 since the hop count would then be 16, which is invalid.

If a router receives a routing update that contains a higher-cost path to a net-work that’s already in its routing table, the update will be ignored.Configuring RIP Routing Example 2Before we move onto learning more about RIP configurations, let’s take a look at Figure 6.16. In this example, we first will find and implement our subnets and then add the RIP configu-ration to the router.FIGURE6.16RIP routing example 2For this configuration, we are going to consider that the Lab_B and Lab_C routers are already configured and we just need to configure the Lab_A router. We will use the network ID of 192.168.164.0/28. The s0/0 interface of Lab_A will use the last available IP address in the eighth subnet and the fa0/0 will use the last available IP address in the second subnet. Do not consider the zero subnet valid.Before we start, you do know that /28 is a 255.255.255.240 mask, right? And that we have a block size of 16 in the fourth octet? It is very important that you know this, and if you need another review of Chapters 2 and 3, that’s okay! Reviewing subnetting will never hurt you.Since we have a block size of 16, our subnets are 16 (remember we are not starting at zero for this example), 32, 48, 64, 80, 96, 112, 128, 144, etc. The eighth subnet (which we will use for the s0/0 interface) is subnet 128. The valid host range for the 128 subnet is 129 through 142, and 143 is the broadcast address of the 128 subnet. The second subnet (which we will use for the fa0/0 interface) is the 32 subnet. The valid hosts are 33 through 46, and 47 is the broad-cast address of the 32 subnet.So, here is what our configuration on the Lab_A router will look like:

Lab_A(config)#interface s0/0Lab_A(config-if)#ip address 192.168.164.142 255.255.255.240Lab_A(config-if)#no shutdown

Lab_A Lab_B Lab_C fa0/0 fa0/0 s0/0 (DCE) s0/0 s0/1 (DCE) s0/0 

10089c06.fm  Page 389  Thursday, August 30, 2007  12:42 PM




390Chapter6 IP RoutingLab_A(config-if)#interface fa0/0Lab_A(config-if)#ip address 192.168.164.46 255.255.255.240Lab_A(config-if)#no shutdownLab_A(config-if)#router ripLab_A(config-router)#network 192.168.164.0Lab_A(config-router)#^Z

Lab_A#Finding the subnets and configuring the last valid host should be pretty straightforward. If not, head back to Chapter 3. However, what I really want you to notice is that although we added two subnets to the Lab_A router, we only had one network statement under RIP. Some-times it is hard to remember that you configure only the classful network statement, which means you turn all host bits off.This was the real purpose of this second RIP configuration example—to remind you of classful network addressing. And it never hurts to practice subnetting, right?Holding Down RIP PropagationsYou probably don’t want your RIP network advertised everywhere on your LAN and WAN. There’s not a whole lot to be gained by advertising your RIP network to the Internet, now, is there?There’s a few different ways to stop unwanted RIP updates from propagating across your LANs and WANs, and the easiest one is through the passive-interface command that I showed you during the R3 configuration. This command prevents RIP update broadcasts from being sent out a specified interface, yet that same interface can still receive RIP updates.Here’s an example of how to configure a passive-interface on a router using the CLI:

Lab_A#config tLab_A(config)#router ripLab_A(config-router)#network 192.168.10.0

Lab_A(config-router)#passive-interface serial 0/0This command will stop RIP updates from being propagated out serial interface 0/0, but serial interface 0/0 can still receive RIP updates. This is easily done within the SDM configu-ration as well, as I demonstrated with the R3 router.RIP Version 2 (RIPv2)Let’s spend a couple of minutes discussing RIPv2 before we move into the distance-vector, Cisco-proprietary routing protocol IGRP.RIP version 2 is mostly the same as RIP version 1. Both RIPv1 and RIPv2 are distance-vector protocols, which means that each router running RIP sends its complete routing tables out all active interfaces at periodic time intervals. Also, the timers and loop-avoidance schemes are the same in both RIP versions (i.e., holddown timers and split horizon rule). Both RIPv1 and RIPv2 are configured as classful addressing (but RIPv2 is considered classless because subnet informa-tion is sent with each route update), and both have the same administrative distance (120).

10089c06.fm  Page 390  Thursday, August 30, 2007  12:42 PM




Routing Information Protocol (RIP)391But there are some important differences that make RIPv2 more scalable than RIPv1. And I’ve got to add a word of advice here before we move on; I’m definitely not advocating using RIP of either version in your network. But since RIP is an open standard, you can use RIP with any brand of router. You can also use OSPF (discussed in Chapter 7) since OSPF is an open standard as well. RIP just requires too much bandwidth, making it pretty intensive to use in your network. Why go there when you have other, more elegant options?Table 6.3 discusses the differences between RIPv1 and RIPv2.

Should We Really Use RIP in an Internetwork?You have been hired as a consultant to install a couple of Cisco routers into a growing net-work. They have a couple of old Unix routers that they want to keep in the network. These routers do not support any routing protocol except RIP. I guess this means you just have to run RIP on the entire network.Well, yes and no. You can run RIP on a router connecting that old network, but you certainly don’t need to run RIP throughout the whole internetwork!You can do what is called redistribution, which is basically translating from one type of routing protocol to another. This means that you can support those old routers using RIP but use Enhanced IGRP, for example, on the rest of your network.This will stop RIP routes from being sent all over the internetwork and eating up all that precious bandwidth.TABLE6.3RIPv1 vs. RIPv2RIPv1RIPv2Distance vectorDistance vectorMaximum hop count of 15Maximum hop count of 15ClassfulClasslessBroadcast basedUses multicast 224.0.0.9No support for VLSMSupports VLSM networksNo authenticationAllows for MD5 authenticationNo support for discontiguous networksSupports discontiguous networks

10089c06.fm  Page 391  Thursday, August 30, 2007  12:42 PM




392Chapter6 IP RoutingRIPv2, unlike RIPv1, is a classless routing protocol (even though it is configured as classful, like RIPv1), which means that it sends subnet mask information along with the route updates. By sending the subnet mask information with the updates, RIPv2 can support Variable Length Subnet Masks (VLSMs) as well as the summarization of network boundaries. In addition, RIPv2 can support discontiguous networking, which I’ll go over more in Chapter 7.Configuring RIPv2 is pretty straightforward. Here’s an example:

Lab_C(config)#router ripLab_C(config-router)#network 192.168.40.0Lab_C(config-router)#network 192.168.50.0

Lab_C(config-router)#version 2That’s it; just add the command version 2 under the (config-router)# prompt and you are now running RIPv2.

RIPv2 is classless and works in VLSM and discontiguous networks.Interior Gateway Routing Protocol (IGRP)Interior Gateway Routing Protocol (IGRP) is a Cisco-proprietary distance-vector routing pro-tocol. This means that to use IGRP in your network, all your routers must be Cisco routers. Cisco created this routing protocol to overcome the problems associated with RIP.IGRP has a maximum hop count of 255 with the default being 100 (same as EIGRP). This is helpful in larger networks and solves the problem of 15 hops being the maximum possible in a RIP network.IGRP also uses a different metric than RIP. IGRP uses bandwidth and delay of the line by default as a metric for determining the best route to an internetwork. This is called a composite metric. Reliability, load, and maximum transmission unit (MTU) can also be used, although they are not used by default.

The main difference between RIP and IGRP configuration is that when you configure IGRP, you supply the autonomous system number. All routers must use the same number in order to share routing table information.Table 6.4 shows a list of IGRP characteristics that you won’t find in RIP.

10089c06.fm  Page 392  Thursday, August 30, 2007  12:42 PM




Verifying Your Configurations393Why is this the end of the IGRP section? Because watch what happens when I try to con-figure IGRP on my router:

R3#config tEnter configuration commands, one per line.  End with CNTL/Z.R3(config)#router igrp 10                   ^% Invalid input detected at '^' marker.

R3(config)#There’s your reason—Cisco no longer supports IGRP. Why should it? All you have to do is put an E in front of IGRP and you’re running a much, much better routing protocol. We’ll get to EIGRP in the next chapter, but first, let’s go through some verification commands for RIP.Verifying Your ConfigurationsIt’s important to verify your configurations once you’ve completed them, or at least once you think you’ve completed them. The following list includes the commands you can use to verify the routed and routing protocols configured on your Cisco routers: show ip route show ip protocols debug ip ripThe first command was covered in the previous section—I’ll go over the others in the sections that follow.TABLE6.4IGRP vs. RIP IGRPRIP Can be used in large internetworksWorks best in smaller networksUses an autonomous system number for activationDoes not use autonomous system numbersGives a full route table update every 90 secondsGives a full route table update every 30 secondsHas an administrative distance of 100Has an administrative distance of 120Uses bandwidth and delay of the line as metric (lowest composite metric), with a maximum hop count of 255Uses only hop count to determine the best path to a remote network, with 15 hops being the maximum

10089c06.fm  Page 393  Thursday, August 30, 2007  12:42 PM




 394 Chapter6  IP Routing The  show ip protocols  Command The  show ip protocols  command shows you the routing protocols that are configured on your router. Looking at the following output, you can see that RIP is running on the router and the timers that RIP uses:

 R3# sh ip protocols Routing Protocol is "rip"  Outgoing update filter list for all interfaces is not set  Incoming update filter list for all interfaces is not set  Sending updates every 30 seconds, next due in 24 seconds  Invalid after 180 seconds, hold down 180, flushed after 240  Redistributing: rip  Default version control: send version 1, receive version 1    Interface             Send  Recv  Triggered RIP  Key-chain    FastEthernet0/1       1     1                                       Serial0/0/1           1     1                                     Automatic network summarization is not in effect  Maximum path: 4  Routing for Networks:    10.0.0.0  Passive Interface(s):    FastEthernet0/0    Serial0/0/0  Routing Information Sources:    Gateway         Distance      Last Update    10.1.11.2            120      00:00:10    10.1.5.1             120      00:00:22

   Distance: (default is 120) Notice in this output that RIP is sending updates every 30 seconds, which is the default. The timers used in distance vector are also shown.Notice further down that RIP is routing for directly connected interfaces f0/1 and s0/0/1. The version is listed to the right of the interfaces—RIPv1.F0/0 and s0/0/0 are listed as passive interfaces (they will not send RIP information out). The neighbors it found are 10.1.11.2 and 10.1.5.1. The last entry is the default AD for RIP (120). Troubleshooting with the  show ip protocols  Command Let’s use a sample router and use the  show ip protocols  command to see what we can deter-mine about routing by looking at this output from a router on another network:

 Router# sh ip protocols Routing Protocol is "rip"

 

10089c06.fm  Page 394  Wednesday, February 27, 2008  5:04 PM




Verifying Your Configurations395  Sending updates every 30 seconds, next due in 6 seconds  Invalid after 180 seconds, hold down 180, flushed after240  Outgoing update filter list for all interfaces is  Incoming update filter list for all interfaces is  Redistributing: rip  Default version control: send version 1, receive any version    Interface        Send  Recv   Key-chain    Serial0/0         1     1 2    Serial0/1         1     1 2  Routing for Networks:    10.0.0.0  Routing Information Sources:    Gateway         Distance    Last Update    10.168.11.14       120      00:00:21

  Distance: (default is 120)Let’s also look at the show ip interface brief command from the same router and see what we find out:

Router#sh ip interface briefInterface        IP-Address      OK?    Method StatusFastEthernet0/0  192.168.18.1    YES    manual  upSerial0/0        10.168.11.17    YES    manual  upFastEthernet0/1  unassigned      YES    NRAM    Administatively down

Serial0/1        192.168.11.21   YES    manual  upUnder the show ip protocols output, you can see that we’re using RIP routing for net-work 10.0.0.0, which means our configuration would look like this:

Router(config)#router rip

Router(config-router)#network 10.0.0.0Also, only serial 0/0 and serial 0/1 are participating in the RIP network. And last, our neigh-bor router is 10.168.11.14.From the output of the show ip interface brief command, you can see that only serial 0/0 is in the 10.0.0.0 network. This means that the router will only send and receive routing updates with the 10.0.0.0 network and not advertise the 192.168.0.0 networks out any interface.The debug ip rip CommandThe debug ip rip command sends routing updates as they are sent and received on the router to the console session. If you are telnetted into the router, you’ll need to use the terminal monitor command to be able to receive the output from the debug commands.

10089c06.fm  Page 395  Thursday, August 30, 2007  12:42 PM




396Chapter6 IP RoutingWe can see in this output that RIP is both sending and receiving (the metric is the hop count):

R3#debug ip ripRIP protocol debugging is onR3#terminal monitor*Mar 17 19:08:34.371: RIP: sending v1 update to 255.255.255.255 via   Serial0/0/1 (10.1.5.2)*Mar 17 19:08:34.371: RIP: build update entries*Mar 17 19:08:34.371:   subnet 10.1.10.0 metric 1*Mar 17 19:08:34.371:   subnet 10.1.11.0 metric 1*Mar 17 19:08:34.371:   subnet 10.1.12.0 metric 2*Mar 17 19:08:40.107: RIP: received v1 update from 10.1.5.1 on   Serial0/0/1*Mar 17 19:08:40.107:      10.1.1.0 in 1 hops*Mar 17 19:08:40.107:      10.1.2.0 in 1 hops*Mar 17 19:08:40.107:      10.1.3.0 in 1 hops*Mar 17 19:08:40.107:      10.1.4.0 in 1 hops*Mar 17 19:08:40.107:      10.1.6.0 in 2 hops*Mar 17 19:08:40.107:      10.1.7.0 in 2 hops*Mar 17 19:08:40.107:      10.1.8.0 in 2 hops*Mar 17 19:08:40.107:      10.1.9.0 in 2 hops*Mar 17 19:08:47.535: RIP: sending v1 update to 255.255.255.255 via   FastEthernet0/1 (10.1.11.1)*Mar 17 19:08:47.535: RIP: build update entries*Mar 17 19:08:47.535:   subnet 10.1.1.0 metric 2*Mar 17 19:08:47.535:   subnet 10.1.2.0 metric 2*Mar 17 19:08:47.535:   subnet 10.1.3.0 metric 2*Mar 17 19:08:47.535:   subnet 10.1.4.0 metric 2*Mar 17 19:08:47.535:   subnet 10.1.5.0 metric 1*Mar 17 19:08:47.535:   subnet 10.1.6.0 metric 3*Mar 17 19:08:47.535:   subnet 10.1.7.0 metric 3*Mar 17 19:08:47.535:   subnet 10.1.8.0 metric 3*Mar 17 19:08:47.535:   subnet 10.1.9.0 metric 3*Mar 17 19:08:47.535:   subnet 10.1.10.0 metric 1*Mar 17 19:08:49.331: RIP: received v1 update from 10.1.11.2 on   FastEthernet0/1*Mar 17 19:08:49.331:      10.1.12.0 in 1 hopsR3#undeug all*Mar 17 19:08:47.535:   subnet 10.1.10.0 metric 1*Mar 17 19:08:49.331: RIP: received v1 update from 10.1.11.2 on

   FastEthernet0/1

10089c06.fm  Page 396  Thursday, August 30, 2007  12:42 PM




Verifying Your Configurations397Let’s talk about the parts I highlighted. First, RIP is sending v1 packet to 255.255.255.255—an “all-hands” broadcast—out interface serial0/0/1 via 10.1.5.2. This is where RIPv2 will come in handy. Why? Because RIPv2 doesn’t send broadcasts; it used the multicast 224.0.0.9. So even though the RIP packets could be transmitted onto a network with no routers, all hosts would just ignore them, making RIPv2 a bit of an improvement over RIPv1. On our R3, we are using the passive-interface so we are not sending broadcasts out to a LAN with no routers connected.Okay—now check out the fact that it’s sending advertisements for all networks except 10.1.11.0 and 10.1.12.0 out FastEthernet0/1, yet the last advertisement out serial0/0/1 is only advertising networks 10.1.10.0, 10.1.11.0, and 10.1.12.0. Why? If you answered split horizon rules, you nailed it! Our R3 router will not advertise all those networks received from the Corp router back to the Corp router.

If the metric of a route shows 16, this is a route poison, and the route being advertised is unreachable.Troubleshooting with the debug ip rip CommandNow let’s use the debug ip rip command to both discover a problem and figure out how RIP was configured on a router from a different sample network:

07:12:58: RIP: sending v1 update to 255.255.255.255 via  FastEthernet0/0 (172.16.1.1)07:12:58:  network 10.0.0.0, metric 107:12:58:  network 192.168.1.0, metric 207:12:58: RIP: sending v1 update to 255.255.255.255 via  Serial0/0 (10.0.8.1)07:12:58:  network 172.16.0.0, metric 107:12:58: RIP: Received v1 update from 10.0.15.2 n Serial0/007:12:58: 192.168.1.0 in one hop

07:12:58: 192.168.168.0 in 16 hops (inaccessible)You can see from the updates that we’re sending out information about networks 10.0.0.0, 192.168.1.0, and 172.16.0.0. But both the 10.0.0.0 network and the 172.16.0.0 network are being advertised with a hop count (metric) of 1, meaning that these networks are directly connected. The 192.168.1.0 is being advertised as a metric of 2, which means that it is not directly connected.For this to be happening, our configuration would have to look like this:

Router(config)#router ripRouter(config-router)#network 10.0.0.0

Router(config-router)#network 172.16.0.0

10089c06.fm  Page 397  Thursday, August 30, 2007  12:42 PM




398Chapter6 IP RoutingAnd there’s something else you can find out by looking at this: There are at least two routers participating in the RIP network because we’re sending out two interfaces but only receiving RIP updates on one interface. Also, notice that the network 192.168.168.0 is being advertised as 16 hops away. RIP has a maximum hop count of 15, so 16 is considered unreachable, making this network inaccessible. So what will happen if you try to ping to a host on network 192.168.168.0? You just will not be successful, that’s what! But if you try any pings to network 10.0.0.0, you should be successful.I have one more output I want to show you—see if you can find the problem. Both a debug ip rip and a show ip route output are shown from our sample router:

07:12:56: RIP: received v1 update from 172.16.100.2 on Serial0/007:12:56:      172.16.10.0 in 1 hops07:12:56:      172.16.20.0 in 1 hops07:12:56:      172.16.30.0 in 1 hopsRouter#sh ip route[output cut]Gateway of last resort is not set   172.16.0.0/24 is subnetted, 8 subnetsC  172.16.150.0 is directly connected, FastEthernet0/0C  172.16.220.0 is directly connected, Loopback2R  172.16.210.0 is directly connected, Loopback1R  172.16.200.0 is directly connected, Loopback0R  172.16.30.0 [120/2] via 172.16.100.2, 00:00:04, Serial0/0S  172.16.20.0 [120/2] via 172.16.150.15R  172.16.10.0 [120/2] via 172.16.100.2, 00:00:04, Serial0/0

R  172.16.100.0 [120/2] is directly connected, Serial0/0Looking at the two outputs, can you tell why users can’t access 172.16.20.0?The debug output shows that network 172.16.20.0 is one hop away and being received on serial0/0 from 172.16.100.2. By checking out the show ip route output, you can see that packets with a destination of 172.16.20.0 are being sent to 172.16.150.15 because of a static route. The output also shows that 172.16.150.0 is directly connected to FastEthernet 0/0 and network 172.16.20.0 is out serial 0/0.Enabling RIPv2 on Our InternetworkBefore we move on to Chapter 7 and configure EIGRP and OSPF, I want to enable RIPv2 on our routers. It’ll only take a second. Here are my configurations:

Corp#config tCorp(config)#router rip

10089c06.fm  Page 398  Thursday, August 30, 2007  12:42 PM




Verifying Your Configurations399Corp(config-router)#version 2Corp(config-router)#^ZR1#config tR1(config)#router ripR1(config-router)#version 2R1(config-router)#^ZR2#config tEnter configuration commands, one per line.  End with CNTL/Z.R2(config)#router ripR2(config-router)#version 2

R2(config-router)#^ZFor the R3 router, I just clicked the Version2 button and clicked OK. Done.

871W#config t871W#(config)#router rip871W#(config-router)#version 2

871W#(config-router)#^Z

10089c06.fm  Page 399  Thursday, August 30, 2007  12:42 PM




400Chapter6 IP RoutingThis was probably the easiest configuration we have done in the book so far. Let’s see if we can find a difference in our routing tables. Here’s the R3 router’s routing table now:

     10.0.0.0/24 is subnetted, 12 subnetsC       10.1.11.0 is directly connected, FastEthernet0/1C       10.1.10.0 is directly connected, FastEthernet0/0R       10.1.9.0 [120/2] via 10.1.5.1, 00:00:23, Serial0/0/1R       10.1.8.0 [120/2] via 10.1.5.1, 00:00:23, Serial0/0/1R       10.1.12.0 [120/1] via 10.1.11.2, 00:00:18, FastEthernet0/1R       10.1.3.0 [120/1] via 10.1.5.1, 00:00:23, Serial0/0/1R       10.1.2.0 [120/1] via 10.1.5.1, 00:00:23, Serial0/0/1R       10.1.1.0 [120/1] via 10.1.5.1, 00:00:23, Serial0/0/1R       10.1.7.0 [120/2] via 10.1.5.1, 00:00:23, Serial0/0/1R       10.1.6.0 [120/2] via 10.1.5.1, 00:00:23, Serial0/0/1C       10.1.5.0 is directly connected, Serial0/0/1R       10.1.4.0 [120/1] via 10.1.5.1, 00:00:23, Serial0/0/1

R3#Well—looks the same to me. I’m going to turn on debugging and see if that shows us any-thing new:

*Mar 17 19:34:00.123: RIP: sending v2 update to 224.0.0.9 via   Serial0/0/1 (10.1.5.2)*Mar 17 19:34:00.123: RIP: build update entries*Mar 17 19:34:00.123:   10.1.10.0/24 via 0.0.0.0, metric 1, tag 0*Mar 17 19:34:00.123:   10.1.11.0/24 via 0.0.0.0, metric 1, tag 0*Mar 17 19:34:00.123:   10.1.12.0/24 via 0.0.0.0, metric 2, tag 0col*Mar 17 19:34:03.795: RIP: received v2 update from 10.1.5.1 on   Serial0/0/1

[output cut]Bingo! Look at that! The networks are still being advertised every 30 seconds, but they’re now sending the advertisements as v2 and as a multicast address of 224.0.0.9. Let’s take a look at the show ip protocols output:

R3#sh ip protocolsRouting Protocol is "rip"  Outgoing update filter list for all interfaces is not set  Incoming update filter list for all interfaces is not set  Sending updates every 30 seconds, next due in 27 seconds  Invalid after 180 seconds, hold down 180, flushed after 240  Redistributing: rip  Default version control: send version 2, receive version 2

10089c06.fm  Page 400  Thursday, August 30, 2007  12:42 PM




Exam Essentials401    Interface             Send  Recv  Triggered RIP  Key-chain    FastEthernet0/1       2     2                                       Serial0/0/1           2     2                                     Automatic network summarization is not in effect  Maximum path: 4  Routing for Networks:    10.0.0.0  Passive Interface(s):    FastEthernet0/0    Serial0/0/0  Routing Information Sources:    Gateway         Distance      Last Update    10.1.11.2            120      00:00:00    10.1.5.1             120      00:00:02

  Distance: (default is 120)We are now sending and receiving RIPv2. Nice when things work out well, huh? You’re ready now to move on to the next chapter!SummaryThis chapter covered IP routing in detail. It’s extremely important that you really understand the basics we covered in this chapter because everything that’s done on a Cisco router typically will have some type of IP routing configured and running.You learned in this chapter how IP routing uses frames to transport packets between rout-ers and to the destination host. From there, we configured static routing on our routers and discussed the administrative distance used by IP to determine the best route to a destination network. If you have a stub network, you can configure default routing, which sets the gate-way of last resort on a router.We then discussed dynamic routing in detail, specifically RIP and how it works on an internet-work (not well). We finished by verifying RIP and then adding RIPv2 to our little internetwork.In the next chapter, we’ll continue on with dynamic routing by discussing EIGRP and OSPF.Exam EssentialsUnderstand the basic IP routing process.You need to remember that the frame changes at each hop but that the packet is never changed or manipulated in any way until it reaches the destination device.

10089c06.fm  Page 401  Thursday, August 30, 2007  12:42 PM




402Chapter6 IP RoutingUnderstand that MAC addresses are always local.A MAC (hardware) address will only be used on a local LAN. It will never pass a router’s interface.Understand that a frame carries a packet to only two places.A frame uses MAC (hard-ware) addresses to send a packet on a LAN. The frame will take the packet to either a host on the LAN or a router’s interface if the packet is destined for a remote networkUnderstand how to configure RIP routing.To configure RIP routing, first you must be in global configuration mode and then you type the command router rip. Then you add all directly connected networks, making sure to use the classful address.Remember how to verify RIP routing.The show ip route command will provide you with the contents of the routing table. An R on the left side of the table indicates a RIP-found route. The debug ip rip command will show you RIP updates being sent and received on your router. If you see a route with a metric of 16, that route is considered down.Remember the differences between RIPv1 and RIPv2.RIPv1 sends broadcasts every 30 sec-onds and has an AD of 120. RIPv2 sends multicasts (224.0.0.9) every 30 seconds and also has an AD of 120. RIPv2 sends subnet mask information with the route updates, which allows it to support classless networks and discontiguous networks. RIPv2 also supports authentica-tion between routers and RIPv1 does not.Written Lab 6Write the answers to the following questions:1.Create a static route to network 172.16.10.0/24 with a next-hop gateway of 172.16.20.1 and an administrative distance of 150.2.From the SDM you have just enabled RIP and the passive-interface box for your serial interface is unchecked. What does this mean?3.What command will you type to create a default route to 172.16.40.1?4.If you are using default routing, what command must also be used?5.You would use a default route on which type of network?6.To see the routing table on your router, what command will you use?7.When creating a static or default route, you don’t have to use the next-hop IP address; you can use the ___________________.8.True/False: To reach a destination host, you must know the MAC address of the remote host.9.True/False: To reach a destination host, you must know the IP address of the remote host.10.If you have a DCE serial interface, what command must you enter for that interface to work?

10089c06.fm  Page 402  Thursday, August 30, 2007  12:42 PM




Hands-on Labs40311.Write the commands used to turn RIP routing on in a router and advertise network 10.0.0.0.12.Write the commands to stop a router from propagating RIP information out serial 1.13.What works with triggered updates to help stop routing loops in distance-vector networks?14.What stops routing loops in distance-vector networks by sending out a maximum hop count as soon as a link fails?15.What stops routing loops in distance-vector networks by not resending information learned on an interface out that same interface?16.What command is used to send RIP routing updates as they are sent and received on the router to the console session? (The answers to Written Lab 6 can be found following the answers to the review questions for this chapter.)Hands-on LabsIn the following hands-on labs, you will configure a network with three routers.

The hands-on labs in this section is included for use with real Cisco routers. If you are using software from RouterSim or Sybex, please use the hands-on labs found in those programs.This chapter includes:Lab 6.1: Creating Static RoutesLab 6.2: Configuring RIP RoutingFigure 6.17 will be used to configure all routers.FIGURE6.17Hands-on lab internetwork

Lab_A Lab_B Lab_C fa0/0 fa0/0 s0/0 (DCE) s0/0 s0/1 (DCE) s0/0 

10089c06.fm  Page 403  Thursday, August 30, 2007  12:42 PM




404Chapter6 IP RoutingTable 6.5 shows our IP addresses for each router (each interface uses a /24 mask).These labs were written without using the LAN interface on the Lab_B router. You can choose to add that LAN into the labs if necessary.Hands-on Lab 6.1: Creating Static RoutesIn this lab, you will create a static route in all three routers so that the routers see all networks. Verify with the Ping program when complete.1.The Lab_A router is connected to two networks, 172.16.10.0 and 172.16.20.0. You need to add routes to networks 172.16.30.0 and 172.16.40.0.Lab_A#config tLab_A(config)#ip route 172.16.30.0 255.255.255.0  172.16.20.2Lab_A(config)#ip route 172.16.40.0 255.255.255.0

  172.16.20.22.Save the current configuration for the Lab_A router by going to the privileged mode, typing copy run start, and pressing Enter.3.On the Lab_B router, you have direct connections to networks 172.16.20.0 and 172.16.30.0. You need to add routes to networks 172.16.10.0 and 172.16.40.0.Lab_B#config tLab_B(config)#ip route 172.16.10.0 255.255.255.0  172.16.20.1Lab_B(config)#ip route 172.16.40.0 255.255.255.0

  172.16.30.2TABLE6.5Our IP AddressesRouterInterfaceIP AddressLab_AF0/0172.16.10.1Lab_AS0/0172.16.20.1Lab_BS0/0172.16.20.2Lab_BS0/1172.16.30.1Lab_CS0/0172.16.30.2Lab_CFa0/0172.16.40.1

10089c06.fm  Page 404  Thursday, August 30, 2007  12:42 PM




 Hands-on Labs 405 4. Save the current configuration for router Lab_B by going to the enabled mode, typing  copy run start , and pressing Enter. 5. On Router Lab_C, create a static route to see networks 172.16.10.0 and 172.16.20.0, which are not directly connected. Create static routes so that Router Lab_C can see all networks, as shown here: Lab_C# config t Lab_C(config)# ip route 172.16.10.0 255.255.255.0  172.16.30.1 Lab_C(config)# ip route 172.16.20.0 255.255.255.0

   172.16.30.1 6. Save the current configuration for Router Lab_C by going to the enable mode, typing  copy run start , and pressing Enter. 7. Check your routing tables to make sure all four networks show up. 8. Now ping from each router to your hosts and from each router to each router. If it is set up correctly, it will work. Hands-on Lab 6.2: Configuring RIP Routing In this lab, we will use the dynamic routing protocol RIP instead of static routing. 1. Remove any static routes or default routes configured on your routers by using the  no   ip   route  command. For example, here is how you would remove the static routes on the Lab_A router: Lab_A# config t Lab_A(config)# no ip route 172.16.30.0 255.255.255.0  172.16.20.2 Lab_A(config)# no ip route 172.16.40.0 255.255.255.0

   172.16.20.2 Do the same thing for routers Lab_B and Lab_C. Verify that only your directly connected networks are in the routing tables. 2. After your static and default routes are clear, go into configuration mode on Router Lab_A by typing  config   t . 3. Tell your router to use RIP routing by typing  router rip  and pressing Enter, as shown here: config t

 router rip 4. Add the network number you want to advertise by typing  network 172.16.0.0  and pressing Enter. 5. Press Ctrl+Z to get out of configuration mode.

 

10089c06.fm  Page 405  Friday, November 7, 2008  11:00 PM




406Chapter6 IP Routing6.Go to routers Lab_B and Lab_C and type the same commands, as shown here:Config tRouter rip

network 172.16.0.07.Verify that RIP is running at each router by typing the following commands at each router:show ip protocolsshow ip route

show running-config or show run8.Save your configurations by typing copy run start or copy running-config startup-config and pressing Enter at each router.9.Verify the network by pinging all remote networks and hosts.

10089c06.fm  Page 406  Thursday, August 30, 2007  12:42 PM




 Review Questions 407 Review Questions

 The following questions are designed to test your understanding of this chapter’s material. For more information on how to get additional ques- tions, please see this book’s Introduction. 1. Network 206.143.5.0 was assigned to the Acme Company to connect to its ISP. The admin-istrator of Acme would like to configure one router with the commands to access the Internet. Which commands could be configured on the Gateway router to allow Internet access to the entire network? (Choose two.) A. Gateway(config)# ip route 0.0.0.0 0.0.0.0 206.143.5.2 B. Gateway(config)# router rip Gateway(config-router)# network 206.143.5.0 C. Gateway(config)# router rip Gateway(config-router)# network 206.143.5.0 default D. Gateway(config)# ip route 206.143.5.0 255.255.255.0 default E. Gateway(config)# ip default-network 206.143.5.0 2. What command is used to stop RIP routing updates from exiting out an interface but still allow the interface to receive RIP route updates? A. Router(config-if)# no routing B. Router(config-if)# passive-interface C. Router(config-router)# passive-interface s0 D. Router(config-router)# no routing updates 3. Which of the following statements are true regarding the command  ip route 172.16.4.0 255.255.255.0 192.168.4.2 ? (Choose two.) A. The command is used to establish a static route. B. The default administrative distance is used. C. The command is used to configure the default route. D. The subnet mask for the source address is 255.255.255.0. E. The command is used to establish a stub network.

 

10089c06.fm  Page 407  Thursday, February 28, 2008  7:50 AM




408Chapter6 IP Routing4.What destination addresses will be used by Host_A to send data to the HTTPS server as shown in the following network? (Choose two.)A.The IP address of the switchB.The MAC address of the remote switchC.The IP address of the HTTPS serverD.The MAC address of the HTTPS serverE.The IP address of RouterA’s Fa0/0 interfaceF.The MAC address of RouterA’s Fa0/0 interface5.Which of the following is true regarding the following output? (Choose two.)04:06:16: RIP: received v1 update from 192.168.40.2 on Serial0/104:06:16:      192.168.50.0 in 16 hops (inaccessible)04:06:40: RIP: sending v1 update to 255.255.255.255 via   FastEthernet0/0 (192.168.30.1)04:06:40: RIP: build update entries04:06:40:       network 192.168.20.0 metric 104:06:40:       network 192.168.40.0 metric 104:06:40:       network 192.168.50.0 metric 1604:06:40: RIP: sending v1 update to 255.255.255.255 via Serial0/1

   (192.168.40.1)A.There are three interfaces on the router participating in this update.B.A ping to 192.168.50.1 will be successful.C.There are at least two routers exchanging information.D.A ping to 192.168.40.2 will be successful.6.What is split horizon?A.Information about a route should not be sent back in the direction from which the original update came.B.It splits the traffic when you have a large bus (horizon) physical network.C.It holds the regular updates from broadcasting to a downed link.D.It prevents regular update messages from reinstating a route that has gone down.

Fa0/0

RouterAHostA

Fa0/1

HTTPS Server

10089c06.fm  Page 408  Thursday, August 30, 2007  12:42 PM




Review Questions4097.Which of the following would be true if HostA is trying to communicate to HostB and interface F0/0 of RouterC goes down? (Choose two.)A.RouterC will use an ICMP to inform HostA that HostB cannot be reached.B.RouterC will use ICMP to inform RouterB that HostB cannot be reached.C.RouterC will use ICMP to inform HostA, RouterA, and RouterB that HostB cannot be reached.D.RouterC will send a destination unreachable message type.E.RouterC will send a router selection message type.F.RouterC will send a source quench message type.8.Which statement is true regarding classless routing protocols? (Choose two.)A.The use of discontiguous networks is not allowed.B.The use of variable length subnet masks is permitted.C.RIPv1 is a classless routing protocol.D.IGRP supports classless routing within the same autonomous system.E.RIPv2 supports classless routing.9.Which two of the following are true regarding the distance-vector and link-state routing protocols?A.Link state sends its complete routing table out all active interfaces on periodic time intervals.B.Distance vector sends its complete routing table out all active interfaces on periodic time intervals.C.Link state sends updates containing the state of its own links to all routers in the internetwork.D.Distance vector sends updates containing the state of its own links to all routers in the internetwork.10.Which command displays RIP routing updates?A.show ip routeB.debug ip ripC.show protocolsD.debug ip route

RouterA

RouterB

RouterC

HostA

HostB

10089c06.fm  Page 409  Thursday, August 30, 2007  12:42 PM




410Chapter6 IP Routing11.What does RIPv2 use to prevent routing loops? (Choose two.)A.CIDRB.Split horizonC.AuthenticationD.Classless maskingE.Holddown timers12.A network administrator views the output from the show ip route command. A network that is advertised by both RIP and IGRP appears in the routing table flagged as an IGRP route. Why is the RIP route to this network not used in the routing table?A.IGRP has a faster update timer.B.IGRP has a lower administrative distance.C.RIP has a higher metric value for that route.D.The IGRP route has fewer hops.E.The RIP path has a routing loop.13.You type debug ip rip on your router console and see that 172.16.10.0 is being advertised to you with a metric of 16. What does this mean?A.The route is 16 hops away.B.The route has a delay of 16 microseconds.C.The route is inaccessible.D.The route is queued at 16 messages a second.14.IGRP uses which of the following as default parameters for finding the best path to a remote network? (Choose two.)A.Hop countB.MTUC.Cumulative interface delayD.STPE.Path bandwidth value15.The Corporate router receives an IP packet with a source IP address of 192.168.214.20 and a destination address of 192.168.22.3. Looking at the output from the Corporate router, what will the router do with this packet?Corp#sh ip route[output cut]R    192.168.215.0 [120/2] via 192.168.20.2, 00:00:23, Serial0/0R    192.168.115.0 [120/1] via 192.168.20.2, 00:00:23, Serial0/0R    192.168.30.0 [120/1] via 192.168.20.2, 00:00:23, Serial0/0C    192.168.20.0 is directly connected, Serial0/0

C    192.168.214.0 is directly connected, FastEthernet0/0

10089c06.fm  Page 410  Thursday, August 30, 2007  12:42 PM




Review Questions411A.The packet will be discarded.B.The packet will be routed out the S0/0 interface.C.The router will broadcast looking for the destination.D.The packet will be routed out the Fa0/0 interface.16.If your routing table has a static, a RIP, and an IGRP route to the same network, which route will be used to route packets by default?A.Any available routeB.RIP routeC.Static routeD.IGRP routeE.They will all load-balance.17.You have the following routing table. Which of the following networks will not be placed in the neighbor routing table?R    192.168.30.0/24 [120/1] via 192.168.40.1, 00:00:12, Serial0C    192.168.40.0/24 is directly connected, Serial0     172.16.0.0/24 is subnetted, 1 subnetsC       172.16.30.0 is directly connected, Loopback0R    192.168.20.0/24 [120/1] via 192.168.40.1, 00:00:12, Serial0R    10.0.0.0/8 [120/15] via 192.168.40.1, 00:00:07, Serial0

C    192.168.50.0/24 is directly connected, Ethernet0A.172.16.30.0B.192.168.30.0C.10.0.0.0D.All of them will be placed in the neighbor routing table.18.Two connected routers are configured with RIP routing. What will be the result when a router receives a routing update that contains a higher-cost path to a network already in its routing table?A.The updated information will be added to the existing routing table.B.The update will be ignored and no further action will occur.C.The updated information will replace the existing routing table entry.D.The existing routing table entry will be deleted from the routing table and all routers will exchange routing updates to reach convergence.19.What is route poisoning?A.It sends back the protocol received from a router as a poison pill, which stops the regular updates.B.It is information received from a router that can’t be sent back to the originating router.C.It prevents regular update messages from reinstating a route that has just come up.D.It describes when a router sets the metric for a downed link to infinity.

10089c06.fm  Page 411  Thursday, August 30, 2007  12:42 PM




412Chapter6 IP Routing20.Which of the following is true regarding RIPv2?A.It has a lower administrative distance than RIPv1.B.It converges faster than RIPv1.C.It has the same timers as RIPv1.D.It is harder to configure than RIPv1.

10089c06.fm  Page 412  Thursday, August 30, 2007  12:42 PM




 Answers to Review Questions 413 Answers to Review Questions 1. A, E. There are actually three different ways to configure the same default route, but only two are shown in the answer. First, you can set a default route with the 0.0.0.0 0.0.0.0 mask and then specify the next hop, as in answer A. Or you can use 0.0.0.0 0.0.0.0 and use the exit interface instead of the next hop. Finally, you can use answer E with the  ip default-network  command. 2. C. The  (config-router)# passive-interface  command stops updates from being sent out an interface, but route updates are still received. 3. A, B. Although answer D almost seems right, it is not; the mask is the mask used on the remote network, not the source network. Since there is no number at the end of the static route, it is using the default administrative distance of 1. 4. C, F. The switches are not used as either a default gateway or other destination. Switches have nothing to do with routing. It is very important to remember that the destination MAC address will always be the router’s interface. The destination address of a frame, from HostA, will be the MAC address of the F0/0 interface of RouterA. The destination address of a packet will be the IP address of the network interface card (NIC) of the HTTPS server. The destination port number in the segment header will have a value of 443 (HTTPS). 5. C, D. The route to 192.168.50.0 is unreachable and only interfaces s0/1 and FastEthernet 0/0 are participating in the RIP update. Since a route update was received, at least two routers are participating in the RIP routing process. Since a route update for network 192.168.40.0 is being sent out f0/0 and a route was received from 192.168.40.2, we can assume a ping to that address will be successful. 6. A. A split horizon will not advertise a route back to the same router it learned the route from. 7. A, D. RouterC will use ICMP to inform HostA that HostB cannot be reached. It will perform this by sending a destination unreachable ICMP message type. 8. B, E. Classful routing means that all hosts in the internetwork use the same mask. Classless routing means that you can use Variable Length Subnet Masks (VLSMs) and can also support discontiguous networking. 9. B, C. The distance-vector routing protocol sends its complete routing table out all active inter-faces at periodic time intervals. Link-state routing protocols send updates containing the state of its own links to all routers in the internetwork. 10. B.  Debug ip rip  is used to show the Internet Protocol (IP) Routing Information Protocol (RIP) updates being sent and received on the router. 11. B, E. RIPv2 uses the same timers and loop-avoidance schemes as RIPv1. Split horizon is used to stop an update from being sent out the same interface it was received on. Holddown timers allow time for a network to become stable in the case of a flapping link. 12. B. RIP has an administrative distance (AD) of 120, while IGRP has an administrative distance of 100, so the router will discard any route with a higher AD than 100. 13. C. You cannot have 16 hops on a RIP network by default. If you receive a route advertised with a metric of 16, this means it is inaccessible.

 

10089c06.fm  Page 413  Monday, March 17, 2008  8:28 AM




414Chapter6 IP Routing14.C, E. IGRP uses bandwidth and delay of the line, by default, to determine the best path to a remote network. Delay of the line can sometimes be called the cumulative interface delay.15.A. Since the routing table shows no route to the 192.168.22.0 network, the router will discard the packet and send an ICMP destination unreachable message out interface FastEthernet 0/0, which is the source LAN where the packet originated from.16.C. Static routes have an administrative distance of 1 by default. Unless you change this, a static route will always be used over any other found route. IGRP has an administrative distance of 100, and RIP has an administrative distance of 120, by default.17.C. The network 10.0.0.0 cannot be placed in the next router’s routing table because it already is at 15 hops. One more hop would make the route 16 hops, and that is not valid in RIP networking.18.B. When a routing update is received by a router, the router first checks the administrative distance (AD) and always chooses the route with the lowest AD. However, if two routes are received and they both have the same AD, then the router will choose the one route with the lowest metrics, or in RIP’s case, hop count.19.D. Another way to avoid problems caused by inconsistent updates and to stop network loops is route poisoning. When a network goes down, the distance-vector routing protocol initiates route poisoning by advertising the network with a metric of 16, or unreachable (sometimes referred to as infinite).20.C. RIPv2 is pretty much just like RIPv1. It has the same administrative distance and timers and is configured just like RIPv1.

10089c06.fm  Page 414  Thursday, August 30, 2007  12:42 PM




 Answers to Written Lab 6 415 Answers to Written Lab 6 1. ip route 172.16.10.0 255.255.255.0 172.16.20.1 150 2. If the box next to an interface is unchecked, this means that passive-interface will not be used and RIP will be sent and received on that interface. 3. ip route 0.0.0.0 0.0.0.0 172.16.40.1 4. Router(config)# ip classless 5. Stub network 6. Router# show ip route 7. Exit interface 8. False. The MAC address would be the router interface, not the remote host. 9. True 10. Router(config-if)# clock rate  speed 11. Router rip, network 10.0.0.0 12. Router rip, passive-interface s1 13. Holddown timers 14. Route poisoning 15. Split horizon 16. debug ip rip

 

10089c06.fm  Page 415  Wednesday, February 27, 2008  5:03 PM




10089c06.fm  Page 416  Thursday, August 30, 2007  12:42 PM




 

Chapter 7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Configure, verify, and troubleshoot basic router operation and routing on Cisco devices  Access and utilize the router to set basic parameters (including: CLI/SDM)   Connect, configure, and verify operation status of a device interface   Verify device configuration and network connectivity using ping, traceroute, telnet, SSH or other utilities   Perform and verify routing configuration tasks for a static or default route given specific routing requirements   Compare and contrast methods of routing and routing protocols   Configure, verify, and troubleshoot OSPF   Configure, verify, and troubleshoot EIGRP   Verify network connectivity (including: using ping, traceroute, and telnet or SSH)   Troubleshoot routing issues   Verify router hardware and software operation using SHOW & DEBUG commands   Implement basic router security 

 

10089.book  Page 417  Monday, July 23, 2007  3:17 PM




 Enhanced Interior Gateway Routing Protocol (EIGRP) is a pro-prietary Cisco protocol that runs on Cisco routers. It is important for you to understand EIGRP because it is probably one of the two most popular routing protocols in use today. In this chapter, I’ll show you the many features of EIGRP and describe how it works, with particular focus on the unique way it discovers, selects, and advertises routes.I’m also going to introduce you to the Open Shortest Path First (OSPF) routing protocol, which is the other popular routing protocol in use today. You’ll build a solid foundation for understanding OSPF by first becoming familiar with the terminology and internal operation of it and then learning about OSPF’s advantages over RIP. Next, we’ll explore the issues surrounding implementations of OSPF in broadcast and non-broadcast networks of various types. I’ll explain how to implement single-area OSPF in different and specific networking environments and demonstrate how to verify that everything is running smoothly.

 For up-to-the minute updates for this chapter, please see  www.lammle.com   and/or  www.sybex.com . EIGRP Features and Operation Enhanced IGRP (EIGRP)  is a classless, enhanced distance-vector protocol that gives us a real edge over another Cisco proprietary protocol, Interior Gateway Routing Protocol (IGRP). That’s basi-cally why it’s called Enhanced IGRP. Like IGRP, EIGRP uses the concept of an autonomous sys-tem to describe the set of contiguous routers that run the same routing protocol and share routing information. But unlike IGRP, EIGRP includes the subnet mask in its route updates. And as you now know, the advertisement of subnet information allows us to use Variable Length Subnet Masks (VLSMs) and summarization when designing our networks!EIGRP is sometimes referred to as a  hybrid routing protocol  because it has characteristics of both distance-vector and link-state protocols. For example, EIGRP doesn’t send link-state packets as OSPF does; instead, it sends traditional distance-vector updates containing information about networks plus the cost of reaching them from the perspective of the advertising router. And EIGRP has link-state characteristics as well—it synchronizes routing tables between neighbors at startup and then sends specific updates only when topology changes occur. This makes EIGRP suitable for very large networks. EIGRP has a maximum hop count of 255 (the default is set to 100).

 

10089.book  Page 418  Monday, July 23, 2007  3:17 PM




 EIGRP Features and Operation 419 There are a number of powerful features that make EIGRP a real standout from IGRP and other protocols. The main ones are listed here:  Support for IP and IPv6 (and some other useless routed protocols) via protocol-dependent modules  Considered classless (same as RIPv2 and OSPF)  Support for VLSM/CIDR  Support for summaries and discontiguous networks  Efficient neighbor discovery  Communication via Reliable Transport Protocol (RTP)  Best path selection via Diffusing Update Algorithm (DUAL)

 Cisco calls EIGRP a distance-vector routing protocol or sometimes an  advanced distance-vector or even a hybrid routing protocol. Protocol-Dependent Modules One of the most interesting features of EIGRP is that it provides routing support for multiple Network layer protocols: IP, IPX, AppleTalk, and now IPv6. (Obviously we won’t use IPX and AppleTalk, but EIGRP does support them.) The only other routing protocol that comes close and supports multiple network layer protocols is  Intermediate System-to-Intermediate System (IS-IS) .EIGRP supports different Network layer protocols through the use of  protocol-dependent modules (PDMs) . Each EIGRP PDM will maintain a separate series of tables containing the routing information that applies to a specific protocol. What this means to you is that there will be IP/EIGRP tables and IPv6/EIGRP tables, for example. Neighbor Discovery Before EIGRP routers are willing to exchange routes with each other, they must become neighbors. There are three conditions that must be met for neighborship establishment:  Hello or ACK received  AS numbers match  Identical metrics (K values)Link-state protocols tend to use Hello messages to establish neighborship (also called adjacencies) because they normally do not send out periodic route updates and there has to be some mechanism to help neighbors realize when a new peer has moved in or an old one has left or gone down. To maintain the neighborship relationship, EIGRP routers must also continue receiving Hellos from their neighbors.

 

10089.book  Page 419  Monday, July 23, 2007  3:17 PM




 420 Chapter7  Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF) EIGRP routers that belong to different autonomous systems (ASes) don’t automatically share routing information and they don’t become neighbors. This behavior can be a real ben-efit when used in larger networks to reduce the amount of route information propagated through a specific AS. The only catch is that you might have to take care of redistribution between the different ASes manually.The only time EIGRP advertises its entire routing table is when it discovers a new neighbor and forms an adjacency with it through the exchange of Hello packets. When this happens, both neighbors advertise their entire routing tables to one another. After each has learned its neighbor’s routes, only changes to the routing table are propagated from then on.When EIGRP routers receive their neighbors’ updates, they store them in a local topology table. This table contains all known routes from all known neighbors and serves as the raw material from which the best routes are selected and placed into the routing table.Let’s define some terms before we move on: Feasible distance This is the best metric along all paths to a remote network, including the metric to the neighbor that is advertising that remote network. This is the route that you will find in the routing table because it is considered the best path. The metric of a feasible distance is the metric reported by the neighbor (called reported or advertised distance) plus the metric to the neighbor reporting the route. Reported/advertised distance This is the metric of a remote network, as reported by a neighbor. It is also the routing table metric of the neighbor and is the same as the second number in paren-theses as displayed in the topology table, the first number being the feasible distance. Neighbor table Each router keeps state information about adjacent neighbors. When a newly discovered neighbor is learned, the address and interface of the neighbor are recorded, and this information is held in the neighbor table, stored in RAM. There is one neighbor table for each protocol-dependent module. Sequence numbers are used to match acknowledgments with update packets. The last sequence number received from the neighbor is recorded so that out-of-order packets can be detected. Topology table The topology table is populated by the protocol-dependent modules and acted upon by the Diffusing Update Algorithm (DUAL). It contains all destinations advertised by neighboring routers, holding each destination address and a list of neighbors that have advertised the destination. For each neighbor, the advertised metric, which comes only from the neighbor’s routing table, is recorded. If the neighbor is advertising this destination, it must be using the route to forward packets.

 The neighbor and topology tables are stored in RAM and maintained through the use of Hello and update packets. Yes, the routing table is also stored in  RAM, but that information is gathered only from the topology table. Feasible successor A feasible successor is a path whose reported distance is less than the feasible distance, and it is considered a backup route. EIGRP will keep up to six feasible successors in the topology table. Only the one with the best metric (the successor) is copied and placed in the routing table. The  show ip eigrp topology  command will display all the EIGRP feasible successor routes known to a router.

 

10089.book  Page 420  Monday, July 23, 2007  3:17 PM




 EIGRP Features and Operation 421

 A feasible successor is a backup route and is stored in the topology table. A successor route is stored in the topology table and is copied and placed in the  routing table. Successor A successor route (think successful!) is the best route to a remote network. A suc-cessor route is used by EIGRP to forward traffic to a destination and is stored in the routing table. It is backed up by a feasible successor route that is stored in the topology table—if one is available.By using the feasible distance, and having feasible successors in the topology table as backup links, the network can converge instantly, and updates to any neighbor make up the only traffic sent from EIGRP. Reliable Transport Protocol (RTP) EIGRP uses a proprietary protocol called  Reliable Transport Protocol (RTP)  to manage the communication of messages between EIGRP-speaking routers. And as the name suggests, reliability is a key concern of this protocol. Cisco has designed a mechanism that leverages multicasts and unicasts to deliver updates quickly and to track the receipt of the data.When EIGRP sends multicast traffic, it uses the Class D address 224.0.0.10. As I said, each EIGRP router is aware of who its neighbors are, and for each multicast it sends out, it maintains a list of the neighbors who have replied. If EIGRP doesn’t get a reply from a neighbor, it will switch to using unicasts to resend the same data. If it still doesn’t get a reply after 16 unicast attempts, the neighbor is declared dead. People often refer to this process as  reliable multicast .Routers keep track of the information they send by assigning a sequence number to each packet. With this technique, it’s possible for them to detect the arrival of old, redundant, or out-of-sequence information.Being able to do these things is highly important because EIGRP is a quiet protocol. It depends upon its ability to synchronize routing databases at startup time and then maintain the consistency of databases over time by only communicating any changes. So the permanent loss of any packets, or the out-of-order execution of packets, can result in corruption of the routing database. Diffusing Update Algorithm (DUAL) EIGRP uses  Diffusing Update Algorithm (DUAL)  for selecting and maintaining the best path to each remote network. This algorithm allows for the following:  Backup route determination if one is available  Support of VLSMs  Dynamic route recoveries  Queries for an alternate route if no route can be found

 

10089.book  Page 421  Monday, July 23, 2007  3:17 PM




 422 Chapter7  Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF) DUAL provides EIGRP with possibly the fastest route convergence time among all protocols. The key to EIGRP’s speedy convergence is twofold: First, EIGRP routers maintain a copy of all of their neighbors’ routes, which they use to calculate their own cost to each remote network. If the best path goes down, it may be as simple as examining the contents of the topology table to select the best replacement route. Second, if there isn’t a good alternative in the local topology table, EIGRP routers very quickly ask their neighbors for help finding one—they aren’t afraid to ask directions! Relying on other routers and leveraging the information they provide accounts for the “diffusing” character of DUAL.And as I said, the whole idea of the Hello protocol is to enable the rapid detection of new or dead neighbors. RTP answers this call by providing a reliable mechanism for conveying and sequencing messages. Building upon this solid foundation, DUAL is responsible for selecting and maintaining information about the best paths. Using EIGRP to Support Large Networks EIGRP includes a bunch of cool features that make it suitable for use in large networks:  Support for multiple ASes on a single router  Support for VLSM and summarization  Route discovery and maintenanceEach of these capabilities adds one small piece to the complex puzzle of supporting a huge number of routers and multiple networks. Multiple ASes EIGRP uses autonomous system numbers to identify the collection of routers that share route information. Only routers that have the same autonomous system numbers share routes. In large networks, you can easily end up with really complicated topology and route tables, and that can markedly slow convergence during diffusing computation operations.So what’s an administrator to do to mitigate the impact of managing really big networks? Well, it’s possible to divide the network into multiple distinct EIGRP autonomous systems, or ASes. Each AS is populated by a contiguous series of routers, and route information can be shared among the different ASes via redistribution.The use of redistribution within EIGRP leads us to another interesting feature. Normally, the administrative distance (AD) of an EIGRP route is 90, but this is true only for what is known as an  internal EIGRP route . These are routes originated within a specific autonomous system by EIGRP routers that are members of the same autonomous system. The other type of route is called an  external EIGRP route  and has an AD of 170, which is not so good. These routes appear within EIGRP route tables courtesy of either manual or automatic redistribu-tion, and they represent networks that originated outside of the EIGRP autonomous system. And it doesn’t matter if the routes originated from another EIGRP autonomous system or from another routing protocol such as OSPF—they’re all considered to be external routes when redistributed within EIGRP.

 

10089.book  Page 422  Monday, July 23, 2007  3:17 PM




 Using EIGRP to Support Large Networks 423 I have some good news regarding EIGRP and redistribution. Let’s say you have an existing company that has IGRP running on all the routers. You have just been hired as the network administrator and have decided to run EIGRP on the network because you read my book and know the many benefits of EIGRP over IGRP.Since you need to migrate slowly over to EIGRP and cannot change all the routers simul-taneously, you need to configure redistribution—right? Not with EIGRP! As long as you use the same autonomous system number for EIGRP that you used for IGRP, EIGRP will auto-matically redistribute the routes from IGRP into EIGRP. Of course, EIGRP will see these as external routes (AD of 170), so this isn’t something you want to use forever. You want to migrate as quickly as possible, but because of this automatic redistribution feature, you don’t have to migrate in one weekend. VLSM Support and Summarization As one of the more sophisticated classless routing protocols, EIGRP supports the use of Variable Length Subnet Masks. This is really important because it allows for the conserva-tion of address space through the use of subnet masks that more closely fit the host require-ments, such as using 30-bit subnet masks for point-to-point networks. And because the subnet mask is propagated with every route update, EIGRP also supports the use of discon-tiguous subnets, something that gives us a lot more flexibility when designing the network’s IP address plan.What’s a discontiguous network? It’s one that has two or more subnetworks of a classful network connected together by different classful networks. Figure 7.1 displays a typical dis-contiguous network.The subnets 172.16.10.0 and 172.16.20.0 are connected together with a 10.3.1.0 network. By default, each router thinks it has the only 172.16.0.0 classful network. FIGURE7.1 A discontiguous network

172.16.20.1/24 E0Host_B172.16.20.2/24

Lab_A

172.16.10.1/24 E0S0S0

Lab_BHost_A172.16.10.2/24172.16.10.0/24172.16.20.0/2410.3.1.2/2410.3.1.1/2410.3.1.0/24

 

10089.book  Page 423  Monday, July 23, 2007  3:17 PM




 424 Chapter7  Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF) It’s important to understand that discontiguous networks just won’t work with RIPv1 or IGRP at all. And they don’t work by default on RIPv2 or EIGRP either, but discontiguous net-works do work on OSPF networks by default because OSPF does not auto-summarize like EIGRP. But no worries about EIGRP—there are ways to make EIGRP work. I’ll show you how to do that a bit later in this chapter.EIGRP also supports the manual creation of summaries at any and all EIGRP routers, which can substantially reduce the size of the route table. However, EIGRP automatically summarizes networks at their classful boundaries, and Figure 7.2 shows how a router running EIGRP would see the network plus the boundaries that it would auto-summarize. FIGURE7.2 EIGRP auto-summarization Obviously, this would never work by default! Make a note to yourself that RIPv1, RIPv2, and IGRP would also auto-summarize these same classful boundaries by default, but OSPF won’t.

 RIPv2 and EIGRP support discontiguous networking, but not by default. OSPF does support discontiguous networking by default because it doesn’t auto- summarize classful boundaries as RIPv2 and EIGRP do. Route Discovery and Maintenance The hybrid nature of EIGRP is fully revealed in its approach to route discovery and maintenance. Like many link-state protocols, EIGRP supports the concept of neighbors that are discovered via a Hello process and whose states are monitored. Like many distance-vector protocols, EIGRP uses the routing-by-rumor mechanism I talked about earlier that implies many routers never hear about a route update firsthand. Instead, they hear about it from another router that may also have heard about it from another one, and so on.

172.16.20.1/24 E0Host_B172.16.20.2/24

Lab_A

172.16.10.1/24 E0S0S0

Lab_BHost_A172.16.10.2/24172.16.10.0/24172.16.20.0/2410.3.1.2/24Network 172.16.0.0is over here!10.3.1.1/24Network 172.16.0.0is over here!10.3.1.0/24

 

10089.book  Page 424  Monday, July 23, 2007  3:17 PM




 Using EIGRP to Support Large Networks 425 Given the huge amount of information that EIGRP routers have to collect, it makes sense that they have a place to store it, right? Well they do—EIGRP uses a series of tables to store important information about its environment: Neighborship table The  neighborship table  (usually referred to as the neighbor table) records information about routers with whom neighborship relationships have been formed. Topology table The  topology table  stores the route advertisements about every route in the internetwork received from each neighbor. Route table The  route table  stores the routes that are currently used to make routing decisions. There would be separate copies of each of these tables for each protocol that is actively being supported by EIGRP, whether it’s IP or IPv6.I am now going to discuss the EIGRP metrics and then move right into the  easy  configura-tion of EIGRP. EIGRP Metrics Another really sweet thing about EIGRP is that unlike many other protocols that use a single factor to compare routes and select the best possible path, EIGRP can use a combination of four:  Bandwidth  Delay  Load  Reliability Like IGRP, EIGRP uses only bandwidth and delay of the line to determine the best path to a remote network by default. Cisco sometimes likes to call these  path bandwidth value and cumulative line delay—go figure.And it’s worth noting that there’s a fifth element, maximum transmission unit (MTU) size. This element has never been used in EIGRP calculations, but it’s a required parameter in some EIGRP-related commands, especially those involving redistribution. The value of the MTU element rep-resents the smallest MTU value encountered along the path to the destination network.Maximum Paths and Hop CountBy default, EIGRP can provide equal-cost load balancing of up to four links (actually, all routing protocols do this). However, you can have EIGRP actually load-balance across up to six links (equal or unequal) by using the following command:

Pod1R1(config)#router eigrp 10Pod1R1(config-router)#maximum-paths ?

  <1-6>  Number of pathsIn addition, EIGRP has a maximum hop count of 100, but it can be set up to 255. Chances are you wouldn’t want to ever change this, but if you did, here is how you would do it:

Pod1R1(config)#router eigrp 10Pod1R1(config-router)#metric maximum-hops ?

  <1-255>  Hop count

10089.book  Page 425  Monday, July 23, 2007  3:17 PM




426Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)As you can see from this router output, EIGRP can be set to a maximum of 255 hops, and even though it doesn’t use hop count in the path metric calculation, it still uses the maximum hop count to limit the scope of the AS.

By default, all routing protocols can load-balance across four equal-cost links. However, EIGRP allows you to load-balance across six, and through the use of the variance command, EIGRP can load-balance up to six unequal-cost links.Configuring EIGRPAlthough EIGRP can be configured for IP, IPv6, IPX, and AppleTalk, as a future Cisco Certified Network Associate, you really only need to focus on the configuration of IP for now. (IPv6 configurations will be shown in Chapter 13.)There are two modes from which EIGRP commands are entered: router configuration mode and interface configuration mode. Router configuration mode enables the protocol, determines which networks will run EIGRP, and sets global characteristics. Interface config-uration mode allows customization of summaries, metrics, timers, and bandwidth.To start an EIGRP session on a router, use the router eigrp command followed by the autonomous system number of your network. You then enter the network numbers connected to the router using the network command followed by the network number.Let’s look at an example of enabling EIGRP for autonomous system 20 on a router con-nected to two networks, with the network numbers being 10.3.1.0/24 and 172.16.10.0/24:

Router#config tRouter(config)#router eigrp 20Router(config-router)#network 172.16.0.0

Router(config-router)#network 10.0.0.0Remember—as with RIP, you use the classful network address, which is all subnet and host bits turned off.

Understand that the AS number is irrelevant—that is, as long as all routers use the same number! You can use any number from 1 to 65,535.Say you need to stop EIGRP from working on a specific interface, such as a BRI interface or a serial connection to the Internet. To do that, you would flag the interface as passive using the passive-interface interface command, as discussed in Chapter 6 with RIP. The following command shows you how to make interface serial 0/1 a passive interface:

Router(config)#router eigrp 20

Router(config-router)#passive-interface serial 0/1

10089.book  Page 426  Monday, July 23, 2007  3:17 PM




 Configuring EIGRP 427 Doing this will prohibit the interface from sending or receiving Hello packets and, as a result, stop it from forming adjacencies. This means that it won’t send or receive route information on this interface.

 The impact of the  passive-interface  command depends upon the routing protocol under which the command is issued. For example, on an interface run-ning RIP, the  passive-interface  command will prohibit the sending of route updates but allow their receipt. Thus, a RIP router with a passive interface will still learn about the networks advertised by other routers. This is different from  EIGRP, where a  passive-interface  will neither send nor receive updates. Okay, let’s configure the same network that we configured in the last chapter with RIP and RIPv2. It doesn’t matter that RIPv2 (as well as our static routes) are already running—unless you’re worried about bandwidth consumption and CPU cycles, of course, because EIGRP has an AD of 90. Remember that our static routes were changed to an AD of 150/151, and RIP is 120, so only EIGRP routes will populate the routing tables, even if RIP and static routing are enabled.Figure 7.3 shows the network that we’ve been working with—the same one we’re going to use to configure with EIGRP. FIGURE7.3 Our internetwork

Remote1 Remote2 

Remote3 

871W F0/0 (DCE) 

1242AP 

BVI1

 10.1.1.0 F0/1 Corp S0/0/0  S0/0/1  S0/1/0 S0/2/0 

HostA HostB HostC HostD (DCE) (DCE) (DCE) VLAN1 10.1.6.0 10.1.7.0 10.1.8.010.1.9.0 10.1.10.0 10.1.11.0 F0/1 D0/3/0 F0/1 F0/0 F0/0 10.1.2.010.1.2.010.1.4.0 10.1.5.0 S0/0/0  s0/0/1  S0/2/0  S0/0/1  10.1.12.0WHA 

WHB 

WHC 

 

10089c07.fm  Page 427  Friday, November 7, 2008  11:02 PM




428Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)So you can use it as a reminder, Table 7.1 contains the IP addresses we’ve been using on each interface.TABLE7.1Network Addressing for the IP Network RouterNetwork AddressInterfaceAddressCorp   Corp10.1.1.0F0/110.1.1.1Corp10.1.2.0S0/0/010.1.2.1Corp10.1.3.0S0/0/1(DCE)10.1.3.1Corp10.1.4.0s0/1/010.1.4.1Corp10.1.5.0s0/2/010.1.5.1R1   R110.1.2.0S0/0/0 (DCE)10.1.2.2R110.1.3.0S0/0/110.1.3.2R110.1.6.0F0/010.1.6.1R110.1.7.0F0/110.1.7.1R2   R210.1.4.0S0/2/0 (DCE)10.1.4.2R210.1.8.0D0/3/010.1.8.1R210.1.9.0F0/010.1.9.1R3   R310.1.5.0S0/0/0/ (DCE)10.1.5.2R310.1.10.0F0/010.1.10.1R310.1.11.0F0/110.1.11.1

10089.book  Page 428  Monday, July 23, 2007  3:17 PM




Configuring EIGRP429It’s actually really easy to add EIGRP to our internetwork—this is the beauty of EIGRP.CorpThe AS number, as shown in the following router output, can be any number from 1 to 65,535. A router can be a member of as many ASes as you want it to be, but for this book’s purposes, we’re just going to configure a single AS:

Corp#config tCorp(config)#router eigrp ?  <1-65535>  Autonomous system numberCorp(config)#router eigrp 10

Corp(config-router)#network 10.0.0.0The router eigrp [as] command turns EIGRP routing on in the router. As with RIPv1, you still need to add the classful network numbers you want to advertise. But unlike RIP, EIGRP uses classless routing—but you still configure it as classful. Classless, which I’m sure you remember, means that the subnet mask information is sent along with routing protocol updates (RIPv2 is classless).R1To configure the R1 router, all you need to do is turn on EIGRP routing using AS 10 and then add the network number like this:

R1#config tR1(config)#router eigrp 10R1(config-router)#network 10.0.0.0*Mar 21 19:18:12.935: %DUAL-5-NBRCHANGE: IP-EIGRP(0) 10: Neighbor   10.1.2.2 (Serial0/0/0) is up: new adjacency*Mar 21 19:18:12.935: %DUAL-5-NBRCHANGE: IP-EIGRP(0) 10: Neighbor

   10.1.3.2 (Serial0/0/1) is up: new adjacency871W   871W10.1.11.0Vlan 110.1.11.2871W10.1.12.0Dot11radio010.1.12.11242 AP   1242 AP10.1.1.0BVI 110.1.1.2TABLE7.1Network Addressing for the IP Network(continued)RouterNetwork AddressInterfaceAddress

10089.book  Page 429  Monday, July 23, 2007  3:17 PM




430Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)The R1 router found the Corp neighbor—the two routers are adjacent! Notice that it found both links connected between the routers. This is a good thing.R2To configure the R2 router, all you need to do is again turn on EIGRP using AS 10:

R2#config tR2(config)#router eigrp 10R2(config-router)#network 10.0.0.0*Mar 21 19:20:29.023: %DUAL-5-NBRCHANGE: IP-EIGRP(0) 10: Neighbor

   10.1.4.2 (Serial0/1/0) is up: new adjacencyThat’s it—really! Most routing protocols are pretty simple to set up, and EIGRP is no exception. But that’s only for the basic configuration, of course.R3Let’s use the SDM to configure EIGRP just as we’ve done throughout the last few chapters. The configuration process itself won’t take long at all—it’s the logging in part that I’m going to do first that’s going to really eat up some of my time!Looking at this first screen shot, we can see that we still have both our static routes and RIPv2 running on our router.:

10089.book  Page 430  Monday, July 23, 2007  3:17 PM




Configuring EIGRP431Let’s enable EIGRP by adding AS 10 and also choosing to set our passive interfaces—only because it is so easy to do so!Last, we can see that EIGRP is now running with AS 10.

10089.book  Page 431  Monday, July 23, 2007  3:17 PM




432Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)That’s it, done.Our configuration seems pretty solid, but remember—only EIGRP routes are going to wind up in the routing table because EIGRP has the lowest AD. So by having RIP running in the background, we’re not only using more memory and CPU cycles on the router, we’re sucking up precious bandwidth across every one of our links! This is definitely not good, and it’s some-thing you’ll really want to keep in mind.Now we’re going to configure our last router. Because the IOS image I have for my 871W router doesn’t support EIGRP, we’ll keep RIPv2 running. I’m going to configure redistribution (translation) from the R3 router to the 871W router—something I think you’ll find pretty interesting, so let’s get started.Redistributing to the 871W Router from R3From the R3 router, we just need to add the redistribution commands under EIGRP and RIP. Understand that the SDM definitely has its limitations, so we’ll need to get this done using the CLI:

R3#config tR3(config)#router eigrp 10R3(config-router)#redistribute rip ?  metric     Metric for redistributed routes  route-map  Route map reference  <cr>R3(config-router)#redistribute rip metric ?  <1-4294967295>  Bandwidth metric in Kbits per secondR3(config-router)#redistribute rip metric 10000000 ?  <0-4294967295>  EIGRP delay metric, in 10 microsecond unitsR3(config-router)#redistribute rip metric 10000000 20000 ?  <0-255>  EIGRP reliability metric where 255 is 100% reliableR3(config-router)#redistribute rip metric 10000000 20000 255 ?  <1-255>  EIGRP Effective bandwidth metric (Loading) where 255 is 100% loadedR3(config-router)#redistribute rip metric 10000000 20000 255 1 ?  <1-65535>  EIGRP MTU of the pathR3(config-router)#redistribute rip metric 10000000 20000 255 1 1500R3(config-router)#do show run | begin router eigrp 10router eigrp 10 redistribute rip metric 10000000 20000 255 1 1500 passive-interface FastEthernet0/0 passive-interface Serial0/0/0 network 10.0.0.0 no auto-summary

!

10089.book  Page 432  Monday, July 23, 2007  3:17 PM




Configuring EIGRP433We needed to change RIP’s metric of hop count to match EIGRP’s bandwidth metric, delay, reliability, load, and MTU. Even though EIGRP may only use bandwidth and delay of the line by default, when you configure redistribution, you have to configure all metric values.Our R3 router is now bilingual and speaks both RIP and EIGRP. It will even serve as a kind of interpreter for the rest of our routers—well, most of them. That means we’re not done just yet—we still need to configure redistribution from EIGRP to RIP too (mutual redistribution) so that the 871W will receive the EIGRP routes as RIP routes because the 871W router only “speaks” RIP:

R3(config)#router ripR3(config-router)#redistribute eigrp 10 ?  metric     Metric for redistributed routes  route-map  Route map reference  <cr>R3(config-router)#redistribute eigrp 10 metric ?  <0-16>       Default metric  transparent  Transparently redistribute metric

R3(config-router)#redistribute eigrp 10 metric 1The preceding output shows us that we’re redistributing EIGRP into RIP and changing the metric to hop count.But the only way to find out if this is really and truly working is to disable RIP on all routers except the R3 router—the one that’s providing the translation to and from the 871W router. Here’s a look at how I did that:

Corp#config tCorp(config)#no router ripR1#config tR1(config)#no router ripR2#config t

R2(config)#no router ripLet’s check out the Corp’s routing table:

Corp#sh ip route     10.0.0.0/24 is subnetted, 12 subnetsD       10.1.11.0 [90/2172416] via 10.1.5.2, 00:04:57, Serial0/2/0D       10.1.10.0 [90/2172416] via 10.1.5.2, 00:04:57, Serial0/2/0D       10.1.9.0 [90/2195456] via 10.1.4.2, 00:04:57, Serial0/1/0D       10.1.8.0 [90/2195456] via 10.1.4.2, 00:04:57, Serial0/1/0D       10.1.12.0 [90/2172416] via 10.1.5.2, 00:03:00, Serial0/2/0C       10.1.3.0 is directly connected, Serial0/0/1C       10.1.2.0 is directly connected, Serial0/0/0

10089.book  Page 433  Monday, July 23, 2007  3:17 PM




434Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)C       10.1.1.0 is directly connected, FastEthernet0/1D       10.1.7.0 [90/2195456] via 10.1.3.2, 00:04:58, Serial0/0/1                 [90/2195456] via 10.1.2.2, 00:04:58, Serial0/0/0D       10.1.6.0 [90/2195456] via 10.1.3.2, 00:04:58, Serial0/0/1                 [90/2195456] via 10.1.2.2, 00:04:58, Serial0/0/0C       10.1.5.0 is directly connected, Serial0/2/0

C       10.1.4.0 is directly connected, Serial0/1/0Okay, cool—all routes are showing up, including 10.1.12.0, which is the wireless LAN connected to the 871W router. And not only is it there, it’s showing up as an EIGRP network because R3 is translating from RIP to EIGRP for us. Here’s the 871W router’s routing table showing that it’s receiving all routes as RIP routes from the R3 router:

871W#sh ip route     10.0.0.0/24 is subnetted, 12 subnetsC       10.1.11.0 is directly connected, Vlan1R       10.1.10.0 [120/1] via 10.1.11.1, 00:00:19, Vlan1R       10.1.9.0 [120/2] via 10.1.11.1, 00:00:19, Vlan1R       10.1.8.0 [120/2] via 10.1.11.1, 00:00:19, Vlan1C       10.1.12.0 is directly connected, Dot11Radio0R       10.1.3.0 [120/2] via 10.1.11.1, 00:00:19, Vlan1R       10.1.2.0 [120/2] via 10.1.11.1, 00:00:19, Vlan1R       10.1.1.0 [120/2] via 10.1.11.1, 00:00:19, Vlan1R       10.1.7.0 [120/2] via 10.1.11.1, 00:00:19, Vlan1R       10.1.6.0 [120/2] via 10.1.11.1, 00:00:19, Vlan1R       10.1.5.0 [120/1] via 10.1.11.1, 00:00:19, Vlan1

R       10.1.4.0 [120/2] via 10.1.11.1, 00:00:19, Vlan1We can see all the networks in the routing table. And as far as the 871W router is con-cerned, the whole network is just running RIP. Pretty sweet! This is a great example of a way to configure a network if you happen to have an old router involved that’s running RIP and you don’t want to install RIP on all your other routers.Configuring Discontiguous NetworksThere’s one more configuration that you need to be aware of that has to do with auto-summarization. Remember Figure 7.1 and how it demonstrated how EIGRP would auto-summarize the boundaries on a discontiguous network? Take a look at that figure again, and then I’ll configure both routers with EIGRP.In the figure, the Lab_A router is connected to a 172.16.10.0/24 network and the 10.3.1.0/24 backbone. The Lab_B router is connected to the 172.16.20.0/24 network and the 10.3.1.0/24 

10089.book  Page 434  Monday, July 23, 2007  3:17 PM




Load Balancing with EIGRP435backbone. Both routers, by default, would automatically summarize the classful boundaries and routing would not work. Here’s the configuration that would make this network work:

Lab_A#config tLab_A(config)#router eigrp 100Lab_A(config-router)#network 172.16.0.0Lab_A(config-router)#network 10.0.0.0Lab_A(config-router)#no auto-summaryLab_B#config tLab_B(config)#router eigrp 100Lab_B(config-router)#network 172.16.0.0Lab_B(config-router)#network 10.0.0.0

Lab_B(config-router)#no auto-summaryBecause I used the no auto-summary command, EIGRP will advertise all the subnets between the two routers. If the networks were larger, you could then provide manual summa-rization on these same boundaries.

When I configured the R3 router using SDM, it automatically added the no auto-summary under EIGRP. It didn’t even ask me if I wanted to or not, and I could only verify this, or disable it, from the CLI.Load Balancing with EIGRPYou might know that by default, EIGRP can load-balance up to four equal-cost links. But did you remember that we can configure EIGRP to load-balance across up to six equal-/unequal-cost links to a remote network? Well, we can, so let’s play with both our Corp and R1 routers and do some load balancing. First, let’s take a look at the R1 routing table and make sure that EIGRP has already found both links between the routers:

R1#sh ip route     10.0.0.0/24 is subnetted, 12 subnetsD       10.1.11.0 [90/2684416] via 10.1.3.1, 00:50:37, Serial0/0/1                  [90/2684416] via 10.1.2.1, 00:50:37, Serial0/0/0D       10.1.10.0 [90/2707456] via 10.1.3.1, 01:04:40, Serial0/0/1                  [90/2707456] via 10.1.2.1, 01:04:40, Serial0/0/0D       10.1.9.0 [90/2707456] via 10.1.3.1, 01:24:09, Serial0/0/1                 [90/2707456] via 10.1.2.1, 01:24:09, Serial0/0/0D       10.1.8.0 [90/2707456] via 10.1.3.1, 01:24:09, Serial0/0/1                 [90/2707456] via 10.1.2.1, 01:24:09, Serial0/0/0

10089.book  Page 435  Monday, July 23, 2007  3:17 PM




436Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)D       10.1.12.0 [90/2684416] via 10.1.3.1, 00:10:10, Serial0/0/1                  [90/2684416] via 10.1.2.1, 00:10:10, Serial0/0/0C       10.1.3.0 is directly connected, Serial0/0/1C       10.1.2.0 is directly connected, Serial0/0/0D       10.1.1.0 [90/2172416] via 10.1.3.1, 01:24:11, Serial0/0/1                 [90/2172416] via 10.1.2.1, 01:24:11, Serial0/0/0C       10.1.7.0 is directly connected, FastEthernet0/1C       10.1.6.0 is directly connected, FastEthernet0/0D       10.1.5.0 [90/2681856] via 10.1.3.1, 01:24:11, Serial0/0/1                 [90/2681856] via 10.1.2.1, 01:24:11, Serial0/0/0D       10.1.4.0 [90/2681856] via 10.1.3.1, 01:24:11, Serial0/0/1

                 [90/2681856] via 10.1.2.1, 01:24:11, Serial0/0/0Now this is new and different—a very interesting routing table indeed! You can see that we have two links to every route in our internetwork, and again, EIGRP will load balance across the s0/0/0 and s0/0/1 links by default because they’re the same metric.EIGRP really does offer some really cool features, and one of them is automatic load balancing. But how about bundling links? Well, EIGRP can allow us to do this too—even with no extra configuration! Let me show you how this works. I’m going to configure the links between our Corp and R1 routers with the same subnet, meaning both links will have all interfaces within the same subnet. Check out my configuration:

Corp#config tCorp(config)#int s0/0/1Corp(config-if)#ip address 10.1.2.4 255.255.255.0R1#config tR1(config)#int s0/0/1R1(config-if)#ip address 10.1.2.3 255.255.255.0R1(config-if)#do show run | begin interfaceinterface Serial0/0/0 description 1st Connection to Corp Router ip address 10.1.2.2 255.255.255.0!interface Serial0/0/1 description 2nd connection to Corp Router

 ip address 10.1.2.3 255.255.255.0Now both links have all four interfaces in the same subnet.

R1(config-if)#do show ip route     10.0.0.0/24 is subnetted, 12 subnetsD       10.1.11.0 [90/2684416] via 10.1.2.4, 00:04:44, Serial0/0/1                  [90/2684416] via 10.1.2.1, 00:04:44, Serial0/0/0

10089.book  Page 436  Monday, July 23, 2007  3:17 PM




Load Balancing with EIGRP437D       10.1.10.0 [90/2707456] via 10.1.2.4, 00:04:44, Serial0/0/1                  [90/2707456] via 10.1.2.1, 00:04:44, Serial0/0/0D       10.1.9.0 [90/2707456] via 10.1.2.4, 00:04:44, Serial0/0/1                 [90/2707456] via 10.1.2.1, 00:04:44, Serial0/0/0D       10.1.8.0 [90/2707456] via 10.1.2.4, 00:04:44, Serial0/0/1                 [90/2707456] via 10.1.2.1, 00:04:44, Serial0/0/0D       10.1.12.0 [90/2684416] via 10.1.2.4, 00:04:44, Serial0/0/1                  [90/2684416] via 10.1.2.1, 00:04:44, Serial0/0/0D       10.1.3.0 [90/3193856] via 10.1.2.4, 00:04:44, Serial0/0/1                 [90/3193856] via 10.1.2.1, 00:04:44, Serial0/0/0C       10.1.2.0 is directly connected, Serial0/0/0                 is directly connected, Serial0/0/1D       10.1.1.0 [90/2172416] via 10.1.2.4, 00:03:56, Serial0/0/1                 [90/2172416] via 10.1.2.1, 00:03:56, Serial0/0/0C       10.1.7.0 is directly connected, FastEthernet0/1C       10.1.6.0 is directly connected, FastEthernet0/0D       10.1.5.0 [90/2681856] via 10.1.2.4, 00:04:46, Serial0/0/1                 [90/2681856] via 10.1.2.1, 00:04:46, Serial0/0/0D       10.1.4.0 [90/2681856] via 10.1.2.4, 00:04:46, Serial0/0/1

                 [90/2681856] via 10.1.2.1, 00:04:46, Serial0/0/0

To make this fabulous configuration work, EIGRP positively must be enabled first. If not, you’ll get an error on your router that the addresses overlap!Did you notice there’s a subtle change or two in the routing table now? Networks 10.1.2.0 and 10.1.3.0 used to show up as individual, directly connected interfaces, but not anymore. Now only the 10.1.2.0 network shows up as two directly connected interfaces, and the router now has a 3MB pipe through that line instead of just two 1.5Mbps T1 links. And just because these changes are subtle doesn’t make them any less cool!But wait a minute here.… Why is 10.1.3.0 still showing up in the routing table, and why is it showing up as a D, meaning DUAL for EIGRP, even though subnet 10.1.3.0 is nonexistent? The answer is pretty simple—when we configured our static routes on our R3 router through SDM, I clicked the Permanent route option. The effect of that command is, “If any static net-work goes down, keep that route in the routing table of R3.” Network 10.1.3.0 was unconfig-ured between the Corp and R1 routers, so the R3 router is advertising 10.1.3.0 as available even though it really isn’t, and it’s doing that because we deployed our redistribution command.

This is a really good reason why not to use the permanent option with static routes, because if you do, your routing tables could actually show you a sub-net that doesn’t even exist!

10089.book  Page 437  Monday, July 23, 2007  3:17 PM




438Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)I am going to add subnet 10.1.3.0 back into the network so we can have some more fun with these dual links. I’ll go to the Corp and R1 s0/0/1 interfaces and configure 10.1.3.1/24 and 10.1.3.2/24. Now 10.1.3.0 is being advertised again, but this time it’s for a network that actually exists! Let’s mix things up a bit and change the metric of the 10.1.3.0 link and see what happens:

R1#config tR1(config)#int s0/0/1R1(config-if)#bandwidth 256000R1(config-if)#delay 300000Corp#config tCorp(config)#int s0/0/1Corp(config-if)#bandwidth 256000

Corp(config-if)#delay 300000Since by default, EIGRP uses bandwidth and delay of the line to determine the best path to each network, I lowered the bandwidth and raised the delay of the s0/0/1 interfaces of the both the R1 and Corp routers. Now, let’s verify EIGRP on our network, plus check out what our dual links are up to now between the R1 and Corp routers.Verifying EIGRPThere are several commands that can be used on a router to help you troubleshoot and verify the EIGRP configuration. Table 7.2 contains all of the most important commands that are used in conjunction with verifying EIGRP operation and offers a brief description of what each command does.TABLE7.2EIGRP Troubleshooting CommandsCommandDescription/Functionshow ip routeShows the entire routing tableshow ip route eigrp Shows only EIGRP entries in the routing table show ip eigrp neighborsShows all EIGRP neighborsshow ip eigrp topologyShows entries in the EIGRP topology tabledebug eigrp packetShows Hello packets sent/received between adjacent routersDebug ip eigrp notificationShows EIGRP changes and updates as they occur on your network

10089.book  Page 438  Monday, July 23, 2007  3:17 PM




Verifying EIGRP439I’ll demonstrate how you would use the commands in Table 7.2 by using them on our inter-network that we just configured—not including the discontiguous network example.The following router output is from the Corp router in our example:

Corp#sh ip route     10.0.0.0/24 is subnetted, 12 subnetsD       10.1.11.0 [90/2172416] via 10.1.5.2, 00:01:05, Serial0/2/0D       10.1.10.0 [90/2195456] via 10.1.5.2, 00:01:05, Serial0/2/0D       10.1.9.0 [90/2195456] via 10.1.4.2, 00:01:05, Serial0/1/0D       10.1.8.0 [90/2195456] via 10.1.4.2, 00:01:05, Serial0/1/0D       10.1.12.0 [90/2172416] via 10.1.5.2, 00:01:05, Serial0/2/0C       10.1.3.0 is directly connected, Serial0/0/1C       10.1.2.0 is directly connected, Serial0/0/0C       10.1.1.0 is directly connected, FastEthernet0/1D       10.1.7.0 [90/2195456] via 10.1.2.2, 00:01:06, Serial0/0/0D       10.1.6.0 [90/2195456] via 10.1.2.2, 00:01:06, Serial0/0/0C       10.1.5.0 is directly connected, Serial0/2/0

C       10.1.4.0 is directly connected, Serial0/1/0You can see that all routes are there in the routing table (10.1.3.0 shows that it’s directly connected again) and we have only one link to networks 10.1.6.0 and 10.1.7.0! Notice that EIGRP routes are indicated with simply a D designation (DUAL) and that the default AD of these routes is 90. This represents internal EIGRP routes. Let’s take a look at the R1 router table now that we’ve changed the metrics:

R1#sh ip route     10.0.0.0/24 is subnetted, 12 subnetsD       10.1.11.0 [90/2684416] via 10.1.2.1, 00:00:09, Serial0/0/0D       10.1.10.0 [90/2707456] via 10.1.2.1, 00:00:09, Serial0/0/0D       10.1.9.0 [90/2707456] via 10.1.2.1, 00:00:09, Serial0/0/0D       10.1.8.0 [90/2707456] via 10.1.2.1, 00:00:09, Serial0/0/0D       10.1.12.0 [90/2684416] via 10.1.2.1, 00:00:09, Serial0/0/0C       10.1.3.0 is directly connected, Serial0/0/1C       10.1.2.0 is directly connected, Serial0/0/0D       10.1.1.0 [90/2172416] via 10.1.2.1, 00:00:09, Serial0/0/0C       10.1.7.0 is directly connected, FastEthernet0/1C       10.1.6.0 is directly connected, FastEthernet0/0D       10.1.5.0 [90/2681856] via 10.1.2.1, 00:00:09, Serial0/0/0

D       10.1.4.0 [90/2681856] via 10.1.2.1, 00:00:09, Serial0/0/0Now we have only one route to each remote network, and the 10.1.3.0 network is our backup link. Obviously, it would be better if we could use both links at the same time, but in my example, I made the 10.1.3.0 network a backup link.

10089.book  Page 439  Monday, July 23, 2007  3:17 PM




440Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)Let’s go back to the Corp router and see what it shows us in the neighbor table:

Corp#sh ip eigrp neighborsIP-EIGRP neighbors for process 10H   Address     Interface  Hold Uptime   SRTT  RTO Q  Seq                            (sec)         (ms)    Cnt Num1   10.1.3.2    Se0/0/1     14 00:35:10    1   200  0  813   10.1.5.2    Se0/2/0     10 02:51:22    1   200  0  312   10.1.4.2    Se0/1/0     13 03:17:20    1   200  0  20

0   10.1.2.2    Se0/0/0     10 03:19:37    1   200  0  80We read the information in this output like this: The H field indicates the order in which the neighbor was discovered. The hold time is how long this router will wait for a Hello packet to arrive from a specific neighbor. The uptime indicates how long the neighborship has been established. The SRTT field is the smooth round-trip timer—an indication of the time it takes for a round-trip from this router to its neighbor and back. This value is used to determine how long to wait after a multicast for a reply from this neighbor. If a reply isn’t received in time, the router will switch to using unicasts in an attempt to complete the communica-tion. The time between multicast attempts is specified by… The Retransmission Time Out (RTO) field, which is the amount of time EIGRP waits before retransmitting a packet from the retransmission queue to a neighbor. The Q value indicates whether there are any outstanding messages in the queue—consistently large values would indicate a problem. The Seq field indicates the sequence number of the last update from that neighbor—something that’s used to maintain synchronization and avoid duplicate or out-of-sequence processing of messages.

The show ip eigrp neighbors command allows you to check the IP addresses as well as the retransmit interval and queue counts for the neighbors that have established an adjacency.Now let’s see what’s in the Corp topology table by using the show ip eigrp topology command—this should be interesting!

Corp#sh ip eigrp topologyIP-EIGRP Topology Table for AS(10)/ID(10.1.5.1)Codes: P - Passive, A - Active, U - Update, Q - Query, R - Reply,       r - reply Status, s - sia StatusP 10.1.11.0/24, 1 successors, FD is 2172416        via 10.1.5.2 (2172416/28160), Serial0/2/0

10089.book  Page 440  Monday, July 23, 2007  3:17 PM




Verifying EIGRP441P 10.1.10.0/24, 1 successors, FD is 2172416        via 10.1.5.2 (2195456/281600), Serial0/2/0P 10.1.9.0/24, 1 successors, FD is 2195456        via 10.1.4.2 (2195456/281600), Serial0/1/0P 10.1.8.0/24, 1 successors, FD is 2195456        via 10.1.4.2 (2195456/72960), Serial0/1/0P 10.1.12.0/24, 1 successors, FD is 2172416        via 10.1.5.2 (2172416/28160), Serial0/2/0P 10.1.3.0/24, 1 successors, FD is 76839936        via Connected, Serial0/0/1        via 10.1.2.2 (9849856/7719936), Serial0/0/0P 10.1.2.0/24, 1 successors, FD is 2169856        via Connected, Serial0/0/0        via 10.1.2.2 (2681856/551936), Serial0/0/0P 10.1.1.0/24, 1 successors, FD is 28160        via Connected, FastEthernet0/1P 10.1.7.0/24, 1 successors, FD is 793600        via 10.1.2.2 (2195456/281600), Serial0/0/0        via 10.1.3.2 (77081600/281600), Serial0/0/1P 10.1.6.0/24, 1 successors, FD is 793600        via 10.1.2.2 (2195456/281600), Serial0/0/0        via 10.1.3.2 (77081600/281600), Serial0/0/1P 10.1.5.0/24, 1 successors, FD is 2169856        via Connected, Serial0/2/0P 10.1.4.0/24, 1 successors, FD is 2169856

        via Connected, Serial0/1/0Notice that every route is preceded by a P. This means that the route is in the passive state, which is a good thing because routes in the active state (A) indicate that the router has lost its path to this network and is searching for a replacement. Each entry also indicates the feasible distance, or FD, to each remote network plus the next-hop neighbor through which packets will travel to their destination. Plus, each entry also has two numbers in parentheses. The first indicates the feasible distance, and the second the advertised distance to a remote network.Now here’s where things get interesting—notice that under the 10.1.7.0 and 10.1.6.0 out-puts there are two links to each network and that the feasible distance and advertised distance are different. What this means is that we have one successor to the networks and one feasible successor—a backup route! So very cool! You need to remember that even though both routes to network 10.1.6.0 and 10.1.7.0 are in the topology table, only the successor route (the one with the lowest metrics) will be copied and placed into the routing table.

In order for the route to be a feasible successor, its advertised distance must be less than the feasible distance of the successor route.

10089.book  Page 441  Monday, July 23, 2007  3:17 PM




442Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)EIGRP will load-balance across both links automatically when they are of equal variance (equal cost), but EIGRP can also load-balance across unequal-cost links as well if we use the variance command. The variance metric is set to 1 by default, meaning that only equal-cost links will load-balance. You can change the metric anywhere up to 128. Changing a variance value enables EIGRP to install multiple, loop-free routes with unequal cost in a local routing table.So basically, if the variance is set to 1, only routes with the same metric as the successor will be installed in the local routing table. And, for example, if the variance is set to 2, any EIGRP-learned route with a metric less than two times the successor metric will be installed in the local routing table (if it is already a feasible successor).Now’s a great time for us to check out some debugging outputs. First, let’s use the debug eigrp packet command that will show our Hello packets being sent between neighbor routers:

Corp#debug eigrp packetEIGRP Packets debugging is on    (UPDATE, REQUEST, QUERY, REPLY, HELLO, IPXSAP, PROBE, ACK, STUB,    SIAQUERY, SIAREPLY)Corp#*Mar 21 23:17:35.050: EIGRP: Sending HELLO on FastEthernet0/1*Mar 21 23:17:35.050:   AS 10, Flags 0x0, Seq 0/0 idbQ 0/0 iidbQ un/rely 0/0*Mar 21 23:17:35.270: EIGRP: Received HELLO on Serial0/1/0 nbr 10.1.4.2*Mar 21 23:17:35.270:   AS 10, Flags 0x0, Seq 0/0 idbQ 0/0 iidbQ   un/rely 0/0 peerQ un/rely 0/0*Mar 21 23:17:35.294: EIGRP: Received HELLO on Serial0/0/0 nbr 10.1.2.2*Mar 21 23:17:35.294:   AS 10, Flags 0x0, Seq 0/0 idbQ 0/0 iidbQ   un/rely 0/0 peerQ un/rely 0/0*Mar 21 23:17:38.014: EIGRP: Received HELLO on Serial0/2/0 nbr 10.1.5.2*Mar 21 23:17:38.014:   AS 10, Flags 0x0, Seq 0/0 idbQ 0/0 iidbQ

   un/rely 0/0 peerQ un/rely 0/0Since my Corp router is connected to three EIGRP neighbors, and because the 224.0.0.10 multicast is sent out every 5 seconds, I didn’t have any problem seeing the updates. The Hello packets are sent out every active interface as well as all the interfaces that we have neighbors connected to. Did you notice that the AS number is provided in the update? This is because if a neighbor doesn’t have the same AS number, the Hello update would just be discarded.I want to show you one more important debugging command—the debug ip eigrp notification command (called debug ip eigrp events on pre-12.4 routers), plus the result-ing output. What’s important, and probably surprising to you, is that this command’s output shouldn’t show you anything at all! That’s right—the only time you’ll see output from this com-mand is if there’s a problem on your network or you’ve added or deleted a network from a router in your internetwork. So because I so humbly know I have a super-cool, problem-free network, I’m going to shut down an interface on my Corp router in order to see some output:

Corp(config)#int f0/1Corp(config-if)#shut

10089.book  Page 442  Monday, July 23, 2007  3:17 PM




Verifying EIGRP443*Mar 21 23:25:43.506: IP-EIGRP(Default-IP-Routing-Table:10): Callback:   route_adjust FastEthernet0/1*Mar 21 23:25:43.506: IP-EIGRP: Callback: ignored connected AS 0 10.1.1.0/24*Mar 21 23:25:43.506:           into: eigrp AS 10*Mar 21 23:25:43.506: IP-EIGRP(Default-IP-Routing-Table:10): Callback:   callbackup_routes 10.1.1.0/24Corp(config-if)#n*Mar 21 23:25:45.506: %LINK-5-CHANGED: Interface FastEthernet0/1,   changed state to administratively down*Mar 21 23:25:46.506: %LINEPROTO-5-UPDOWN: Line protocol on Interface   FastEthernet0/1, changed state to downCorp(config-if)#no shutCorp(config-if)#^Z*Mar 21 23:25:49.570: %LINK-3-UPDOWN: Interface FastEthernet0/1,   changed state to up*Mar 21 23:25:49.570: IP-EIGRP(Default-IP-Routing-Table:10): Callback:   lostroute 10.1.1.0/24*Mar 21 23:25:49.570: IP-EIGRP(Default-IP-Routing-Table:0): Callback:   redist connected (config change) FastEthernet0/1*Mar 21 23:25:49.570: IP-EIGRP(Default-IP-Routing-Table:0): Callback:   redist connected (config change) Serial0/0/0*Mar 21 23:25:49.570: IP-EIGRP(Default-IP-Routing-Table:0): Callback:   redist connected (config change) Serial0/0/1*Mar 21 23:25:49.570: IP-EIGRP(Default-IP-Routing-Table:0): Callback:   redist connected (config change) Serial0/1/0*Mar 21 23:25:49.570: IP-EIGRP(Default-IP-Routing-Table:0): Callback:   redist connected (config change) Serial0/2/0*Mar 21 23:25:49.570: IP-EIGRP(Default-IP-Routing-Table:10): Callback:   route_adjust FastEthernet0/1*Mar 21 23:25:50.570: %LINEPROTO-5-UPDOWN: Line protocol on Interface

   FastEthernet0/1, changed state to up

I’m going to repeat this because it’s that important—you don’t want to see output from this command on your production network! If you do, odds are you have a problem or two that you’d better find and fix.I know you’ve learned a lot about EIGRP so far, but stick around because you’re not done with this chapter just yet! It’s now time to give you the skinny on OSPF!

10089.book  Page 443  Monday, July 23, 2007  3:17 PM




444Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)Open Shortest Path First (OSPF) BasicsOpen Shortest Path First (OSPF) is an open standard routing protocol that’s been imple-mented by a wide variety of network vendors, including Cisco. If you have multiple routers and not all of them are Cisco (what!), then you can’t use EIGRP, can you? So your remaining CCNA objective options are basically RIP, RIPv2, and OSPF. If it’s a large network, then, really, your only options are OSPF and something called route redistribution—a translation service between routing protocols that we discussed earlier in this chapter.OSPF works by using the Dijkstra algorithm. First, a shortest path tree is constructed, and then the routing table is populated with the resulting best paths. OSPF converges quickly, although perhaps not as quickly as EIGRP, and it supports multiple, equal-cost routes to the same destination. Like EIGRP, it does support both IP and IPv6 routed protocols.OSPF provides the following features: Consists of areas and autonomous systems Minimizes routing update traffic Allows scalability Supports VLSM/CIDR Has unlimited hop count Allows multi-vendor deployment (open standard)OSPF is the first link-state routing protocol that most people are introduced to, so it’s useful to see how it compares to more traditional distance-vector protocols such as RIPv2 and RIPv1. Table 7.3 gives you a comparison of these three protocols.TABLE7.3OSPF and RIP comparison CharacteristicOSPFRIPv2RIPv1Type of protocolLink stateDistance vectorDistance vectorClassless supportYesYesNoVLSM supportYesYesNoAuto-summarizationNoYesYesManual summarizationYesNoNoDiscontiguous supportYesYesNoRoute propagationMulticast on changePeriodic multicastPeriodic broadcastPath metricBandwidthHopsHopsHop count limitNone1515

10089.book  Page 444  Monday, July 23, 2007  3:17 PM




Open Shortest Path First (OSPF) Basics445OSPF has many features beyond the few I’ve listed in Table 7.3, and all of them contribute to a fast, scalable, and robust protocol that can be actively deployed in thousands of produc-tion networks.OSPF is supposed to be designed in a hierarchical fashion, which basically means that you can separate the larger internetwork into smaller internetworks called areas. This is the best design for OSPF.The following are reasons for creating OSPF in a hierarchical design: To decrease routing overhead To speed up convergence To confine network instability to single areas of the networkThis does not make configuring OSPF easier, but more elaborate and difficult.Figure 7.4 shows a typical OSPF simple design. Notice how each router connects to the back-bone—called area 0, or the backbone area. OSPF must have an area 0, and all other areas should connect to this area. (Areas that do not connect directly to area 0 by using virtual links are beyond the scope of this book.) Routers that connect other areas to the backbone area within an AS are called Area Border Routers (ABRs). Still, at least one interface of the ABR must be in area 0.FIGURE7.4OSPF design exampleConvergenceFastSlowSlowPeer authenticationYesYesNoHierarchical networkYes (using areas)No (flat only)No (flat only)UpdatesEvent triggeredRoute table updatesRoute table updatesRoute computationDijkstraBellman-FordBellman-FordTABLE7.3OSPF and RIP comparison(continued)CharacteristicOSPFRIPv2RIPv1

Autonomous systemBackbone router

Area 0

Area 1

Area 2

Autonomous systemborder router (ASBR)Area borderrouter (ABR)

10089.book  Page 445  Monday, July 23, 2007  3:17 PM




446Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)OSPF runs inside an autonomous system, but it can also connect multiple autonomous sys-tems together. The router that connects these ASes is called an Autonomous System Boundary Router (ASBR).Ideally, you would create other areas of networks to help keep route updates to a minimum and to keep problems from propagating throughout the network. But that’s beyond the scope of this chapter. Just make note of it.As in the section on EIGRP, I’ll first cover the essential terminology you need to under-stand OSPF.OSPF TerminologyImagine how challenging it would be if you were given a map and compass but had no knowl-edge of east or west, north or south, river or mountain, lake or desert. You’d probably not get very far putting your new tools to good use without knowing about this stuff. For this reason, you’ll begin your exploration of OSPF with a long list of terms that will prevent you from get-ting lost in the later sections. The following are important OSPF terms to familiarize yourself with before you proceed:LinkA link is a network or router interface assigned to any given network. When an inter-face is added to the OSPF process, it’s considered by OSPF to be a link. This link, or interface, will have state information associated with it (up or down) as well as one or more IP addresses.Router IDThe Router ID (RID) is an IP address used to identify the router. Cisco chooses the Router ID by using the highest IP address of all configured loopback interfaces. If no loop-back interfaces are configured with addresses, OSPF will choose the highest IP address of all active physical interfaces.NeighborNeighbors are two or more routers that have an interface on a common network, such as two routers connected on a point-to-point serial link.AdjacencyAn adjacency is a relationship between two OSPF routers that permits the direct exchange of route updates. OSPF is really picky about sharing routing information—unlike EIGRP, which directly shares routes with all of its neighbors. Instead, OSPF directly shares routes only with neighbors that have also established adjacencies. And not all neighbors will become adja-cent—this depends upon both the type of network and the configuration of the routers.Hello protocolThe OSPF Hello protocol provides dynamic neighbor discovery and main-tains neighbor relationships. Hello packets and Link State Advertisements (LSAs) build and maintain the topological database. Hello packets are addressed to 224.0.0.5.Neighborship databaseThe neighborship database is a list of all OSPF routers for which Hello packets have been seen. A variety of details, including the Router ID and state, are main-tained on each router in the neighborship database.Topological databaseThe topological database contains information from all of the Link State Advertisement packets that have been received for an area. The router uses the information from the topology database as input into the Dijkstra algorithm that computes the shortest path to every network.

10089.book  Page 446  Monday, July 23, 2007  3:17 PM




Open Shortest Path First (OSPF) Basics447

LSA packets are used to update and maintain the topological database.Link State AdvertisementA Link State Advertisement (LSA) is an OSPF data packet con-taining link-state and routing information that’s shared among OSPF routers. There are dif-ferent types of LSA packets, and I’ll go into these shortly. An OSPF router will exchange LSA packets only with routers to which it has established adjacencies.Designated routerA Designated Router (DR) is elected whenever OSPF routers are con-nected to the same multi-access network. Cisco likes to call these “broadcast” networks, but really, they are networks that have multiple recipients. Try not to confuse multi-access with multipoint, which can be easy to do sometimes.A prime example is an Ethernet LAN. To minimize the number of adjacencies formed, a DR is chosen (elected) to disseminate/receive routing information to/from the remaining routers on the broadcast network or link. This ensures that their topology tables are synchronized. All routers on the shared network will establish adjacencies with the DR and backup designated router (BDR)—I’ll define this next. The election is won by the router with the highest priority, and the Router ID is used as a tiebreaker if the priority of more than one router turns out to be the same.Backup designated routerA Backup Designated Router (BDR) is a hot standby for the DR on multi-access links (remember that Cisco sometimes likes to call these “broadcast” networks). The BDR receives all routing updates from OSPF adjacent routers but doesn’t flood LSA updates.OSPF areasAn OSPF area is a grouping of contiguous networks and routers. All routers in the same area share a common Area ID. Because a router can be a member of more than one area at a time, the Area ID is associated with specific interfaces on the router. This would allow some interfaces to belong to area 1 while the remaining interfaces can belong to area 0. All of the routers within the same area have the same topology table. When configuring OSPF, you’ve got to remember that there must be an area 0 and that this is typically configured on the routers that connect to the backbone of the network. Areas also play a role in establishing a hierarchical network organization—something that really enhances the scalability of OSPF!Broadcast (multi-access)Broadcast (multi-access) networks such as Ethernet allow multiple devices to connect to (or access) the same network as well as provide a broadcast ability in which a single packet is delivered to all nodes on the network. In OSPF, a DR and a BDR must be elected for each broadcast multi-access network.Non-broadcast multi-accessNon-broadcast multi-access (NBMA) networks are types such as Frame Relay, X.25, and Asynchronous Transfer Mode (ATM). These networks allow for multi-access but have no broadcast ability like Ethernet. So, NBMA networks require special OSPF configuration to function properly and neighbor relationships must be defined.

DR and BDR are elected on broadcast and non-broadcast multi-access net-works. Elections are covered in detail later in this chapter.

10089.book  Page 447  Monday, July 23, 2007  3:17 PM




448Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)Point-to-pointPoint-to-point refers to a type of network topology consisting of a direct con-nection between two routers that provides a single communication path. The point-to-point connection can be physical, as in a serial cable directly connecting two routers, or it can be log-ical, as in two routers that are thousands of miles apart yet connected by a circuit in a Frame Relay network. In either case, this type of configuration eliminates the need for DRs or BDRs—but neighbors are discovered automatically.Point-to-multipointPoint-to-multipoint refers to a type of network topology consisting of a series of connections between a single interface on one router and multiple destination routers. All of the interfaces on all of the routers sharing the point-to-multipoint connection belong to the same network. As with point-to-point, no DRs or BDRs are needed.All of these terms play an important part in understanding the operation of OSPF, so again, make sure you’re familiar with each of them. Reading through the rest of this chapter will help you to place the terms within their proper context.SPF Tree CalculationWithin an area, each router calculates the best/shortest path to every network in that same area. This calculation is based upon the information collected in the topology database and an algorithm called shortest path first (SPF). Picture each router in an area constructing a tree—much like a family tree—where the router is the root and all other networks are arranged along the branches and leaves. This is the shortest path tree used by the router to insert routes into the routing table.It’s important to understand that this tree contains only networks that exist in the same area as the router itself does. If a router has interfaces in multiple areas, then separate trees will be constructed for each area. One of the key criteria considered during the route selection pro-cess of the SPF algorithm is the metric or cost of each potential path to a network. But this SPF calculation doesn’t apply to routes from other areas.OSPF uses a metric referred to as cost. A cost is associated with every outgoing interface included in an SPF tree. The cost of the entire path is the sum of the costs of the outgoing inter-faces along the path. Because cost is an arbitrary value as defined in RFC 2338, Cisco had to implement its own method of calculating the cost for each OSPF-enabled interface. Cisco uses a simple equation of 108/bandwidth. The bandwidth is the configured bandwidth for the inter-face. Using this rule, a 100Mbps Fast Ethernet interface would have a default OSPF cost of 1 and a 10Mbps Ethernet interface would have a cost of 10.

An interface set with a bandwidth of 64,000 would have a default cost of 1,563.This value may be overridden by using the ip ospf cost command. The cost is manipulated by changing the value to a number within the range of 1 to 65,535. Because the cost is assigned to each link, the value must be changed on the interface that you want to change the cost.

10089.book  Page 448  Monday, July 23, 2007  3:17 PM




Configuring OSPF449

Cisco bases link cost on bandwidth. Other vendors may use other metrics to calculate a given link’s cost. When connecting links between routers from dif-ferent vendors, you may have to adjust the cost to match another vendor’s router. Both routers must assign the same cost to the link for OSPF to work.Configuring OSPFConfiguring basic OSPF isn’t as simple as RIP, IGRP, and EIGRP, and it can get really complex once the many options that are allowed within OSPF are factored in. But that’s okay—for your studies, you should be interested in the basic single-area OSPF configuration. The following sec-tions describe how to configure single-area OSPF.These two elements are the basic elements of OSPF configuration: Enabling OSPF Configuring OSPF areasEnabling OSPFThe easiest and also least scalable way to configure OSPF is to just use a single area. Doing this requires a minimum of two commands.The command you use to activate the OSPF routing process is as follows:

Lab_A(config)#router ospf ?

<1-65535>A value in the range 1–65,535 identifies the OSPF Process ID. It’s a unique number on this router that groups a series of OSPF configuration commands under a specific running process. Different OSPF routers don’t have to use the same Process ID in order to communicate. It’s purely a local value that essentially has little meaning, but it cannot start at 0; it has to start at a minimum of 1.You can have more than one OSPF process running simultaneously on the same router if you want, but this isn’t the same as running multi-area OSPF. The second process will main-tain an entirely separate copy of its topology table and manage its communications indepen-dently of the first process. And because the CCNA objectives only cover single-area OSPF with each router running a single OSPF process, that’s what I’m going to focus on in this book.

The OSPF Process ID is needed to identify a unique instance of an OSPF data-base and is locally significant.

10089.book  Page 449  Monday, July 23, 2007  3:17 PM




450Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)Configuring OSPF AreasAfter identifying the OSPF process, you need to identify the interfaces that you want to activate OSPF communications on as well as the area in which each resides. This will also configure the networks you’re going to advertise to others. OSPF uses wildcards in the configuration—which are also used in access-list configurations (covered in Chapter 11).Here’s an OSPF basic configuration example for you:

Lab_A#config tLab_A(config)#router ospf 1Lab_A(config-router)#network 10.0.0.0 0.255.255.255 area ?  <0-4294967295>  OSPF area ID as a decimal value  A.B.C.D         OSPF area ID in IP address formatLab_A(config-router)#network 10.0.0.0 0.255.255.255

 area 0

The areas can be any number from 0 to 4.2 billion. Don’t get these numbers confused with the Process ID, which is from 1 to 65,535.Remember, the OSPF Process ID number is irrelevant. It can be the same on every router on the network, or it can be different—doesn’t matter. It’s locally significant and just enables the OSPF routing on the router.The arguments of the network command are the network number (10.0.0.0) and the wild-card mask (0.255.255.255). The combination of these two numbers identifies the interfaces that OSPF will operate on and will also be included in its OSPF LSA advertisements. OSPF will use this command to find any interface on the router configured in the 10.0.0.0 network, and it will place any interface it finds into area 0. Notice that you can create about 4.2 billion areas. (I doubt that a router would let you actually create that many, but you can certainly name them using the numbers up to 4.2 billion.) You can also label an area using an IP address format.A quick review of wildcards: A 0 octet in the wildcard mask indicates that the corresponding octet in the network must match exactly. On the other hand, a 255 indicates that you don’t care what the corresponding octet is in the network number. A network and wildcard mask combina-tion of 1.1.1.1 0.0.0.0 would match 1.1.1.1 only, and nothing else. This is really useful if you want to activate OSPF on a specific interface in a very clear and simple way. If you insist on matching a range of networks, the network and wildcard mask combination of 1.1.0.0 0.0.255.255 would match anything in the range 1.1.0.0–1.1.255.255. Because of this, it’s simpler and safer to stick to using wildcard masks of 0.0.0.0 and identify each OSPF interface individually.The final argument is the area number. It indicates the area to which the interfaces identified in the network and wildcard mask portion belong. Remember that OSPF routers will only become neighbors if their interfaces share a network that’s configured to belong to the same area number. The format of the area number is either a decimal value from the range 1–4,294,967,295 or a value 

10089.book  Page 450  Monday, July 23, 2007  3:17 PM




Configuring OSPF451represented in standard dotted-decimal notation. For example, area 0.0.0.0 is a legitimate area and is identical to area 0.Wildcard ExampleBefore getting down to configuring our network, let’s take a quick peek at a harder OSPF net-work configuration to find out what our OSPF network statements would be if we were using subnets and wildcards.You have a router with these four subnets connected to four different interfaces: 192.168.10.64/28 192.168.10.80/28 192.168.10.96/28 192.168.10.8/30All interfaces need to be in area 0. Seems to me, the easiest configuration would be this:

Test#config tTest(config)#router ospf 1

Test(config-router)#network 192.168.10.0 0.0.0.255 area 0But easy isn’t always best, so although this is easier, what fun is that? And worse, it’s not likely to cover the CCNA objectives for you! So let’s create a separate network statement for each interface using the subnet numbers and wildcards. Doing that would look something like this:

Test#config tTest(config)#router ospf 1Test(config-router)#network 192.168.10.64 0.0.0.15 area 0Test(config-router)#network 192.168.10.80 0.0.0.15 area 0Test(config-router)#network 192.168.10.96 0.0.0.15 area 0

Test(config-router)#network 192.168.10.8 0.0.0.3 area 0Wow, now that’s a different looking config! Truthfully, OSPF would work exactly the same way as in the easy configuration I showed you first—but unlike the easy configuration, this one covers the CCNA objectives!Just remember, when configuring wildcards, they’re always one less than the block size. A /28 is a block size of 16, so we’d add our network statement using the subnet number and then add a wildcard of 15 in the interesting octet. For the /30, which is a block size of 4, we’d use a wildcard of 3.Let’s use Figure 7.5 as an example and configure that network with OSPF using wildcards to make sure you have a solid grip on this. Figure 7.5 shows a three-router network with the IP addresses of each interface.The very first thing you need to be able to do is to look at each interface and determine the subnet that the addresses are in. Hold on, I know what you’re thinking: “Why don’t I just use the exact IP addresses of the interface with the 0.0.0.0 wildcard?” Well, you can, but we’re paying attention to CCNA objectives here, not just what’s easiest, remember?

10089.book  Page 451  Monday, July 23, 2007  3:17 PM




452Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)FIGURE7.5Sample OSPF wildcard configurationThe IP addresses for each interface are shown in the figure. The Lab_A router has two directly connected subnets: 192.168.10.64/29 and 10.255.255.80/30. Here’s the OSPF con-figuration using wildcards:

Lab_A#config tLab_A(config)#router ospf 1Lab_A(config-router)#network 192.168.10.64 0.0.0.7 area 0

Lab_A(config-router)#network 10.255.255.80 0.0.0.3 area 0The Lab_A router is using a /29 or 255.255.255.248 mask on the ethernet0 interface. This is a block size of 8, which is a wildcard of 7. The s0 interface is a mask of 255.255.255.252—block size of 4, with a wildcard of 3. You can’t configure OSPF this way if you can’t look at the IP address and slash notation and then figure out the subnet, mask, and wildcard, can you?Here are our other two configurations:

Lab_B#config tLab_B(config)#router ospf 1Lab_B(config-router)#network 192.168.10.48 0.0.0.7 area 0

Lab_B Lab_C e0: 192.168.10.17/29 s0: 10.255.255.10/30 e0 Area 0 

Lab_A e0 

s0 s1 s0 

Lab_C e0 

s0 Lab_A e0:192.168.10.65/29 s0:10.255.255.81/30 Lab_B e0: 192.168.10.49/29 s1: 10.255.255.82/30 s0: 10.255.255.9/30 

10089.book  Page 452  Monday, July 23, 2007  3:17 PM




Configuring OSPF453Lab_B(config-router)#network 10.255.255.80 0.0.0.3 area 0Lab_B(config-router)#network 10.255.255.8 0.0.0.3 area 0Lab_C#config tLab_C(config)#router ospf 1Lab_C(config-router)#network 192.168.10.16 0.0.0.7 area 0

Lab_C(config-router)#network 10.255.255.8 0.0.0.3 area 0As I mentioned with the Lab_A configuration, you’ve got to be able to determine the sub-net, mask, and wildcard just by looking at the IP address of an interface. If you can’t do that, you won’t be able to configure OSPF using wildcards as I just demonstrated. So go over this until you’re really comfortable with it!Configuring Our Network with OSPFOkay—now we get to have some fun! Let’s configure our internetwork with OSPF using just area 0. Before we do that, we’ve got to remove EIGRP because OSPF has an administrative dis-tance of 110. (EIGRP is 90—but you already knew that, right?) I know we’ve already removed RIP from our routers, but now we can also remove RIP from the 871W and R3 routers because the 871W supports the OSPF routing protocol.There’s a bunch of different ways to configure OSPF, and as I said, the simplest and easiest is to use the wildcard mask of 0.0.0.0. But I want to demonstrate that we can configure each router differently with OSPF and still come up with the exact same result. This is one reason why OSPF is more fun than other routing protocols—it gives us all a lot more ways to screw things up!CorpHere’s the Corp router’s configuration:

Corp#config tCorp(config)#no router eigrp 10Corp(config)#router ospf 132Corp(config-router)#network 10.1.1.1 0.0.0.0 area 0Corp(config-router)#network 10.1.2.1 0.0.0.0 area 0Corp(config-router)#network 10.1.3.1 0.0.0.0 area 0Corp(config-router)#network 10.1.4.1 0.0.0.0 area 0

Corp(config-router)#network 10.1.5.1 0.0.0.0 area 0Hmmmm—it seems we have a few things to discuss here. First, I removed EIGRP, then added OSPF. So why did I use OSPF 132? It really doesn’t matter—the number is irrelevant!The network commands are pretty straightforward. I typed in the IP address of each inter-face and used the wildcard mask of 0.0.0.0, which means that the IP address must match each octet exactly. But if this is one of those times where easier is better—just do this:

Corp(config)#router ospf 132

Corp(config-router)#network 10.1.0.0 0.0.255.255 area 0

10089.book  Page 453  Monday, July 23, 2007  3:17 PM




454Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)One line instead of five! I really want you to understand that no matter which way you con-figure the network statement, OSPF will work the same here. Now, let’s move on to R1. To keep things simple, we’re going to use our same sample configuration.R1The R1 router has four directly connected networks. Instead of typing in each interface, I can use the one network command example and still make it work exactly the same:

R1#config tR1(config)#no router eigrp 10R1(config)#router ospf 1R1(config-router)#network 10.1.0.0 0.0.255.255 area0                                                   ^% Invalid input detected at '^' marker.

R1(config-router)#network 10.1.0.0 0.0.255.255 area 0Okay—other than my little typo, where I forgot to place a space between the area com-mand and the area number, this is truly a fast and efficient configuration.All I did was to first disable EIGRP, and then I turned on OSPF routing process 1 and added the network command 10.1.0.0 with a wildcard of 0.0.255.255. What this did is basi-cally say, “Find any interface that starts with 10.1, and place those interfaces into area 0.” Quick, easy, and slick!R2Let’s give the R2 router that’s directly connected to three networks some attention:

R2#config tR2(config)#no router eigrp 10R2(config)#router ospf 45678

R2(config-router)#network 10.0.0.0 0.255.255.255 area 0I can use any process ID I want—as long as it’s a value from 1 to 65,535. And notice I used the 10.0.0.0 with wildcard 0.255.255.255. This works well too.R3For the R3 router, we need to turn off RIP and EIGRP, although RIP won’t bother OSPF since OSPF has a lower AD. But we should turn it off anyway. And, as usual, we’ll use the SDM.

10089.book  Page 454  Monday, July 23, 2007  3:17 PM




Configuring OSPF455Our first screen shot shows RIP disabled.Our next graphic shows EIGRP being disabled. (I just clicked the Delete button.)

10089.book  Page 455  Monday, July 23, 2007  3:17 PM




456Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)After clicking on the OSPF tab, I clicked Add Network and added my OSPF information. I then clicked OK, chose my passive interfaces, and clicked OK again.We’re good—you can see that we have only OSPF running on the R3 router.

10089.book  Page 456  Monday, July 23, 2007  3:17 PM




Verifying OSPF Configuration457871WFinally, our last router! Let’s disable RIP and add OSPF:

871W#config t871W(config)#no router rip871W(config)#router ospf 1871W(config-router)#network 10.1.11.0 0.0.0.255 area 0

871W(config-router)#network 10.1.12.0 0.0.0.255 area 0Cool! Now that we’ve configured all the routers with OSPF, what’s next? Miller Time? Nope—not yet. It’s that verification thing again. We still have to make sure that OSPF is really working. That’s exactly what we’re going to do next.Verifying OSPF ConfigurationThere are several ways to verify proper OSPF configuration and operation, and in the follow-ing sections I’ll show you the OSPF show commands you need to know in order to do this. We’re going to start by taking a quick look at the routing table of the Corp router:So, let’s issue a show ip route command on the Corp router:

     10.0.0.0/24 is subnetted, 12 subnetsO       10.1.11.0 [110/65] via 10.1.5.2, 00:01:31, Serial0/2/0O       10.1.10.0 [110/65] via 10.1.5.2, 00:01:31, Serial0/2/0O       10.1.9.0 [110/74] via 10.1.4.2, 00:01:31, Serial0/1/0O       10.1.8.0 [110/65] via 10.1.4.2, 00:01:31, Serial0/1/0O       10.1.12.0 [110/66] via 10.1.5.2, 00:01:31, Serial0/2/0C       10.1.3.0 is directly connected, Serial0/0/1C       10.1.2.0 is directly connected, Serial0/0/0C       10.1.1.0 is directly connected, FastEthernet0/1O       10.1.7.0 [110/74] via 10.1.3.2, 00:01:32, Serial0/0/1                 [110/74] via 10.1.2.2, 00:01:32, Serial0/0/0O       10.1.6.0 [110/74] via 10.1.3.2, 00:01:32, Serial0/0/1                 [110/74] via 10.1.2.2, 00:01:32, Serial0/0/0C       10.1.5.0 is directly connected, Serial0/2/0

C       10.1.4.0 is directly connected, Serial0/1/0The Corp router shows the found routes for all 12 of our networks, with the O representing OSPF internal routes (the Cs are obviously our directly connected networks). It also found the dual routes to networks 10.1.6.0 and 10.1.7.0. I removed the bandwidth and delay commands from under the interface so the defaults are being used to determine the metric. But remember, OSPF only uses bandwidth to determine the best path to a network.

10089.book  Page 457  Monday, July 23, 2007  3:17 PM




458Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)

Important note: OSPF can load-balance only across links of equal costs. It can’t load-balance across unequal-cost links as EIGRP can.It’s time to show you all the OSPF verification commands that you need to know.The show ip ospf CommandThe show ip ospf command is used to display OSPF information for one or all OSPF processes running on the router. Information contained therein includes the Router ID, area information, SPF statistics, and LSA timer information. Let’s check out the output from the Corp router:

Corp#sh ip ospf Routing Process "ospf 132" with ID 10.1.5.1 Start time: 04:32:04.116, Time elapsed: 01:27:10.156 Supports only single TOS(TOS0) routes Supports opaque LSA Supports Link-local Signaling (LLS) Supports area transit capability Router is not originating router-LSAs with maximum metric Initial SPF schedule delay 5000 msecs Minimum hold time between two consecutive SPFs 10000 msecs Maximum wait time between two consecutive SPFs 10000 msecs Incremental-SPF disabled Minimum LSA interval 5 secs Minimum LSA arrival 1000 msecs LSA group pacing timer 240 secs Interface flood pacing timer 33 msecs Retransmission pacing timer 66 msecs Number of external LSA 0. Checksum Sum 0x000000 Number of opaque AS LSA 0. Checksum Sum 0x000000 Number of DCbitless external and opaque AS LSA 0 Number of DoNotAge external and opaque AS LSA 0 Number of areas in this router is 1. 1 normal 0 stub 0 nssa Number of areas transit capable is 0 External flood list length 0    Area BACKBONE(0)        Number of interfaces in this area is 5        Area has no authentication        SPF algorithm last executed 00:14:52.220 ago

10089.book  Page 458  Monday, July 23, 2007  3:17 PM




Verifying OSPF Configuration459        SPF algorithm executed 14 times        Area ranges are        Number of LSA 6. Checksum Sum 0x03C06F        Number of opaque link LSA 0. Checksum Sum 0x000000        Number of DCbitless LSA 0        Number of indication LSA 0        Number of DoNotAge LSA 0

        Flood list length 0Notice the Router ID (RID) of 10.1.5.1, which is the highest IP address configured on the router.The show ip ospf database CommandUsing the show ip ospf database command will give you information about the number of routers in the internetwork (AS) plus the neighboring router’s ID (this is the topology data-base I mentioned earlier). Unlike the show ip eigrp topology command, this command shows the “OSPF routers,” not each and every link in the AS as EIGRP does.The output is broken down by area. Here’s a sample output, again from Corp:

Corp#sh ip ospf database            OSPF Router with ID (10.1.5.1) (Process ID 132)                Router Link States (Area 0)Link ID         ADV Router      Age         Seq#       Checksum Link count10.1.5.1        10.1.5.1        72          0x80000002 0x00F2CA 910.1.7.1        10.1.7.1        83          0x80000004 0x009197 610.1.9.1        10.1.9.1        73          0x80000001 0x00DA1C 410.1.11.1       10.1.11.1       67          0x80000005 0x00666A 410.1.12.1       10.1.12.1       67          0x80000004 0x007631 2                Net Link States (Area 0)Link ID         ADV Router      Age         Seq#       Checksum

10.1.11.2       10.1.12.1       68          0x80000001 0x00A337You can see all five routers and the RID of each router (the highest IP address on each router). The router output shows the link ID—remember that an interface is also a link—and the RID of the router on that link under the ADV router, or advertising router.

10089.book  Page 459  Monday, July 23, 2007  3:17 PM




460Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)The show ip ospf interface CommandThe show ip ospf interface command displays all interface-related OSPF information. Data is displayed about OSPF information for all interfaces or for specified interfaces. (I’ll bold some of the important things.)

Corp#sh ip ospf interface f0/1FastEthernet0/1 is up, line protocol is up  Internet Address 10.1.1.1/24, Area 0  Process ID 132, Router ID 10.1.5.1, Network Type BROADCAST, Cost: 1  Transmit Delay is 1 sec, State DR, Priority 1  Designated Router (ID) 10.1.5.1, Interface address 10.1.1.1  No backup designated router on this network  Timer intervals configured, Hello 10, Dead 40, Wait 40, Retransmit 5    oob-resync timeout 40    Hello due in 00:00:01  Supports Link-local Signaling (LLS)  Index 1/1, flood queue length 0  Next 0x0(0)/0x0(0)  Last flood scan length is 0, maximum is 0  Last flood scan time is 0 msec, maximum is 0 msec  Neighbor Count is 0, Adjacent neighbor count is 0

  Suppress hello for 0 neighbor(s)The following information is displayed by this command: Interface IP address Area assignment Process ID Router ID Network type Cost Priority DR/BDR election information (if applicable) Hello and Dead timer intervals Adjacent neighbor informationThe reason I used the show ip ospf interface f0/1 command is because I knew that there would be a designated router elected on the FastEthernet broadcast multi-access net-work. We’ll get into DR and DBR elections in detail in a minute.

10089.book  Page 460  Monday, July 23, 2007  3:17 PM




 Verifying OSPF Configuration 461 The  show ip ospf neighbor  Command The  show ip ospf neighbor  command is super-useful because it summarizes the pertinent OSPF information regarding neighbors and the adjacency state. If a DR or BDR exists, that information will also be displayed. Here’s a sample:

 Corp# sh ip ospf neighbor Neighbor ID  Pri  State   Dead Time    Address     Interface10.1.11.1     0   FULL/  - 00:00:37    10.1.5.2     Serial0/2/010.1.9.1      0   FULL/  - 00:00:34    10.1.4.2     Serial0/1/010.1.7.1      0   FULL/  - 00:00:38    10.1.3.2     Serial0/0/1

 10.1.7.1      0   FULL/  - 00:00:34    10.1.2.2     Serial0/0/0 This is a super-important command to understand because it’s extremely useful in produc-tion networks. Let’s take a look at the R3 and 871W routers outputs:

 R3# sh ip ospf neighbor Neighbor ID  Pri  State   Dead Time    Address      Interface10.1.5.1      0   FULL/  -  00:00:39    10.1.5.1    Serial0/0/110.1.11.2     1   FULL/BDR  00:00:31    10.1.11.2   FastEthernet0/1871W# sh ip ospf nei Neihbor ID   Pri  State   Dead Time    Address      Interface

 10.1.11.1     1   FULL/DR   00:00:30    10.1.11.1     Vlan1 Since there’s an Ethernet link (broadcast multi-access) on the R3 router, there’s going to be an election to determine who will be the designated router and who will be the non-designated router. We can see that the 871W became the designated router, and it won because it had the highest IP address on the network. You can change this, but that’s the default.The reason that the Corp connections to R1, R2, and R3 don’t have a DR or BDR listed in the output is that by default, elections don’t happen on point-to-point links. But we can see that the Corp router is fully adjacent to all three routers (and on both connections to R1) from its output. The  show ip protocols  Command The  show ip protocols  command is also useful, whether you’re running OSPF, EIGRP, IGRP, RIP, BGP, IS-IS, or any other routing protocol that can be configured on your router. It provides an excellent overview of the actual operation of all currently running protocols.Check out the output from the Corp router:

 Corp# sh ip protocols Routing Protocol is "ospf 132"  Outgoing update filter list for all interfaces is not set  Incoming update filter list for all interfaces is not set

 

10089c07.fm  Page 461  Friday, November 7, 2008  11:04 PM




 462 Chapter7  Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)   Router ID 10.1.5.1  Number of areas in this router is 1. 1 normal 0 stub 0 nssa  Maximum path: 4  Routing for Networks:    10.1.1.1 0.0.0.0 area 0    10.1.2.1 0.0.0.0 area 0    10.1.3.1 0.0.0.0 area 0    10.1.4.1 0.0.0.0 area 0    10.1.5.1 0.0.0.0 area 0 Reference bandwidth unit is 100 mbps  Routing Information Sources:    Gateway         Distance      Last Update    10.1.11.1            110      00:28:53    10.1.11.2            110      00:28:53    10.1.9.1             110      00:28:53    10.1.7.1             110      00:28:53

   Distance: (default is 110) From looking at this output, you can determine the OSPF Process ID, OSPF Router ID, type of OSPF area, networks and areas configured for OSPF, and the OSPF Router IDs of neigh-bors—that’s a lot. Read, efficient! And hold on a second. Did you notice the absence of timers like the ones we were shown before in the RIP outputs from this command? That’s because link-state routing protocols don’t use timers to keep the network stable like distance-vector routing algorithms do. Debugging OSPF Debugging is a great tool for any protocol, so let’s take a look in Table 7.4 at a few debugging commands for troubleshooting OSPF. TABLE7.4 Debugging Commands for Troubleshooting OSPF CommandDescription/Function debug ip ospf packet Shows Hello packets being received on your router debug ip ospf hello Shows Hello packets being sent and received on your router. Shows more detail than the  debug ip ospf packet  output debug ip ospf adj Shows DR and DBR elections on a broadcast and non-broadcast multi-access network

 

10089c07.fm  Page 462  Wednesday, February 27, 2008  5:06 PM




Verifying OSPF Configuration463I’ll start by showing you the output from the Corp router I got using the debug ip ospf packet command:

Corp#debug ip ospf packetOSPF packet debugging is on*Mar 23 01:20:42.199: OSPF: rcv. v:2 t:1 l:48 rid:172.16.10.3      aid:0.0.0.0 chk:8075 aut:0 auk: from Serial0/1/0Corp#*Mar 23 01:20:45.507: OSPF: rcv. v:2 t:1 l:48 rid:172.16.10.2      aid:0.0.0.0 chk:8076 aut:0 auk: from Serial0/0/0*Mar 23 01:20:45.531: OSPF: rcv. v:2 t:1 l:48 rid:172.16.10.2      aid:0.0.0.0 chk:8076 aut:0 auk: from Serial0/0/1*Mar 23 01:20:45.531: OSPF: rcv. v:2 t:1 l:48 rid:172.16.10.4      aid:0.0.0.0 chk:8074 aut:0 auk: from Serial0/2/0*Mar 23 01:20:52.199: OSPF: rcv. v:2 t:1 l:48 rid:172.16.10.3      aid:0.0.0.0 chk:8075 aut:0 auk: from Serial0/1/0*Mar 23 01:20:55.507: OSPF: rcv. v:2 t:1 l:48 rid:172.16.10.2      aid:0.0.0.0 chk:8076 aut:0 auk: from Serial0/0/0*Mar 23 01:20:55.527: OSPF: rcv. v:2 t:1 l:48 rid:172.16.10.2      aid:0.0.0.0 chk:8076 aut:0 auk: from Serial0/0/1*Mar 23 01:20:55.531: OSPF: rcv. v:2 t:1 l:48 rid:172.16.10.4

      aid:0.0.0.0 chk:8074 aut:0 auk: from Serial0/2/0In the preceding output, we can see that our router is both sending and receiving Hello packets every 10 seconds from neighbor (adjacent) routers. The next command will provide us with the same information, but with more detail. For example, we can see the multicast address used (224.0.0.5) and the area:

Corp#debug ip ospf hello*Mar 23 01:18:41.103: OSPF: Send hello to 224.0.0.5 area 0 on   Serial0/1/0 from 10.1.4.1*Mar 23 01:18:41.607: OSPF: Send hello to 224.0.0.5 area 0 on   FastEthernet0/1 from 10.1.1.1*Mar 23 01:18:41.607: OSPF: Send hello to 224.0.0.5 area 0 on   Serial0/0/0 from 10.1.2.1*Mar 23 01:18:41.611: OSPF: Send hello to 224.0.0.5 area 0 on   Serial0/2/0 from 10.1.5.1*Mar 23 01:18:41.611: OSPF: Send hello to 224.0.0.5 area 0 on   Serial0/0/1 from 10.1.3.1*Mar 23 01:18:42.199: OSPF: Rcv hello from 172.16.10.3 area 0 from   Serial0/1/0 10.1.4.2*Mar 23 01:18:42.199: OSPF: End of hello processing

10089.book  Page 463  Monday, July 23, 2007  3:17 PM




464Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)*Mar 23 01:18:45.519: OSPF: Rcv hello from 172.16.10.2 area 0 from   Serial0/0/0 10.1.2.2*Mar 23 01:18:45.519: OSPF: End of hello processing*Mar 23 01:18:45.543: OSPF: Rcv hello from 172.16.10.2 area 0 from   Serial0/0/1 10.1.3.2*Mar 23 01:18:45.543: OSPF: End of hello processing*Mar 23 01:18:45.543: OSPF: Rcv hello from 172.16.10.4 area 0 from   Serial0/2/0 10.1.5.2

*Mar 23 01:18:45.543: OSPF: End of hello processingThe last debug command I’m going show you is the debug ip ospf adj command that will show us elections as they occur on broadcast and non-broadcast multi-access networks:

Corp#debug ip ospf adjOSPF adjacency events debugging is on*Mar 23 01:24:34.823: OSPF: Interface FastEthernet0/1 going Down*Mar 23 01:24:34.823: OSPF: 172.16.10.1 address 10.1.1.1 on   FastEthernet0/1 is dead, state DOWN*Mar 23 01:24:34.823: OSPF: Neighbor change Event on interface   FastEthernet0/1*Mar 23 01:24:34.823: OSPF: DR/BDR election on FastEthernet0/1*Mar 23 01:24:34.823: OSPF: Elect BDR 0.0.0.0*Mar 23 01:24:34.823: OSPF: Elect DR 0.0.0.0*Mar 23 01:24:34.823: OSPF: Elect BDR 0.0.0.0*Mar 23 01:24:34.823: OSPF: Elect DR 0.0.0.0*Mar 23 01:24:34.823:        DR: none    BDR: none*Mar 23 01:24:34.823: OSPF: Flush network LSA immediately*Mar 23 01:24:34.823: OSPF: Remember old DR 172.16.10.1 (id)*Mar 23 01:24:35.323: OSPF: We are not DR to build Net Lsa for   interface FastEthernet0/1*Mar 23 01:24:35.323: OSPF: Build router LSA for area 0, router ID   172.16.10.1, seq 0x80000006*Mar 23 01:24:35.347: OSPF: Rcv LS UPD from 172.16.10.2 on Serial0/0/1   length 148 LSA count 1*Mar 23 01:24:40.703: OSPF: Interface FastEthernet0/1 going Up*Mar 23 01:24:41.203: OSPF: Build router LSA for area 0, router ID   172.16.10.1, seq 0x80000007*Mar 23 01:24:41.231: OSPF: Rcv LS UPD from 172.16.10.2 on Serial0/0/1

   length 160 LSA count 1All right—let’s move on and discover how elections occur in an OSPF network.

10089.book  Page 464  Monday, July 23, 2007  3:17 PM




OSPF DR and BDR Elections465OSPF DR and BDR ElectionsIn this chapter, I have discussed OSPF in detail; however, I need to expand the section on des-ignated routers and backup designated routers that I’ve only briefly touched on so far. I’m also going to delve deeper into verifying the election process as well as provide you with a hands-on lab at the end of the chapter to help you understand that process even better.To start with, I need to make sure you fully understand the terms neighbors and adjacencies again because they’re really crucial to the DR and BDR election process. The election process happens when a broadcast or non-broadcast multi-access network is connected to a router and the link comes up. (Think Ethernet or Frame Relay.)NeighborsRouters that share a common segment become neighbors on that segment. These neighbors are elected via the Hello protocol. Hello packets are sent periodically out of each interface using IP multicast.Two routers won’t become neighbors unless they agree on the following:Area IDThe idea here is that the two routers’ interfaces have to belong to the same area on a particular segment. And of course, those interfaces have to belong to the same subnet.AuthenticationOSPF allows for the configuration of a password for a specific area. Although authentication between routers isn’t required, you have the option to set it if you need to do so. Also, keep in mind that in order for routers to become neighbors, they need to have the same password on a segment if you’re using authentication.Hello and Dead intervalsOSPF exchanges Hello packets on each segment. This is a keepalive system used by routers to acknowledge their existence on a segment and for electing a designated router (DR) on both broadcast and non-broadcast multi-access segments.The Hello interval specifies the number of seconds between Hello packets. The Dead interval is the number of seconds that a router’s Hello packets can go without being seen before its neighbors declare the OSPF router dead (down). OSPF requires these intervals to be exactly the same between two neighbors. If any of these intervals are different, the routers won’t become neighbors on that segment. You can see these timers with the show ip ospf interface command.AdjacenciesIn the election process, adjacency is the next step after the neighboring process. Adjacent routers are routers that go beyond the simple Hello exchange and proceed into the database exchange process. In order to minimize the amount of information exchanged on a particular segment, OSPF elects one router to be a designated router (DR) and one router to be a backup designated router (BDR) on each multi-access segment.The BDR is elected as a backup router in case the DR goes down. The idea behind this is that routers have a central point of contact for information exchange. Instead of each router 

10089.book  Page 465  Monday, July 23, 2007  3:17 PM




466Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)exchanging updates with every other router on the segment, every router exchanges informa-tion with the DR and BDR. The DR and BDR then relay the information to everybody else.DR and BDR ElectionsDR and BDR election is accomplished via the Hello protocol. Hello packets are exchanged via IP multicast packets on each segment. However, only segments that are broadcast and non-broadcast multi-access networks (such as Ethernet and Frame Relay) will perform DR and BDR elections. Point-to-point links, like a serial WAN for example, will not have a DR election process.On a broadcast or non-broadcast multi-access network, the router with the highest OSPF pri-ority on a segment will become the DR for that segment. This priority is shown with the show ip ospf interface command, which is set to 1 by default. If all routers have the default priority set, the router with the highest Router ID (RID) will win.As you know, the RID is determined by the highest IP address on any interface at the moment of OSPF startup. This can be overridden with a loopback (logical) interface, which I’ll talk about in the next section.If you set a router’s interface to a priority value of zero, that router won’t participate in the DR or BDR election on that interface. The state of the interface with priority zero will then be DROTHER.Now let’s play with the RID on an OSPF router.OSPF and Loopback InterfacesConfiguring loopback interfaces when using the OSPF routing protocol is important, and Cisco suggests using them whenever you configure OSPF on a router.Loopback interfaces are logical interfaces, which are virtual, software-only interfaces; they are not real router interfaces. Using loopback interfaces with your OSPF configuration ensures that an interface is always active for OSPF processes.They can be used for diagnostic purposes as well as OSPF configuration. The reason you want to configure a loopback interface on a router is because if you don’t, the highest IP address on a router will become that router’s RID. The RID is used to advertise the routes as well as elect the DR and BDR.

By default, OSPF uses the highest IP address on any active interface at the moment of OSPF startup. However, this can be overridden by a logical inter-face. The highest IP address of any logical interface will always become a router’s RID.In the following sections, you will see how to configure loopback interfaces and how to verify loopback addresses and RIDs.

10089.book  Page 466  Monday, July 23, 2007  3:17 PM




OSPF and Loopback Interfaces467Configuring Loopback InterfacesConfiguring loopback interfaces rocks mostly because it’s the easiest part of OSPF configuration, and we all need a break about now—right? So hang on—we’re in the home stretch!First, let’s see what the RID is on the Corp router with the show ip ospf command:

Corp#sh ip ospf Routing Process "ospf 132" with ID 10.1.5.1

[output cut]We can see that the RID is 10.1.5.1, or the serial 0/2/0 interface of the router. So let’s con-figure a loopback interface using a completely different IP addressing scheme:

Corp(config)#int loopback 0*Mar 22 01:23:14.206: %LINEPROTO-5-UPDOWN: Line protocol on Interface   Loopback0, changed state to up

Corp(config-if)#ip address 172.16.10.1 255.255.255.255The IP scheme really doesn’t matter here, but each router has to be in a separate subnet. By using the /32 mask, we can use any IP address we want as long as the addresses are never the same on any two routers.Let’s configure the other routers:

R1#config tR1(config)#int loopback 0*Mar 22 01:25:11.206: %LINEPROTO-5-UPDOWN: Line protocol on Interface   Loopback0, changed state to up

R1(config-if)#ip address 172.16.10.2 255.255.255.255Here’s the configuration of the loopback interface on R2:

R2#config tR2(config)#int loopback 0*Mar 22 02:21:59.686: %LINEPROTO-5-UPDOWN: Line protocol on Interface   Loopback0, changed state to up

R2(config-if)#ip address 172.16.10.3 255.255.255.255Here’s the configuration of the loopback interface on R3:

R3#config tR3(config)#int loopback 0*Mar 22 02:01:49.686: %LINEPROTO-5-UPDOWN: Line protocol on Interface   Loopback0, changed state to up

R3(config-if)#ip address 172.16.10.4 255.255.255.255

10089.book  Page 467  Monday, July 23, 2007  3:17 PM




468Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)I’m going to hold off on setting a logical interface on the 871W router. You’ll find out why in a minute.I’m pretty sure you’re wondering what the IP address mask of 255.255.255.255 (/32) means and why we don’t just use 255.255.255.0 instead. Well, either mask works, but the /32 mask is called a host mask and works fine for loopback interfaces, and it allows us to save subnets. Notice how I was able to use 172.16.10.1, .2, .3, and .4? If I didn’t use the /32, I’d have to use a separate subnet for each and every router!Now, before we move on, did we actually change the RIDs of our router by setting the loop-back interfaces? Let’s check into that by taking a look at the Corp’s RID:

Corp#sh ip ospf

 Routing Process "ospf 132" with ID 10.1.5.1What happened? You’d think that because we set logical interfaces, the IP addresses under the logical interfaces automatically become the RID of the router, right? Well, sort of—but only if you do one of two things: either reboot the router or delete OSPF and re-create the data-base on your router. And neither is really that great an option.I’m going with rebooting the Corp router because it’s the easier of the two.Now let’s look and see what our RID is:

Corp#sh ip ospf

 Routing Process "ospf 132" with ID 172.16.1.1Okay, that did it. The Corp router now has a new RID! So I guess I’ll just go ahead and reboot all my routers (except the 871W) to get their RIDs reset to our logical addresses.Or not—there is one other way. What would you say about adding a new RID for the router right under the router ospf process-id command instead? I’d say let’s give it a shot! Here’s an example of doing that on the 871W router:

871W#sh ip ospf Routing Process "ospf 1" with ID 10.1.12.1871W#config t871W(config)#router ospf 1871W(config-router)#router-id 172.16.10.5Reload or use "clear ip ospf process" command, for this to take effect871W(config-router)#do clear ip ospf processReset ALL OSPF processes? [no]: yes*Mar 23 01:33:00.051: OSPF: Rcv LS UPD from 172.16.10.4 on Serial0/2/0   length 76 LSA count 1*Mar 23 01:33:00.071: OSPF: Rcv LS UPD from 172.16.10.2 on Serial0/0/1   length 76 LSA count 1871W(config-router)#do sh ip ospf

Routing Process "ospf 1" with ID 172.16.10.5

10089.book  Page 468  Monday, July 23, 2007  3:17 PM




OSPF and Loopback Interfaces469Take a look at that—it worked! We changed the RID without reloading the router! But wait—remember, we didn’t set a loopback (logical interface) yet. So let’s try that now—let’s set a logical interface IP address, reload the router, and see if the loopback interface overrides the router-id command we just used:

871W(config-router)#int lo0871W(config-if)#ip address 172.16.10.6 255.255.255.255871W(config-if)#^Z871W#reloadSystem configuration has been modified. Save? [yes/no]: yBuilding configuration...871W#sh ip ospf

 Routing Process "ospf 1" with ID 172.16.10.5Well, there’s our answer. A logical (loopback) interface will not override the router-id command, and we don’t have to reboot the router to make it take effect as the RID.The only thing left now is to decide whether you want to advertise the loopback interfaces under OSPF. There are pros and cons to using an address that won’t be advertised versus using an address that will be. Using an unadvertised address saves on real IP address space, but the address won’t appear in the OSPF table, which means you can’t ping it.So basically, what you’re faced with here is a choice that equals a trade-off between the ease of debugging the network and conservation of address space—what to do? A really tight strategy is to use a private IP address scheme as I did. Do this, and all will be well!OSPF Interface PrioritiesAnother way to configure DRs and BDRs in OSPF is to “fix” elections instead of using loopback interfaces. We can do this by configuring interfaces on our router to gain a better priority over another router when elections occur. In other words, we can use priorities instead of logical addresses to force a certain router to become the DR or BDR in a network.Let’s use Figure 7.6 as an example. Looking at Figure 7.6, what options would you use to ensure that the R2 router will be elected the designated router (DR) for the LAN (broadcast multi-access) segment? The first thing you’d need to do is determine what the RID is of each router and which router is the default DR for the 172.16.1.0 LAN.At this point, we can see that R3 will be the DR by default because it has the highest RID of 192.168.11.254. That gives us three options to ensure that R2 will be elected the DR for the LAN segment 172.16.1.0/24: Configure the priority value of the Fa0/0 interface of the R2 router to a higher value than any other interface on the Ethernet network. Configure a loopback interface on the R2 with an IP address higher than any IP address on the other routers. Change the priority value of the Fa0/0 interface of R1 and R3 to zero.

10089.book  Page 469  Monday, July 23, 2007  3:17 PM




470Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)FIGURE7.6Ensuring your designated routerIf we set a priority of zero (0) on the R1 and R3 routers, they wouldn’t be allowed to par-ticipate in the election process. But that may not be the best way to go—we might just be better off choosing options one and two.Since you already know how to configure a loopback (logical) interface, here’s how to set a priority on the Fa0/0 interface on the R2 router:

R2#config tR2(config)#int f0/0R2(config-if)#ip ospf priority ?  <0-255>  Priority

R2(config-if)#ip ospf priority 2That’s it! All router interfaces default to a priority of 1, so by setting this interface to 2, I’ve ensured that it will automatically become the DR of the LAN segment. Setting an interface to 255 means that no one can beat your router!Hold on though. Even if you change the priority of the interface, the router will not become the DR of the LAN segment until both the existing DR and the BDR are shut down. Once an election occurs, that’s all she wrote, and the election won’t happen again until the DR and BDR are reloaded and/or shut down. Just having a router with a better RID come up on your network doesn’t mean your DR or BDR will change!You can see your priority with the show ip ospf interface command:

R2(config-if)#do show ip ospf int f0/0FastEthernet0/0 is up, line protocol is up

What options can you configure that will ensure that R2 will be the DR of the LAN segment?  

R2 Area 0 

R1 192.168.10.254/24 Fa0/0 

R3 Fa0/1 Fa0/0 192.168.0.101/15 s0/0 Fa0/0 172.16.1.123/24 172.16.1.124/24 Switch 172.16.1.125/24 Fa0/1 192.168.11.254/24 

10089.book  Page 470  Monday, July 23, 2007  3:17 PM




Troubleshooting OSPF471  Internet Address 10.1.13.1/24, Area 0  Process ID 132, Router ID 172.16.30.1, Network Type BROADCAST,Cost:1

  Transmit Delay is 1 sec, State UP, Priority 2

Remember, you can see the elections occur on a broadcast or non-broadcast multi-access network with the debug ip ospf adj command.Troubleshooting OSPFThis section will have you verify sample OSPF configurations and configuration outputs in order to troubleshoot, maintain, and fix OSPF-related issues.If you see a configuration as shown here, you must know that there is no way a router will accept this input because the wildcard is incorrect:

Router(config)#router ospf 1

Router(config-router)#network 10.0.0.0 255.0.0.0 area 0This would be correct statement:

Router(config)#router ospf 1

Router(config-router)#network 10.0.0.0 0.255.255.255 area 0Next, let’s take a look at a figure and see if we can determine which of the routers will become the designated router of the area. Figure 7.7 shows a network with six routers con-nected by two switches and a WAN link.Looking at Figure 7.7, which routers are likely to be elected as a designated router (DR)? All the router OSPF priorities are at the default.Notice the RIDs of each router. The routers with the highest RIDs are routers A and B, since they have the highest IP addresses. RouterB should be the DR and RouterA should be the BDR. Okay, now here’s the thing: Since elections do not occur on point-to-point links by default, the top LAN would have its own election. But since you’re reading this because you’re studying for the CCNA exam objectives, RouterB is the best answer.Let’s use another command to verify an OSPF configuration: the show ip ospf interface command. Look at the following output for routers A and B and see if you can determine why the two directly connected routers cannot establish an adjacency:

RouterA#sh ip ospf interface e0/0Ethernet0/0 is up, line protocol is up  Internet Address 172.16.1.2/16, Area 0  Process ID 2, Router ID 172.126.1.1, Network Type BROADCAST, Cost: 10  Transmit Delay is 1 sec, State DR, Priority 1  Designated Router (ID) 172.16.1.2, interface address 172.16.1.1

10089.book  Page 471  Monday, July 23, 2007  3:17 PM




472Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)  No backup designated router on this network  Timer intervals configured, Hello 5, Dead 20, Wait 20, Retransmit 5RouterB#sh ip ospf interface e0/0Ethernet0/0 is up, line protocol is up  Internet Address 172.16.1.1/16, Area 0  Process ID 2, Router ID 172.126.1.1, Network Type BROADCAST, Cost: 10  Transmit Delay is 1 sec, State DR, Priority 1  Designated Router (ID) 172.16.1.1, interface address 172.16.1.2  No backup designated router on this network

  Timer intervals configured, Hello 10, Dead 40, Wait 40, Retransmit 5Everything in the two outputs looks pretty good, except that the Hello and Dead timers are not the same. RouterA has Hello and Dead timers of 5 and 20, and RouterB has Hello and Dead timers of 10 and 40, which are the default timers for OSPF. If two directly connected routers do not have the timers set the same, they will not form an adjacency. Notice also that the show ip ospf interface command will show you who the designated and backup des-ignated routers (DR/BDR) are for your area.FIGURE7.7Designated router example

RID: 192.166.2.22/24RID: 172.16.10.1/24RouterF

RID: 10.1.1.2/24RouterE

RID: 172.31.100.2/24RouterD

RID: 192.168.168.1/24RouterB

RID: 192.168.10.1/24RouterA

RouterC

10089.book  Page 472  Monday, July 23, 2007  3:17 PM




Troubleshooting OSPF473Take a look at the network shown in Figure 7.8 with four routers and two different routing protocols.If all parameters are set to default and redistribution is not configured, which path do you think RouterA will use to reach RouterD? Since IGRP has an AD of 100 and OSPF has an AD of 110, RouterA will send packets to RouterD through RouterC.Study Figure 7.9 carefully. You are running OSPF on the routers as shown and an ISDN link provides connectivity to the remote sales office.What type of route should be configured on the Corporate router to connect to the sales office’s remote network while minimizing network overhead on the ISDN link as shown in Figure 7.9?The best solution to this problem is to dump the ISDN link and connect a broadband link from the remote office to the Internet and then create a VPN from the Corporate office to the remote office through the Internet. Yeah, well, wouldn’t that be nice? Anyway, the question asks how we can make this work with the ISDN link and minimize network overhead. The only way we can do that is to create a static route on the Corporate router to connect to the remote network; anything else would be too bandwidth intensive.FIGURE7.8Multiple routing protocols and OSPF

RouterA 

RouterD

RouterCRouterB

OSPFArea 0IGRP 1

10089.book  Page 473  Monday, July 23, 2007  3:17 PM




474Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)FIGURE7.9OSPF and ISDN connectivityConfiguring EIGRP and OSPF Summary RoutesThis section will provide you with the commands to summarize both EIGRP and OSPF. Although OSPF can be summarized a few different ways, I’ll provide the most common OSPF summary com-mand, which summarizes multiple-area OSPF networks into area 0.You learned in Chapter 3 how to determine summary routes for a network. This section will have you apply the summary routes to a router’s configuration.Figure 7.10 shows a contiguous network design—yes, contiguous networks do not happen by accident; they have to be planned! Figure 7.10 shows six networks with four block sizes of 4 (WAN links) and two block sizes of 8 (LAN connections). This network design fits nicely into a block size of 32. The network address used is 192.168.10.64 and with a block size of 32, the mask would be 255.255.255.224—because as you know, 224 provides a block size of 32.On the core (backbone connection) router, for EIGRP we’ll place the summary route on Ethernet0, which will advertise our summary route out to the backbone network (10.10.10.0 network). This will stop all six of our networks from being advertised individually and instead advertise them as one route to the other routers in the internetwork. However, it is imperative that no other router outside our contiguous networks have a subnet in this advertised block behind it, which would allow it to advertise conflicting routes.

ISP

OSPFArea 1Remote Office LAN

Corporate 

10089.book  Page 474  Monday, July 23, 2007  3:17 PM




Configuring EIGRP and OSPF Summary Routes475Here is the complete configuration of EIGRP on the core router:

Core#config tCore(config)#router eigrp 10Core(config-router)#network 192.168.10.0Core(config-router)#network 10.0.0.0Core(config-router)#no auto-summaryCore(config-router)#interface ethernet 0

Core(config-if)#ip summary-address eigrp 10 192.168.10.64 255.255.255.224FIGURE7.10Contiguous network designThe above EIGRP configuration for autonomous system 10 advertises directly connected networks 192.168.10.0 and 10.0.0.0. Since EIGRP auto-summarizes at classful boundaries, you must use the no auto-summary command as well. The summary route we will advertise to the backbone network is placed on the interface connected to the backbone, not under the routing process. This summary route tells EIGRP to find all networks in the 192.168.10.64 network with a block size of 32 and advertise them as one route out interface E0. This means, basically, that any packet with a destination IP address of 192.168.10.64 through 192.168.10.95 will be forwarded via this summary route.To summarize the contiguous network with OSPF we used with the EIGRP example we need to configure OSPF into multiple areas, as shown in Figure 7.11.To summarize area 1 into the area 0 backbone, use the following command under the OSPF Process ID. Here is the complete OSPF configuration for the Core (backbone) router:

Core#config tCore(config)#router ospf 1Core(config-router)#network 192.168.10.64 0.0.0.3 area 1Core(config-router)#network 192.168.10.68 0.0.0.3 area 1

192.168.10.80/29192.168.10.88/29192.168.10.72/30192.168.10.76/30192.168.10.64/30192.168.10.68/30E0: 10.10.10.0/24

10089.book  Page 475  Monday, July 23, 2007  3:17 PM




476Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)Core(config-router)#network 10.10.10.0 0.0.0.255 area 0

Core(config-router)#area 1 range 192.168.10.64 255.255.255.224The no auto-summary command is not needed since OSPF does not summarize at any boundary by default. The preceding OSPF configuration will summarize all the networks from area 1 to the backbone area as one entry of 192.168.10.64/27.FIGURE7.11OSPF multiple area designSummaryI know—this chapter has been, you could say, a touch on the extensive side. But it’s really impor-tant! EIGRP, the main focus of the chapter, is a hybrid of link-state routing and distance-vector protocols. It allows for unequal-cost load balancing, controlled routing updates, and formal neighbor adjacencies.EIGRP uses the capabilities of the Reliable Transport Protocol (RTP) to communicate between neighbors and utilizes the Diffusing Update Algorithm (DUAL) to compute the best path to each remote network.EIGRP also supports large networks through features such as support for VLSM, discon-tiguous networks, and summarization. The ability to configure EIGRP behavior over NBMA networks also makes it a really hot protocol for large networks.I also went over the configuration of EIGRP and explored a number of troubleshooting commands.This chapter also provided you with a great deal of information about OSPF. It’s really dif-ficult to include everything about OSPF because so much of it falls outside the scope of this chapter and book, but I’ve given you a few tips here and there, so you’re good to go—as long as you make sure you’ve got what I presented to you dialed in, that is!

192.168.10.68/30

192.168.10.64/30E0 10.10.10.0/24Area1Area 0

10089.book  Page 476  Monday, July 23, 2007  3:17 PM




Written Lab 7477I talked about a lot of OSPF topics, including terminology, operations, and configuration as well as verification and monitoring.Each of these topics encompasses quite a bit of information—the terminology section just scratched the surface of OSPF. But you’ve got the goods for your studies—things like config-uring single-area OSPF, VLSM implementation, and summarizing contiguous boundaries. Finally, I gave you a tight survey of commands useful in observing the operation of OSPF so you can verify that things are moving along as they should. So eat it all up, and you’re set!Exam EssentialsKnow EIGRP features.EIGRP is a classless, advanced distance-vector protocol that sup-ports IP, IPX, AppleTalk, and now IPv6. EIGRP uses a unique algorithm, called DUAL, to maintain route information and uses RTP to communicate with other EIGRP routers reliably.Know how to configure EIGRP.Be able to configure basic EIGRP. This is configured the same as IGRP with classful addresses.Know how to verify EIGRP operation.Know all of the EIGRP show commands and be familiar with their output and the interpretation of the main components of their output.Compare OSPF and RIPv1.OSPF is a link-state protocol that supports VLSM and classless routing; RIPv1 is a distance-vector protocol that does not support VLSM and supports only classful routing.Know how OSPF routers become neighbors and/or adjacent.OSPF routers become neigh-bors when each router sees the other’s Hello packets.Know the different OSPF NBMA network types.There are five different OSPF network types that Cisco routers can be configured to support. Two of these are non-proprietary based (non-broadcast and point-to-multipoint) and three are Cisco proprietary (broadcast, point-to-point, and point-to-multipoint non-broadcast). Each network type is further characterized by how routers become adjacent and whether they require the election of a DR/BDR.Be able to configure single-area OSPF.A minimal single-area configuration involves only two commands: router ospf process-id and network x.x.x.x y.y.y.y area Z.Be able to verify the operation of OSPF.There are many show commands that provide use-ful details on OSPF, and it is useful to be completely familiar with the output of each: show ip ospf, show ip ospf database, show ip ospf interface, show ip ospf neighbor, and show ip protocols.Written Lab 71.What four routed protocols are supported by EIGRP?2.When is redistribution required for EIGRP?

10089.book  Page 477  Monday, July 23, 2007  3:17 PM




478Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)3.What command would be used to enable EIGRP with an autonomous system number of 300?4.What command will tell EIGRP that it is connected to network 172.10.0.0?5.What type of EIGRP interface will neither send nor receive Hello packets?6.Write the command that will enable OSPF process 101 on a router.7.Write the command that will display details of all OSPF routing processes enabled on a router.8.Write the command that will display interface-specific OSPF information.9.Write the command that will display all OSPF neighbors.10.Write the command that will display all different OSPF route types that are currently known by the router.(The answers to Written Lab 7 can be found following the answers to the Review Questions for this chapter.)Hands-on LabsIn this section, you will use the following network and add EIGRP and OSPF routing.The first lab (Lab 7.1) requires you to configure three routers for EIGRP and then view the configuration. In the last four labs, you will be asked to enable OSPF routing on the same net-work. Note that the labs in this chapter were written to be used with real equipment.

You must remove EIGRP before starting Labs 7.2–7.4 because the routing pro-tocols have a lower administrative distance than OSPF.The labs in this chapter are as follows:Lab 7.1: Configuring and Verifying EIGRPLab 7.2: Enabling the OSPF ProcessLab 7.3 Configuring OSPF NeighborsLab 7.4: Verifying OSPF OperationLab 7.5: OSPF DR and DBR Elections

172.16.10.0172.16.20.0172.16.30.0172.16.40.0172.16.50.0

S0

E02501A

F0/02621A

S1

S0

E02501B

E0

S02501C

10089.book  Page 478  Monday, July 23, 2007  3:17 PM




 Hands-on Labs 479 Table 7.5 shows our IP addresses for each router (each interface uses a /24 mask). Hands-on Lab 7.1: Configuring and Verifying EIGRP 1. Implement EIGRP on 2621A: 2621A# conf t Enter configuration commands, one per line.  End with CNTL/Z.2621A(config)# router eigrp 100 2621A(config-router)# network 172.16.0.0 2621A(config-router)# ^Z

 2621A# 2. Implement EIGRP on 2501A: 2501A# conf t Enter configuration commands, one per line.  End with CNTL/Z.2501A(config)# router eigrp 100 2501A(config-router)# network 172.16.0.0 2501A(config-router)# exit

 2501A# TABLE7.5 Our IP Addresses routerInterfaceIP address 2621AF0/0172.16.10.12501AE0172.16.10.22501AS0172.16.20.12501BE0172.16.30.12501BS0172.16.20.22501BS1172.16.40.12501CS0172.16.40.22501CE0172.16.50.1

 

10089c07.fm  Page 479  Friday, November 7, 2008  11:04 PM




480Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)3.Implement EIGRP on 2501B:2501B#conf tEnter configuration commands, one per line.  End with CNTL/Z.2501B(config)#router eigrp 1002501B(config-router)#network 172.16.0.02501B(config-router)#^Z

2501B#4.Implement EIGRP on 2501C:2501C#conf tEnter configuration commands, one per line.  End with CNTL/Z.2501C(config)#router eigrp 1002501C(config-router)#network 172.16.0.02501C(config-router)#^Z

2501C#5.Display the topology table for 2501B:

2501B#show ip eigrp topology6.Display the routing table on the 2501B router:

2501B#show ip route7.Display the neighbor table on the 2501B router:

2501B#show ip eigrp neighborHands-on Lab 7.2: Enabling the OSPF Process1.Enable OSPF process 100 on 2621A:2621A#conf tEnter configuration commands, one per line.  End with CNTL/Z.2621A(config)#router ospf 100

2621A(config-router)#^Z2.Enable OSPF process 101 on 2501A:2501A#conf tEnter configuration commands, one per line.  End with CNTL/Z.2501A(config)#router ospf 101

2501A(config-router)#^Z

10089.book  Page 480  Monday, July 23, 2007  3:17 PM




Hands-on Labs4813.Enable OSPF process 102 on 2501B:2501B#conf tEnter configuration commands, one per line.  End with CNTL/Z.2501B(config)#router ospf 102

2501B(config-router)#^Z4.Enable OSPF process 103 on 2501C:2501C#conf tEnter configuration commands, one per line.  End with CNTL/Z.Router(config)#router ospf 103

2501C(config-router)#^ZHands-on Lab 7.3: Configuring OSPF Neighbors1.Configure the network between 2621A and 2501A. Assign it to area 0:2621A#conf tEnter configuration commands, one per line.  End with CNTL/Z.2621A(config)#router ospf 1002621A(config-router)#network 172.16.10.1 0.0.0.0 area 02621A(config-router)#^Z

2621A#2.Configure the networks on the 2501A router. Assign them to area 0:2501A#conf tEnter configuration commands, one per line.  End with CNTL/Z.2501A(config)#router ospf 1012501A(config-router)#network 172.16.10.2 0.0.0.0 area 02501A(config-router)#network 172.16.20.1 0.0.0.0  area 02501A(config-router)#^Z

2501A#3.Configure the networks on the 2501B router. Assign them to area 0:2501B#conf tEnter configuration commands, one per line.  End with CNTL/Z.2501B(config)#router ospf 1022501B(config-router)#network 172.16.20.2 0.0.0.0 area 0

10089.book  Page 481  Monday, July 23, 2007  3:17 PM




482Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)2501B(config-router)#network 172.16.30.1 0.0.0.0 area 02501B(config-router)#network 172.16.40.1 0.0.0.0 area 02501B(config-router)#^Z

2501B#4.Configure the networks on the 2501C router. Assign them to area 0:2501C#conf tEnter configuration commands, one per line.  End with CNTL/Z.2501C(config)#router ospf 1032501C(config-router)#network 172.16.40.2 0.0.0.0 area 02501C(config-router)#network 172.16.50.1 0.0.0.0 area 02501C(config-router)#^Z

2501C#Hands-on Lab 7.4: Verifying OSPF Operation1.Execute a show ip ospf neighbors command from the 2621 router and view the results:

2621A#sho ip ospf neig2.Execute a show ip route command to verify that all other routers are learning all routes:

2621A#sho ip routeHands-on Lab 7.5: OSPF DR and BDR ElectionsIn this lab, you’ll watch the DR and BDR elections on your test network by forcing and ver-ifying the election process. You’re going to start by using Figure 7.12 to build your network. The more routers you have the better, but you need at least three routers connected via a LAN segment to complete this lab.FIGURE7.12OSPF hands-on lab network diagram

In this lab, I am using 2500 series routers, but you can use any type of router with any type of LAN interface. Or you can use the Sybex or RouterSim soft-ware products instead of real routers.

Lab_A Lab_B Lab_C EO EO EO 

10089.book  Page 482  Monday, July 23, 2007  3:17 PM




Hands-on Labs4831.First, connect the network together as shown in Figure 7.12. Create an IP scheme for the network—something simple like 10.1.1.1/24, 10.1.1.2/24, and 10.1.1.3/24 will work great.2.Now configure OSPF, and place all routers into area 0. Only the Ethernet LAN interface needs to be configured in this lab because, as you know, elections don’t take place on serial connections.3.Next, type show ip ospf interface e0 on each router to verify Area ID, DR, BDR infor-mation, and the Hello and Dead timers of the interface connected to the LAN network4.By looking at the show ip ospf interface e0 output, determine which router is the DR and which router is the BDR.5.Now verify the network type of your router. Because the connection is on an Ethernet LAN, the network type is BROADCAST. If you were viewing a serial connection, you’d want a point-to-point network.6.Here you have to set the priority for the router. The priority of all routers, by default, is 1. If you were to change the priority to 0, the router would never participate in the election process for the LAN. (Remember that elections do not occur on serial point-to-point links.)7.Now you need to decide which router will be the new DR.8.Next, enable the debugging process that allows you to see the DR and BDR election take place. Type debug ip ospf adjacency on all your routers.

Try to open more than one console connection by telnetting into the other routers. Remember to use the terminal monitor command on the Telnet ses-sion or you won’t see any debugging output.9.Here, set the priority of the new DR Ethernet 0 interface to 3 by typing ip ospf priority 3.10.Next, shut down the Ethernet interface of the DR router and bring it back up with the no shutdown command. Obviously, if you’re telnetted into that router, you’ll lose your session at this point.11.Here’s where the election should take place and the router you picked to be the DR should now actually be the DR.12.Finally, type show ip ospf interface e0 to verify the DR and BDR information on each router.

The priority of a router’s interface can be set all the way up to 255, which means it will always be the DR of the area. You can then set a router in your test net-work with a higher priority and see that the priority takes precedence over a high RID on a router, even if you are using a loopback (logical) interface.

10089.book  Page 483  Monday, July 23, 2007  3:17 PM




484Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)Review Questions

The following questions are designed to test your understanding of this chapter’s material. For more information on how to get additional ques-tions, please see this book’s Introduction.1.Your company is running IGRP using an AS of 10. You want to configure EIGRP on the net-work but want to migrate slowly to EIGRP and don’t want to configure redistribution. What command would allow you to migrate over time to EIGRP without configuring redistribution?A.router eigrp 11B.router eigrp 10C.router eigrp 10 redistribute igrpD.router igrp combine eigrp 102.Which EIGRP information is held in RAM and maintained through the use of Hello and update packets? (Choose two.)A.Neighbor tableB.STP tableC.Topology tableD.DUAL table3.Which of the following describe the process identifier that is used to run OSPF on a router? (Choose two.)A.It is locally significant.B.It is globally significant.C.It is needed to identify a unique instance of an OSPF database.D.It is an optional parameter required only if multiple OSPF processes are running on the router.E.All routes in the same OSPF area must have the same Process ID if they are to exchange routing information.4.Where are EIGRP successor routes stored?A.In the routing table onlyB.In the neighbor table onlyC.In the topology table onlyD.In the routing table and neighbor tableE.In the routing table and the topology tableF.In the topology table and the neighbor table

10089.book  Page 484  Monday, July 23, 2007  3:17 PM




Review Questions4855.Which command will display all the EIGRP feasible successor routes known to a router?A.show ip routes *B.show ip eigrp summaryC.show ip eigrp topologyD.show ip eigrp adjacenciesE.show ip eigrp neighbors detail6.You get a call from a network administrator who tells you that he typed the following into his router:Router(config)#router ospf 1

Router(config-router)#network 10.0.0.0 255.0.0.0 area 0He tells you he still can’t see any routes in the routing table. What configuration error did the administrator make?A.The wildcard mask is incorrect.B.The OSPF area is wrong.C.The OSPF Process ID is incorrect.D.The AS configuration is wrong.7.Which of the following protocols support VLSM, summarization, and discontiguous networking? (Choose three.)A.RIPv1B.IGRPC.EIGRPD.OSPFE.BGPF.RIPv28.Which of the following are true regarding OSPF areas? (Choose three.)A.You must have separate loopback interfaces configured in each area.B.The numbers you can assign an area go up to 65,535.C.The backbone area is also called area 0.D.If your design is hierarchical, then you don’t need multiple areas.E.All areas must connect to area 0.F.If you have only one area, it must be called area 1.

10089.book  Page 485  Monday, July 23, 2007  3:17 PM




486Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)9.Which of the following network types have a designated router and a backup designated router assigned? (Choose two.)A.BroadcastB.Point-to-pointC.NBMAD.NBMA point-to-pointE.NBMA point-to-multipoint10.A network administrator needs to configure a router with a distance-vector protocol that allows classless routing. Which of the following satisfies those requirements?A.IGRPB.OSPFC.RIPv1D.EIGRPE.IS-IS11.You need the IP address of the devices with which the router has established an adjacency. Also, the retransmit interval and the queue counts for the adjacent routers need to be checked. What command will display the required information?A.show ip eigrp adjacencyB.show ip eigrp topologyC.show ip eigrp interfacesD.show ip eigrp neighbors12.For some reason, you cannot establish an adjacency relationship on a common Ethernet link between two routers. Looking at the output below, what is the cause of the problem?RouterA#Ethernet0/0 is up, line protocol is up  Internet Address 172.16.1.2/16, Area 0  Process ID 2, Router ID 172.126.1.1, Network Type BROADCAST, Cost: 10  Transmit Delay is 1 sec, State DR, Priority 1  Designated Router (ID) 172.16.1.2, interface address 172.16.1.1  No backup designated router on this network  Timer intervals configured, Hello 5, Dead 20, Wait 20, Retransmit 5RouterB#Ethernet0/0 is up, line protocol is up  Internet Address 172.16.1.1/16, Area 0  Process ID 2, Router ID 172.126.1.1, Network Type BROADCAST, Cost: 10  Transmit Delay is 1 sec, State DR, Priority 1

10089.book  Page 486  Monday, July 23, 2007  3:17 PM




Review Questions487  Designated Router (ID) 172.16.1.1, interface address 172.16.1.2  No backup designated router on this network

  Timer intervals configured, Hello 10, Dead 40, Wait 40, Retransmit 5A.The OSPF area is not configured properly.B.The priority on RouterA should be set higher.C.The cost on RouterA should be set higher.D.The Hello and Dead timers are not configured properly.E.A backup designated router needs to be added to the network.F.The OSPF Process ID numbers must match.13.Which is true regarding EIGRP successor routes? (Choose two.)A.A successor route is used by EIGRP to forward traffic to a destination.B.Successor routes are saved in the topology table to be used if the primary route fails.C.Successor routes are flagged as “active” in the routing table.D.A successor route may be backed up by a feasible successor route.E.Successor routes are stored in the neighbor table following the discovery process.14.Which type of OSPF network will elect a backup designated router? (Choose two.)A.Broadcast multi-accessB.Non-broadcast multi-accessC.Point-to-pointD.Broadcast multipoint15.Which two of the following commands will place network 10.2.3.0/24 into area 0? (Choose two.)A.router eigrp 10B.router ospf 10C.router ripD.network 10.0.0.0E.network 10.2.3.0 255.255.255.0 area 0F.network 10.2.3.0 0.0.0.255 area0G.network 10.2.3.0 0.0.0.255 area 016.With which network type will OSPF establish router adjacencies but not perform the DR/BDR election process?A.Point-to-pointB.Backbone area 0C.Broadcast multi-accessD.Non-broadcast multi-access

10089.book  Page 487  Monday, July 23, 2007  3:17 PM




488Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)17.What are three reasons for creating OSPF in a hierarchical design? (Choose three.)A.To decrease routing overheadB.To speed up convergenceC.To confine network instability to single areas of the networkD.To make configuring OSPF easier18.What is the administrative distance of OSPF?A.90B.100C.110D.12019.You have an internetwork as shown in the following illustration. However, the two networks are not sharing routing table route entries. Which command is needed to fix the problem?A.version 2B.no auto-summaryC.redistribute eigrp 10D.default-information originate20.If routers in a single area are configured with the same priority value, what value does a router use for the OSPF Router ID in the absence of a loopback interface?A.The lowest IP address of any physical interfaceB.The highest IP address of any physical interfaceC.The lowest IP address of any logical interfaceD.The highest IP address of any logical interface

172.16.20.1/24 E0Host_B

172.16.20.2/24

Lab_A

172.16.10.1/24 E0S0

S0

Lab_BHost_A172.16.10.2/24172.16.10.0/24172.16.20.0/2410.3.1.2/2410.3.1.1/2410.3.1.0/24

10089.book  Page 488  Monday, July 23, 2007  3:17 PM




Answers to Review Questions489Answers to Review Questions1.B. If you enable EIGRP on a router with the same autonomous system (AS) number, EIGRP will automatically redistribute IGRP into EIGRP. You will see the IGRP injected routes as external (EX) routes with an EIGRP AD of 170. This is a nice feature that lets you migrate slowly to EIGRP with no extra configuration.2.A, C. EIGRP holds three tables in RAM: neighbor, topology, and routing. The neighbor and topology tables are built and maintained with the use of Hello packets.3.A, C. The Process ID for OSPF on a router is only locally significant and you can use the same number on each router, or each router can have a different number—it just doesn’t matter. The numbers you can use are from 1 to 65,535. Don’t get this confused with area numbers, which can be from 0 to 4.2 billion.4.E. Successor routes are going to be in the routing table since they are the best path to a remote network. However, the topology table has a link to each and every network, so the best answer is topology table and routing table. Any secondary route to a remote network is considered a feasible successor, and those routes are only found in the topology table and used as backup routes in case of primary route failure.5.C. Any secondary route to a remote network is considered a feasible successor, and those routes are only found in the topology table and used as backup routes in case of primary route failure. You can see the topology table with the show ip eigrp topology command.6.A. The administrator typed in the wrong wildcard mask configuration. The wildcard should have been 0.0.0.255.7.C, D, F. RIPv1 and IGRP are true distance-vector routing protocols and can’t do much, really—except build and maintain routing tables and use a lot of bandwidth! RIPv2, EIGRP, and OSPF build and maintain routing tables, but they also provide classless routing, which allows for VLSM, summarization, and discontiguous networking.8.C, D, E. Loopback interfaces are created on a router, and the highest IP address on a loopback (logical) interface becomes the RID of the router but has nothing to do with areas and is optional, so option A is wrong. The numbers you can create an area with are from 0 to 4,294,967,295—option B is wrong. The backbone area is called area 0, so option C is correct. All areas must con-nect to area 0, so option E is correct. If you have only one area, it must be called area 0, so option F is incorrect. This leaves option D, which must be correct; it doesn’t make much sense, but it is the best answer.9.A, C. No DR is assigned on any type of point-to-point link. No DR/BDR is assigned on the NBMA point-to-multipoint due to the hub/spoke topology. DR and BDR are elected on broad-cast and non-broadcast multi-access networks. Frame Relay is a non-Broadcast Multi-Access (NBMA) network by default.10.D. In this question, we’re calling EIGRP just plain old distance vector. EIGRP is an “advanced” distance-vector routing protocol, sometimes called a hybrid routing protocol because it uses the characteristics of both distance-vector and link-state routing protocols.

10089.book  Page 489  Monday, July 23, 2007  3:17 PM




490Chapter7 Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF)11.D. The show ip eigrp neighbors command allows you to check the IP addresses as well as the retransmit interval and queue counts for the neighbors that have established an adjacency.12.D. The Hello and Dead timers must be set the same on two routers on the same link or they will not form an adjacency (relationship). The default timers for OSPF are 10 seconds for the Hello timer and 40 seconds for the Dead timer.13.A, D. Successor routes are the routes picked from the topology table as the best route to a remote network, so these are the routes that IP uses in the routing table to forward traffic to a remote des-tination. The topology table contains any route that is not as good as the successor route and is con-sidered a feasible successor, or backup route. Remember that all routes are in the topology table, even successor routes.14.A, B. DR and BDR are elected on broadcast and non-broadcast multi-access networks. Frame Relay is a non-broadcast multi-access (NBMA) network by default. No DR is assigned on any type of point-to-point link. No DR/BDR is assigned on the NBMA point-to-multipoint due to the hub/spoke topology.15.B, G. To enable OSPF, you must first start OSPF using a Process ID. The number is irrelevant; just choose a number from 1 to 65,535 and you’re good to go. After you start the OSPF pro-cess, you must configure any network that you want advertised via OSPF using wildcards and the area command. Answer F is wrong because there must be a space after the parameter area and before you list the area number.16.A. No DR is assigned on any type of point-to-point link. No DR/BDR is assigned on the NBMA point-to-multipoint due to the hub/spoke topology. DR and BDR are elected on broad-cast and non-broadcast multi-access networks. Frame Relay is a non-broadcast multi-access (NBMA) network by default.17.A, B, C. OSPF is created in a hierarchical design, not a flat design like RIP. This decreases routing overhead, speeds up convergence, and confines network instability to a single area of the network.18.C. The administrative distance (AD) is a very important parameter in a routing protocol. The lower the AD, the more trusted the route. If you have IGRP and OSPF running, by default IGRP routes would be placed in the routing table because IGRP has a lower AD of 100. OSPF has an AD of 110. RIPv1 and RIPv2 both have an AD of 120, and EIGRP is the lowest, at 90.19.B. The network in the diagram is considered a discontiguous network because you have one class-ful address subnetted and separated by another classful address. Only RIPv2, OSPF, and EIGRP can work with discontiguous networks, but RIPv2 and EIGRP won’t work by default. You must use the no auto-summary command under the routing protocol configuration.20.B. At the moment of OSPF process startup, the highest IP address on any active interface will be the Router ID (RID) of the router. If you have a loopback interface configured (logical interface), then that will override the interface IP address and become the RID of the router automatically.

10089.book  Page 490  Monday, July 23, 2007  3:17 PM




Answers to Written Lab 7491Answers to Written Lab 71.The four routed protocols supported by EIGRP are IP, IPv6, IPX, and AppleTalk.2.Redistribution is required when more than one EIGRP session or process is running and they are identified with different ASNs. Redistribution shares topology information between EIGRP sessions.3.router eigrp 3004.network 172.10.0.05.Passive interface6.router ospf 1017.show ip ospf8.show ip ospf interface9.show ip ospf neighbor10.show ip ospf database

10089.book  Page 491  Monday, July 23, 2007  3:17 PM




10089.book  Page 492  Monday, July 23, 2007  3:17 PM




 

Chapter 8 Layer 2 Switching and Spanning Tree Protocol (STP)

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Configure, verify, and troubleshoot a switch with VLANs and interswitch communications  Select the appropriate media, cables, ports, and connectors to connect switches to other network devices and hosts   Explain the technology and media access control method for Ethernet networks   Explain network segmentation and basic traffic management concepts   Explain basic switching concepts and the operation of Cisco switches   Perform and verify initial switch configuration tasks including remote access management   Verify network status and switch operation using basic utilities (including: ping, traceroute, telnet, SSH, arp, ipconfig), SHOW & DEBUG commands   Identify, prescribe, and resolve common switched network media issues, configuration issues, auto negotiation, and switch hardware failures 

 

10089.book  Page 493  Monday, July 23, 2007  3:17 PM




 When folks at Cisco discuss switching, they’re talking about layer 2 switching unless they say otherwise. Layer 2 switching is the process of using the hardware address of devices on a LAN to segment a network. Since you’ve got the basic ideas down, I’m now going to focus on the particulars of layer 2 switching and nail down how it works.You know that switching breaks up large collision domains into smaller ones and that a collision domain is a network segment with two or more devices sharing the same bandwidth. A hub network is a typical example of this type of technology. But since each port on a switch is actually its own collision domain, you can make a much better Ethernet LAN network just by replacing your hubs with switches!Switches truly have changed the way networks are designed and implemented. If a pure switched design is properly implemented, it absolutely will result in a clean, cost-effective, and resilient internetwork. In this chapter, we’ll survey and compare how networks were designed before and after switching technologies were introduced.Routing protocols (such as RIP, which you learned about in Chapter 6, “IP Routing”) have processes for stopping network loops from occurring at the Network layer. However, if you have redundant physical links between your switches, routing protocols won’t do a thing to stop loops from occurring at the Data Link layer. That’s exactly the reason Spanning Tree Pro-tocol was developed—to put a stop to loops in a layer 2 switched network. The essentials of this vital protocol, as well as how it works within a switched network, are also important sub-jects that we’ll cover thoroughly in this chapter.I’ll be using three switches to start our configuration of a switched network, and we’ll actually continue with their configuration in Chapter 9, “Virtual LANs (VLANs).”

  For up-to-the minute updates for this chapter, please see  www.lammle.com   and/or  www.sybex.com . Before Layer 2 Switching Let’s go back in time a bit and take a look at the condition of networks before switches and how switches have helped segment the corporate LAN. Before LAN switching, the typical net-work design looked like the network in Figure 8.1.The design in Figure 8.1 was called a collapsed backbone because all hosts would need to go to the corporate backbone to reach any network services—both LAN and mainframe.

 

10089.book  Page 494  Monday, July 23, 2007  3:17 PM




 Before Layer 2 Switching 495 FIGURE8.1 Before switching Going back even further, before networks like the one shown in Figure 8.1 had physical segmentation devices such as routers and hubs, there was the mainframe network. This net-work included the mainframe (IBM, Honeywell, Sperry, DEC, etc.), controllers, and dumb terminals that connected into the controller. Any remote sites were connected to the main-frame with bridges.And then the PC began its rise to stardom and the mainframe was connected to the Ethernet or to a Token Ring LAN where the servers were installed. These servers were usually OS/2 or LAN Manager because this was “pre-NT.” Each floor of a building ran either coax or twisted-pair wir-ing to the corporate backbone and was then connected to a router. PCs ran an emulating software program that allowed them to connect to the mainframe services, giving those PCs the ability to access services from the mainframe and LAN simultaneously. Eventually the PC became robust enough to allow application developers to port applications more effectively than they could ever before—an advance that markedly reduced networking prices and enabled businesses to grow at a much faster rate.When Novell became more popular in the late 1980s and early 1990s, OS/2 and LAN Man-ager servers were by and large replaced with NetWare servers. This made the Ethernet net-work even more popular, because that’s what Novell 3. x  servers used to communicate with client/server software.So that’s the story about how the network in Figure 8.1 came into being. There was only one problem—the corporate backbone grew and grew, and as it grew, network services became slower. A big reason for this was that, at the same time this huge burst in growth was taking place, LAN services needed even faster service and the network was becoming totally saturated. Everyone 

Corporate

Remote Branch

Server Farm

Token RingHubs

 

10089.book  Page 495  Monday, July 23, 2007  3:17 PM




 496 Chapter8  Layer 2 Switching and Spanning Tree Protocol (STP) was dumping the Macs and dumb terminals used for the mainframe service in favor of those slick new PCs so they could more easily connect to the corporate backbone and network services.All this was taking place before the Internet’s momentous popularity, so everyone in the company needed to access the corporate network’s services. Why? Because without the Internet, all network services were internal—exclusive to the company network. This created a screaming need to segment that one humongous and plodding corporate net-work, connected with sluggish old routers. At first, Cisco just created faster routers (no doubt about that), but more segmentation was needed, especially on the Ethernet LANs. The invention of FastEthernet was a very good and helpful thing, too, but it didn’t address that network segmentation need at all.But devices called bridges did, and they were first used in the network to break up collision domains. Bridges were sorely limited by the number of ports and other network services they could provide, and that’s when layer 2 switches came to the rescue. These switches saved the day by breaking up collision domains on each and every port—like a bridge—and switches could provide hundreds of ports! This early, switched LAN looked like the network pictured in Figure 8.2. FIGURE8.2 The first switched LAN

Switches

Corporate

Remote Branch

Server Farm

Token RingHubs

 

10089.book  Page 496  Monday, July 23, 2007  3:17 PM




 Switching Services 497 Each hub was placed into a switch port, an innovation that vastly improved the network. Now, instead of each building being crammed into the same collision domain, each hub became its own separate collision domain. But there was a catch—switch ports were still very new, hence unbelievably expensive. Because of that, simply adding a switch into each floor of the building just wasn’t going to happen—at least, not yet. Thanks to whomever you choose to thank for these things, the price has dropped dramatically, so now having every one of your users plugged into a switch port is both good and feasible.So there it is—if you’re going to create a network design and implement it, including switching services is a must. A typical contemporary network design would look something like Figure 8.3, a complete switched network design and implementation. FIGURE8.3 The typical switched network design “But I still see a router in there,” you say! Yes, it’s not a mirage—there  is  a router in there. But its job has changed. Instead of performing physical segmentation, it now creates and handles logical segmentation. Those logical segments are called VLANs, and I promise I’ll explain them thoroughly—both in the duration of this chapter and in Chapter 9, where they’ll be given a starring role. Switching Services Unlike bridges, which use software to create and manage a filter table, switches use application-specific integrated circuits (ASICs) to build and maintain their filter tables. But it’s still okay to think of a layer 2 switch as a multiport bridge because their basic reason for being is the same: to break up collision domains.Layer 2 switches and bridges are faster than routers because they don’t take up time looking at the Network layer header information. Instead, they look at the frame’s hardware addresses before deciding to either forward, flood or drop the frame.

 

10089.book  Page 497  Monday, July 23, 2007  3:17 PM




 498 Chapter8  Layer 2 Switching and Spanning Tree Protocol (STP) Switches create private, dedicated collision domains and provide independent bandwidth on each port, unlike hubs. Figure 8.4 shows five hosts connected to a switch—all running 10Mbps half-duplex to the server. Unlike with a hub, each host has 10Mbps dedicated com-munication to the server. FIGURE8.4 Switches create private domains. Layer 2 switching provides the following:  Hardware-based bridging (ASIC)  Wire speed  Low latency  Low costWhat makes layer 2 switching so efficient is that no modification to the data packet takes place. The device only reads the frame encapsulating the packet, which makes the switching process considerably faster and less error-prone than routing processes are.And if you use layer 2 switching for both workgroup connectivity and network segmenta-tion (breaking up collision domains), you can create a flatter network design with more net-work segments than you can with traditional routed networks.Plus, layer 2 switching increases bandwidth for each user because, again, each connection (interface) into the switch is its own collision domain. This feature makes it possible for you to connect multiple devices to each interface.In the following sections, I will dive deeper into the layer 2 switching technology. Limitations of Layer 2 Switching Since we commonly stick layer 2 switching into the same category as bridged networks, we also tend to think it has the same hang-ups and issues that bridged networks do. Keep in mind that bridges are good and helpful things if we design the network correctly, keeping their features as 

Server10Mbps Half-duplex links

 

10089.book  Page 498  Monday, July 23, 2007  3:17 PM




 Switching Services 499 well as their limitations in mind. And to design well with bridges, these are the two most impor-tant considerations:  We absolutely must break up the collision domains correctly.  The right way to create a functional bridged network is to make sure that its users spend 80 percent of their time on the local segment.Bridged networks break up collision domains, but remember, that network is still one large broadcast domain. Neither layer 2 switches nor bridges break up broadcast domains by default—something that not only limits your network’s size and growth potential, but also can reduce its overall performance.Broadcasts and multicasts, along with the slow convergence time of spanning trees, can give you some major grief as your network grows. These are the big reasons layer 2 switches and bridges cannot completely replace routers (layer 3 devices) in the internetwork. Bridging vs. LAN Switching It’s true—layer 2 switches really are pretty much just bridges that give us a lot more ports, but there are some important differences you should always keep in mind:  Bridges are software based, while switches are hardware based because they use ASIC chips to help make filtering decisions.  A switch can be viewed as a multiport bridge.  There can be only one spanning-tree instance per bridge, while switches can have many. (I’m going to tell you all about spanning trees in a bit.)  Switches have a higher number of ports than most bridges.  Both bridges and switches forward layer 2 broadcasts.  Bridges and switches learn MAC addresses by examining the source address of each frame received.  Both bridges and switches make forwarding decisions based on layer 2 addresses. Three Switch Functions at Layer 2 There are three distinct functions of layer 2 switching (you need to remember these!):  address learning ,  forward/filter decisions , and  loop avoidance . Address learning Layer 2 switches and bridges remember the source hardware address of each frame received on an interface, and they enter this information into a MAC database called a forward/filter table. Forward/filter decisions When a frame is received on an interface, the switch looks at the destination hardware address and finds the exit interface in the MAC database. The frame is only forwarded out the specified destination port.

 

10089.book  Page 499  Monday, July 23, 2007  3:17 PM




 500 Chapter8  Layer 2 Switching and Spanning Tree Protocol (STP) Loop avoidance If multiple connections between switches are created for redundancy pur-poses, network loops can occur. Spanning Tree Protocol (STP) is used to stop network loops while still permitting redundancy.I’m going to talk about address learning, forward/filtering decisions, and loop avoidance in detail in the next sections. Address Learning When a switch is first powered on, the MAC forward/filter table is empty, as shown in Figure 8.5. FIGURE8.5 Empty forward/filter table on a switch When a device transmits and an interface receives a frame, the switch places the frame’s source address in the MAC forward/filter table, allowing it to remember which interface the sending device is located on. The switch then has no choice but to flood the network with this frame out of every port except the source port because it has no idea where the destination device is actually located.If a device answers this flooded frame and sends a frame back, then the switch will take the source address from that frame and place that MAC address in its database as well, associating this address with the interface that received the frame. Since the switch now has both of the relevant MAC addresses in its filtering table, the two devices can now make a point-to-point connection. The switch doesn’t need to flood the frame as it did the first time because now the frames can and will be forwarded only between the two devices. This is exactly the thing that makes layer 2 switches better than hubs. In a hub network, all frames are forwarded out all ports every time—no matter what. Figure 8.6 shows the processes involved in building a MAC database.

MAC Forward/Filter TableE0/0:E0/1:E0/2:E0/3:E0/0E0/3E0/2E0/1

Host AHost BHost CHost D

 

10089.book  Page 500  Monday, July 23, 2007  3:17 PM




 Switching Services 501 FIGURE8.6 How switches learn hosts’ locations In this figure, you can see four hosts attached to a switch. When the switch is powered on, it has nothing in its MAC address forward/filter table, just as in Figure 8.5. But when the hosts start communicating, the switch places the source hardware address of each frame in the table along with the port that the frame’s address corresponds to.Let me give you an example of how a forward/filter table is populated: 1. Host A sends a frame to Host B. Host A’s MAC address is 0000.8c01.000A; Host B’s MAC address is 0000.8c01.000B. 2. The switch receives the frame on the E0/0 interface and places the source address in the MAC address table. 3. Since the destination address is not in the MAC database, the frame is forwarded out all interfaces—except the source port. 4. Host B receives the frame and responds to Host A. The switch receives this frame on inter-face E0/1 and places the source hardware address in the MAC database. 5. Host A and Host B can now make a point-to-point connection and only the two devices will receive the frames. Hosts C and D will not see the frames, nor are their MAC addresses found in the database because they haven’t yet sent a frame to the switch.If Host A and Host B don’t communicate to the switch again within a certain amount of time, the switch will flush their entries from the database to keep it as current as possible. Forward/Filter Decisions When a frame arrives at a switch interface, the destination hardware address is compared to the forward/filter MAC database. If the destination hardware address is known and listed in the database, the frame is only sent out the correct exit interface. The switch doesn’t transmit 

MAC Forward/Filter TableE0/0: 0000.8c01.000A  step 2E0/1: 0000.8c01.000B  step 4E0/2:E0/3:E0/0E0/3E0/2E0/1

Step 13334

Host A

Host B

Host C

Host D

 

10089.book  Page 501  Monday, July 23, 2007  3:17 PM




 502 Chapter8  Layer 2 Switching and Spanning Tree Protocol (STP) the frame out any interface except for the destination interface. This preserves bandwidth on the other network segments and is called  frame filtering .But if the destination hardware address is not listed in the MAC database, then the frame is flooded out all active interfaces except the interface the frame was received on. If a device answers the flooded frame, the MAC database is updated with the device’s location (interface).If a host or server sends a broadcast on the LAN, the switch will flood the frame out all active ports except the source port by default. Remember, the switch creates smaller collision domains, but it’s still one large broadcast domain by default.In Figure 8.7, Host A sends a data frame to Host D. What will the switch do when it receives the frame from Host A? FIGURE8.7 Forward/filter table Since Host A’s MAC address is not in the forward/filter table, the switch will add the source address and port to the MAC address table and then forward the frame to Host D. If Host D’s MAC address was not in the forward/filter table, the switch would have flooded the frame out all ports except for port Fa0/3.Now let’s take a look at the output of a  show mac address-table :

 Switch# sh mac address-table Vlan    Mac Address       Type        Ports----    -----------       --------    -----   1    0005.dccb.d74b    DYNAMIC     Fa0/1   1    000a.f467.9e80    DYNAMIC     Fa0/3   1    000a.f467.9e8b    DYNAMIC     Fa0/4   1    000a.f467.9e8c    DYNAMIC     Fa0/3   1    0010.7b7f.c2b0    DYNAMIC     Fa0/3   1    0030.80dc.460b    DYNAMIC     Fa0/3   1    0030.9492.a5dd    DYNAMIC     Fa0/1

    1    00d0.58ad.05f4    DYNAMIC     Fa0/1

Switch#sh mac address-table Vlan    Mac Address       Ports----    -----------       -----   1    0005.dccb.d74b    Fa0/4   1    000a.f467.9e80    Fa0/5   1    000a.f467.9e8b    Fa0/6Fa0/3Fa0/4Fa0/5Fa0/6ABCD

 

10089.book  Page 502  Monday, July 23, 2007  3:17 PM




 Switching Services 503 Suppose the preceding switch received a frame with the following MAC addresses:Source MAC:  0005.dccb.d74b Destination MAC:  000a.f467.9e8c How will the switch handle this frame? Answer: The destination MAC address will be found in the MAC address table and the frame will be forwarded out Fa0/3 only. Remember that if the destination MAC address is not found in the forward/filter table, it will forward the frame out all ports of the switch looking for the destination device. Now that we can see the MAC address table and how switches add hosts addresses to the forward filter table, how can we secure it from unauthorized users? Port Security So just how do you stop someone from simply plugging a host into one of your switch ports—or worse, adding a hub, switch, or access point into the Ethernet jack in their office? By default, MAC addresses will just dynamically appear in your MAC forward/filter database. You can stop them in their tracks by using port security. Here are your options:

 Switch# config t Switch(config)# int f0/1 Switch(config-if)# switchport port-security ?    aging           Port-security aging commands   mac-address     Secure mac address   maximum         Max secure addresses   violation       Security violation mode

    <cr> You can see clearly in the preceding output that the  switchport port-security  command can be used with four options. Personally, I like the  port-security  command because it allows me to easily control users on my network. You can use the  switchport port-security mac-address  mac-address  command to assign individual MAC addresses to each switch port, but if you choose to go there, you’d better have a lot of time on your hands!If you want to set up a switch port to allow only one host per port, and to shut down the port if this rule is violated, use the following commands:

  Switch#config t Switch(config)#int f0/1 Switch(config-if)#switchport port-security maximum 1

 Switch(config-if)#switchport port-security violation shutdownThese commands are probably the most popular because they prevent users from connect-ing to a switch or access point that’s in their office. The maximum setting of 1 means only one MAC address can be used on that port; if the user tries to add another host on that segment, the switch port will shut down. If that happens, you’d have to manually go into the switch and enable the port with a no shutdown command.

10089.book  Page 503  Monday, July 23, 2007  3:17 PM




504Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)Probably one of my favorite commands is the sticky command. Not only does it perform a cool function, it’s got a cool name! You can find this command under the mac-address command:

Switch(config-if)#switchport port-security mac-address stickySwitch(config-if)#switchport port-security maximum 2

Switch(config-if)#switchport port-security violation shutdownBasically, what this does is provide static MAC address security without having to type in everyone’s MAC address on the network. As I said—cool!In the preceding example, the first two MAC addresses into the port “stick” as static addresses and will stay that way for however long you set the aging command for. Why did I set it to 2? Well, I needed one for the PC/data and one for telephony/phone. I’ll cover this type of configuration more in the next chapter, which is about VLANs.

I’ll be going over port security again in the configuration examples later in this chapter.Loop AvoidanceRedundant links between switches are a good idea because they help prevent complete net-work failures in the event one link stops working.Sounds great, but even though redundant links can be extremely helpful, they often cause more problems than they solve. This is because frames can be flooded down all redundant links simultaneously, creating network loops as well as other evils. Here’s a list of some of the ugliest problems: If no loop avoidance schemes are put in place, the switches will flood broadcasts endlessly throughout the internetwork. This is sometimes referred to as a broadcast storm. (But most of the time it’s referred to in ways we’re not permitted to repeat in print!) Figure 8.8 illustrates how a broadcast can be propagated throughout the network. Observe how a frame is continually being flooded through the internetwork’s physical network media.FIGURE8.8Broadcast storm

Segment 1Segment 2

Broadcast

Switch A

Switch B

10089.book  Page 504  Monday, July 23, 2007  3:17 PM




Spanning Tree Protocol (STP)505 A device can receive multiple copies of the same frame since that frame can arrive from dif-ferent segments at the same time. Figure 8.9 demonstrates how a whole bunch of frames can arrive from multiple segments simultaneously. The server in the figure sends a unicast frame to Router C. Since it’s a unicast frame, Switch A forwards the frame and Switch B provides the same service—it forwards the broadcast. This is bad because it means that Router C receives that unicast frame twice, causing additional overhead on the network.FIGURE8.9Multiple frame copies You may have thought of this one: The MAC address filter table could be totally confused about the device’s location because the switch can receive the frame from more than one link. And what’s more, the bewildered switch could get so caught up in constantly updating the MAC filter table with source hardware address locations that it will fail to forward a frame! This is called thrashing the MAC table. One of the nastiest things that can happen is multiple loops generating throughout a net-work. This means that loops can occur within other loops, and if a broadcast storm were to also occur, the network wouldn’t be able to perform frame switching—period!All of these problems spell disaster (or at least close to it) and are decidedly evil situations that must be avoided, or at least fixed somehow. That’s where the Spanning Tree Protocol comes into the game. It was developed to solve each and every one of the problems I just told you about.Spanning Tree Protocol (STP)Once upon a time a company called Digital Equipment Corporation (DEC) was purchased and renamed Compaq. But before that happened, DEC created the original version of Span-ning Tree Protocol, or STP. The IEEE later created its own version of STP called 802.1D. The bad news is that by default, Cisco switches run the IEEE 802.1D version of STP, which isn’t compatible with the DEC version. The good news is that Cisco is moving toward another 

Segment 1Segment 2

Unicast

Unicast

Unicast

Router C

Switch A

Switch B

10089.book  Page 505  Monday, July 23, 2007  3:17 PM




506Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)industry standard in its newer switches called 802.1w. I’ll cover that STP version in this section as well, but first, let’s define some important STP basics.STP’s main task is to stop network loops from occurring on your layer 2 network (bridges or switches). It vigilantly monitors the network to find all links, making sure that no loops occur by shutting down any redundant links. STP uses the spanning-tree algorithm (STA) to first create a topology database and then search out and destroy redundant links. With STP running, frames will be forwarded only on the premium, STP-picked links.In the following sections, I am going to hit the nitty-gritty of the Spanning Tree Protocol.

STP is a layer 2 protocol that is used to maintain a loop-free switched network.The Spanning Tree Protocol is necessary in networks such as the one shown in Figure 8.10.FIGURE8.10A switched network with switching loopsIn Figure 8.10, we have a switched network with a redundant topology (switching loops). Without some type of layer 2 mechanism to stop the network loop, we would have the prob-lems we discussed previously: broadcast storms and multiple frame copies.

Understand that the network in Figure 8.10 would actually sort of work, albeit extremely slowly. This clearly demonstrates the danger of switching loops. And to make matters worse, it can be super hard to find this problem once it starts!Spanning Tree TermsBefore I get into describing the details of how STP works in the network, you need to under-stand some basic ideas and terms and how they relate within the layer 2 switched network:Root bridgeThe root bridge is the bridge with the best bridge ID. With STP, the key is for all the switches in the network to elect a root bridge that becomes the focal point in the net-work. All other decisions in the network—such as which port is to be blocked and which port is to be put in forwarding mode—are made from the perspective of this root bridge.

10089.book  Page 506  Monday, July 23, 2007  3:17 PM




Spanning Tree Protocol (STP)507BPDUAll the switches exchange information to use in the selection of the root switch as well as in subsequent configuration of the network. Each switch compares the parameters in the Bridge Protocol Data Unit (BPDU) that it sends to one neighbor with the one that it receives from another neighbor.Bridge IDThe bridge ID is how STP keeps track of all the switches in the network. It is deter-mined by a combination of the bridge priority (32,768 by default on all Cisco switches) and the base MAC address. The bridge with the lowest bridge ID becomes the root bridge in the network.Nonroot bridgesThese are all bridges that are not the root bridge. Nonroot bridges exchange BPDUs with all bridges and update the STP topology database on all switches, preventing loops and providing a measure of defense against link failures.Port costPort cost determines the best path when multiple links are used between two switches and none of the links is a root port. The cost of a link is determined by the bandwidth of a link.Root portThe root port is always the link directly connected to the root bridge, or the shortest path to the root bridge. If more than one link connects to the root bridge, then a port cost is determined by checking the bandwidth of each link. The lowest-cost port becomes the root port. If multiple links have the same cost, the bridge with the lower advertising bridge ID is used. Since multiple links can be from the same device, the lowest port number will be used.Designated portA designated port is one that has been determined as having the best (lowest) cost. A designated port will be marked as a forwarding port.Nondesignated portA nondesignated port is one with a higher cost than the designated port. Nondesignated ports are put in blocking mode—they are not forwarding ports.Forwarding portA forwarding port forwards frames.Blocked portA blocked port is the port that, in order to prevent loops, will not forward frames. However, a blocked port will always listen to frames.Spanning Tree OperationsAs I’ve said before, STP’s job is to find all links in the network and shut down any redundant ones, thereby preventing network loops from occurring.STP accomplishes this by first electing a root bridge that will forward through all ports and act as a point of reference for all other devices in the STP domain. Once all switches agree on who the root bridge is, every bridge must find its one and only allotted root port. Each and every link between two switches must have one, and only one, designated port—the port on that link that provides the highest bandwidth to the root. It’s really important to remember that a bridge can go through many other bridges to get to the root, meaning that it’s not always the shortest path but the fastest (highest bandwidth) path that will be the one used.Obviously, every port on the root switch is a designated port, as you can get no closer to the root than being the root. After the dust settles, any port that is not either a root port or a designated port—which means it is a nonroot, nondesignated port—is placed in the blocking state, thus breaking the switching loop.

10089.book  Page 507  Monday, July 23, 2007  3:17 PM




508Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)Things tend to go a lot more smoothly when you don’t have more than one person making a navigational decision, so similarly there can be only one root bridge in any given network. I’ll discuss the root bridge election process more completely in the next section.Selecting the Root BridgeThe bridge ID is used to elect the root bridge in the STP domain and to determine the root port for each of the remaining devices in the STP domain. This ID is 8 bytes long and includes both the priority and the MAC address of the device. The default priority on all devices running the IEEE STP version is 32,768.To determine the root bridge, you combine the priority of each bridge with its MAC address. If two switches or bridges happen to have the same priority value, the MAC address becomes the tiebreaker for figuring out which one has the lowest (best) ID. It’s like this: If two switches—I’ll name them A and B—both use the default priority of 32768, then the MAC address will be used instead. If Switch A’s MAC address is 0000.0c00.1111 and Switch B’s MAC address is 0000.0c00.2222, then Switch A would become the root bridge. Just remem-ber that the lower value is the better one when it comes to electing a root bridge.By default, BPDUs are sent every two seconds out all active ports on a bridge/switch—again, the bridge with the lowest (best) bridge ID is elected the root bridge. You can change the bridge’s ID by lowering its priority so that it will become a root bridge automatically. Being able to do that is important in a large switched network—it ensures that the best paths are chosen. Efficiency is what you’re after here!Figure 8.11 shows a typical switched network with redundant switched paths. First, let’s find out which switch is the root; then we can have the nonroot bridge become the root by changing the priority of the switch.FIGURE8.11A switched network with redundant switched pathsBy looking at Figure 8.11, you can tell that Switch A is the root bridge because it’s the one with the lowest bridge ID. Switch B must shut down one of its ports connected to Switch A to prevent a switching loop from occurring. Remember that even though Switch B won’t transmit out the blocked port, it will still receive through it—including BPDUs.To determine which port STP will shut down on Switch B, it will first check each link’s amount of bandwidth and then shut down the link with the lowest bandwidth value. Since both links between Switch A and Switch B are 100Mbps, STP will typically shut down the higher of the port numbers, but not always. In this example, 12 is higher than 11, so port 12 would be put into blocking mode.

Block  Switch A Default priority 32768  MAC 0c0011111111  Switch B Default priority 32768 MAC 0c0022222222 100Mbps 100Mbps Fa0/11 Fa0/12 

10089.book  Page 508  Monday, July 23, 2007  3:17 PM




Spanning Tree Protocol (STP)509Changing the default priority is the best way to choose a root bridge. This is important because you want the core switch (the one closest to the center of your network) to be the root bridge in your network so STP will converge quickly.Let’s have some fun and make Switch B the root in our network. Here’s the output from Switch B that shows the default priority. We’ll use the show spanning-tree command:

Switch B(config)#do show spanning-treeVLAN0001  Spanning tree enabled protocol ieee  Root ID    Priority    32769             Address     0005.74ae.aa40             Cost        19             Port        1 (FastEthernet0/1)             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec  Bridge ID  Priority    32769  (priority 32768 sys-id-ext 1)             Address     0012.7f52.0280             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec             Aging Time 300

[output cut]There are two things to notice right off the bat here: Switch B is running the IEEE 801.d pro-tocol, and the first output (Root ID) is the root bridge information for the switched network. But it’s not Switch B. Switch B’s port (called root port) to the root bridge is port 1. The Bridge ID is the actual spanning tree information for Switch B and for VLAN 1, listed as VLAN0001—each VLAN can have a different root bridge, although this is rare. Switch B’s MAC address is listed as well, and you can see that it’s different than the root bridge’s MAC address.Switch B’s priority is 32,768—the default for every switch. You see it listed as 32769, but the actual VLAN ID is added, so in this case it shows up as 32769 for VLAN 1. VLAN 2 would be 32770, and so on.As I said, you can change the priority to force a switch to become the root of your STP network, so let’s do that now for Switch B. Use the following command to change a bridge priority on a Catalyst switch:

Switch B(config)#spanning-tree vlan 1 priority ?  <0-61440>  bridge priority in increments of 4096

Switch B(config)#spanning-tree vlan 1 priority 4096You can set the priority to any value from 0 through 61440. Setting it to zero (0) means that the switch will always be a root bridge, and the bridge priority is set in increments of 4096. If you want to set a switch to be the root bridge for every VLAN in your network, then you have to change the priority for each VLAN, with 0 being the lowest priority you can use. It would not be advantageous to set all switches to a priority of 0.

10089.book  Page 509  Monday, July 23, 2007  3:17 PM




510Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)Check out the following output—now that we’ve changed the priority of Switch B for VLAN 1 to 4096, we’ve successfully forced this switch to become the root:

Switch B(config)#do show spanning-treeVLAN0001  Spanning tree enabled protocol ieee  Root ID    Priority    4097             Address     0012.7f52.0280             This bridge is the root             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec  Bridge ID  Priority    4097   (priority 4096 sys-id-ext 1)             Address     0012.7f52.0280             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec             Aging Time 15

[output cut]Both the root’s MAC address and the bridge ID of Switch B are now the same, meaning that Switch B is now the root bridge, and it tells us so. Knowing the show spanning-tree command is a very important thing; we’ll use it again toward the end of this chapter.

Believe it or not, there’s yet another command that you can use to set your root bridge, and I promise to tell you all about it when I show you my switch configuration examples later in this chapter. Spanning-Tree Port StatesThe ports on a bridge or switch running STP can transition through five different states:BlockingA blocked port won’t forward frames; it just listens to BPDUs. The purpose of the blocking state is to prevent the use of looped paths. All ports are in blocking state by default when the switch is powered up.ListeningThe port listens to BPDUs to make sure no loops occur on the network before pass-ing data frames. A port in listening state prepares to forward data frames without populating the MAC address table.LearningThe switch port listens to BPDUs and learns all the paths in the switched network. A port in learning state populates the MAC address table but doesn’t forward data frames. Forward delay means the time it takes to transition a port from listening to learning mode, which is set to 15 seconds by default and can be seen in the show spanning-tree output.ForwardingThe port sends and receives all data frames on the bridged port. If the port is still a designated or root port at the end of the learning state, it enters the forwarding state.DisabledA port in the disabled state (administratively) does not participate in the frame for-warding or STP. A port in the disabled state is virtually nonoperational.

10089.book  Page 510  Monday, July 23, 2007  3:17 PM




Spanning Tree Protocol (STP)511

Switches populate the MAC address table in learning and forwarding modes only.Switch ports are most often in either the blocking or forwarding state. A forwarding port is one that has been determined to have the lowest (best) cost to the root bridge. But when and if the network experiences a topology change (because of a failed link or because someone adds in a new switch), you’ll find the ports on a switch in listening and learning states.As I mentioned, blocking ports is a strategy for preventing network loops. Once a switch determines the best path to the root bridge, all other redundant ports will be in blocking mode. Blocked ports can still receive BPDUs—they just don’t send out any frames.If a switch determines that a blocked port should now be the designated or root port because of a topology change, it will go into listening mode and check all BPDUs it receives to make sure it won’t create a loop once the port goes to forwarding mode.ConvergenceConvergence occurs when all ports on bridges and switches have transitioned to either for-warding or blocking modes. No data will be forwarded until convergence is complete. And before data can begin being forwarded again, all devices must be updated. Yes—you read that right: When STP is converging, all host data stops transmitting! So if you want to remain on speaking terms with your network’s users (or remain employed for any length of time), you positively must make sure that your switched network is physically designed really well so that STP can converge quickly.Figure 8.12 shows you some really great considerations for designing and implementing your switched network so that STP converges efficiently.FIGURE8.12An optimal hierarchical switch designConvergence is truly important because it ensures that all devices have the same database. But as I’ve drilled into you, it does cost you some time. It usually takes 50 seconds to go from blocking to forwarding mode, and I don’t recommend changing the default STP timers. (But 

6500

3560

2960

2960

3560

2960

2960

3560

2960

2960Create core switch as STP root for fastest STP convergenceSTP root Bridge priority 4096 Bridge priority8192 

10089.book  Page 511  Monday, July 23, 2007  3:17 PM




512Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)you can adjust those timers if necessary.) By creating your physical switch design in a hierar-chical manner, as shown in Figure 8.12, you can make your core switch the STP root, which will then make STP convergence time nice and quick.Because the typical spanning-tree topology’s time to convergence from blocking to for-warding on a switch port is 50 seconds, this could create time-out problems on your servers or hosts—for example, when you reboot them. To address this hitch, you can disable spanning tree on individual ports using PortFast.Spanning Tree PortFastIf you have a server or other devices connected into your switch that you’re totally sure won’t create a switching loop if STP is disabled, you can use something called portfast on these ports. Using it means the port won’t spend the usual 50 seconds to come up into forwarding mode while STP is converging.Here are the commands—they’re pretty simple:

Switch(config-if)#spanning-tree portfast ?  disable  Disable portfast for this interface  trunk    Enable portfast on the interface even in trunk mode

  <cr>We haven’t discussed trunk ports yet, but basically, these are used to connect switches together and pass VLAN information between them. You have to specifically tell portfast if you want to enable it on a trunk port. This isn’t a typical configuration because ports between switches should usually run STP. So let’s take a look at the message I get when I turn on port-fast on an interface:

Switch(config-if)#spanning-tree portfast%Warning: portfast should only be enabled on ports connected to a   single host. Connecting hubs, concentrators, switches, bridges,   etc... to this interface  when portfast is enabled, can cause   temporary bridging loops. Use with CAUTION%Portfast has been configured on FastEthernet0/1 but will only have effect when the interface is in a non-trunking mode.

Switch(config-if)#Portfast is enabled on port f0/1, but notice that you get a pretty long message telling you to be careful. One last helpful interface command I want to tell you about is the range com-mand, which you can use on switches to help you configure multiple ports at the same time. Here’s an example:

Switch(config)#int range fastEthernet 0/1 - 12

Switch(config-if-range)#spanning-tree portfast

10089.book  Page 512  Monday, July 23, 2007  3:17 PM




Spanning Tree Protocol (STP)513The preceding range command allows me to set all 12 of my switch ports into portfast mode by typing in one command and then simply pressing the Enter key. Sure hope I didn’t create any loops! Again, just be super careful with the portfast command. I also want you to know that the interface range command can be used in conjunction with any command. I just used it with the portfast command as an example.Spanning Tree UplinkFastUplinkFast is a Cisco-specific feature that improves the convergence time of STP in case of a link failure. And beware, just as with the portfast command, you’ve got to be really careful where you use this command! The UplinkFast feature is designed to run in a switched envi-ronment when the switch has at least one alternate/backup root port (a port in blocking state). This is why Cisco recommends that UplinkFast be enabled only for switches with blocked ports and, typically, at the Access layer.UplinkFast allows a switch to find alternate paths to the root bridge before the primary link fails. This means that if the primary link fails, the secondary link would come up more quickly—the port wouldn’t wait for the normal STP convergence time of 50 seconds. So if you’re running the 802.1d STP and you have redundant links on your Access layer switches, you definitely want to turn on UplinkFast. But don’t use it on switches without the implied topology knowledge of an alternative/backup root link that’s typically used for distribution and core switches in Cisco multilayer design.Spanning Tree BackboneFastUnlike UplinkFast, which is used to determine and quickly fix link failures on the local switch, another Cisco-proprietary STP extension called BackboneFast is used for speeding up convergence when a link that’s not directly connected to the switch fails. If a switch running BackboneFast receives an inferior BPDU from its designated bridge, it knows that a link on the path to the root has failed. Just to make sure you’re clear on this, an inferior BPDU is one that lists the same switch for the root bridge and the designated bridge.And again, unlike UplinkFast, which is only configured on Access layer switches or switches with redundant links and at least one link in blocking mode, BackboneFast should be enabled on all Catalyst switches to allow for detection of indirect link failures. Enabling BackboneFast is also beneficial because it starts the spanning tree reconfiguration more quickly—it can save 20 seconds on the default 50-second STP convergence time.Rapid Spanning Tree Protocol (RSTP) 802.1wHow would you like to have a good STP configuration running on your switched network (regardless of the brand of switches) and have all the features we just discussed built in and enabled on every switch? Absolutely—yes! Well then, welcome to the world of Rapid Span-ning Tree Protocol (RSTP).Cisco created PortFast, UplinkFast, and BackboneFast to “fix” the holes and liabilities the IEEE 802.1d standard presented. The drawbacks to these enhancements are only that they are Cisco proprietary and need additional configuration. But the new 802.1w standard (RSTP) addresses all these “issues” in one tight package—just turn on RSTP and you’re good to go. 

10089.book  Page 513  Monday, July 23, 2007  3:17 PM




514Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)It’s important that you make sure all the switches in your network are running the 802.1w protocol for 802.1w to work properly!

It might come as a surprise, but RSTP actually can interoperate with legacy STP protocols. Just know that the inherently fast convergence ability of 802.1w is lost when it interacts with legacy bridges.I’ll show you how to configure RSTP later in this chapter. It’s pretty easy, actually.EtherChannelInstead of having redundant links and allowing STP to put one of the links in BLK (blocked) mode, we can bundle the links and create a logical aggregation so that our multiple links will then appear as a single one. Since doing this would still provide the same redundancy as STP, why wouldn’t we want to bundle our redundant links?Well, as usual, there’s the Cisco version of EtherChannel and the IEEE version to choose from—take your pick. Cisco’s version is called Port Aggregation Protocol (PAgP) and the IEEE 802.3ad standard is called Link Aggregation Control Protocol (LACP). Both versions work equally as well, but how you configure each is different. I’m going to bundle some links toward the end of this chapter to demonstrate this just for fun. And no worries—I’m also going to cover all configurations for the STP extensions coming right up in the next section.Configuring Catalyst SwitchesCisco Catalyst switches come in many flavors—some run 10Mbps, and some jam all the way up to 10Gbps switched ports with a combination of twisted-pair and fiber. These newer switches (specifically the 2960s and 3560s) have more intelligence, so they can give you data fast—video and voice services, too.It’s time to get down to it—I’m going to show you how to start up and configure a Cisco Cat-alyst switch using the command-line interface (CLI). After you get the basic commands down in this chapter, in the next chapter, I’ll show you how to configure virtual LANs (VLANs) plus Inter-Switch Link (ISL), 802.1q routing, and Cisco’s Virtual Trunk Protocol (VTP).Here’s a list of the basic tasks we’ll be covering in this section: Administrative functions Configuring the IP address and subnet mask Setting the IP default gateway Setting port security Setting PortFast Enabling BPDUGuard and BPDUFilter Enabling UplinkFast

10089.book  Page 514  Monday, July 23, 2007  3:17 PM




 Configuring Catalyst Switches 515  Enabling BackboneFast  Enabling RSTP (802.1w)  Enabling EtherChannel  Configuring an STP root switch  Using the CNA to configure a switch

 You can learn all about the Cisco family of Catalyst switches at  www.cisco  .com/en/US/products/hw/switches/index.html . Catalyst Switch Configuration Just as we did with the routers we configured in Chapters 6 and 7, we’ll use a diagram and switch setup to configure in this chapter as well as in Chapter 9, “Virtual LANs (VLANs).” Figure 8.13 shows the switched network we’ll be working on. FIGURE8.13 Our switched network I’m going to use a new 3560, a 2960, and a 3550 switch. Keep in mind that the hosts, phones, and router shown in the network will become more important later when we get to Chapter 9. But before we actually get into configuring one of the Catalyst switches, I’ve got to fill you in regarding the bootup process of these switches, just as I did with the routers in Chapter 4. Figure 8.14 shows the detail of a typical Cisco Catalyst switch, and I need to tell you about the different interfaces and features of this product.

2960

3550

3560

Clients 

Phone A 

Clients 

Phone B

IVR S1 S2 Core 

 

10089c08.fm  Page 515  Friday, November 7, 2008  11:07 PM




516Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)FIGURE8.14A Cisco Catalyst switchThe first thing I want you to know is that the console port for the Catalyst switches are typ-ically located on the back of the switch. But on a smaller switch like the 3560 shown in the fig-ure, the console is right in the front to make it easier to use. (The eight-port 2960 looks exactly the same.) If the POST completes successfully, the system LED turns green; if the POST fails, it will turn amber. And seeing the amber glow is a very bad thing—typically fatal. So you may just want to keep a spare switch around—especially in case it happens to be a production switch that’s croaked! The bottom button is used to show you which lights are providing Power over Ethernet (PoE). You can see this by pressing the Mode button. The PoE is a very nice feature of these switches. It allows me to power my access point and phone by just con-necting them into the switch with an Ethernet cable! Sweet.After a switch boots up, you can use the Express Setup HTTP screen. Figure 8.15 shows the screen you’ll get when you connect to a new switch and use 10.0.0.1 in the HTTP field of your browser. Oh, and obviously your host needs to be in the same subnet.FIGURE8.15Express Setup HTTP screenThe screen shows us that we can set some basic functions. To me, it’s easier to configure the information from the CLI, which I’ll show you next, but this is actually just one of your options. You can configure the IP address, mask, and default gateway of the switch, plus the passwords. You can also configure the management VLAN, but I’m going to hold off on that 

System LEDPoE

10089.book  Page 516  Monday, July 23, 2007  3:17 PM




Configuring Catalyst Switches517for now and show you how to do that in the next chapter. Moving on, optionally, you can con-figure the hostname, system contact, and location and set up Telnet access. And last, the Express Setup HTTP screen provides you with some simple help on setting the switch up with SNMP so your Network Management System (NMS) can find it.Now if we connect our switches to each other, as shown in Figure 8.13, remember that first we’ll need a crossover cable between the switches. My 2960 and 3560 switches autodetect the connection type, so I was able to use straight-through cables. But a 2950 or 3550 switch won’t autodetect the cable type. Different switches have different needs and abilities, so just keep this in mind when connecting your various switches together.When you first connect the switch ports to each other, the link lights are orange and then turn green indicating normal operation. This is spanning-tree converging, and as you already know, this process takes around 50 seconds with no extensions enabled. But if you connect into a switch port and the switch port LED is alternating green and amber, this means the port is experiencing errors. If this happens, check the host NIC card or the cabling.S1Okay—let’s start our configuration by connecting into each switch and setting the administrative functions. We’ll also assign an IP address to each switch, but this isn’t really necessary to make our network function. The only reason we’re going to do that is so we can manage/administer it. Let’s use a simple IP scheme like 192.168.10.16/28. This mask should be familiar to you! Check out the following output.

Switch>enSwitch#config tEnter configuration commands, one per line.  End with CNTL/Z.Switch(config)#hostname S1S1(config)#enable secret toddS1(config)#int f0/1S1(config-if)#description 1st Connection to Core SwitchS1(config-if)#int f0/2S1(config-if)#description 2nd Connection to Core SwitchS1(config-if)#int f0/3S1(config-if)#description Connection to HostAS1(config-if)#int f0/4S1(config-if)#description Connection to PhoneAS1(config-if)#int f0/8S1(config-if)#description Connection to IVRS1(config-if)#line console 0S1(config-line)#password consoleS1(config-line)#loginS1(config-line)#exitS1(config)#line vty 0 ?

10089.book  Page 517  Monday, July 23, 2007  3:17 PM




518Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)  <1-15>  Last Line number  <cr>S1(config)#line vty 0 15S1(config-line)#password telnetS1(config-line)#loginS1(config-line)#int vlan 1S1(config-if)#ip address 192.168.10.17 255.255.255.240S1(config-if)#no shutS1(config-if)#exitS1(config)#banner motd # This is the S1 switch #S1(config)#exitS1#copy run startDestination filename [startup-config]? [enter]Building configuration...[OK]

S1#The first thing to notice about this is that there’s no IP address configured on the switch’s interfaces. Since all ports on a switch are enabled by default, there’s not so much to configure. The IP address is configured under a logical interface, called a management domain or VLAN. You would typically use the default VLAN 1 to manage a switched network just as we’re doing here. The rest of the configuration is basically the same as the process you go through for router configuration. Remember, no IP addresses on switch interfaces, no routing protocols, and so on. We’re performing layer 2 switching at this point, not routing! Also, note that there is no aux port on Cisco switches.S2Here is the S2 configuration:

Switch#config tSwitch(config)#hostname S2S2(config)#enable secret toddS2(config)#int fa0/12(config-if)#description 1st Connection to CoreS2(config-if)#int fa0/2S2(config-if)#description 2nd Connection to Core S2(config-if)#int fa0/3S2(config-if)#description Connection to HostBS2(config-if)#int fa0/4S2(config-if)#description Connection to PhoneBS2(config-if)#line con 0S2(config-line)#password consoleS2(config-line)#login

10089.book  Page 518  Monday, July 23, 2007  3:17 PM




Configuring Catalyst Switches519S2(config-line)#exit      S2(config)#line vty 0 ?  <1-15>  Last Line number  <cr>S2(config)#line vty 0 15S2(config-line)#password telnetS2(config-line)#loginS2(config-line)#int vlan 1S2(config-if)#ip address 192.168.10.18 255.255.255.240S2(config-if)#no shutS2(config-if)#exitS2(config)#banner motd # This is my S2 Switch #S2(config)#exitS2#copy run startDestination filename [startup-config]?[enter]Building configuration...[OK]

S2#We should now be able to ping from S2 to S1. Let’s try it:

S2#ping 192.168.10.17Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 192.168.10.17, timeout is 2 seconds:.!!!!Success rate is 80 percent (4/5), round-trip min/avg/max = 1/1/1 ms

S2#I have two questions for you: How can I ping through the core switch if I haven’t configured it yet, and why did I get only four pings to work instead of five? (The first period [.] is a time-out; the exclamation point [!] is a success.)Both are good questions. Here’s why: First, you don’t need the switch configured to make it work. All ports are enabled by default, so by just turning it on you should be able to com-municate between hosts. Second, the first ping didn’t work because of the time that ARP takes to resolve the IP address to a hardware MAC address.CoreHere is the Core switch configuration:

Switch>enSwitch#config tSwitch(config)#hostname CoreCore(config)#enable secret todd

10089.book  Page 519  Monday, July 23, 2007  3:17 PM




 520 Chapter8  Layer 2 Switching and Spanning Tree Protocol (STP) Core(config)# int f0/5 Core(config-if)# description 1st Connection to S2     Core(config-if)# int fa0/6 Core(config-if)# description 2nd Connection to S2 Core(config-if)# int f0/7 Core(config-if)# desc 1st Connection to S1 Core(config-if)# int f0/8 Core(config-if)# desc 2nd Connection to S1 Core(config-if)# line con 0 Core(config-line)# password console Core(config-line)# login Core(config-line)# line vty 0 15 Core(config-line)# password telnet Core(config-line)# login Core(config-line)# int vlan 1 Core(config-if)# ip address 192.168.10.19 255.255.255.240 Core(config-if)# no shut Core(config-if)# exit Core(config)# banner motd # This is the Core Switch # Core(config)# exit Core# copy run start Destination filename [startup-config]? [enter] Building configuration...[OK]

 Core# Now let’s ping to S1 and S2 from the Core switch and see what happens:

 Core# ping 192.168.10.17 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 192.168.10.17, timeout is 2 seconds:.!!!!Success rate is 80 percent (4/5), round-trip min/avg/max = 1/1/1 msCore# ping 192.168.10.18 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 192.168.10.18, timeout is 2 seconds:.!!!!Success rate is 80 percent (4/5), round-trip min/avg/max = 1/1/1 msCore# sh ip arp Protocol  Address          Age (min)  Hardware Addr   Type   InterfaceInternet  192.168.10.18           0   001a.e2ce.ff40  ARPA   Vlan1

 

10089c08.fm  Page 520  Friday, November 7, 2008  11:08 PM




 Configuring Catalyst Switches 521 Internet  192.168.10.19           -   000d.29bd.4b80  ARPA   Vlan1Internet  192.168.10.17           0   001b.2b55.7540  ARPA   Vlan1

 Core# Now, before we move on to verifying the switch configurations, there’s one more command you need to know about, even though we don’t need it in our current network because we don’t have a router involved. It’s the  ip default-gateway  command. If you want to manage your switches from outside your LAN, you need to set a default gateway on the switches, just as you would with a host. You do this from global config. Here’s an example where we intro-duce our router with an IP address using the last IP address in our subnet range (we’ll use the router in our next chapter, on VLANs):

 Core# config t Enter configuration commands, one per line.  End with CNTL/Z.Core(config)# ip default-gateway 192.168.10.30 Core(config)# exit

 Core# Now that we have all three switches basically configured, let’s have some fun with them. Port Security As I said earlier in the chapter, it’s usually not a good thing to have your switches available for anyone to just plug into and play around with. I mean, you demand wireless security, so why wouldn’t you want switch security just as much?The answer is, you do, and by using port security, you can limit the number of MAC addresses that can be assigned dynamically to a port, set a static MAC address, and—here’s my favorite part—set penalties for users who abuse your policy. Personally, I like to have the port shut down when the security policy is violated and then make the abusers bring me a memo from their boss explaining to me why they violated the security policy before I’ll enable their port again. That usually really helps them remember to behave!A secured switch port can associate anywhere from 1 to 8,192 MAC addresses, but the ’50 series can support only 192, which seems like enough to me. You can choose to allow the switch to learn these values dynamically, or you can set a static address for each port using the  switchport port-security mac-address  mac-address  command.So let’s set port security on our S1 switch now. Ports fa0/3 and fa0/4 have only one device con-nected in our lab. By using port security, we can know for certain that no other device can connect once our host in port fa0/3 and the phone in fa0/4 are connected. Here’s how we’ll do that:

 S1# config t Enter configuration commands, one per line.  End with CNTL/Z.S1(config)# int range fa0/3 - 4 S1(config-if-range)# switchport port-security maximum ?   <1-8192>  Maximum addressesS1(config-if-range)# switchport port-security maximum 1

 

10089c08.fm  Page 521  Friday, November 7, 2008  11:09 PM




 522 Chapter8  Layer 2 Switching and Spanning Tree Protocol (STP) S1(config-if-range)# switchport port-security mac-address sticky S1(config-if-range)# switchport port-security violation ?   protect   Security violation protect mode  restrict  Security violation restrict mode  shutdown  Security violation shutdown modeS1(config-if-range)# switchport port-security violation shutdown

 S1(config-if-range)# exit The preceding command set port security on port fa0/3 and fa0/4 to allow a maximum association of one MAC address, and only the first MAC address associated to the port will be able to send frames through the switch. If a second device with a different MAC address were to try and send a frame into the switch, the port would be shut down because of our  violation  command. I use the  sticky  command because I am way too lazy to type in all the MAC addresses of each device by hand!There are two other modes you can use instead of just shutting down the port. The protect mode means that another host can connect but its frames will just be dropped. Restrict mode is also pretty cool—it alerts you via SNMP that a violation has occurred on a port. You can then call the abuser and tell them they’re so busted—you can see them, you know what they did, and they’re in big-time trouble!In our connection between switches we have redundant links, so it’s best to let STP run on those links (for now). But on our S1 and S2 switches, we also have hosts connected to port fa0/3 and fa0/4 (not the Core). So let’s turn STP off on those ports.

 Understand that hosts can directly connect physically to the back of phones since phones typically will have an Ethernet jack. So we really need only one port on the switch for both devices. I’ll go over that in depth in the telephony  section of Chapter 9. PortFast If we use the  portfast  command on our switches, we prevent the problem of our hosts pos-sibly being unable to receive a DHCP address because STP takes way too long to converge and the host’s DHCP request times out. So I’m going to use PortFast on port fa0/3 and fa0/4 on both the S1 and S2 switches:

 S1# config t S1(config)# int range f0/3-4 S1(config-if-range)# spanning-tree portfast ?   disable  Disable portfast for this interface  trunk    Enable portfast on the interface even in trunk mode  <cr>S1(config-if-range)# spanning-tree portfast %Warning: portfast should only be enabled on ports connected to a

 

10089c08.fm  Page 522  Wednesday, February 27, 2008  5:09 PM




Configuring Catalyst Switches523   single host. Connecting hubs, concentrators, switches, bridges,   etc... to this interface when portfast is enabled, can cause   temporary bridging loops. Use with CAUTION%Portfast has been configured on FastEthernet0/2 but will only have effect when the interface is in a non-trunking mode.

S1(config-if-range)#Now that I have configured S1, I won’t show you the next output, but I’m going over to S2 to enable PortFast on fa0/3-4 as well. Again, just a reminder to be careful when using PortFast—you definitely do not want to create a network loop! Why? Because if you let this happen, even though the network may still work (well, kind of), data will pass super slowly, and worse, it could take you a really long time to find the source of the problem, making you very unpopular. So proceed carefully.It would be good to know that there are some safeguard commands you can use when using PortFast in case someone accidentally causes a loop in a port that’s configured with PortFast enabled. Here they are.BPDUGuardI talked about this a bit earlier: If you turn on PortFast for a switch port, turning on BPDUGuard is a really good idea. If a switch port that has PortFast enabled receives a BPDU on that port, it will place the port into error disabled state. This stops an administrator from accidentally con-necting another switch or hub port into a switch port configured with PortFast. Basically, you’re preventing (guarding) this from happening and bringing down, or at least severely crippling, your network. You’d only configure this command on your Access layer switches—switches where users are directly connected—so we wouldn’t configure this on our Core switch.BPDUFilterAnother helpful command to use with PortFast is BPDUFilter. Since a switch port that has PortFast enabled will still receive BPDUs by default, you can use BPDUFilter to completely stop BPDUs from coming to or going from that port. BPDUFilter filtering will immediately take a port out of PortFast if it receives a BPDU and force the port to be part of the STP topol-ogy again. Unlike BPDUGuard, which places the port into error disabled state, the BPDUFilter will keep a port up, but without PortFast running. There’s just no reason to have BPDUs received on an interface configured with PortFast. To be perfectly honest, I have no idea why BPDUGuard or BPDUFilter are not enabled by default when PortFast is enabled.So let’s configure our S1 and S2 interfaces that are already configured with PortFast with both the BPDUGuard and BPDUFilter now—it’s easy:

S1(config-if-range)#spanning-tree bpduguard ?  disable  Disable BPDU guard for this interface  enable   Enable BPDU guard for this interfaceS1(config-if-range)#spanning-tree bpduguard enable

10089.book  Page 523  Monday, July 23, 2007  3:17 PM




524Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)S1(config-if-range)#spanning-tree bpdufilter ?     disable  Disable BPDU filtering for this interface  enable   Enable BPDU filtering for this interfaceS1(config-if-range)#spanning-tree bpdufilter enableS2(config-if-range)#spanning-tree bpduguard enable

S2(config-if-range)#spanning-tree bpdufilter enableUnderstand that you typically would use one command or the other because both bpduguard and bpdufilter accomplish the same thing, so configuring both commands is somewhat overkill. We’re also going to configure a couple more STP 802.1d extensions that you can use when configuring STP.UplinkFastHere’s how to configure UplinkFast on our Access layer switches (S1 and S2):

S1#config tS1(config)#spanning-tree uplinkfastS2#config tS2(config)#spanning-tree uplinkfastS1(config)#do show spanning-tree uplinkfastUplinkFast is enabledStation update rate set to 150 packets/sec.UplinkFast statistics-----------------------Number of transitions via uplinkFast (all VLANs)            : 1Number of proxy multicast addresses transmitted (all VLANs) : 8Name                 Interface List-------------------- ------------------------------------VLAN0001             Fa0/1(fwd), Fa0/2

S1(config)#The uplinkfast command is a global command and it’s enabled on every port.BackboneFastHere’s how you would configure BackboneFast on a switch:

S1(config)#spanning-tree backbonefastS2(config)#spanning-tree backbonefastCore(config)#spanning-tree backbonefast

10089.book  Page 524  Monday, July 23, 2007  3:17 PM




Configuring Catalyst Switches525S2(config)#do show spanning-tree backbonefastBackboneFast is enabledBackboneFast statistics-----------------------Number of transition via backboneFast (all VLANs)           : 0Number of inferior BPDUs received (all VLANs)               : 2Number of RLQ request PDUs received (all VLANs)             : 0Number of RLQ response PDUs received (all VLANs)            : 1Number of RLQ request PDUs sent (all VLANs)                 : 1Number of RLQ response PDUs sent (all VLANs)                : 0

S2(config)#Notice that unlike what I did with UplinkFast, I configured BackboneFast on all switches in the network, not just the Access layer switches. Remember, BackboneFast is used to deter-mine link failures on a remote switch, unlike UplinkFast, which is used to both determine and quickly fix link failures on the local switch.RSTP (802.1w)Configuring RSTP actually is as easy as configuring any of our other 802.1d extensions. Considering how much better it is than 802.1d, you’d think the configuration would be more complex, but we’re in luck—it’s not. So let’s turn it on in the Core switch now and see what happens:

Core#config tCore(config)#spanning-tree mode ?  mst         Multiple spanning tree mode  pvst        Per-Vlan spanning tree mode  rapid-pvst  Per-Vlan rapid spanning tree modeCore(config)#spanning-tree mode rapid-pvstCore(config)#1d02h: %LINEPROTO-5-UPDOWN: Line protocol on Interface Vlan1, changed state to down1d02h: %LINEPROTO-5-UPDOWN: Line protocol on Interface Vlan1,

 changed state to upSweet! The Core switch is now running the 802.1w STP. Let’s verify that:

Core(config)#do show spanning-treeVLAN0001  Spanning tree enabled protocol rstp  Root ID    Priority    32769             Address     000d.29bd.4b80

10089.book  Page 525  Monday, July 23, 2007  3:17 PM




526Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)             This bridge is the root             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec  Bridge ID  Priority    32769  (priority 32768 sys-id-ext 1)             Address     000d.29bd.4b80             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec             Aging Time 300Interface        Role Sts Cost      Prio.Nbr Type---------------- ---- --- --------- -------- ------------Fa0/5            Desg FWD 19        128.5    P2p Peer(STP)Fa0/6            Desg FWD 19        128.6    P2p Peer(STP)Fa0/7            Desg FWD 19        128.7    P2p Peer(STP)

Fa0/8            Desg FWD 19        128.8    P2p Peer(STP)Interesting… it looks like nothing really happened. I can see on my two other switches that all ports have converged. Once everything was up, everything looked the same. 802.1d and 802.1w seem to be cohabiting with no problem.But, if we were to look under the hood more closely, we’d see that the 802.1w switch has changed from 802.1w BPDUs to 802.1d BPDUs on the ports connecting to the other switches running 802.1d (which is all of them).The S1 and S2 switches believe that the Core switch is actually running 802.1d because the Core reverted back to 802.1d BPDUs just for them. And even though the S1 and S2 switches receive the 802.1w BPDUs, they don’t understand them, so they simply drop them. However, the Core does receive the 802.1d BPDUs and accepts them from the S1 and S2 switches, now knowing which ports to run 802.1d on. In other words, turning 802.1w on for just one switch didn’t really help our network at all!One small annoying issue is that once the Core switch knows to send 802.1d BPDUs out the ports connected to S1 and S2, it won’t change this automatically if the S1 and S2 switches were later configured with 802.1w—we’d still need to reboot the Core switch to stop the 802.1d BPDUs.EtherChannelThe easiest way to configure EtherChannel is through the Cisco Network Assistant, and I’ll show you this at the very end of the chapter. For now, I’m going with the CLI because you need to know CLI commands too. Remember, there are two versions of EtherChannel, the Cisco version and the IEEE version. I’m going to use the Cisco version and bundle the links between the S1 switch and the Core.I’ll use the interface port-channel global command and the channel-group and the channel-protocol interface commands on the S1 and Core switches. Here’s what that looks like:

S1#config tS1(config)#int port-channel 1S1(config-if)#int range f0/1-2

10089.book  Page 526  Monday, July 23, 2007  3:17 PM




Configuring Catalyst Switches527S1(config-if-range)#switchport mode trunk1d03h: %SPANTREE_FAST-7-PORT_FWD_UPLINK: VLAN0001 FastEthernet0/2 moved to Forwarding (UplinkFast).S1(config-if-range)#switchport nonegotiateS1(config-if-range)#channel-group 1 mode desirableS1(config-if-range)#do sh int fa0/1 etherchannelPort state    = Up Sngl-port-Bndl Mstr Not-in-BndlChannel group = 1      Mode = Desirable-Sl    Gcchange = 0Port-channel  = null   GC   = 0x00010001   Pseudo port-channel = Po1Port index    = 0      Load = 0x00         Protocol =   PAgP[output cut]Core#config tCore(config)#int port-channel 1Core(config-if)#int range f0/7-8Core(config-if-range)#switchport trunk encap dot1qCore(config-if-range)#switchport mode trunk1d03h: %SPANTREE_FAST-7-PORT_FWD_UPLINK: VLAN0001 FastEthernet0/2 moved to Forwarding (UplinkFast).Core(config-if-range)#switchport nonegotiateCore(config-if-range)#channel-group 1 mode desirable1d04h: %SPANTREE_FAST-7-PORT_FWD_UPLINK: VLAN0001 FastEthernet0/2 moved to Forwarding (UplinkFast).1d04h: %SPANTREE_FAST-7-PORT_FWD_UPLINK: VLAN0001 FastEthernet0/2 moved to Forwarding (UplinkFast).1d04h: %LINK-3-UPDOWN: Interface Port-channel1, changed state to up1d04h: %LINEPROTO-5-UPDOWN: Line protocol on InterfacePort-channel1, changed state to upCore(config-if-range)#do show int port-channel 1Port-channel1 is up, line protocol is up (connected)  Hardware is EtherChannel, address is 001b.2b55.7501 (bia 001b.2b55.7501)  MTU 1500 bytes, BW 200000 Kbit, DLY 100 usec,     reliability 255/255, txload 1/255, rxload 1/255  Encapsulation ARPA, loopback not set  Full-duplex, 100Mb/s, link type is auto, media type is unknown

[output cut]I added the switchport nonegotiate interface command to stop the switches from trying to autodetect the link types and also to automatically set up trunking; instead, I statically con-figured my trunk links. The two links between the S1 and the Core are now bundled using the Cisco EtherChannel version of PAgP.

10089.book  Page 527  Monday, July 23, 2007  3:17 PM




528Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)Okay—but wait, we still need to verify our switch configurations and play with our root bridge before we can learn about Virtual LANs in the next chapter.Verifying Cisco Catalyst SwitchesThe first thing I like to do with any router or switch is to run through the configurations with a show running-config command. Why? Because doing this gives me a really great headshot of each device. However, it’s time consuming, and showing you all the configs would take up a whole bunch of pages in this book. Besides, we can run other commands that will still stock us with really good information.For example, to verify the IP address set on a switch, we can use the show interface command. Here is the output:

S1#sh int vlan 1Vlan1 is up, line protocol is up  Hardware is EtherSVI, address is 001b.2b55.7540 (bia 001b.2b55.7540)  Internet address is 192.168.10.17/28  MTU 1500 bytes, BW 1000000 Kbit, DLY 10 usec,     reliability 255/255, txload 1/255, rxload 1/255  Encapsulation ARPA, loopback not set, reliability 255/255, txload 1/255, rxload 1/255

  [output cut]

Remember that IP addresses aren’t needed on a switch. The only reason we would set an IP address, mask, and default gateway is for management purposes.show mac address-tableI’m sure you remember being shown this command earlier in the chapter. Using it displays the forward filter table, also called a content addressable memory (CAM) table. Here’s the output from the S1 switch:

S1#sh mac address-table          Mac Address Table-------------------------------------------Vlan    Mac Address       Type        Ports----    -----------       --------    ----- All    0100.0ccc.cccc    STATIC      CPU All    ffff.ffff.ffff    STATIC      CPU[output cut]   1    0002.1762.b235    DYNAMIC     Po1   1    0009.b79f.c080    DYNAMIC     Po1

10089.book  Page 528  Monday, July 23, 2007  3:17 PM




 Configuring Catalyst Switches 529    1    000d.29bd.4b87    DYNAMIC     Po1   1    000d.29bd.4b88    DYNAMIC     Po1   1    0016.4662.52b4    DYNAMIC     Fa0/4   1    0016.4677.5eab    DYNAMIC     Po1   1    001a.2f52.49d8    DYNAMIC     Po1   1    001a.2fe7.4170    DYNAMIC     Fa0/8   1    001a.e2ce.ff40    DYNAMIC     Po1   1    0050.0f02.642a    DYNAMIC     Fa0/3Total Mac Addresses for this criterion: 31

 S1# The switches use what are called base MAC addresses that are assigned to the CPU, and the 2960s use 20. From the preceding output, you can see that we have seven MAC addresses dynamically assigned to EtherChannel port 1. Ports Fa0/3, Fa0/8, and Fa0/4 only have one MAC address assigned, and all ports are assigned to VLAN 1.Let’s take a look at the S2 switch CAM and see what we can find. Keep in mind that the S2 switch doesn’t have EtherChannel configured as the S1 switch does, so STP will shut down one of the redundant links to the Core switch:

 S2# sh mac address-table           Mac Address Table-------------------------------------------Vlan    Mac Address       Type        Ports----    -----------       --------    ----- All    0008.205a.85c0    STATIC      CPU All    0100.0ccc.cccc    STATIC      CPU All    0100.0ccc.cccd    STATIC      CPU All    0100.0cdd.dddd    STATIC      CPU[output cut]   1    0002.1762.b235    DYNAMIC     Fa0/3   1    000d.29bd.4b80    DYNAMIC     Fa0/1   1    000d.29bd.4b85    DYNAMIC     Fa0/1   1    0016.4662.52b4    DYNAMIC     Fa0/1   1    0016.4677.5eab    DYNAMIC     Fa0/4   1    001b.2b55.7540    DYNAMIC     Fa0/1Total Mac Addresses for this criterion: 26

 S2# We can see in the preceding output that we have four MAC addresses assigned to Fa0/1. And of course, we can also see that we have one connection for each host on ports 3 and 4. But where’s port 2? Since port 2 is a redundant link, STP placed Fa0/2 into blocking mode. I’ll get into more about this again in a minute.

 

10089c08.fm  Page 529  Wednesday, February 27, 2008  5:08 PM




530Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)Assigning Static MAC AddressesYou can set a static MAC address in the MAC address table, but like setting static MAC port security, it’s a ton of work. But in case you want to do it, here’s how it’s done:

S1#config tS1(config)#mac-address-table static aaaa.bbbb.cccc vlan 1 int fa0/5S1(config)#do show mac address-table          Mac Address Table-------------------------------------------Vlan    Mac Address       Type        Ports----    -----------       --------    ----- All    0100.0ccc.cccc    STATIC      CPU[output cut]   1    0002.1762.b235    DYNAMIC     Po1   1    0009.b79f.c080    DYNAMIC     Po1   1    000d.29bd.4b87    DYNAMIC     Po1   1    000d.29bd.4b88    DYNAMIC     Po1   1    0016.4662.52b4    DYNAMIC     Fa0/4   1    0016.4677.5eab    DYNAMIC     Po1   1    001a.2f52.49d8    DYNAMIC     Po1   1    001a.2fe7.4170    DYNAMIC     Fa0/8   1    001a.e2ce.ff40    DYNAMIC     Po1   1    0050.0f02.642a    DYNAMIC     Fa0/3   1    aaaa.bbbb.cccc    STATIC      Fa0/5Total Mac Addresses for this criterion: 31

S1(config)#You can see that a static MAC address is now assigned permanently to interface Fa0/5 and that it’s also assigned to VLAN 1 only.show spanning-treeBy this time you know that the show spanning-tree command is important. With it, you can see who the root bridge is and what our priorities are set to for each VLAN.Understand that Cisco switches run what is called Per-VLAN Spanning Tree (PVST), which basically means that each VLAN runs its own instance of the STP protocol. If we typed show spanning-tree, we’d receive information for each VLAN, starting with VLAN 1. So, say we’ve got multiple VLANs and we want to see what’s up with VLAN 2—we’d use the com-mand show spanning-tree vlan 2.Here is an output from the show spanning-tree command from switch S1. Since we are only using VLAN 1, we don’t need to add the VLAN number to the command:

S1#sh spanning-treeVLAN0001

10089.book  Page 530  Monday, July 23, 2007  3:17 PM




Configuring Catalyst Switches531  Spanning tree enabled protocol ieee  Root ID    Priority    32769             Address     000d.29bd.4b80             Cost        3012             Port        56 (Port-channel1)             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec  Bridge ID  Priority    49153  (priority 49152 sys-id-ext 1)             Address     001b.2b55.7500             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec             Aging Time 15  Uplinkfast enabledInterface        Role Sts Cost      Prio.Nbr Type---------------- ---- --- --------- -------- ----------Fa0/3            Desg FWD 3100      128.3    Edge ShrFa0/4            Desg FWD 3019      128.4    Edge P2pFa0/8            Desg FWD 3019      128.8    P2p

Po1              Root FWD 3012      128.56   P2pSince we only have VLAN 1 configured, there’s no more output for this command, but if we had more, we would get another page for each VLAN configured on the switch. The default pri-ority is 32768, but there’s something called the system ID extension (sys-id-ext), which is the VLAN identifier. The Bridge ID priority is incremented by the number of that VLAN. And since we only have VLAN 1, we increment by one to 32769. But understand, by default, Backbone-Fast raises the default priority to 49152 to prevent that bridge from becoming the root.The top of the output shows us who the root bridge is:

VLAN0001    Root ID  Priority    32769             Address     000d.29bd.4b80             Cost        3012             Port        56 (Port-channel1)

             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 secEtherChannel Port 1 is our root port, which means that it’s our chosen path to the root bridge, and it has an identifier of 000d.29bd.4b80. That can only be either the Core switch or S2, and we’ll find out which one it is in a minute.The last output from the command displays the ports that are running STP and have a con-nection to another device. Because we’re running EtherChannel, we have no blocked ports. One way to determine if your bridge is the root is to look to see whether there are any Altn BLK ports (meaning blocked ports that are alternates). A root bridge would never have a blocked port on any interface, but all our ports on S1 show forwarding (FWD) because of our EtherChannel configuration.

10089.book  Page 531  Monday, July 23, 2007  3:17 PM




532Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)Determining Our Root BridgeTo determine our root bridge, we would obviously use the show spanning-tree command. Let’s take a look at our other two switches and see which switch is the default root bridge. Make a mental note of the Bridge ID MAC address as well as the priority of the S1 switch. Here’s the S2 output:

S2#sh spanning-treeVLAN0001  Spanning tree enabled protocol ieee  Root ID    Priority    32769             Address     000d.29bd.4b80             Cost        3019             Port        2 (FastEthernet0/1)             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec  Bridge ID  Priority    49153  (priority 49152 sys-id-ext 1)             Address     001a.e2ce.ff00             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec             Aging Time 300  Uplinkfast enabledInterface        Role Sts Cost      Prio.Nbr Type---------------- ---- --- --------- -------- ------------Fa0/1            Root FWD 3019      128.2    P2pFa0/2            Altn BLK 3019      128.3    P2pFa0/3            Desg FWD 3100      128.4    Edge ShrFa0/4            Desg FWD 3019      128.5    Edge P2p

S2#We can see that port Fa0/2 is blocked, so this cannot be our root bridge. A root bridge can-not have blocked ports. Again, pay special attention to the Bridge ID MAC address and the priority. Here’s the output from the Core switch:

Core#sh spanning-treeVLAN0001  Spanning tree enabled protocol rstp  Root ID    Priority    32769             Address     000d.29bd.4b80             This bridge is the root             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec

10089.book  Page 532  Monday, July 23, 2007  3:17 PM




Configuring Catalyst Switches533  Bridge ID  Priority    32769  (priority 32768 sys-id-ext 1)             Address     000d.29bd.4b80             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec             Aging Time 300Interface        Role Sts Cost      Prio.Nbr Type---------------- ---- --- --------- -------- --------------Fa0/5            Desg FWD 19        128.5    P2p Peer(STP)Fa0/6            Desg FWD 19        128.6    P2p Peer(STP)

Po1              Desg FWD 12        128.66   P2p Peer(STP)Well there you have it— “This bridge is the root.”But think about this—why does the Core switch just have the default of 32768 and not 49152 like the other switches? Because it’s running the 802.1w version of STP, and BackBone-Fast is disabled by default. Let’s take a look at the bridge MAC address of each switch: S1 address: 001b.2b55.7500 S2 address: 001a.e2ce.ff00 Core address: 000d.29bd.4b80By checking out the MAC addresses, and if all switches are set to the default priority, which switch do you think will be the root switch? Start reading the MAC addresses from the left, moving toward the right. Core is obviously the lowest MAC address, and by looking at the output of the show spanning-tree command, we can see that it is, indeed, our root bridge (even if all switches had the same priority). It’s good practice to figure out the root bridge by comparing the MAC addresses of the switches once in awhile.Setting Our Root BridgeIt’s kind of convenient that the Core switch is our root bridge by default because that’s right where I’d typically choose to set the root. But just for fun, let’s change it. Here’s how we’ll do that:

S1#config tS1(config)#spanning-tree vlan 1 priority ?  <0-61440>  bridge priority in increments of 4096S1(config)#spanning-tree vlan 1 priority 16384S1(config)#do show spanning-treeVLAN0001Spanning tree enabled protocol ieee  Root ID    Priority    16385             Address     001b.2b55.7500             This bridge is the root             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec

10089.book  Page 533  Monday, July 23, 2007  3:17 PM




534Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)  Bridge ID  Priority    16385  (priority 16384 sys-id-ext 1)             Address     001b.2b55.7500             Hello Time   2 sec  Max Age 20 sec  Forward Delay 15 sec             Aging Time 300Interface        Role Sts Cost      Prio.Nbr Type---------------- ---- --- --------- -------- -----------Fa0/3            Desg FWD 100       128.3    Edge ShrFa0/4            Desg FWD 19        128.4    Edge P2pFa0/8            Desg FWD 19        128.8    P2p

Po1              Desg FWD 12        128.56   P2pWhen you lower the S1 priority to 16384, the S1 switch immediately became the root bridge. You can set your priorities all the way from 0 to 61440. Zero (0) means that the switch will always be the root bridge, and 61440 means the switch will never be a root.There’s one last command I want to tell you about, if you want to skip all this verification and configuration of the root bridge stuff—and no, you don’t get to skip all that if you want to pass the Cisco exams! Here’s a simple command you can run on a switch to set it as a root bridge:

S1(config)#spanning-tree vlan 1 root ?  primary    Configure this switch as primary root for this spanning tree  secondary  Configure switch as secondary root

S1(config)#spanning-tree vlan 1 root primaryUnderstand that this does not override a low-priority switch—this command would only work for you if all your switches had the same priority, or higher, set. Oh, and did I mention that you would have to configure this per VLAN, and that you can also set a primary and sec-ondary switch as roots? Yep, you can, and it’s certainly a whole lot easier than how we’ve done it in this chapter! But this is, first and foremost, a guide to prepare you for the CCNA exam—something you definitely want to pass. So make sure you know how to do it like we did even though it really is the hard way!Cisco Network AssistantThe Cisco Network Assistant (CNA) can make configuring your switches a breeze, which, as with the SDM, is both good and bad. It’s good in that it makes it easier for us to create much harder configs, and it’s bad because it makes it easier for everyone else to do that as well. But still, it can be a little tricky at first, so download it and get familiar with the CNA as much as possible.

10089.book  Page 534  Monday, July 23, 2007  3:17 PM




Cisco Network Assistant535Once we connect to a switch using the CNA, we’ll get a screen like this one.We get a nice topology view of all connected devices. We can see my IP phone, the Core switch, and the inter-VLAN router (IVR), which happens to be the hostname of my 2811 router. But you need to understand that we see only directly connected devices in this output, which means we can’t see devices on the other side of the Core.Probably one of the most helpful menu choices on the left side of the CNA screen is Smart-ports. Once you click Smartports, you get a close-up view of the switch.

10089.book  Page 535  Monday, July 23, 2007  3:17 PM




536Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)All you need to do is highlight a port, or all of them, and then right-click and choose the type of device that port, or group of ports, will be connecting to. Nice feature! However, an interesting aspect of Smartports is what exactly it configures. For example, I used Smartports to configure port 6 of my S1 switch to be connected to a desktop (meaning a PC). Here are the commands it downloaded to my switch:

!interface FastEthernet0/6 switchport mode access switchport port-security switchport port-security aging time 2 switchport port-security violation restrict switchport port-security aging type inactivity macro description cisco-desktop spanning-tree portfast spanning-tree bpduguard enable

! Holy smokes! Check that out—I don’t even need to understand any of the features we dis-cussed in this chapter to configure my switch. Instead, I just need to choose the type of device connected to the interface and, wham—the switch is automatically configured with the com-mands that it “thinks” are the smart choices for that type of port. Wow—this little beauty really rocks!Now before we move on with the CNA, let’s talk a bit about the configuration that was placed under the interface. We’ve already discussed port-security, the spanning-tree portfast, and bpduguard in this chapter, but what about the macro and port-security aging commands that it also set? The macros are the default programs that are stored on the new Cisco switches that are run when a Smartport is set. The port-security aging command does just that—it sets the aging rate of the port. There are two options you can use with the aging command: absolute and inactivity. The absolute option will delete the secure addresses on a port after the specified time—anywhere from 0 to 1440 minutes. The inactivity option means that the addresses on the port will be deleted when they are inactive for the specified time. That time is also set in minutes from 0 to 1440. So, when you use the command switchport port-security aging time 2 with the command switchport port-security aging type inactivity, all the MAC addresses associated to the port will be deleted after two minutes. And not only that, the command switchport port-security violation restrict means that a trap will be sent to an SNMP server, or Network Management Station (NMS). Even though this seems a little overboard, to me it’s still pretty cool.You can view the port security on an interface with this command:

S1#sh port-security interface f0/6Port Security              : Enabled

10089.book  Page 536  Monday, July 23, 2007  3:17 PM




Cisco Network Assistant537Port Status                : Secure-downViolation Mode             : RestrictAging Time                 : 2 minsAging Type                 : InactivitySecureStatic Address Aging : DisabledMaximum MAC Addresses      : 1Total MAC Addresses        : 0Configured MAC Addresses   : 0Sticky MAC Addresses       : 0Last Source Address:Vlan   : 0000.0000.0000:0

Security Violation Count   : 0Wait a minute—what’s happening with that macro command in there? Well, no worries. It’s just a switch command that lets you create and run macros, and Cisco’s new switches have six of those preconfigured. Weirdly, you can’t see them in the running configuration; you can only view them with the show parser command. Here’s the macro that was run on our port f0/6:

S1#sh parser macroTotal number of macros = 6--------------------------------------------------------------Macro name : cisco-desktopMacro type : default interface# macro keywords $access_vlan# Basic interface - Enable data VLAN only# Recommended value for access vlan should not be 1switchport access vlan $access_vlanswitchport mode access# Enable port security limiting port to a single# MAC address -- that of desktopswitchport port-securityswitchport port-security maximum 1# Ensure port-security age is greater than one minute# and use inactivity timerswitchport port-security violation restrictswitchport port-security aging time 2switchport port-security aging type inactivity# Configure port as an edge network port

10089.book  Page 537  Monday, July 23, 2007  3:17 PM




538Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)spanning-tree portfastspanning-tree bpduguard enable

[output cut]All right, all’s good. At least now we know how all those command were configured on our switch—there’s a macro for each of the devices under Smartports, as shown by the show parser macro brief command:

S1#sh parser macro brief    default global   : cisco-global    default interface: cisco-desktop    default interface: cisco-phone    default interface: cisco-switch    default interface: cisco-router

    default interface: cisco-wirelessAnyway, getting back to the CNA, under Ports, you’ll find Port Settings.

10089.book  Page 538  Monday, July 23, 2007  3:17 PM




Cisco Network Assistant539Here’s where you’ll find all the ports with basic configuration information that you can both view and change. Also under Ports, you’ll find EtherChannels.This may be an easier way to bundle some ports instead of using the CLI. With it, you can bundle up to eight ports between switches. After clicking EtherChannel, just choose your group and indicate the ports you want to bundle and the protocol you’re going to use. That’s it!

10089.book  Page 539  Monday, July 23, 2007  3:17 PM




540Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)And this is very cool—by clicking Device Properties, you can set many of your switch’s features from here: the IP addresses, hostname, users and passwords, and more.From the Topology View screen, you can right-click a device and click on a variety of options, but the Device Manager is interesting.

10089.book  Page 540  Monday, July 23, 2007  3:17 PM




Summary541One you choose Device Manager, your default HTTP browser will open, and it’s now time to begin configuring and verifying your switch from an HTTP browser—however, it has a slower response time than through the CNA so I don’t personally use it.There’s a lot more to the CNA, and I recommend that you take the time to download it and play with it as much as possible.Now admit it—even though this was a huge chapter, you really did learn a lot, and well, maybe you even had a little fun along the way! You’ve now configured and verified all switches, set port security, and navigated STP extensions as well as set your root bridge. That means you’re now ready to learn all about virtual LANs! I’m going to save all our switch configurations so we’ll be able to start right from here in Chapter 9, “Virtual LANs (VLANs).”SummaryIn this chapter, I talked about the differences between switches and bridges and how they both work at layer 2 and create a MAC address forward/filter table in order to make decisions on whether to forward or flood a frame.I also discussed problems that can occur if you have multiple links between bridges (switches) and how to solve these problems by using the Spanning Tree Protocol (STP).I also covered detailed configuration of Cisco’s Catalyst switches, including verifying the configuration, setting the Cisco STP extensions, and changing the root bridge by setting a bridge priority.Last, I went through the Cisco Network Assistant, which can help you tremendously with your switch configurations.

10089.book  Page 541  Monday, July 23, 2007  3:17 PM




542Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)Exam EssentialsRemember the three switch functions.Address learning, forward/filter decisions, and loop avoidance are the functions of a switch.Remember the command show mac address-table.The command show mac address-table will show you the forward/filter table used on the LAN switchUnderstand the main purpose of the Spanning Tree Protocol in a switched LAN.The main purpose of STP is to prevent switching loops in a network with redundant switched paths.Remember the states of STP.The purpose of the blocking state is to prevent the use of looped paths. A port in listening state prepares to forward data frames without populating the MAC address table. A port in learning state populates the MAC address table but doesn’t for-ward data frames. A port in forwarding state sends and receives all data frames on the bridged port. Last, a port in the disabled state is virtually nonoperational.Remember the command show spanning-tree.You must be familiar with the command show spanning-tree and how to determine who the root bridge is.Written Lab 8Write the answers to the following questions:1.What command will show you the forward/filter table?2.If a destination MAC address is not in the forward/filter table, what will the switch do with the frame?3.What are the three switch functions at layer 2?4.If a frame is received on a switch port and the source MAC address is not in the forward/filter table, what will the switch do?5.What is used at layer 2 to prevent switching loops?6. 802.1w is also called what?7.When is STP considered to be converged?8.Switches break up _________ domains.9.What is used to prevent switching loops in a network with redundant switched paths?10.Which Cisco 802.1d extension stops BPDU from being transmitted out a port?(The answers to Written Lab 8 can be found following the review questions for this chapter.)

10089.book  Page 542  Monday, July 23, 2007  3:17 PM




Review Questions543Review Questions

The following questions are designed to test your understanding of this chapter’s material. For more information on how to get additional ques-tions, please see this book’s Introduction.1.Which of the following is a layer 2 protocol used to maintain a loop-free network?A.VTPB.STPC.RIPD.CDP2.What command will display the forward/filter table?A.show mac filterB.show runC.show mac address-tableD.show mac filter-table3.What is the result of segmenting a network with a bridge (switch)? (Choose two options.)A.It increases the number of collision domains.B.It decreases the number of collision domains.C.It increases the number of broadcast domains.D.It decreases the number of broadcast domains.E.It makes smaller collision domains.F.It makes larger collision domains.4.Which statement describes a spanning-tree network that has converged?A.All switch and bridge ports are in the forwarding state.B.All switch and bridge ports are assigned as either root or designated ports.C.All switch and bridge ports are in either the forwarding or blocking state.D.All switch and bridge ports are either blocking or looping.5.What is the purpose of Spanning Tree Protocol in a switched LAN?A.To provide a mechanism for network monitoring in switched environmentsB.To prevent routing loops in networks with redundant pathsC.To prevent switching loops in networks with redundant switched pathsD.To manage the VLAN database across multiple switchesE.To create collision domains

10089.book  Page 543  Monday, July 23, 2007  3:17 PM




544Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)6.What are the three distinct functions of layer 2 switching that increase available bandwidth on the network? (Choose three.)A.Address learningB.RoutingC.Forwarding and filteringD.Creating network loopsE.Loop avoidanceF.IP addressing7.Your switch has a port status LED that is alternating between green and amber. What could this indicate?A.The port is experiencing errors.B.The port is shut down.C.The port is in STP blocking mode.D.Nothing; this is normal.8.Which of the following statements is true?A.A switch creates a single collision domain and a single broadcast domain. A router creates a single collision domain.B.A switch creates separate collision domains but one broadcast domain. A router provides a separate broadcast domain.C.A switch creates a single collision domain and separate broadcast domains. A router pro-vides a separate broadcast domain as well.D.A switch creates separate collision domains and separate broadcast domains. A router pro-vides separate collision domains.9.You need to configure a Catalyst switch so it can be managed remotely. Which of the following would you use to accomplish this task?A.Switch(configs)#int fa0/1Switch(configs-if)#ip address 192.168.10.252 255.255.255.0Switch(configs-if)#no shutB.Switch(configs)#int vlan 1Switch(configs-if)#ip address 192.168.10.252 255.255.255.0Switch(configs-if)#ip default-gateway 192.168.10.254 255.255.255.0C.Switch(configs)#ip default-gateway 192.168.10.254Switch(configs)#int vlan 1Switch(configs-if)#ip address 192.168.10.252 255.255.255.0Switch(configs-if)#no shutD.Switch(configs)#ip default-network 192.168.10.254Switch(configs)#int vlan 1Switch(configs-if)#ip address 192.168.10.252 255.255.255.0Switch(configs-if)#no shut

10089.book  Page 544  Monday, July 23, 2007  3:17 PM




Review Questions54510.What does a switch do when a frame is received on an interface and the destination hardware address is unknown or not in the filter table?A.Forwards the switch to the first available linkB.Drops the frameC.Floods the network with the frame looking for the deviceD.Sends back a message to the originating station asking for a name resolution11.If a switch receives a frame and the source MAC address is not in the MAC address table but the destination address is, what will the switch do with the frame?A.Discard it and send an error message back to the originating hostB.Flood the network with the frameC.Add the source address and port to the MAC address table and forward the frame out the destination portD.Add the destination to the MAC address table and then forward the frame12.You want to run the new 802.1w on your switches. Which of the following would enable this protocol?A.Switch(config)#spanning-tree mode rapid-pvstB.Switch#spanning-tree mode rapid-pvstC.Switch(config)#spanning-tree mode 802.1wD.Switch#spanning-tree mode 802.1w13.In which circumstance are multiple copies of the same unicast frame likely to be transmitted in a switched LAN?A.During high-traffic periodsB.After broken links are reestablishedC.When upper-layer protocols require high reliabilityD.In an improperly implemented redundant topology14.Which command was used to produce the following output:Vlan    Mac Address       Type        Ports----    -----------       --------    -----   1    0005.dccb.d74b    DYNAMIC     Fa0/1   1    000a.f467.9e80    DYNAMIC     Fa0/3   1    000a.f467.9e8b    DYNAMIC     Fa0/4   1    000a.f467.9e8c    DYNAMIC     Fa0/3   1    0010.7b7f.c2b0    DYNAMIC     Fa0/3

   1    0030.80dc.460b    DYNAMIC     Fa0/3A.show vlanB.show ip routeC.show mac address-tableD.show mac address-filter

10089.book  Page 545  Monday, July 23, 2007  3:17 PM




546Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)15.If you want to disable STP on a port connected to a server, which command would you use?A.disable spanning-treeB.spanning-tree offC.spanning-tree securityD.spanning-tree portfast16.Refer to the Graphic. Why does the switch have two MAC addresses assigned to the Fast-Ethernet 0/1 port in the switch address table?A.Data from HostC and HostD have been received by the switch port FastEthernet 0/1.B.Data from two of the devices connected to the switch have been forwarded out to HostD.C.HostC and HostD had their NIC replaced.D.HostC and HostD are on different VLANs.17. Layer 2 switching provides which of the following? (Choose four.)A.Hardware-based bridging (ASIC)B.Wire speedC.Low latencyD.Low costE.RoutingF.WAN services

HostA HostB HostC HostD MAC Address Type Ports 0005.dccb.d74b 000a.f467.9e80 000a.f467.9e8b 000a.f467.9e8c  DYNAMIC  DYNAMIC  DYNAMIC  DYNAMIC  Fa0/1 Fa0/1 Fa0/4 Fa0/3 

10089.book  Page 546  Monday, July 23, 2007  3:17 PM




Review Questions54718.You type show mac address-table and receive the following output:Switch#sh mac address-tableVlan    Mac Address       Type        Ports----    -----------       --------    -----   1    0005.dccb.d74b    DYNAMIC     Fa0/1   1    000a.f467.9e80    DYNAMIC     Fa0/3   1    000a.f467.9e8b    DYNAMIC     Fa0/4   1    000a.f467.9e8c    DYNAMIC     Fa0/3   1    0010.7b7f.c2b0    DYNAMIC     Fa0/3

   1    0030.80dc.460b    DYNAMIC     Fa0/3Suppose the above switch received a frame with the following MAC addresses: Source MAC: 0005.dccb.d74b Destination MAC: 000a.f467.9e8cWhat will it do?A.It will discard the frame.B.It will forward the frame out port Fa0/3 only.C.It will forward it out Fa0/1 only.D.It will send it out all ports except Fa0/1.19.You need to allow one host to be permitted to attach dynamically to each switch interface. Which two commands must you configure on your catalyst switch to meet this policy? (Choose two.)A.Switch(config-if)#ip access-group 10B.Switch(config-if)#switchport port-security maximum 1C.Switch(config)#access-list 10 permit ip host 1D.Switch(config-if)#switchport port-security violation shutdownE.Switch(config)#mac-address-table secure20.You have two switches connected together with two crossover cables for redundancy, and STP is disabled. Which of the following will happen between the switches?A.The routing tables on the switches will not update.B.The MAC forward/filter table will not update on the switch.C.Broadcast storms will occur on the switched network.D.The switches will automatically load-balance between the two links.

10089.book  Page 547  Monday, July 23, 2007  3:17 PM




548Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)Answers to Review Questions1.B. The Spanning Tree Protocol is used to stop switching loops in a switched network with redundant paths.2.C. The command show mac address-table displays the forward/filter table on the switch.3.A, E. Bridges break up collision domains, which would increase the number of collision domains in a network and also make smaller collision domains.4.C. Convergence occurs when all ports on bridges and switches have transitioned to either the forwarding or blocking states. No data is forwarded until convergence is complete. Before data can be forwarded again, all devices must be updated.5.C. The Spanning Tree Protocol (STP) was designed to stop layer 2 loops. All Cisco switches have the STP on by default.6.A, C, E. Layer 2 features include address learning, forwarding and filtering of the network, and loop avoidance.7.A. When you connect to a switch port, at first the link lights are orange/amber, and then they turn green, indicating normal operation. If the link light is blinking, you have a problem.8.B. Switches break up collision domains, and routers break up broadcast domains.9.C. To manage a switch remotely, you must set an IP address under the management VLAN, which is, by default, interface vlan 1. Then, from global configuration mode, you set the default gateway with the ip default-gateway command.10.C. Switches flood all frames that have an unknown destination address. If a device answers the frame, the switch will update the MAC address table to reflect the location of the device.11.C. Since the source MAC address is not in the MAC address table, the switch will add the source address and the port it is connected to into the MAC address table and then forward the frame to the outgoing port.12.A. 802.1w is the also called Rapid Spanning Tree Protocol. It is not enabled by default on Cisco switches, but it is a better STP to run since it has all the fixes that the Cisco extensions provide with 802.1d.13.D. If the Spanning Tree Protocol is not running on your switches and you connect them together with redundant links, you will have broadcast storms and multiple frame copies.14.C. The command show mac address-table will display the forward/filter table, also called a CAM table on a switch.15.D. If you have a server or other devices connected into your switch that you’re totally sure won’t create a switching loop if STP is disabled, you can use something called portfast on these ports. Using it means the port won’t spend the usual 50 seconds to come up while STP is converging.16.A. A switch can have multiple MAC addresses associated with a port. In the Graphic, a hub is connected to port Fa0/1, which has two hosts connected.

10089c08.fm  Page 548  Wednesday, October 31, 2007  8:04 AM




Answers to Review Questions54917.A, B, C, D. Switches, unlike bridges, are hardware based. Cisco says its switches are wire speed and provide low latency, and I guess they are low cost compared to their prices in the 1990s.18.B. Since the destination MAC address is in the MAC address table (forward/filter table), it will send it out port Fa0/3 only.19.B, D. switchport port-security is an important command, and it’s super easy with the CNA; however, from the CLI, you can set the maximum number of MAC addresses allowed into the port, and then set the penalty if this maximum has been passed.20.C. If spanning tree is disabled on a switch and you have redundant links to another switch, broadcast storms will occur, among other possible problems.

10089.book  Page 549  Monday, July 23, 2007  3:17 PM




550Chapter8 Layer 2 Switching and Spanning Tree Protocol (STP)Answers to Written Lab 8.11.show mac address-table2.Flood the frame out all ports except the port it was received on.3.Address learning, forward/filter decisions, and loop avoidance4.It will add the source MAC address in the forward/filter table and associate it with the port the frame was received on.5.Spanning Tree Protocol (STP)6.Rapid Spanning Tree Protocol (RSTP)7.When all ports are in either the blocking or forwarding mode8.Collision9.Spanning Tree Protocol (STP)10.PortFast

10089.book  Page 550  Monday, July 23, 2007  3:17 PM




 

Chapter 9 Virtual LANs (VLANs)

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Describe how a network works   Describe the impact of applications (Voice Over IP and Video Over IP) on a network   Configure, verify and troubleshoot a switch with VLANs and interswitch communications  Verify network status and switch operation using basic utilities (including: ping, traceroute, telnet, SSH, arp, ipconfig), SHOW & DEBUG commands   Identify, prescribe, and resolve common switched network media issues, configuration issues, auto negotiation, and switch hardware failures   Describe enhanced switching technologies (including: VTP, RSTP, VLAN, PVSTP, 802.1q)   Describe how VLANs create logically separate networks and the need for routing between them   Configure, verify, and troubleshoot VLANs   Configure, verify, and troubleshoot trunking on Cisco switches   Configure, verify, and troubleshoot interVLAN routing   Configure, verify, and troubleshoot VTP   Configure, verify, and troubleshoot RSTP operation   Interpret the output of various show and debug commands to verify the operational status of a Cisco switched network   Implement basic switch security (including: port security, trunk access, management vlan other than vlan1, etc.) 

 

10089.book  Page 551  Monday, July 23, 2007  3:17 PM




 I know I keep telling you this, but I’ve got to be sure you never forget it, so here I go, one last time: By default, switches break up collision domains and routers break up broadcast domains. Okay, I feel better! Now we can move on.In contrast to the networks of yesterday that were based on collapsed backbones, today’s network design is characterized by a flatter architecture—thanks to switches. So now what? How do we break up broadcast domains in a pure switched internetwork? By creating a virtual local area network (VLAN). A VLAN is a logical grouping of network users and resources connected to administratively defined ports on a switch. When you create VLANs, you’re given the ability to create smaller broadcast domains within a layer 2 switched inter-network by assigning different ports on the switch to different subnetworks. A VLAN is treated like its own subnet or broadcast domain, meaning that frames broadcast onto the network are only switched between the ports logically grouped within the same VLAN.So, does this mean we no longer need routers? Maybe yes; maybe no. It really depends on what you want or what your needs are. By default, hosts in a specific VLAN cannot commu-nicate with hosts that are members of another VLAN, so if you want inter-VLAN communi-cation, the answer is that you still need a router.In this chapter, you’re going to learn, in detail, exactly what a VLAN is and how VLAN mem-berships are used in a switched network. Also, I’m going to tell you all about how VLAN Trunk Protocol (VTP) is used to update switch databases with VLAN information and how trunking is used to send information from all VLANs across a single link. I’ll wrap things up by demon-strating how you can make inter-VLAN communication happen by introducing a router into a switched network.Of course, we’ll configure our switched network with VLANs and inter-VLAN routing, and we’ll finish the chapter by using the Cisco Network Assistant (CNA) to configure VLANs on our switches.

 For up-to-the-minute updates to this chapter, please see  www.lammle.com  and/or  www.sybex.com . VLAN Basics Figure 9.1 shows how layer 2 switched networks are typically designed—as flat networks. With this configuration, every broadcast packet transmitted is seen by every device on the net-work regardless of whether the device needs to receive that data or not.

 

10089.book  Page 552  Monday, July 23, 2007  3:17 PM




 VLAN Basics 553 By default, routers allow broadcasts to occur only within the originating network, while switches forward broadcasts to all segments. Oh, and by the way, the reason it’s called a  flat network  is because it’s one  broadcast domain , not because the actual design is physically flat. In Figure 9.1 we see Host A sending out a broadcast and all ports on all switches forwarding it—all except the port that originally received it. FIGURE9.1 Flat network structure Now check out Figure 9.2. It pictures a switched network and shows Host A sending a frame with Host D as its destination. What’s important is that, as you can see, that frame is only for-warded out the port where Host D is located. This is a huge improvement over the old hub net-works, unless having one  collision domain  by default is what you really want. (Probably not!)Now you already know that the largest benefit you gain by having a layer 2 switched network is that it creates individual collision domain segments for each device plugged into each port on the switch. This scenario frees us from the Ethernet distance constraints, so now larger networks can be built. But often, each new advance comes with new issues. For instance, the larger the number of users and devices, the more broadcasts and packets each switch must handle. FIGURE9.2 The benefit of a switched network

Host A

Host AHost D

 

10089.book  Page 553  Monday, July 23, 2007  3:17 PM




 554 Chapter9  Virtual LANs (VLANs) And here’s another issue: security! This one’s real trouble because within the typical layer 2 switched internetwork, all users can see all devices by default. And you can’t stop devices from broadcasting, plus you can’t stop users from trying to respond to broadcasts. This means your security options are dismally limited to placing passwords on your servers and other devices.But wait—there’s hope! That is, if you create a  virtual LAN (VLAN) . You can solve many of the problems associated with layer 2 switching with VLANs, as you’ll soon see.Here’s a short list of ways VLANs simplify network management:  Network adds, moves, and changes are achieved with ease by just configuring a port into the appropriate VLAN.  A group of users that need an unusually high level of security can be put into its own VLAN so that users outside of the VLAN can’t communicate with them.  As a logical grouping of users by function, VLANs can be considered independent from their physical or geographic locations.  VLANs greatly enhance network security.  VLANs increase the number of broadcast domains while decreasing their size.Coming up, I’m going to tell you all about switching characteristics and thoroughly describe how switches provide us with better network services than hubs can in our networks today. Broadcast Control Broadcasts occur in every protocol, but how often they occur depends upon three things:  The type of protocol  The application(s) running on the internetwork  How these services are usedSome older applications have been rewritten to reduce their bandwidth appetites, but there’s a new generation of applications that are incredibly bandwidth greedy that will con-sume any and all they can find. These bandwidth gluttons are multimedia applications that use both broadcasts and multicasts extensively. And faulty equipment, inadequate segmentation, and poorly designed firewalls seriously compound the problems that these broadcast-intensive applications create. All of this has added a major new dimension to network design and pre-sents a bunch of new challenges for an administrator. Positively making sure your network is properly segmented so you can quickly isolate a single segment’s problems to prevent them from propagating throughout your entire internetwork is imperative! And the most effective way to do that is through strategic switching and routing.Since switches have become more affordable lately, a lot of companies are replacing their flat hub networks with pure switched network and VLAN environments. All devices within a VLAN are members of the same broadcast domain and receive all broadcasts. By default, these broadcasts are filtered from all ports on a switch that aren’t members of the same VLAN. This is great because you get all the benefits you would with a switched design without getting hit with all the problems you’d have if all your users were in the same broadcast domain—sweet!

 

10089.book  Page 554  Monday, July 23, 2007  3:17 PM




 VLAN Basics 555 Security Okay, I know. There’s always a catch, though, right? Time to get back to those security issues. A flat internetwork’s security used to be tackled by connecting hubs and switches together with routers. So it was basically the router’s job to maintain security. This arrangement was pretty ineffective for several reasons. First, anyone connecting to the physical network could access the network resources located on that particular physical LAN. Second, all anyone had to do to observe any and all traffic happening in that network was to simply plug a network analyzer into the hub. And similar to that last ugly fact, users could join a workgroup by just plugging their workstations into the existing hub. That’s about as secure as an open barrel of honey in a bear enclosure!But that’s exactly what makes VLANs so cool. If you build them and create multiple broad-cast groups, you have total control over each port and user! So the days when anyone could just plug their workstations into any switch port and gain access to network resources are his-tory because now you get to control each port, plus whatever resources that port can access. What’s more, with the new 2960/3560 switches, this actually happens automatically!And it doesn’t end there my friends, because VLANs can be created in accordance with the network resources a given user requires, plus switches can be configured to inform a network management station of any unauthorized access to network resources. And if you need inter-VLAN communication, you can implement restrictions on a router to make that happen. You can also place restrictions on hardware addresses, protocols, and applications.  Now  we’re talk-ing security—the honey barrel is now sealed, shrouded in razor wire, and made of solid titanium! Flexibility and Scalability If you were paying attention to what you’ve read so far, you know that layer 2 switches only read frames for filtering—they don’t look at the Network layer protocol. And by default, switches forward all broadcasts. But if you create and implement VLANs, you’re essentially creating smaller broadcast domains at layer 2.What this means is that broadcasts sent out from a node in one VLAN won’t be forwarded to ports configured to belong to a different VLAN. So by assigning switch ports or users to VLAN groups on a switch or group of connected switches, you gain the flexibility to add only the users you want into that broadcast domain regardless of their physical location. This setup can also work to block broadcast storms caused by a faulty network interface card (NIC) as well as prevent an intermediate device from propagating broadcast storms throughout the entire internetwork. Those evils can still happen on the VLAN where the problem originated, but the disease will instead be quarantined to that one ailing VLAN.Another advantage is that when a VLAN gets too big, you can create more VLANs to keep the broadcasts from consuming too much bandwidth—the fewer users in a VLAN, the fewer users affected by broadcasts. This is all well and good, but you seriously need to keep network services in mind and understand how the users connect to these services when you create your VLAN. It’s a good move to try to keep all services, except for the email and Internet access that everyone needs, local to all users whenever possible.

 

10089.book  Page 555  Monday, July 23, 2007  3:17 PM




 556 Chapter9  Virtual LANs (VLANs) To understand how a VLAN looks to a switch, it’s helpful to begin by first looking at a traditional network. Figure 9.3 shows how a network was created by using hubs to connect physical LANs to a router. FIGURE9.3 Physical LANs connected to a router Here you can see that each network is attached with a hub port to the router (each segment also has its own logical network number even though this isn’t obvious looking at the figure). Each node attached to a particular physical network has to match that network’s number in order to be able to communicate on the internetwork. Notice that each department has its own LAN, so if you needed to add new users to, let’s say, Sales, you would just plug them into the Sales LAN and they would automatically be part of the Sales collision and broadcast domain. This design really did work well for many years.But there was one major flaw: What happens if the hub for Sales is full and we need to add another user to the Sales LAN? Or, what do we do if there’s no more physical space where the Sales team is located for this new employee? Well, let’s say there just happens to be plenty of room in the Finance section of the building. That new Sales team member will just have to sit on the same side of the building as the Finance people, and we’ll just plug the poor soul into the hub for Finance.Doing this obviously makes the new user part of the Finance LAN, which is very bad for many reasons. First and foremost, we now have a major security issue. Because the new Sales employee is a member of the Finance broadcast domain, the newbie can see all the same servers and access all network services that the Finance folks can. Second, for this user to access the Sales network services they need to get their job done, they would have to go through the router to log in to the Sales server—not exactly efficient!Now let’s look at what a switch accomplishes for us. Figure 9.4 demonstrates how switches come to the rescue by removing the physical boundary to solve our problem. It also shows how six VLANs (numbered 2 through 7) are used to create a broadcast domain for each depart-ment. Each switch port is then administratively assigned a VLAN membership, depending on the host and which broadcast domain it’s placed in.

Finance

Management

Engineering

Sales

Marketing

Shipping

Hubs

 

10089.book  Page 556  Monday, July 23, 2007  3:17 PM




 VLAN Basics 557 FIGURE9.4 Switches removing the physical boundary So now, if we needed to add another user to the Sales VLAN (VLAN 7), we could just assign the port to VLAN 7 regardless of where the new Sales team member is physically located—nice! This illustrates one of the sweetest advantages to designing your network with VLANs over the old collapsed backbone design. Now, cleanly and simply, each host that needs to be in the Sales VLAN is merely assigned to VLAN 7. And by using the new switches with the pre-defined macros, we can just use CNA and Smartports to configure the port to be a Desktop connection and voilà! The port configuration is simply completed for us.Notice that I started assigning VLANs with VLAN number 2. The number is irrelevant, but you might be wondering what happened to VLAN 1? Well that VLAN is an administrative VLAN, and even though it can be used for a workgroup, Cisco recommends that you use it for administrative purposes only. You can’t delete or change the name of VLAN 1, and by default, all ports on a switch are members of VLAN 1 until you change them.Since each VLAN is considered a broadcast domain, it’s got to also have its own subnet number (refer again to Figure 9.4). And if you’re also using IPv6, then each VLAN must also be assigned its own IPv6 network number. So you don’t get confused, just keep thinking of VLANs as separate subnets or networks.Now let’s get back to that “because of switches, we don’t need routers anymore” misconception. Looking at Figure 9.4, notice that there are seven VLANs, or broadcast domains, counting VLAN 1. The nodes within each VLAN can communicate with each other but not with anything in a different VLAN because the nodes in any given VLAN “think” that they’re actually in a collapsed backbone, as illustrated in Figure 9.3.

VLAN2

VLAN3

VLAN4

VLAN2

VLAN7

VLAN3

VLAN3

VLAN6

VLAN5

VLAN5

VLAN6

VLAN4

Provides inter-VLANcommunication andWAN services

Marketing VLAN2 172.16.20.0/24Shipping VLAN3 172.16.30.0/24Engineering VLAN4 172.16.40.0/24Finance VLAN5 172.16.50.0/24Management VLAN6 172.16.60.0/24Sales VLAN7 172.16.70.0/24

 

10089.book  Page 557  Monday, July 23, 2007  3:17 PM




 558 Chapter9  Virtual LANs (VLANs) So what handy little tool do we need to enable the hosts in Figure 9.4 to communicate to a node or host on a different VLAN? You guessed it—a router! Those nodes positively need to go through a router, or some other layer 3 device, just as when they’re configured for inter-network communication (as shown in Figure 9.3). It works the same way it would if we were trying to connect different physical networks. Communication between VLANs must go through a layer 3 device. So don’t expect mass router extinction any time soon!

 We’ll use both a router and the 3560 switch to provide inter-VLAN routing on our switched network toward the end of this chapter. We can actually employ  the 3560 to be a layer 3 switch, just like a router. VLAN Memberships Most of the time, VLANs are created by a sys admin who proceeds to assign switch ports to each VLAN. VLANs of this type are known as  static VLANs . If you don’t mind doing a little more work when you begin this process, assign all the host devices’ hardware addresses into a database so your switches can be configured to assign VLANs dynamically any time you plug a host into a switch. I hate saying things like “obviously,” but obviously, this type of VLAN is known as a  dynamic VLAN . I’ll be covering both static and dynamic VLANs in the next couple of sections. Static VLANs Creating static VLANs is the most common way to create a VLAN, and one of the reasons for that is because static VLANs are the most secure. This security stems from the fact that any switch port you’ve assigned a VLAN association to will always maintain it unless you change the port assignment manually.Static VLAN configuration is pretty easy to set up and supervise, and it works really well in a networking environment where any user movement within the network needs to be con-trolled. It can be helpful to use network management software to configure the ports, but you don’t have to use it if you don’t want to.In Figure 9.4, each switch port was configured manually with a VLAN membership based upon which VLAN the host needed to be a member of—remember, the device’s actual physical location doesn’t matter a bit. Which broadcast domain your hosts become members of is purely up to you. And again, remember that each host also has to have the correct IP address information. For instance, you must configure each host in VLAN 2 into the 172.16.20.0/24 network for them to become members of that VLAN. It’s also a good idea to keep in mind that if you plug a host into a switch, you have to verify the VLAN membership of that port. If the membership is different than what’s needed for that host, the host   won’t be able to gain access to the network services that it needs, such as a workgroup server.

 

10089.book  Page 558  Monday, July 23, 2007  3:17 PM




 Identifying VLANs 559

 Static access ports are either manually assigned to a VLAN or are assigned  through a RADIUS server for use with IEEE 802.1x. Dynamic VLANs On the other hand, a dynamic VLAN determines a node’s VLAN assignment automatically. Using intelligent management software, you can base VLAN assignments on hardware (MAC) addresses, protocols, or even applications that create dynamic VLANs.For example, let’s say MAC addresses have been entered into a centralized VLAN manage-ment application and you hook up a new node. If you attached it to an unassigned switch port, the VLAN management database can look up the hardware address and both assign and con-figure the switch port into the correct VLAN. Needless to say, this makes management and configuration much easier because if a user moves, the switch will simply assign them to the correct VLAN automatically. But here again, there’s a catch: You’ve got to do a lot more work initially setting up the database. It can be very worthwhile though!And here’s some good news: You can use the VLAN Management Policy Server (VMPS) service to set up a database of MAC addresses to be used for the dynamic addressing of your VLANs. The VMPS database automatically maps MAC addresses to VLANs.A dynamic-access port can belong to one VLAN (VLAN ID 1 all the way up to 4094) and, as I said, is dynamically assigned by the VMPS. The Catalyst 2960 switch can be a VMPS client only. You can have dynamic-access ports and trunk ports on the same switch, but you have to connect the dynamic-access port to an end station or hub— not  to another switch! Identifying VLANs Know that switch ports are layer 2–only interfaces that are associated with a physical port. A switch port can belong to only one VLAN if it is an access port or all VLANs if it is a trunk port. You can manually configure a port as an access or trunk port, or you can let the Dynamic Trunking Protocol (DTP) operate on a per-port basis to set the switchport mode. DTP does this by negotiating with the port on the other end of the link.Switches are definitely pretty busy devices. As frames are switched throughout the network, they’ve got to be able to keep track of all the different types plus understand what to do with them depending on the hardware address. And remember—frames are handled differently according to the type of link they’re traversing.There are two different types of links in a switched environment: Access ports An access port belongs to and carries the traffic of only one VLAN. Traffic is both received and sent in native formats with no VLAN tagging whatsoever. Anything arriving on an access port is simply assumed to belong to the VLAN assigned to the port. So, what do you think will happen if an access port receives a tagged packet, like IEEE 802.1Q tagged? Right—

 

10089.book  Page 559  Monday, July 23, 2007  3:17 PM




 560 Chapter9  Virtual LANs (VLANs) that packet would simply be dropped. But why? Well, because an access port doesn’t look at the source address, so tagged traffic can be forwarded and received only on trunk ports.With an access link, this can be referred to as the  configured VLAN  of the port. Any device attached to an  access link  is unaware of a VLAN membership—the device just assumes it’s part of the same broadcast domain, but it doesn’t have the big picture, so it doesn’t understand the physical network topology at all.Another good bit of information to know is that switches remove any VLAN information from the frame before it’s forwarded out to an access-link device. Remember that access-link devices can’t communicate with devices outside their VLAN unless the packet is routed. And you can only create a switch port to be either an access port or a trunk port—not both. So you’ve got to choose one or the other and know that if you make it an access port, that port can be assigned to one VLAN only. Voice access ports Not to confuse you, but all that I just said about the fact that an access port can be assigned to only one VLAN is really only sort of true. Nowadays, most switches will allow you to add a second VLAN to an access port on a switch port for your voice traf-fic; it’s called the voice VLAN. The voice VLAN used to be called the auxiliary VLAN, which allowed it to be overlaid on top of the data VLAN, enabling both types of traffic through the same port. Even though this is technically considered to be a different type of link, it’s still just an access port that can be configured for both data and voice VLANs. This allows you to connect both a phone and a PC device to one switch port but still have each device in a separate VLAN. I’ll go into voice VLANs in detail and clear all this up for you in the section “Telephony: Configuring Voice VLANs” later in this chapter. Trunk Ports Believe it or not, the term  trunk port  was inspired by the telephone system trunks that carry multiple telephone conversations at a time. So it follows that trunk ports can similarly carry multiple VLANs at a time.A  trunk link  is a 100- or 1000Mbps point-to-point link between two switches, between a switch and router, or even between a switch and server, and it carries the traffic of multiple VLANs—from 1 to 4,094 at a time (though it’s really only up to 1,005 unless you’re going with extended VLANs).Trunking can be a real advantage because with it, you get to make a single port part of a whole bunch of different VLANs at the same time. This is a great feature because you can actually set ports up to have a server in two separate broadcast domains simultaneously so your users won’t have to cross a layer 3 device (router) to log in and access it. Another benefit to trunking comes into play when you’re connecting switches. Trunk links can carry various amounts of VLAN information across the link, but by default, if the links between your switches aren’t trunked, only information from the configured VLAN will be switched across that link.It’s good to know that all VLANs send information on a trunked link unless you clear each VLAN by hand, and no worries, I’ll show you how to clear individual VLANs from a trunk in a bit.Check out Figure 9.5. It shows how the different links are used in a switched network. All hosts connected to the switches can communicate to all ports in their VLAN because of the trunk link between them. Remember, if we used an access link between the switches, this 

 

10089.book  Page 560  Monday, July 23, 2007  3:17 PM




 Identifying VLANs 561 would allow only one VLAN to communicate between switches. As you can see, these hosts are using access links to connect to the switch, so they’re communicating in one VLAN only. That means that without a router, no host can communicate outside its own VLAN, but they can send data over trunked links to hosts on another switch configured in their same VLAN. FIGURE9.5 Access and trunk links in a switched network Okay—it’s finally time to tell you about frame tagging and the VLAN identification methods used in it. Frame Tagging As you now know, you can set up your VLANs to span more than one connected switch. You can see that going on in Figure 9.4, which depicts hosts from various VLANs spread across a bunch of switches. This flexible, power-packed capability is probably the main advantage to implementing VLANs.But it can get kind of complicated—even for a switch—so there needs to be a way for each one to keep track of all the users and frames as they travel the switch fabric and VLANs. When I say “switch fabric,” I’m just referring to a group of switches that share the same VLAN infor-mation. And this just happens to be where  frame tagging  enters the scene. This frame identi-fication method uniquely assigns a user-defined ID to each frame. Sometimes people refer to it as a “VLAN ID” or even “color.”Here’s how it works: Each switch that the frame reaches must first identify the VLAN ID from the frame tag. It then finds out what to do with the frame by looking at the information in what’s known as the filter table. If the frame reaches a switch that has another trunked link, the frame will be forwarded out the trunk-link port.

SYSTEM

RPS

1x2x3x4x5x6x7x8x

9x10x11x12x13x14x15x16x

17x18x19x20x21x22x23x24x

10BaseT

MODECISCO  YSTEMSS

Æ

UTLFDUPSTATCatalyst 1900

AxBx

100BaseTX

SYSTEM

RPS

1x2x3x4x5x6x7x8x

9x10x11x12x13x14x15x16x

17x18x19x20x21x22x23x24x

10BaseT

MODECISCO  YSTEMSS

Æ

UTLFDUPSTATCatalyst 1900

AxBx

100BaseTX

Trunk LinkVLANs can span across multiple switchesby using trunk links, which carry trafficfor multiple VLANs

Red VLAN

Blue VLAN

Green VLAN

Red VLAN

Blue VLAN

Green VLAN

 

10089.book  Page 561  Monday, July 23, 2007  3:17 PM




 562 Chapter9  Virtual LANs (VLANs) Once the frame reaches an exit that’s determined by the forward/filter table to be an access link matching the frame’s VLAN ID, the switch will remove the VLAN identifier. This is so the destination device can receive the frames without being required to understand their VLAN identification.Another thing about trunk ports is that they will support tagged and untagged traffic simul-taneously (if you are using 802.1Q trunking, which we will talk about in the next section). The trunk port is assigned a default port VLAN ID (PVID) for a VLAN that all untagged traffic will travel on. This VLAN is also called the native VLAN and is always VLAN 1 by default (but can be changed to any VLAN number).Similarly, any untagged or tagged traffic with a NULL (unassigned) VLAN ID is assumed to belong to the VLAN with the port default PVID (again, VLAN 1 by default). A packet with a VLAN ID equal to the outgoing port default PVID is sent untagged and can only commu-nicate to hosts or devices in VLAN 1. All other VLAN traffic has to be sent with a VLAN tag to communicate in a particular VLAN that corresponds with that tag. VLAN Identification Methods VLAN identification is what switches use to keep track of all those frames as they’re traversing a switch fabric. It’s how switches identify which frames belong to which VLANs, and there’s more than one trunking method. Inter-Switch Link (ISL) Inter-Switch Link (ISL)  is a way of explicitly tagging VLAN information onto an Ethernet frame. This tagging information allows VLANs to be multiplexed over a trunk link through an external encapsulation method (ISL), which allows the switch to identify the VLAN mem-bership of a frame over the trunked link.By running ISL, you can interconnect multiple switches and still maintain VLAN informa-tion as traffic travels between switches on trunk links. ISL functions at layer 2 by encapsulat-ing a data frame with a new header and cyclic redundancy check (CRC).Of note is that this is proprietary to Cisco switches, and it’s used for Fast Ethernet and Gigabit Ethernet links only.  ISL routing  is pretty versatile and can be used on a switch port, router interfaces, and server interface cards to trunk a server. IEEE 802.1Q Created by the IEEE as a standard method of frame tagging, IEEE 802.1Q actually inserts a field into the frame to identify the VLAN. If you’re trunking between a Cisco switched link and a different brand of switch, you’ve got to use 802.1Q for the trunk to work.It works like this: You first designate each port that is going to be a trunk with 802.1Q encapsulation. The ports must be assigned a specific VLAN ID, which makes them the native VLAN, in order for them to communicate. The ports that populate the same trunk create a group with this native VLAN, and each port gets tagged with an identification number reflect-ing that, again the default being VLAN 1. The native VLAN allows the trunks to carry infor-mation that was received without any VLAN identification or frame tag.

 

10089.book  Page 562  Monday, July 23, 2007  3:17 PM




 VLAN Trunking Protocol (VTP) 563 The 2960s support only the IEEE 802.1Q trunking protocol, but the 3560s will support both the ISL and IEEE methods.

 The basic purpose of ISL and 802.1Q frame-tagging methods is to provide inter-switch VLAN communication. Also, remember that any ISL or 802.1Q frame tagging is removed if a frame is forwarded out an access link—tagging  is used across trunk links only! VLAN Trunking Protocol (VTP) Cisco created this one too. The basic goals of  VLAN Trunking Protocol (VTP)  are to manage all configured VLANs across a switched internetwork and to maintain consistency throughout that network VTP allows you to add, delete, and rename VLANs—information that is then propagated to all other switches in the VTP domain.Here’s a list of some of the cool features VTP has to offer:  Consistent VLAN configuration across all switches in the network VLAN trunking over mixed networks, such as Ethernet to ATM LANE or even FDDI Accurate tracking and monitoring of VLANs Dynamic reporting of added VLANs to all switches in the VTP domain Plug and Play VLAN addingVery nice, but before you can get VTP to manage your VLANs across the network, you have to create a VTP server. All servers that need to share VLAN information must use the same domain name, and a switch can be in only one domain at a time. So basically, this means that a switch can only share VTP domain information with other switches if they’re configured into the same VTP domain. You can use a VTP domain if you have more than one switch connected in a network, but if you’ve got all your switches in only one VLAN, you just don’t need to use VTP. Do keep in mind that VTP information is sent between switches only via a trunk port.Switches advertise VTP management domain information as well as a configuration revi-sion number and all known VLANs with any specific parameters. But there’s also something called VTP transparent mode. In it, you can configure switches to forward VTP information through trunk ports but not to accept information updates or update their VTP databases.If you’ve got sneaky users adding switches to your VTP domain behind your back, you can include passwords, but don’t forget—every switch must be set up with the same password. And as you can imagine, this little snag can be a real hassle administratively!Switches detect any added VLANs within a VTP advertisement, then prepare to send infor-mation on their trunk ports with the newly defined VLAN in tow. Updates are sent out as revi-sion numbers that consist of the notification plus 1. Any time a switch sees a higher revision number, it knows the information it’s getting is more current, so it will overwrite the existing database with the latest information.

10089.book  Page 563  Monday, July 23, 2007  3:17 PM




564Chapter9 Virtual LANs (VLANs)You should know these three requirements for VTP to communicate VLAN information between switches: The VTP management domain name of both switches must be set the same. One of the switches has to be configured as a VTP server. No router is necessary.Now that you’ve got that down, we’re going to delve deeper in the world of VTP with VTP modes and VTP pruning.VTP Modes of OperationFigure 9.6 shows you all three different modes of operation within a VTP domain:ServerThis is the default mode for all Catalyst switches. You need at least one server in your VTP domain to propagate VLAN information throughout that domain. Also important: The switch must be in server mode to be able to create, add, and delete VLANs in a VTP domain. VTP information has to be changed in server mode, and any change made to a switch in server mode will be advertised to the entire VTP domain. In VTP server mode, VLAN configurations are saved in NVRAM.ClientIn client mode, switches receive information from VTP servers, but they also send and receive updates, so in this way, they behave like VTP servers. The difference is that they can’t create, change, or delete VLANs. Plus, none of the ports on a client switch can be added to a new VLAN before the VTP server notifies the client switch of the new VLAN. Also good to know is that VLAN information sent from a VTP server isn’t stored in NVRAM, which is important because it means that if the switch is reset or reloaded, the VLAN information will be deleted. Here’s a hint: If you want a switch to become a server, first make it a client so it receives all the correct VLAN information, then change it to a server—so much easier!So basically, a switch in VTP client mode will forward VTP summary advertisements and pro-cess them. This switch will learn about but won’t save the VTP configuration in the running configuration, and it won’t save it in NVRAM. Switches that are in VTP client mode will only learn about and pass along VTP information—that’s it!FIGURE9.6VTP modes

Client

Transparent

ServerServer configuration: Saved in NVRAMTransparent configuration: Saved in NVRAMClient configuration: Not saved in NVRAM

10089.book  Page 564  Monday, July 23, 2007  3:17 PM




VLAN Trunking Protocol (VTP)565TransparentSwitches in transparent mode don’t participate in the VTP domain or share its VLAN database, but they’ll still forward VTP advertisements through any configured trunk links. They can create, modify, and delete VLANs because they keep their own database—one they keep secret from the other switches. Despite being kept in NVRAM, the VLAN database in transparent mode is actually only locally significant. The whole purpose of transparent mode is to allow remote switches to receive the VLAN database from a VTP server-configured switch through a switch that is not participating in the same VLAN assignments.VTP only learns about normal-range VLANs, with VLAN IDs 1 to 1005; VLANs with IDs greater than 1005 are called extended-range VLANs and they’re not stored in the VLAN data-base. The switch must be in VTP transparent mode when you create VLAN IDs from 1006 to 4094, so it would be pretty rare that you’d ever use these VLANs. One other thing: VLAN IDs 1 and 1002 to 1005 are automatically created on all switches and can’t be removed.VTP PruningVTP gives you a way to preserve bandwidth by configuring it to reduce the amount of broad-casts, multicasts, and unicast packets. This is called pruning. VTP pruning enabled switches sends broadcasts only to trunk links that actually must have the information.Here’s what this means: If Switch A doesn’t have any ports configured for VLAN 5 and a broadcast is sent throughout VLAN 5, that broadcast wouldn’t traverse the trunk link to Switch A. By default, VTP pruning is disabled on all switches. Seems to me this would be a good default parameter.

So, When Do I Need to Consider Using VTP?Here’s a scenario for you. Bob, a senior network administrator at Acme Corporation in San Francisco, has about 25 switches all connected together, and he wants to configure VLANs to break up broadcast domains. When do you think he should start to consider using VTP?If you answered that he should have used VTP the moment he had more than one switch and multiple VLANs, you’re right. If you have only one switch, then VTP is irrelevant. It also isn’t a player if you’re not configuring VLANs in your network. But if you do have multiple switches that use multiple VLANs, you’d better configure your VTP server and clients, and you better do it right!When you first bring up your switched network, verify that your main switch is a VTP server and that all the other ones are VTP clients. When you create VLANs on the main VTP server, all switches will receive the VLAN database.If you have an existing switched network and you want to add a new switch, make sure to con-figure it as a VTP client before you install it. If you don’t, it’s possible—okay, highly probable—that your new little beauty will send out a new VTP database to all your other switches, effec-tively wiping out all your existing VLANs like a nuclear blast. No one needs that!

10089.book  Page 565  Monday, July 23, 2007  3:17 PM




566Chapter9 Virtual LANs (VLANs)When you enable pruning on a VTP server, you enable it for the entire domain. By default, VLANs 2 through 1001 are pruning eligible, but VLAN 1 can never prune because it’s an administrative VLAN. VTP pruning is supported with both VTP version 1 and version 2.By using the show interface trunk command, we can see that all VLANs are allowed across a trunked link by default:

S1#sh int trunkPort        Mode         Encapsulation  Status        Native vlanFa0/1       auto         802.1q         trunking      1Fa0/2       auto         802.1q         trunking      1Port        Vlans allowed on trunkFa0/1       1-4094Fa0/2       1-4094Port        Vlans allowed and active in management domainFa0/1       1Fa0/2       1Port        Vlans in spanning tree forwarding state and not prunedFa0/1       1Fa0/2       none

S1#Looking at the preceding output, you can see that VTP pruning is disabled by default. I’m going to go ahead and enable pruning. It only takes one command and it is enabled on your entire switched network for the listed VLANs. Let’s see what happens:

S1#config tS1(config)#int f0/1S1(config-if)#switchport trunk ?  allowed  Set allowed VLAN characteristics when interface is  in trunking mode  native   Set trunking native characteristics when interface  is in trunking mode  pruning  Set pruning VLAN characteristics when interface is  in trunking modeS1(config-if)#switchport trunk pruning ?  vlan  Set VLANs enabled for pruning when interface is in  trunking mode

S1(config-if)#switchport trunk pruning vlan 3-4The valid VLANs that can be pruned are 2 to 1001. Extended-range VLANs (VLAN IDs 1006 to 4094) can’t be pruned, and these pruning-ineligible VLANs can receive a flood of traffic.

10089.book  Page 566  Monday, July 23, 2007  3:17 PM




Routing between VLANs567Routing between VLANsHosts in a VLAN live in their own broadcast domain and can communicate freely. VLANs create network partitioning and traffic separation at layer 2 of the OSI, and as I said when I told you why we still need routers, if you want hosts or any other IP-addressable device to communicate between VLANs, you just have to have a layer 3 device—period.For this, you can use a router that has an interface for each VLAN or a router that supports ISL or 802.1Q routing. The least expensive router that supports ISL or 802.1Q routing is the 2600 series router. (You’d have to buy that from a used-equipment reseller, because they are end of life, or EOL.) The 1600, 1700, and 2500 series don’t support ISL or 802.1Q routing. I’d recommend at least a 2800 as a bare minimum, and that only supports 802.1Q—Cisco is really moving away from ISL, so you probably should only be using 802.1Q anyway. (Some IOSs on the 2800 may support both ISL and 802.1Q—I just have never seen it supported.)As shown in Figure 9.7, if you had only a few VLANs (two or three), you could get by with a router equipped with two or three Fast Ethernet connections. And 10BaseT is okay for home use, and I mean only for home use, but for anything else I’d honestly recommend Fast Ethernet or Gigabit interfaces for something serious under the hood.FIGURE9.7Router with individual VLAN associationsWhat we see in Figure 9.7 is that each router interface is plugged into an access link. This means that each of the routers’ interface IP addresses would then become the default gateway address for each host in each VLAN.If you have more VLANs available than router interfaces, you can configure trunking on one Fast Ethernet interface or buy a layer 3 switch, like the Cisco 3560 or a higher-end switch like a 6500.Instead of using a router interface for each VLAN, you can use one Fast Ethernet interface and run ISL or 802.1Q trunking. Figure 9.8 shows how a Fast Ethernet interface on a router will look when configured with ISL or 802.1Q trunking. This allows all VLANs to commu-nicate through one interface. Cisco calls this a “router on a stick.”

Router connecting three VLANstogether for inter-VLAN communication,one interface for each VLAN.

10089.book  Page 567  Monday, July 23, 2007  3:17 PM




568Chapter9 Virtual LANs (VLANs)FIGURE9.8“Router on a stick”I need to point out that this creates a bottleneck, as well as a single point of failure, so your host/VLAN count is limited. How many? That depends on your traffic level. To really make things really right, you’d be better off using a higher-end switch and routing on the backplane, but if you just happen to have a router sitting around, configuring this method is free, right?Configuring VLANsIt may come as a surprise to you, but configuring VLANs is actually pretty easy. Figuring out which users you want in each VLAN is not; it’s extremely time consuming. But once you’ve decided on the number of VLANs you want to create and established which users you want to belong to each one, it’s time to bring your first VLAN into the world.To configure VLANs on a Cisco Catalyst switch, use the global config vlan command. In the following example, I’m going to demonstrate how to configure VLANs on the S1 switch by creating three VLANs for three different departments—again, remember that VLAN 1 is the native and administrative VLAN by default:

S1#config tS1(config)#vlan ?  WORD      ISL VLAN IDs 1-4094  internal  internal VLANS1(config)#vlan 2 S1(config-vlan)#name SalesS1(config-vlan)#vlan 3S1(config-vlan)#name Marketing

Router connecting all VLANs togetherallowing for inter-VLAN communication,using only one router interface(router on a stick).

10089.book  Page 568  Monday, July 23, 2007  3:17 PM




Configuring VLANs569S1(config-vlan)#vlan 4S1(config-vlan)#name AccountingS1(config-vlan)#^Z

S1#From the preceding above, you can see that you can create VLANs from 2 to 4094. This is only mostly true. As I said, VLANs can really only be created up to 1005, and you can’t use, change, rename, or delete VLANs 1 and 1002 through 1005 because they’re reserved. The VLAN numbers above that are called extended VLANs and won’t be saved in the database unless your switch is set to VTP transparent mode. You won’t see these VLAN numbers used too often in production. Here’s an example of setting my S1 switch to VLAN 4000 when my switch is set to VTP server mode (the default VTP mode):

S1#config tS1(config)#vlan 4000S1(config-vlan)#^Z% Failed to create VLANs 4000Extended VLAN(s) not allowed in current VTP mode.

%Failed to commit extended VLAN(s) changes.After you create the VLANs that you want, you can use the show vlan command to check them out. But notice that, by default, all ports on the switch are in VLAN 1. To change the VLAN associated with a port, you need to go to each interface and tell it which VLAN to be a part of.

Remember that a created VLAN is unused until it is assigned to a switch port or ports and that all ports are always assigned in VLAN 1 unless set otherwise.Once the VLANs are created, verify your configuration with the show vlan command (sh vlan for short):

S1#sh vlanVLAN Name                  Status    Ports---- -----------------------------------------------------------1    default                active    Fa0/3, Fa0/4, Fa0/5, Fa0/6                                      Fa0/7, Fa0/8, Gi0/12    Sales                  active  3    Marketing              active  4    Accounting             active 

 [output cut]

10089.book  Page 569  Monday, July 23, 2007  3:17 PM




570Chapter9 Virtual LANs (VLANs)This may seem repetitive, but it’s important, and I want you to remember it: You can’t change, delete, or rename VLAN 1 because it’s the default VLAN and you just can’t change that—period. It’s the native VLAN of all switches by default, and Cisco recommends that you use it as your administrative VLAN. Basically, any packets that aren’t specifically assigned to a different VLAN will be sent down to the native VLAN.In the preceding S1 output, you can see that ports Fa0/3 through Fa0/8 and the Gi0/1 uplink are all in VLAN 1, but where are ports 1 and 2? Remember that in the previous chapter I trunked and created an EtherChannel bundle. Any port that is a trunk port won’t show up in the VLAN database. You have to use the show interface trunk command to see your trunked ports.Now that we can see the VLANs created, we can assign switch ports to specific ones. Each port can be part of only one VLAN, with the exception of our voice access ports. With the trunking we went over earlier, you can make a port available to traffic from all VLANs. I’ll cover that next.Assigning Switch Ports to VLANsYou configure a port to belong to a VLAN by assigning a membership mode that specifies the kind of traffic the port carries, plus the number of VLANs to which it can belong. You can configure each port on a switch to be in a specific VLAN (access port) by using the interface switchport command. You can also configure multiple ports at the same time with the interface range command we talked about in Chapter 8.Remember that you can configure either static memberships or dynamic memberships on a port. For this book’s purpose, I’m only going to cover the static flavor. In the following example, I’ll configure interface fa0/3 to VLAN 3. This is the connection from the S1 switch to the HostA device:

S1#config tS1(config)#int fa0/3S1(config-if)#switchport ?  access         Set access mode characteristics of the interface  backup         Set backup for the interface  block          Disable forwarding of unknown uni/multi cast addresses  host           Set port host  mode           Set trunking mode of the interface  nonegotiate    Device will not engage in negotiation protocol on this                 interface  port-security  Security related command  priority       Set appliance 802.1p priority  protected      Configure an interface to be a protected port  trunk          Set trunking characteristics of the interface

  voice          Voice appliance attributes

10089.book  Page 570  Monday, July 23, 2007  3:17 PM




Configuring VLANs571Well now, what do we have here? There’s some new stuff showing up in the preceding out-put. We can see various commands—some that I’ve already covered, but no worries; I’m going to cover the access, mode, nonegotiate, trunk, and voice commands very soon in this chapter. Let’s start with setting an access port on S1, which is probably the most widely used type of port on production switches that has VLANs configured:

S1(config-if)#switchport mode ?  access   Set trunking mode to ACCESS unconditionally  dynamic  Set trunking mode to dynamically negotiate access or  trunk mode  trunk    Set trunking mode to TRUNK unconditionallyS1(config-if)#switchport mode access

S1(config-if)#switchport access vlan 3By starting with the switchport mode access command, you’re telling the switch that this is a layer 2 port. You can then assign a VLAN to the port with the switchport access command. Remember, you can choose many ports to configure at the same time if you use the interface range command. The dynamic and trunk commands are used for trunk ports exclusively.That’s it. Well, sort of. If you plugged devices into each VLAN port, they can only talk to other devices in the same VLAN. We want to enable inter-VLAN communication and we’re going to do that, but first you need to learn a bit more about trunking.Configuring Trunk PortsThe 2960 switch only runs the IEEE 802.1Q encapsulation method. To configure trunking on a Fast Ethernet port, use the interface command trunk [parameter]. It’s a tad different on the 3560 switch, and I’ll show you that in the next section.The following switch output shows the trunk configuration on interface fa0/8 as set to trunk on:

S1#config tS1(config)#int fa0/8

S1(config-if)#switchport mode trunkThe following list describes the different options available when configuring a switch interface:switchport mode accessI discussed this in the previous section, but this puts the inter-face (access port) into permanent nontrunking mode and negotiates to convert the link into a nontrunk link. The interface becomes a nontrunk interface regardless of whether the neigh-boring interface is a trunk interface. The port would be a dedicated layer 2 port.switchport mode dynamic autoThis mode makes the interface able to convert the link to a trunk link. The interface becomes a trunk interface if the neighboring interface is set to trunk or desirable mode. This is now the default switchport mode for all Ethernet interfaces on all new Cisco switches.

10089.book  Page 571  Monday, July 23, 2007  3:17 PM




572Chapter9 Virtual LANs (VLANs)switchport mode dynamic desirableThis one makes the interface actively attempt to convert the link to a trunk link. The interface becomes a trunk interface if the neighboring interface is set to trunk, desirable, or auto mode. I used to see this mode as the default on some older switches, but not any longer. The default is dynamic auto now.switchport mode trunkPuts the interface into permanent trunking mode and negotiates to convert the neighboring link into a trunk link. The interface becomes a trunk interface even if the neighboring interface isn’t a trunk interface.switchport nonegotiatePrevents the interface from generating DTP frames. You can use this command only when the interface switchport mode is access or trunk. You must manually configure the neighboring interface as a trunk interface to establish a trunk link.

Dynamic Trunking Protocol (DTP) is used for negotiating trunking on a link between two devices, as well as negotiating the encapsulation type of either 802.1Q or ISL. I use the nonegotiate command when I want dedicated trunk ports no questions asked.To disable trunking on an interface, use the switchport mode access command, which sets the port back to a dedicated layer 2 switch port.Trunking with the Cisco Catalyst 3560 switchOkay, let’s take a look at one more switch—the Cisco Catalyst 3560. The configuration is pretty much the same as it is for a 2960, with the exception that the 3560 can provide layer 3 services and the 2960 can’t. Plus, the 3560 can run both the ISL and the IEEE 802.1Q trunking encap-sulation methods—the 2960 can only run 802.1Q. With all this in mind, let’s take a quick look at the VLAN encapsulation difference regarding the 3560 switch.The 3560 has the encapsulation command, which the 2960 switch doesn’t:

Core(config-if)#switchport trunk encapsulation ?  dot1q      Interface uses only 802.1q trunking encapsulation when trunking  isl        Interface uses only ISL trunking encapsulation when trunking  negotiate  Device will negotiate trunking encapsulation with peer on             interfaceCore(config-if)#switchport trunk encapsulation dot1q

Core(config-if)#switchport mode trunkAs you can see, we’ve got the option to add either the IEEE 802.1Q (dot1q) encapsulation or the ISL encapsulation to the 3560 switch. After you set the encapsulation, you still have to set the interface mode to trunk. Honestly, it’s pretty rare that you’d continue to use the ISL encapsulation method. Cisco is moving away from ISL—its new routers don’t even support it.

10089.book  Page 572  Monday, July 23, 2007  3:17 PM




Configuring VLANs573Defining the Allowed VLANs on a TrunkAs I’ve mentioned, trunk ports send and receive information from all VLANs by default, and if a frame is untagged, it’s sent to the management VLAN. This applies to the extended range VLANs as well.But we can remove VLANs from the allowed list to prevent traffic from certain VLANs from traversing a trunked link. Here’s how you’d do that:

S1#config tS1(config)#int f0/1S1(config-if)#switchport trunk allowed vlan ?  WORD    VLAN IDs of the allowed VLANs when this port is in  trunking mode  add     add VLANs to the current list  all     all VLANs  except  all VLANs except the following  none    no VLANs  remove  remove VLANs from the current listS1(config-if)#switchport trunk allowed vlan remove ?  WORD  VLAN IDs of disallowed VLANS when this port is in trunking mode

S1(config-if)#switchport trunk allowed vlan remove 4The preceding command stopped the trunk link configured on S1 port f0/1, causing it to drop all traffic sent and received for VLAN 4. You can try to remove VLAN 1 on a trunk link, but it will still send and receive management like CDP, PAgP, LACP, DTP, and VTP, so what’s the point?To remove a range of VLANs, just use the hyphen:

S1(config-if)#switchport trunk allowed vlan remove 4-8If by chance someone has removed some VLANs from a trunk link and you want to set the trunk back to default, just use this command:

S1(config-if)#switchport trunk allowed vlan allOr this command to accomplish the same thing:

S1(config-if)#no switchport trunk allowed vlanNext, I want to show you how to configure pruning for VLANs before we start routing between VLANs.Changing or Modifying the Trunk Native VLANYou really don’t want to change the trunk port native VLAN from VLAN 1, but you can, and some people do it for security reasons. To change the native VLAN, use the following command:

S1#config tS1(config)#int f0/1

10089.book  Page 573  Monday, July 23, 2007  3:17 PM




574Chapter9 Virtual LANs (VLANs)S1(config-if)#switchport trunk ?  allowed  Set allowed VLAN characteristics when interface is  in trunking mode  native   Set trunking native characteristics when interface  is in trunking mode  pruning  Set pruning VLAN characteristics when interface is  in trunking modeS1(config-if)#switchport trunk native ?  vlan  Set native VLAN when interface is in trunking modeS1(config-if)#switchport trunk native vlan ?  <1-4094>  VLAN ID of the native VLAN when this port is in  trunking modeS1(config-if)#switchport trunk native vlan 40

S1(config-if)#^ZSo we’ve changed our native VLAN on our trunk link to 40, and by using the show running-config command, I can see the configuration under the trunk link:

!interface FastEthernet0/1 switchport trunk native vlan 40 switchport trunk allowed vlan 1-3,9-4094 switchport trunk pruning vlan 3,4

!Hold on there partner! You didn’t think it would be this easy and would just start working, did you? Sure you didn’t. Here’s the rub: If all switches don’t have the same native VLAN con-figured on the trunk links, then we’ll start to receive this error:

19:23:29: %CDP-4-NATIVE_VLAN_MISMATCH: Native VLAN mismatchdiscovered on FastEthernet0/1 (40), with Core FastEthernet0/7 (1).19:24:29: %CDP-4-NATIVE_VLAN_MISMATCH: Native VLAN mismatch

discovered on FastEthernet0/1 (40), with Core FastEthernet0/7 (1).Actually, this is a good, noncryptic error, so either we go to the other end of our trunk link(s) and change the native VLAN or we set the native VLAN back to the default. Here’s how we’d do that:

S1(config-if)#no switchport trunk native vlanNow our trunk link is using the default VLAN 1 as the native VLAN. Just remember that all switches must use the same native VLAN or you’ll have some serious problems. Now, let’s mix it up by connecting a router into our switched network and configuring inter-VLAN communication.

10089.book  Page 574  Monday, July 23, 2007  3:17 PM




Configuring VLANs575Configuring Inter-VLAN RoutingBy default, only hosts that are members of the same VLAN can communicate. To change this and allow inter-VLAN communication, you need a router or a layer 3 switch. I’m going to start with the router approach.To support ISL or 802.1Q routing on a Fast Ethernet interface, the router’s interface is divided into logical interfaces—one for each VLAN. These are called subinterfaces. From a Fast Ethernet or Gigabit interface, you can set the interface to trunk with the encapsulation command:

ISR#config tISR(config)#int f0/0.1ISR(config-subif)#encapsulation ?  dot1Q  IEEE 802.1Q Virtual LANISR(config-subif)#encapsulation dot1Q ?

  <1-4094>  IEEE 802.1Q VLAN IDNotice that my 2811 router (named ISR) only supports 802.1Q. We’d need an older-model router to run the ISL encapsulation, but why bother?The subinterface number is only locally significant, so it doesn’t matter which subinterface numbers are configured on the router. Most of the time, I’ll configure a subinterface with the same number as the VLAN I want to route. It’s easy to remember that way since the subinter-face number is used only for administrative purposes. It’s really important that you understand that each VLAN is a separate subnet. True, I know—they don’t have to be. But it really is a good idea to configure your VLANs as separate subnets, so just do that.Now, I need to make sure you’re fully prepared to configure inter-VLAN routing, as well as determine the port IP addresses of hosts connected in a switched VLAN environment. And as always, it’s also a good idea to be able to fix any problems that may arise. To set you up for suc-cess, let me give you few examples.First, start by looking at Figure 9.9, and read the router and switch configuration within it. By this point in the book, you should be able to determine the IP address, masks, and default gateways of each of the hosts in the VLANs.The next step after that is to figure out which subnets are being used. By looking at the router configuration in the figure, you can see that we’re using 192.168.1.64/26 with VLAN 1 and 192.168.1.128/27 with VLAN 10. And by looking at the switch configura-tion, you can see that ports 2 and 3 are in VLAN 1 and port 4 is in VLAN 10. This means that HostA and HostB are in VLAN 1 and HostC is in VLAN 10.Here’s what the hosts’ IP addresses should be:HostA: 192.168.1.66, 255.255.255.192, default gateway 192.168.1.65HostB: 192.168.1.67, 255.255.255.192, default gateway 192.168.1.65HostC: 192.168.1.130, 255.255.255.224, default gateway 192.168.1.129The hosts could be any address in the range—I just choose the first available IP address after the default gateway address. That wasn’t so hard, was it?

10089.book  Page 575  Monday, July 23, 2007  3:17 PM




576Chapter9 Virtual LANs (VLANs)FIGURE9.9Configuring Inter-VLAN example 1Now, again using Figure 9.9, let’s go through the commands necessary to configure switch port 1 to establish a link with the router and provide inter-VLAN communication using the IEEE version for encapsulation. Keep in mind that the commands can vary slightly depending on what type of switch you’re dealing with.For a 2960 switch, use the following:

2960#config t2960(config)#interface fa0/1

2960(config-if)#switchport mode trunkAs you already know, the 2960 switch can only run the 802.1Q encapsulation so there’s no need to specify it. You can’t anyway! For a 3560, it’s basically the same, but since it can run ISL and 802.1Q, you have to specify the trunking protocol you’re going to use.

Remember that when you create a trunked link, all VLANs are allowed to pass data by default.Let’s take a look at Figure 9.10 and see what we can learn from it. This figure shows three VLANs, with two hosts in each of them.The router in Figure 9.10 is connected to the fa0/1 switch port, and VLAN 2 is configured on port f0/6. Looking at the diagram, these are the things that Cisco expects you to know: The router is connected to the switch using subinterfaces. The switch port connecting to the router is a trunk port. The switch ports connecting to the clients and the hub are access ports, not trunk ports.

interface fastethernet 0/1.1  encapsulation dot1q 1  ip address 192.168.1.65 255.255.255.192interface fastethernet 0/1.10  encapsulation dot1q 10  ip address 192.168.1.129 255.255.255.224

Internet

fa0/11234HostA

HostB

HostC

Port 1: dot1q trunkPorts 2, 3: VLAN 1Port 4: VLAN 10

10089.book  Page 576  Monday, July 23, 2007  3:17 PM




Configuring VLANs577FIGURE9.10Inter-VLAN example 2The configuration of the switch would look something like this:

2960#config t2960(config)#int f0/12960(config-if)#switchport mode trunk2960(config-if)#int f0/22960(config-if)#switchport access vlan 12960(config-if)#int f0/32960(config-if)#switchport access vlan 12960(config-if)#int f0/42960(config-if)#switchport access vlan 32960(config-if)#int f0/52960(config-if)#switchport access vlan 32960(config-if)#int f0/6

2960(config-if)#switchport access vlan 2Before we configure the router, we need to design our logical network:VLAN 1: 192.168.10.16/28VLAN 2: 192.168.10.32/28VLAN 3: 192.168.10.48/28The configuration of the router would then look like this:

ISR#config tISR(config)#int f0/0

HostE

Internet

Fa0/0Fa0/1Fa0/2Fa0/3Fa0/4Fa0/5Fa0/6HostC

HostD

HostF

HostA

HostB

VLAN 1VLAN 3VLAN 2

10089.book  Page 577  Monday, July 23, 2007  3:17 PM




578Chapter9 Virtual LANs (VLANs)ISR(config-if)#no ip addressISR(config-if)#no shutdownISR(config-if)#int f0/0.1ISR(config-subif)#encapsulation dot1q 1ISR(config-subif)#ip address 192.168.10.17 255.255.255.240ISR(config-subif)#int f0/0.2ISR(config-subif)#encapsulation dot1q 2ISR(config-subif)#ip address 192.168.10.33 255.255.255.240ISR(config-subif)#int f0/0.3ISR(config-subif)#encapsulation dot1q 3

ISR(config-subif)#ip address 192.168.10.49 255.255.255.240The hosts in each VLAN would be assigned an address from their subnet range, and the default gateway would be the IP address assigned to the router’s subinterface in that VLAN.Now, let’s take a look at another figure and see if you can determine the switch and router configurations without looking at the answer—no cheating! Figure 9.11 shows a router con-nected to a 2960 switch with two VLANs. One host in each VLAN is assigned an IP address. What are your router and switch configurations based on these IP addresses?Since the hosts don’t list a subnet mask, you have to look for the number of hosts used in each VLAN to figure out the block size. VLAN 1 has 85 hosts and VLAN 2 has 115 hosts. Each of these will fit in a block size of 128, which is a /25 mask, or 255.255.255.128.FIGURE9.11Inter-VLAN example 3

VLAN 185 HostsHostA

F0/1172.16.10.129

F0/2F0/3172.16.10.126VLAN 2115 HostsHostB

10089.book  Page 578  Monday, July 23, 2007  3:17 PM




Configuring VLANs579You should know by now that the subnets are 0 and 128; the 0 subnet (VLAN 1) has a host range of 1–126, and the 128 subnet (VLAN 2) has a range of 129–254. You can almost be fooled since HostA has an IP address of 126, which makes it almost seem that HostA and B are in the same subnet. But they’re not, and you’re way too smart by now to be fooled by this one!Here is the switch configuration:

2960#config t2960(config)#int f0/12960(config-if)#switchport mode trunk2960(config-if)#int f0/22960(config-if)#switchport access vlan 12960(config-if)#int f0/3

2960(config-if)#switchport access vlan 2Here is the router configuration:

ISR#config tISR(config)#int f0/0ISR(config-if)#no ip addressISR(config-if)#no shutdownISR(config-if)#int f0/0.1ISR(config-subif)#encapsulation dot1q 1ISR(config-subif)#ip address 172.16.10.1 255.255.255.128ISR(config-subif)#int f0/0.2ISR(config-subif)#encapsulation dot1q 2

ISR(config-subif)#ip address 172.16.10.254 255.255.255.128I used the first address in the host range for VLAN 1 and the last address in the range for VLAN 2, but any address in the range would work. You just have to configure the host’s default gateway to whatever you make the router’s address.Now, before we go on to the next example, I need to make sure you know how to set the IP address on the switch. Since VLAN 1 is typically the administrative VLAN, we’ll use an IP address from that pool of addresses. Here’s how to set the IP address of the switch (I’m not nagging, but you really should already know this!):

2960#config t2960(config)#int vlan 12960(config-if)#ip address 172.16.10.2 255.255.255.128

2960(config-if)#no shutdownYes, you have to do a no shutdown on the VLAN interface.One more example, and then we’ll move on to VTP—another important subject that you definitely don’t want to miss! In Figure 9.12 there are two VLANs. By looking at the router configuration, what’s the IP address, mask, and default gateway of HostA? Use the last IP address in the range for HostA’s address:

10089.book  Page 579  Monday, July 23, 2007  3:17 PM




 580 Chapter9  Virtual LANs (VLANs) FIGURE9.12 Inter-VLAN example 4 If you really look carefully at the router configuration (the hostname in this figure is just Router), there is a simple and quick answer. Both subnets are using a /28, or 255.255.255.240 mask, which is a block size of 16. The router’s address for VLAN 1 is in subnet 128. The next subnet is 144, so the broadcast address of VLAN 1 is 143 and the valid host range is 129–142. So the host address would be this: IP Address:  192.168.10.142 Mask:  255.255.255.240 Default Gateway:  192.168.10.129 Configuring VTP All Cisco switches are configured to be VTP servers by default. To configure VTP, first you have to configure the domain name you want to use. And of course, once you configure the VTP information on a switch, you need to verify it.

VLAN 1HostA

F0/1VLAN 2

F0/2F0/3

HostBRouter#config tRouter(config)#int f0/0Router(config-if)#no ip addressRouter(config-if)#no shutdownRouter(config-if)#int f0/0.1Router(config-subif)# encapsulation dot1q 1 Router(config-subif)# ip address 192.168.10.129 255.255.255.240Router(config-subif)# int f0/0.2Router(config-subif)# encapsulation dot1q 2 Router(config-subif)# ip address 192.168.10.46 255.255.255.240

 

10089c09.fm  Page 580  Wednesday, October 29, 2008  2:57 PM




Configuring VTP581When you create the VTP domain, you have a bunch of options, including setting the domain name, password, operating mode, and pruning capabilities of the switch. Use the vtp global con-figuration mode command to set all this information. In the following example, I’ll set the S1 switch to vtp server, the VTP domain to Lammle, and the VTP password to todd:

S1#config tS1#(config)#vtp mode serverDevice mode already VTP SERVER.S1(config)#vtp domain LammleChanging VTP domain name from null to LammleS1(config)#vtp password toddSetting device VLAN database password to toddS1(config)#do show vtp passwordVTP Password: toddS1(config)#do show vtp statusVTP Version                     : 2Configuration Revision          : 0Maximum VLANs supported locally : 255Number of existing VLANs        : 8VTP Operating Mode              : ServerVTP Domain Name                 : LammleVTP Pruning Mode                : DisabledVTP V2 Mode                     : DisabledVTP Traps Generation            : DisabledMD5 digest                      : 0x15 0x54 0x88 0xF2 0x50 0xD9 0x03 0x07Configuration last modified by 192.168.24.6 at 3-14-93 15:47:32Local updater ID is 192.168.24.6 on interface Vl1 (lowest numbered VLAN 

interface found)Please make sure you remember that all switches are set to VTP server mode by default, and if you want to change any VLAN information on a switch, you absolutely must be in VTP server mode. After you configure the VTP information, you can verify it with the show vtp command as shown in the preceding output. The preceding switch output shows the VTP domain, the VTP password, and the switch’s mode.Before we move onward to configuring the Core and the S2 switch with VTP information, take a minute to reflect on the fact that the show vtp status output shows that the maximum number of VLANs supported locally is only 255. Since you can create over 1,000 VLANs on a switch, this seems like it would definitely be a problem if you have more then 255 switches and you’re using VTP. And, well, yes, it is problem—if you are trying to configure the 256th VLAN on a switch, you’ll get a nice little error message stating that there are not enough hard-ware resources available, and then it will shut down the VLAN and the 256th VLAN will show up in suspended state in the output of the show vlan command. Not so good!

10089.book  Page 581  Monday, July 23, 2007  3:17 PM




582Chapter9 Virtual LANs (VLANs)Let’s go to the Core and S2 switches and set them into the Lammle VTP domain. It is very important to remember that the VTP domain name is case sensitive! VTP is not forgiving—one teeny small mistake and it just won’t work.

Core#config tCore(config)#vtp mode clientSetting device to VTP CLIENT mode.Core(config)#vtp domain LammleChanging VTP domain name from null to LammleCore(config)#vtp password toddSetting device VLAN database password to toddCore(config)#do show vtp statusVTP Version                     : 2Configuration Revision          : 0Maximum VLANs supported locally : 1005Number of existing VLANs        : 5VTP Operating Mode              : ServerVTP Domain Name                 : LammleVTP Pruning Mode                : DisabledVTP V2 Mode                     : DisabledVTP Traps Generation            : DisabledMD5 digest                      : 0x2A 0x6B 0x22 0x17 0x04 0x4F 0xB8 0xC2Configuration last modified by 192.168.10.19 at 3-1-93 03:13:16Local updater ID is 192.168.24.7 on interface Vl1 (first interface found)S2#config tS2(config)#vtp mode clientSetting device to VTP CLIENT mode.S2(config)#vtp domain LammleChanging VTP domain name from null to LammleS2(config)#vtp password toddSetting device VLAN database password to toddS2(config)#do show vtp statusVTP Version                     : 2Configuration Revision          : 0Maximum VLANs supported locally : 1005Number of existing VLANs        : 5VTP Operating Mode              : ClientVTP Domain Name                 : LammleVTP Pruning Mode                : DisabledVTP V2 Mode                     : DisabledVTP Traps Generation            : DisabledMD5 digest                      : 0x02 0x11 0x18 0x4B 0x36 0xC5 0xF4 0x1F

Configuration last modified by 0.0.0.0 at 0-0-00 00:00:00

10089.book  Page 582  Monday, July 23, 2007  3:17 PM




Configuring VTP583Nice—now that all our switches are set to the same VTP domain and password, the VLANs I created earlier on the S1 switch should be advertised to the Core and S2 VTP client switches. Let’s take a look using the show vlan brief command on the Core and S2 switch:

Core#sh vlan briefVLAN Name                 Status    Ports---- ------------------ --------- ---------------------1    default              active    Fa0/1,Fa0/2,Fa0/3,Fa0/4                                    Fa0/9,Fa0/10,Fa0/11,Fa0/12                                    Fa0/13,Fa0/14,Fa0/15,                                    Fa0/16,Fa0/17, Fa0/18, Fa0/19,                                    Fa0/20,Fa0/21, Fa0/22, Fa0/23,                                    Fa0/24, Gi0/1, Gi0/22    Sales                 active  3    Marketing             active  4    Accounting            active  [output cut]S2#sh vlan briVLAN Name                   Status    Ports---- ---------------------- --------- ---------------------1    default                active    Fa0/3, Fa0/4, Fa0/5, Fa0/6                                      Fa0/7, Fa0/8, Gi0/12    Sales                  active  3    Marketing              active  4    Accounting             active

[output cut]The VLAN database that I created on the S1 (2960) switch earlier in this chapter was uploaded to the Core and S2 switch via VTP advertisements. VTP is a great way to keep VLAN naming consistent across the switched network. We can now assign VLANs to the ports on the Core and S1 switches and they’ll communicate with the hosts in the same VLANs on the S1 switch across the trunked ports between switches.

It’s imperative that you can assign a VTP domain name, set the switch to VTP server mode, and create a VLAN!Troubleshooting VTPYou connect your switches with crossover cables, the lights go green on both ends, and you’re up and running! Yeah—in a perfect world, right? Don’t you wish it was that easy? Well, actually, it pretty much is—without VLANs, of course. But if you’re using VLANs—and you definitely should be—then you need to use VTP if you have multiple VLANs configured in your switched network.

10089.book  Page 583  Monday, July 23, 2007  3:17 PM




584Chapter9 Virtual LANs (VLANs)But here there be monsters: If VTP is not configured correctly, it (surprise!) will not work, so you absolutely must be capable of troubleshooting VTP. Let’s take a look at a couple of configurations and solve the problems. Study the output from the two following switches:

SwitchA#sh vtp statusVTP Version                     : 2Configuration Revision          : 0Maximum VLANs supported locally : 64Number of existing VLANs        : 7VTP Operating Mode              : ServerVTP Domain Name                 : RouterSimVTP Pruning Mode                : DisabledVTP V2 Mode                     : DisabledVTP Traps Generation            : DisabledSwitchB#sh vtp statusVTP Version                     : 2Configuration Revision          : 1Maximum VLANs supported locally : 64Number of existing VLANs        : 7VTP Operating Mode              : ServerVTP Domain Name                 : GlobalNetVTP Pruning Mode                : DisabledVTP V2 Mode                     : Disabled

VTP Traps Generation            : DisabledSo what’s happening with these two switches? Why won’t they share VLAN information? At first glance, it seems that both servers are in VTP server mode, but that’s not the problem. Servers in VTP server mode will share VLAN information using VTP. The problem is that they’re in two different VTP domains. SwitchA is in VTP domain RouterSim and SwitchB is in VTP domain GlobalNet. They will never share VTP information because the VTP domain names are configured differently.Now that you know how to look for common VTP domain configuration errors in your switches, let’s take a look at another switch configuration:

SwitchC#sh vtp statusVTP Version                     : 2Configuration Revision          : 1Maximum VLANs supported locally : 64Number of existing VLANs        : 7VTP Operating Mode              : ClientVTP Domain Name                 : ToddVTP Pruning Mode                : Disabled

10089.book  Page 584  Monday, July 23, 2007  3:17 PM




Configuring VTP585VTP V2 Mode                     : Disabled

VTP Traps Generation            : DisabledThere you are just trying to create a new VLAN on SwitchC and what do you get for your trouble? A loathsome error! Why can’t you create a VLAN on SwitchC? Well, the VTP domain name isn’t the important thing in this example. What is critical here is the VTP mode. The VTP mode is client, and a VTP client cannot create, delete, add, or change VLANs, remember? VTP clients only keep the VTP database in RAM, and that’s not saved to NVRAM. So, in order to create a VLAN on this switch, you’ve got to make the switch a VTP server first.Here’s what will happen when you have the preceding VTP configuration:

SwitchC(config)#vlan 50

VTP VLAN configuration not allowed when device is in CLIENT mode.So to fix this problem, here’s what you need to do:

SwitchC(config)#vtp mode serverSetting device to VTP SERVER modeSwitchC(config)#vlan 50

SwitchC(config-vlan)#Wait, we’re not done. Now take a look at the output from these two switches and determine why SwitchB is not receiving VLAN information from SwitchA:

SwitchA#sh vtp statusVTP Version                     : 2Configuration Revision          : 4Maximum VLANs supported locally : 64Number of existing VLANs        : 7VTP Operating Mode              : ServerVTP Domain Name                 : GlobalNetVTP Pruning Mode                : DisabledVTP V2 Mode                     : DisabledVTP Traps Generation            : DisabledSwitchB#sh vtp statusVTP Version                     : 2Configuration Revision          : 14Maximum VLANs supported locally : 64Number of existing VLANs        : 7VTP Operating Mode              : ServerVTP Domain Name                 : GlobalNetVTP Pruning Mode                : DisabledVTP V2 Mode                     : Disabled

VTP Traps Generation            : Disabled

10089.book  Page 585  Monday, July 23, 2007  3:17 PM




586Chapter9 Virtual LANs (VLANs)You may be tempted to say it’s because they’re both VTP servers, but that is not the prob-lem. All your switches can be servers and they can still share VLAN information. As a matter of fact, Cisco actually suggests that all switches stay VTP servers and that you just make sure the switch you want to advertise VTP VLAN information has the highest revision number. If all switches are VTP servers, then all of the switches will save the VLAN database. But SwitchB isn’t receiving VLAN information from SwitchA because SwitchB has a higher revision num-ber than SwitchA. It’s very important that you can recognize this problem.There are a couple ways to go about resolving this issue. The first thing you could do is to change the VTP domain name on SwitchB to another name, then set it back to GlobalNet, which will reset the revision number to zero (0) on SwitchB. The second approach would be to create or delete VLANs on SwitchA until the revision number passes the revision number on SwitchB. I didn’t say the second way was better; I just said it’s another way to fix it!Telephony: Configuring Voice VLANsIf you do yoga, meditate, chain smoke, or consume mass quantities of comfort food when stressed, take a little break and do that now because, and I’m going to be honest, this isn’t the easiest part of the chapter—or even the book, for that matter. But I promise that I’ll do my best to make this as painless for you as possible.The voice VLAN feature enables access ports to carry IP voice traffic from an IP phone. When a switch is connected to a Cisco IP phone, the IP phone sends voice traffic with layer 3 IP precedence and layer 2 class of service (CoS) values, which are both set to 5 for voice traffic; all other traffic defaults to 0.Because the sound quality of an IP phone call can deteriorate if the data is unevenly sent, the switch supports quality of service (QoS) based on IEEE 802.1p CoS. (802.1p provides a mechanism for implementing QoS at the MAC level.) The 802.1p field is carried in the 802.1Q trunk header. If you look at the fields in an 802.1Q tag, you will see a field called the priority field; this is where the 802.1p information goes. QoS uses classification and scheduling to send network traffic from the switch in an organized, predictable manner.The Cisco IP phone is a configurable device, and you can configure it to forward traffic with an IEEE 802.1p priority. You can also configure the switch to either trust or override the traffic priority assigned by an IP phone—which is exactly what we’re going to do. The Cisco phone basically has a three-port switch: one to connect to the Cisco switch, one to a PC device, and one to the actual phone, which is internal.You can also configure an access port with an attached Cisco IP phone to use one VLAN for voice traffic and another VLAN for data traffic from a device attached to the phone—like a PC. You can configure access ports on the switch to send Cisco Discovery Protocol (CDP) packets that instruct an attached Cisco IP phone to send voice traffic to the switch in any of these ways: In the voice VLAN tagged with a layer 2 CoS priority value In the access VLAN tagged with a layer 2 CoS priority value In the access VLAN, untagged (no layer 2 CoS priority value)

10089.book  Page 586  Monday, July 23, 2007  3:17 PM




Telephony: Configuring Voice VLANs587The switch can also process tagged data traffic (traffic in IEEE 802.1Q or IEEE 802.1p frame types) from the device attached to the access port on the Cisco IP phone. You can con-figure layer 2 access ports on the switch to send CDP packets that instruct the attached Cisco IP phone to configure the IP phone access port in one of these modes: In trusted mode, all traffic received through the access port on the Cisco IP phone passes through the IP phone unchanged. In untrusted mode, all traffic in IEEE 802.1Q or IEEE 802.1p frames received through the access port on the IP phone receive a configured layer 2 CoS value. The default layer 2 CoS value is 0. Untrusted mode is the default.Configuring the Voice VLANBy default, the voice VLAN feature is disabled; you enable it by using the interface command switchport voice vlan. When the voice VLAN feature is enabled, all untagged traffic is sent according to the default CoS priority of the port. The CoS value is not trusted for IEEE 802.1p or IEEE 802.1Q tagged traffic.These are the voice VLAN configuration guidelines: You should configure voice VLAN on switch access ports; voice VLAN isn’t supported on trunk ports, even though you can actually configure it! The voice VLAN should be present and active on the switch for the IP phone to correctly communicate on it. Use the show vlan privileged EXEC command to see if the VLAN is present—if it is, it’ll be listed in the display. Before you enable the voice VLAN, it’s recommend that you enable QoS on the switch by entering the mls qos global configuration command and set the port trust state to trust by entering the mls qos trust cos interface configuration command. You must make sure that CDP is enabled on the switch port connected to the Cisco IP phone to send the configuration. This is on by default, so unless you disabled it, you shouldn’t have a problem. The PortFast feature is automatically enabled when the voice VLAN is configured, but when you disable the voice VLAN, the PortFast feature isn’t automatically disabled. To return the port to its default setting, use the no switchport voice vlan interface configuration command.Configuring IP Phone Voice TrafficYou can configure a port connected to the Cisco IP phone to send CDP packets to the phone to configure the way in which the phone sends voice traffic. The phone can carry voice traffic in IEEE 802.1Q frames for a specified voice VLAN with a layer 2 CoS value. It can use IEEE 802.1p priority tagging to give voice traffic a higher priority as well as forward all voice traffic through the native (access) VLAN. The IP phone can also send untagged voice traffic, or use its own configuration to send voice traffic in the access VLAN. In all configurations, the voice traffic carries a layer 3 IP precedence value—again, for voice the setting is usually 5.

10089.book  Page 587  Monday, July 23, 2007  3:17 PM




588Chapter9 Virtual LANs (VLANs)I think it’s about time to give you some actual examples to make this clear to you. This example shows you how to configure four things:1.How to configure a port connected to an IP phone to use the CoS value for classifying incoming traffic2.How to configure the port to use IEEE 802.1p priority tagging for voice traffic3.How to configure it to use the Voice VLAN (10) to carry all voice traffic4.And last, how to configure VLAN 3 to carry PC data

Switch#configure tSwitch(config)#mls qosSwitch(config)#interface f0/1Switch(config-if)#switchport priority extend ?  cos    Override 802.1p priority of devices on appliance  trust  Trust 802.1p priorities of devices on applianceSwitch(config-if)#switchport priority extend trustSwitch(config-if)#mls qos trust cosSwitch(config-if)#switchport voice vlan dot1pSwitch(config-if)#switchport mode accessSwitch(config-if)#switchport access vlan 3

Switch(config-if)#switchport voice vlan 10The command mls qos trust cos will configure the interface to classify incoming traffic packets by using the packet CoS value. For untagged packets, the port’s default CoS value will be used. But before configuring the port trust state, you must first globally enable QoS by using the mls qos global configuration command.Notice how I added two access VLANs to the same port? I can only do this if I have one for a data VLAN and another one for a voice VLAN.This section was probably the hardest part of this entire book, and I honestly created the simplest configuration you should use to get you through it! So you can relax a bit because using the CNA with the preconfigured macros makes configuring phones connected to your switch a day at the beach. You just really should know how to do it both ways. I’ll demon-strate configuration using the CNA in the next section.Using the CNA to Configure VLANs and Inter-VLAN RoutingThis chapter has focused pretty heavily on enabling routing between VLANs through the use of an actual router, but increasingly, you’ll find that you’ll use a switch backplane for routing 

10089.book  Page 588  Monday, July 23, 2007  3:17 PM




Using the CNA to Configure VLANs and Inter-VLAN Routing589most often in a large corporate environment. But this time, instead of using the command-line interface (CLI), I’m going to use the CNA to show you both how to configure VLANs on a 2960 and how to perform inter-VLAN routing on a 3560 switch. After that, I’m going to con-figure our switch ports with our phone ports using the CNA Smartports to show you how easy it is to do that using the new CNA instead of grinding through all the work we did in the pre-vious section. And trust me, even though it probably seems like we did a lot, we really just scratched the surface of telephony in this chapter.I’m going to get started by connecting to my 2960 switch (S1) that already has the three new VLANs configured from earlier in this chapter and taking a look at them. Then I’m going to add a new voice VLAN. At the first screen, I clicked Configure, Switching, and VLANs and received a new screen that shows the status of my ports.Now we can see that ports 1 and 2 are trunked dynamically, and since they’re set to dynamic auto by default, they’ll automatically became trunk links with the connections to the Core switch. In order to make this happen before, I erased the configuration of the switch and reloaded, which of course deleted my EtherChannel configuration. But the VLAN database that’s stored in flash memory is still there. You can also see that port 3 is a member of VLAN 3—the VLAN access port I set earlier in the chapter.

10089.book  Page 589  Monday, July 23, 2007  3:17 PM




590Chapter9 Virtual LANs (VLANs)A nice feature of this screen is the Modify button you find toward the bottom right. I high-lighted port 1, clicked Modify, and that took me to this new screen.Moving right along, from this screen, I can change the different administrative modes and encapsulations plus set the VLANs allowed on the trunk port as well as set my VTP pruning. But what I’m most interested in is the Configure VLANs tab on the VLANs screen.

10089.book  Page 590  Monday, July 23, 2007  3:17 PM




Using the CNA to Configure VLANs and Inter-VLAN Routing591From here, I can see my configured VLANs, and I can also modify, add, and delete them. (Remember, you have to be a VTP server, not just a client, in order to do this.) I clicked the Create button and received this screen.I then clicked Create, added a new VLAN named Todd (how creative), and clicked OK.

10089.book  Page 591  Monday, July 23, 2007  3:17 PM




592Chapter9 Virtual LANs (VLANs)Now, let’s have some fun and create a voice VLAN. I clicked Voice VLAN under Configure and received this screen.I then highlighted port 4 where my phone is connected and clicked Modify. I created a new voice VLAN (Voice VLAN 10) and clicked OK.

10089.book  Page 592  Monday, July 23, 2007  3:17 PM




Using the CNA to Configure VLANs and Inter-VLAN Routing593All good—now I want to go to the 3560 switch and configure inter-VLAN routing using the switch instead of using a router as we’ve done so far.So now I’m connected to my Core switch (3560). Under Configure I clicked Routing and Enable/Disable. From the screen that appeared, I clicked Enable IP Routing, and it automat-ically added my configured default gateway—way cool!After I clicked OK, I clicked Inter-VLAN Routing Wizard and this screen appeared.

10089.book  Page 593  Monday, July 23, 2007  3:17 PM




594Chapter9 Virtual LANs (VLANs)Well, the party is kind of over—we’ve reached the point where it really isn’t much easier than configuring from a command-line interface. When you see the finished configuration in RAM, you’ll know what I mean. So, after clicking Next, I got this screen.Not much to do here but click Next to go to the next screen.

10089.book  Page 594  Monday, July 23, 2007  3:17 PM




Using the CNA to Configure VLANs and Inter-VLAN Routing595Now this screen is where things happen—you get to do the inter-VLAN routing configu-ration. I clicked on the VLANs that I wanted to provide inter-VLAN communication between, added new subnets and subnet masks for each separate VLAN, and then clicked Next to get to this screen.This screen already had the IP default gateway set as the default route of the switch, which is fine, so I again clicked Next.Here’s where the party starts again—I just sat back and watched the router auto-configure itself. You can see that there’s a separate logical interface for each VLAN with the IP address I specified for each. IP routing becomes enabled, and then the default route is set. To be honest, I could have typed this in faster using the CLI, but now, by looking at the commands, you know 

10089.book  Page 595  Monday, July 23, 2007  3:17 PM




596Chapter9 Virtual LANs (VLANs)both ways to configure inter-VLAN routing on a switch. To finish, I again clicked Next, and the configuration was then uploaded to the running-config.And here’s the running-config’s output:

!interface Vlan1 ip address 192.168.24.7 255.255.255.0!interface Vlan3 ip address 192.168.25.7 255.255.255.0!interface Vlan10 ip address 192.168.26.7 255.255.255.0

!Pretty simple, straightforward, and looking good—all of our hosts/phones should now be able to communicate freely between VLANs. However, I do want you to understand that I’m not saying that you should actually route between all your VLANs—but for demonstration purposes, this configuration works great!Before I finish the chapter, I want to connect to the 2960 and use a Smartport macro to con-figure the port my phone is plugged into. Now, in contrast to what I said about the routing configuration on the 3560 and how it is just as easy to use the CLI, the phone configuration via the CLI, as you already know, is Godzilla, so let’s do it the easy way.I opened the CNA for the 2960 (S1) and clicked Smartports. Then I highlighted port 4, right-clicked, and chose IP Phone+Desktop.

10089.book  Page 596  Monday, July 23, 2007  3:17 PM




Summary597I then chose the access VLAN—VLAN 3, which my PC is using—and the voice VLAN I created earlier (10). Once I clicked OK, the macro was run and the configuration was loaded into the running-config. Here’s the output of the running-config after the cisco-phone macro ran on port 4:

!interface FastEthernet0/4 switchport access vlan 3 switchport mode access switchport voice vlan 10 switchport port-security maximum 2 switchport port-security switchport port-security aging time 2 switchport port-security violation restrict switchport port-security aging type inactivity srr-queue bandwidth share 10 10 60 20 srr-queue bandwidth shape  10  0  0  0 mls qos trust device cisco-phone mls qos trust cos macro description cisco-phone auto qos voip cisco-phone spanning-tree portfast spanning-tree bpduguard enable

!   Wow, and I thought the Desktop macro added a lot of configuration lines to the router interface! And this does not count the 44—that’s right, 44—lines of queuing it added to the running-config as well. If someone looks at your configurations and doesn’t know you used the CNA, you’re going to look like a genius for sure!Now I can connect both a PC-type device and a phone to the same port and they will run in separate VLANs.Now this was quite a chapter! I can’t stress enough that you have to get hands-on experi-ence with at least 2960 switches, and you must be able to configure the switches using both the CLI and the CNA.SummaryThis chapter introduced you to the world of virtual LANs and described how Cisco switches can use them. We talked about how VLANs break up broadcast domains in a switched inter-network—a very important, necessary thing because layer 2 switches only break up collision domains and, by default, all switches make up one large broadcast domain. I also described access links to you and we went over how trunked VLANs work across a Fast Ethernet link.

10089.book  Page 597  Monday, July 23, 2007  3:17 PM




598Chapter9 Virtual LANs (VLANs)Trunking is a crucial technology to understand well when you’re dealing with a network populated by multiple switches that are running several VLANs. I also talked at length about VLAN Trunk Protocol (VTP), which, in reality, has nothing to do with trunking. You learned that it does send VLAN information down a trunked link but that the trunk configuration in and of itself isn’t part of VTP.And then there was the telephony gauntlet—something you may want to forget, but if you want to succeed, you’ll make sure you’ve got it down even if it means going through it all again!I finished the chapter off by providing troubleshooting and configuration examples of VTP, trunking, and VLAN configurations as well as how to configure 2960 and 3560 switches using the CNA.Exam EssentialsUnderstand the term frame tagging.Frame tagging refers to VLAN identification; this is what switches use to keep track of all those frames as they’re traversing a switch fabric. It’s how switches identify which frames belong to which VLANs.Understand the ISL VLAN identification method.Inter-Switch Link (ISL) is a way of explicitly tagging VLAN information onto an Ethernet frame. This tagging information allows VLANs to be multiplexed over a trunk link through an external encapsulation method, which allows the switch to identify the VLAN membership of a frame over the link. ISL is a Cisco-proprietary frame-tagging method that can only be used with Cisco switches and routers.Understand the 802.1Q VLAN identification method.This is a nonproprietary IEEE method of frame tagging. If you’re trunking between a Cisco switched link and a different brand of switch, you have to use 802.1Q for the trunk to work.Remember how to set a trunk port on a 2960 switch.To set a port to trunking on a 2960, use the switchport mode trunk command.Remember to check a switch port’s VLAN assignment when plugging in a new host.If you plug a new host into a switch, then you must verify the VLAN membership of that port. If the membership is different than what is needed for that host, the host will not be able to reach the needed network services, such as a workgroup server.Understand the purpose and configuration of VTPVTP provides propagation of the VLAN database throughout your switched network. All switches must be in the same VTP domain.Remember how to create a Cisco “router on a stick” to provide inter-VLAN communication.You can use a Cisco Fast Ethernet or Gigabit Ethernet interface to provide inter-VLAN routing. The switch port connected to the router must be a trunk port; then you must create virtual inter-faces (subinterfaces) on the router port for each VLAN connecting. The hosts in each VLAN will use this subinterface address as their default gateway address.

10089.book  Page 598  Monday, July 23, 2007  3:17 PM




Written Lab 9599Written Lab 9In this section, write the answers to the following questions:1.What VTP mode can only accept VLAN information and not change it?2.What VLAN identification method is proprietary to Cisco routers?3.VLANs break up ________ domains.4.Switches, by default, only break up ________ domains.5.What is the default VTP mode?6.What does trunking provide?7.What is frame tagging?8.True/False: The ISL encapsulation is removed from the frame if the frame is forwarded out an access link.9.What type of link is only part of one VLAN and is referred to as the “native VLAN” of the port?10.What type of Cisco tagging information allows VLANs to be multiplexed over a trunk link through an external encapsulation method?(The answers to Written Lab 9 can be found following the answers to the review questions for this chapter.)

10089.book  Page 599  Monday, July 23, 2007  3:17 PM




600Chapter9 Virtual LANs (VLANs)Review Questions

The following questions are designed to test your understanding of this chapter’s material. For more information on how to get additional ques-tions, please see this book’s Introduction.1.Which of the following is true regarding VLANs?A.You must have at least two VLANs defined in every Cisco switched network.B.All VLANs are configured at the fastest switch and, by default, propagate this information to all other switches.C.You should not have more than 10 switches in the same VTP domain.D.VTP is used to send VLAN information to switches in a configured VTP domain.2.According to the following diagram, which of the following describes the router port config-uration and the switch port configuration as shown in the topology? (Choose three.)A.The router WAN port is configured as a trunk port.B.The router port connected to the switch is configured using subinterfaces.C.The router port connected to the switch is configured at 10Mbps.D.The switch port connected to the hub is configured as full duplex.E.The switch port connected to the router is configured as a trunking port.F.The switch ports connected to the hosts are configured as access ports.

HostE

Internet

Fa0/0Fa0/1Fa0/2Fa0/3Fa0/4Fa0/5Fa0/6HostC

HostD

HostF

HostA

HostB

VLAN 1VLAN 3VLAN 2

10089.book  Page 600  Monday, July 23, 2007  3:17 PM




Review Questions6013.A switch has been configured for three different VLANs: VLAN2, VLAN3, and VLAN4. A router has been added to provide communication between the VLANs. What type of interface is necessary on the router if only one connection is to be made between the router and the switch?A.10Mbps EthernetB.56Kbps SerialC.100Mbps EthernetD.1Gbps Ethernet4.You want to improve network performance by increasing the bandwidth available to hosts and limit the size of the broadcast domains. Which of the following options will achieve this goal?A.Managed hubsB.BridgesC.SwitchesD.Switches configured with VLANs5.Which of the following protocols are used to configure trunking on a switch? (Choose two.)A.VLAN Trunk ProtocolB.VLANC.802.1QD.ISL6.When a new trunk link is configured on an IOS-based switch, which VLANs are allowed over the link?A.By default, all VLANs are allowed on the trunk.B.No VLAN’s are allowed, you must configure each VLAN by hand.C.Only configured VLAN’s are allowed on the link.D.Only extended VLAN’s are allowed by default.7.Which switching technology reduces the size of a broadcast domain?A.ISLB.802.1QC.VLANsD.STP8.What VTP mode allows you to change VLAN information on the switch?A.ClientB.STPC.ServerD.Transparent

10089.book  Page 601  Monday, July 23, 2007  3:17 PM




602Chapter9 Virtual LANs (VLANs)9.Which command will configure a switch port to use the IEEE standard method of inserting VLAN membership information into Ethernet frames?A.Switch(config)#switchport trunk encapsulation islB.Switch(config)#switchport trunk encapsulation ietfC.Switch(config)#switchport trunk encapsulation dot1qD.Switch(config-if)#switchport trunk encapsulation islE.Switch(config-if)#switchport trunk encapsulation ietfF.Switch(config-if)#switchport trunk encapsulation dot1q10.Which of the following is true regarding VTP?A.All switches are VTP servers by default.B.All switches are VTP transparent by default.C.VTP is on by default with a domain name of Cisco on all Cisco switches.D.All switches are VTP clients by default.11.Which protocol reduces administrative overhead in a switched network by allowing the configuration of a new VLAN to be distributed to all the switches in a domain?A.STPB.VTPC.DHCPD.ISL12.Which of the following commands sets a trunk port on a 2960 switch?A.trunk onB.trunk allC.switchport trunk onD.switchport mode trunk13.Which of the following is an IEEE standard for frame tagging?A.ISLB.802.3ZC.802.1QD.802.3U14.You connect a host to a switch port, but the new host cannot log into the server that is plugged into the same switch. What could the problem be? (Choose the most likely answer.)A.The router is not configured for the new host.B.The VTP configuration on the switch is not updated for the new host.C.The host has an invalid MAC address.D.The switch port the host is connected to is not configured to the correct VLAN membership.

10089.book  Page 602  Monday, July 23, 2007  3:17 PM




Review Questions60315.According to the diagram, which three commands can be used to establish a link with the router’s Fast Ethernet interface using the IEEE version of frame tagging? (Choose three.)A.Switch(config)#interface fastethernet 0/1B.Switch(config-if)#switchport mode accessC.Switch(config-if)#switchport mode trunkD.Switch(config-if)#switchport access vlan 1E.Switch(config-if)#switchport trunk encapsulation islF.Switch(config-if)#switchport trunk encapsulation dot1q16.These two switches are not sharing VLAN information. From the following output, what is the reason these switches are not sharing VTP messages?SwitchA#sh vtp statusVTP Version                     : 2Configuration Revision          : 0Maximum VLANs supported locally : 64Number of existing VLANs        : 7VTP Operating Mode              : ServerVTP Domain Name                 : RouterSimVTP Pruning Mode                : DisabledSwitchB#sh vtp statusVTP Version                     : 2Configuration Revision          : 1Maximum VLANs supported locally : 64

interface fastethernet 0/1.1  encapsulation dot1q 1  ip address 192.168.1.65 255.255.255.192interface fastethernet 0/1.10  encapsulation dot1q 10  ip address 192.168.1.129 255.255.255.224

Internet

fa0/11234HostA

HostB

HostC

Port 1: dot1q trunkPorts 2, 3: VLAN 1Port 4: VLAN 10

10089.book  Page 603  Monday, July 23, 2007  3:17 PM




604Chapter9 Virtual LANs (VLANs)Number of existing VLANs        : 7VTP Operating Mode              : ServerVTP Domain Name                 : GlobalNet

VTP Pruning Mode                : DisabledA.One of the switches needs to be set to VTP version 1.B.Both switches are set to VTP server and one must be set to client.C.The VTP domain names are not configured correctly.D.VTP pruning is disabled.17.Which of the following provide inter-switch VLAN communication? (Choose two.)A.ISLB.VTPC.802.1QD.802.3Z18.To configure the VLAN trunking protocol to communicate VLAN information between two switches, what two requirements must be met? (Choose two.)A.Each end of the trunk link must be set to the IEEE 802.1e encapsulation.B.The VTP management domain name of both switches must be set the same.C.All ports on both the switches must be set as access ports.D.One of the two switches must be configured as a VTP server.E.A rollover cable is required to connect the two switches together.F.A router must be used to forward VTP traffic between VLANs.19.Which of the following are benefits of VLANs? (Choose three.)A.They increase the size of collision domains.B.They allow logical grouping of users by function.C.They can enhance network security.D.They increase the size of broadcast domains while decreasing the number of collision domains.E.They simplify switch administration.F.They increase the number of broadcast domains while decreasing the size of the broadcast domains.20.Which of the following modes are valid when a switch port is used as a VLAN trunk? (Choose three.)A.BlockingB.Dynamic autoC.Dynamic desirableD.NonegotiateE.AccessF.Learning

10089.book  Page 604  Monday, July 23, 2007  3:17 PM




Answers to Review Questions605Answers to Review Questions1.D. Switches do not propagate VLAN information by default; you must configure the VTP domain. VLAN Trunk Protocol (VTP) is used to propagate VLAN information across a trunked link.2.B, E, F. A router connected to a switch that provides inter-VLAN communication is config-ured using subinterfaces. The switch port connected to the router must be using either ISL or 802.1Q trunking protocol, and the hosts are all connected as access ports, which is the default on all switch ports.3.C. Although you can use either 100Mbps or 1Gbps Ethernet, the 100Mbps is necessary at a minimum and is the best answer to this question. You need to trunk the link from the switch to the router to make this connection work with inter-VLAN communication.4.D. By creating and implementing VLANs in your switched network, you can break up broad-cast domains at layer 2. For hosts on different VLANs to communicate, you must have a router or layer 3 switch.5.C, D. Cisco has a proprietary trunking protocol called ISL. The IEEE version is 802.1Q.6.A. By default, all VLANs are allowed on the trunk link and you must remove by hand each VLAN that you don’t want traversing the trunked link.7.C. Virtual LANs break up broadcast domains in layer 2 switched internetworks.8.C. Only in server mode can you change VTP information on a switch.9.F. If you are on a 2950 switch, then the interface command is just switchport mode trunk, since the 2950 can only run the IEEE 802.1Q version. However, a 3550 can run both ISL and 802.1Q, so you must use the encapsulation command. The argument to choose 802.1Q for a trunking protocol is dot1q.10.A. All Cisco switches are VTP servers by default. No other VTP information is configured on a Cisco switch by default. You must set the VTP domain name on all switches to be the same domain name or they will not share the VTP database.11.B. Virtual Trunk Protocol (VTP) is used to pass a VLAN database to any or all switches in the switched network. The three VTP modes are server, client, and transparent.12.D. To set a switch port to trunk mode, which allows all VLAN information to pass down the link, use the switchport mode trunk command.13.C. 802.1Q was created to allow trunked links between disparate switches.14.D. This question is a little vague, but the best answer is that the VLAN membership for the port is not configured.15.A, C, F. To create a trunked link on a switch port, you first need to go to the interface—in this question, interface FastEthernet 0/1. Then you choose your trunking command, either switchport mode trunk for the 2950 (IEEE 802.1Q is the only version the 2960 switch runs) or switchport trunk encapsulation dot1q for a 3560 switch.

10089.book  Page 605  Monday, July 23, 2007  3:17 PM




606Chapter9 Virtual LANs (VLANs)16.C. Although one of the switches can be set to client, that would not stop them from sharing VLAN information through VTP. However, they will not share VLAN information through VTP if the domain names are not set the same.17.A, C. ISL is a Cisco-proprietary frame-tagging method. IEEE 802.1Q is the nonproprietary version of frame tagging.18.B, D. You must have the same VTP domain name on all switches in order to share VLAN information between the switches. At least one of the switches must be a VTP server; the other switches should be set to VTP client.19.B, C, F. VLANs break up broadcast domains in a switched layer 2 network, which means smaller broadcast domains. They allow configuration by logical function instead of physical location and can create some security if configured correctly.20.B, C, D. The valid modes of a VLAN trunk on a switch are dynamic auto, dynamic desirable, trunk (on), and nonegotiate.

10089.book  Page 606  Monday, July 23, 2007  3:17 PM




Answers to Written Lab 9.1607Answers to Written Lab 9.11.Client2.Inter-Switch Link (ISL)3.Broadcast4.Collision5.Server6.Trunking allows you to make a single port part of multiple VLANs at the same time.7.Frame identification (frame tagging) uniquely assigns a user-defined ID to each frame. This is sometimes referred to as a VLAN ID or color.8.True9.Access link10.Inter-Switch Link (ISL)

10089.book  Page 607  Monday, July 23, 2007  3:17 PM




10089.book  Page 608  Monday, July 23, 2007  3:17 PM




 

Chapter 10 Security

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Identify security threats to a network and describe general methods to mitigate those threats  Describe today's increasing network security threats and explain the need to implement a comprehensive security policy to mitigate the threats   Explain general methods to mitigate common security threats to network devices, hosts, and applications   Describe the functions of common security appliances and applications   Describe security recommended practices including initial steps to secure network devices   Configure, verify, and troubleshoot basic router operation and routing on Cisco devices  Implement basic router security   Implement, verify, and troubleshoot NAT and ACLs in a medium-size Enterprise branch office network  Describe the purpose and types of ACLs    Configure and apply ACLs based on network filtering requirements (including: CLI/SDM)    Configure and apply an ACLs to limit telnet and SSH access to the router using (including: SDM/CLI)    Verify and monitor ACLs in a network environment   Troubleshoot ACL issues 

 

10089.book  Page 609  Monday, July 23, 2007  3:17 PM




 If you’re a sys admin, it’s my guess that shielding sensitive, crit-ical data, as well as your network’s resources, from every possi-ble evil exploit is a top priority of yours. Right? Good to know you’re on the right page—Cisco has some really effective security solutions that will arm you with the tools you need to make this happen.Basically, covering you and your network’s posterior is going to be the focus of this chapter. You’ll learn a lot about deterring the most common threats to your network’s security with Cisco routers and IOS firewalls that, together, offer quite a powerful, integrated detection package against many types of invasions. I’m going to give you the lowdown on how the Cisco IOS Firewall provides actual security and policy enforcement for both your internal and exter-nal networking needs. And I’ll also show you how to create secure connections to any remote locations you may have living on the fringes, too.Access control lists (ACLs)are an integral part of Cisco’s security solution, and I’ll show you the keys of both simple and advanced access lists that will equip you with the ability to ensure internetwork security as well as how to mitigate most security-oriented network threats.The proper use and configuration of access lists is a vital part of router configuration because access lists are such versatile networking accessories. Contributing mightily to the effi-ciency and operation of your network, access lists give network managers a huge amount of control over traffic flow throughout the enterprise. With access lists, managers can gather basic statistics on packet flow and security policies can be implemented. Sensitive devices can also be protected from unauthorized access.In this chapter, we’ll discuss access lists for TCP/IP and MAC access lists on a layer 2 switch as well as cover some of the tools available to test and monitor the functionality of applied access lists.After we cover the Cisco IOS Firewall and configuring access lists using the CLI, I’ll show how easy it is to do using Cisco’s Security Device Manager (SDM).Although virtual private networks (VPNs) can be an important part of your corporate security, I’ll cover VPNs in Chapter 14, “Wide Area Networks.”

 For up-to-the minute updates for this chapter, please see  www.lammle.com  and/or  www.sybex.com . Perimeter, Firewall, and Internal Routers You see this a lot—typically, in medium to large enterprise networks, the various strategies for security are based on a some recipe of internal and perimeter routers plus firewall devices. 

 

10089.book  Page 610  Monday, July 23, 2007  3:17 PM




 Recognizing Security Threats 611 Internal routers provide additional security to the network by screening traffic to various parts of the protected corporate network, and they do this using access lists. You can see where each of these types of devices are found in Figure 10.1. FIGURE10.1 A typical secured network I’ll use the terms  trusted network  and  untrusted network  throughout this chapter and in Chapter 11, “Network Address Translation (NAT),” so it’s important that you can see where they are found in a typical secured network. The demilitarized zone (DMZ) can be global (real) Internet addresses or private addresses, depending on how you configure your firewall, but this is typically where you’ll find the HTTP, DNS, email, and other Internet-type corporate servers.Instead of having routers, we can (as you already know) use virtual local area networks (VLANs) with switches on the inside trusted network. Multilayer switches containing their own security features can sometimes replace internal (LAN) routers to provide higher perfor-mance in VLAN architectures.Let’s discuss the security threats a typical secured internetwork faces; then I’ll provide some ways of protecting the internetwork using the Cisco IOS Firewall feature set and access lists. Recognizing Security Threats Yes, it’s true: Security attacks vary considerably in their complexity and threat level, and some even happen because of WUI, or witless user ignorance. (This term isn’t an exam objective, but it does occur more than you’d think!)You see, it all comes down to planning, or rather, lack thereof. Basically, the vital tool that the Internet has become today was absolutely unforeseen by those who brought it into being. This is a big reason why security is now such an issue—most IP implementations are innately 

Perimeter (premises) router 

Firewall Internal (local network) router 

Internet MailserverUntrusted network Corporate  trusted) network DMZ Web server 

 

10089.book  Page 611  Monday, July 23, 2007  3:17 PM




 612 Chapter10  Security insecure. No worries though, because Cisco has a few tricks up its sleeve to help us with this. But first, let’s examine some common attack profiles: Application-layer attacks These attacks commonly zero in on well-known holes in the software that’s typically found running on servers. Favorite targets include FTP, sendmail, and HTTP. Because the permissions level granted to these accounts is most often “privileged,” bad guys simply access and exploit the machine that’s running one of the applications I just mentioned. Autorooters You can think of these as a kind of hacker automaton. Bad guys use something called a rootkit to probe, scan, and then capture data on a strategically positioned computer that’s poised to give them “eyes” into entire systems—automatically! Backdoors These are simply paths leading into a computer or network. Through simple invasions, or via more elaborate “Trojan horse” code, bad guys can use their implanted inroads into a specific host or even a network whenever they want to—until you detect and stop them, that is! Denial of service (DoS) and distributed denial of service (DDoS) attacks These are bad—pretty tough to get rid of too! But even hackers don’t respect other hackers that execute them because, though nasty, they’re really easy to accomplish. (This means that some 10-year-old could actually bring you to your knees, and that is just wrong!) Basically, a service is made unavailable by overwhelming the system that normally provides it. And there are several different flavors: TCP SYN flood Begins when a client initiates a seemingly run-of-the-mill TCP connec-tion and sends a SYN message to a server. The server predictably responds by sending a SYN-ACK message back to the client machine, which then establishes the connection by returning an ACK message. Sounds fine, but it’s actually during this process—when the connection is only halfway open—that the victim machine is literally flooded with a deluge of half-open connections and pretty much becomes paralyzed. “Ping of death” attacks You probably know that TCP/IP’s maximum packet size is 65,536 octets. It’s okay if you didn’t know that—just understand that this attack is executed by simply pinging with oversized packets, causing a device to keep rebooting incessantly, freeze up, or just totally crash. Tribe Flood Network (TFN) and Tribe Flood Network 2000 (TFN2K) These nasty little numbers are more complex in that they initiate synchronized DoS attacks from multiple sources and can target multiple devices. This is achieved, in part, by something known as “IP spoofing,” which I’ll be describing soon. Stacheldraht This attack is actually a mélange of methods, and it translates from the German term for barbed wire. It basically incorporates TFN and adds a dash of encryption. It all begins with a huge invasion at the root level, followed up with a DoS attack finale. IP spoofing This is pretty much what it sounds like it is—a bad guy from within or outside of your network masquerades as a trusted host machine by doing one of two things: presenting with an IP address that’s inside your network’s scope of trusted addresses or using an approved, trusted external IP address. Because the hacker’s true identity is veiled behind the spoofed address, this is often just the beginning of your problems.

 

10089.book  Page 612  Monday, July 23, 2007  3:17 PM




 Recognizing Security Threats 613 Man-in-the-middle attacks Interception! But it’s not a football, it’s a bunch of your network’s packets—your precious data! A common guilty party could be someone working for your very own ISP using a tool known as a sniffer (discussed later) and augmenting it with routing and transport protocols. Network reconnaissance Before breaking into a network, hackers often gather all the infor-mation they can about it, because the more they know about the network, the better they can compromise it. They accomplish their objectives through methods like port scans, DNS queries, and ping sweeps. Packet sniffers This is the tool I mentioned earlier, but I didn’t tell you what it is, and it may come as a surprise that it’s actually software. Here’s how it works—a network adapter card is set to promiscuous mode so it will send all packets snagged from the network’s physical layer through to a special application to be viewed and sorted out. A packet sniffer can nick some highly valuable, sensitive data including, but not limited to, passwords and usernames, making them prized among identity thieves. Password attacks These come in many flavors, and even though they can be achieved via more sophisticated types of attacks like IP spoofing, packet sniffing, and Trojan horses, their sole purpose is to—surprise—discover user passwords so the thief can pretend they’re a valid user and then access that user’s privileges and resources. Brute force attack Another software-oriented attack that employs a program running on a targeted network that tries to log in to some type of shared network resource like a server. For the hacker, it’s ideal if the accessed accounts have a lot of privileges because then the bad guys can form back doors to use for gaining access later and bypass the need for passwords entirely. Port redirection attacks This approach requires a host machine the hacker has broken into and uses to get wonky traffic (that normally wouldn’t be allowed passage) through a firewall. Trojan horse attacks and viruses These two are actually pretty similar—both Trojan horses and viruses infect user machines with malicious code and mess it up with varying degrees of paralysis, destruction, even death! But they do have their differences—viruses are really just nasty programs attached to command.com, which just happens to be the main interpreter for all Windows systems. Viruses then run amok, deleting files and infecting any flavor of command.com it finds on the now diseased machine. The difference between a virus and a Trojan horse is that Trojans are actually complete applications encased inside code that makes them appear to be a completely different entity—say, a simple, innocent game—than the ugly implements of destruction they truly are! Trust exploitation attacks These happen when someone exploits a trust relationship inside your network. For example, a company’s perimeter network connection usually shelters important things like SMTP, DNS, and HTTP servers, making the servers really vulnerable because they’re all on the same segment.To be honest, I’m not going to go into detail on how to mitigate each and every one of the security threats I just talked about, not only because that would be outside the scope of this book, but also because the methods I am going to teach you will truly protect you from being attacked in general. You will learn enough tricks to make all but the most determined bad guys give up on you and search for easier prey. So basically, think of this as a chapter on how to practice “safe networking.”

 

10089.book  Page 613  Monday, July 23, 2007  3:17 PM




 614 Chapter10  Security Mitigating Security Threats Hmm…what solution should we use to mitigate security threats? Something from Juniper, McAfee, or some other firewall product? Nah—we probably should use something from Cisco. Cisco has a very cool product called the Adaptive Security Appliance, or ASA. But there’s a catch or two—it’s a pretty pricey little beauty that scales in cost depending on the modules you choose (for example, intrusion prevention). Plus, the ASA is actually above the objectives of this book. I just personally think is the best product on the market—it truly rocks!Cisco IOS software runs on upwards of 80 percent of the Internet backbone routers out there; it’s probably the most critical part of network infrastructure. So let’s just keep it real and use the Cisco IOS’s software-based security, known as the Cisco IOS Firewall feature set, for our end-to-end Internet, intranet, and remote-access network security solutions. It’s a good idea to go with this because Cisco ACLs really are quite efficient tools for mitigating many of the most common threats around—and if you just happen to be studying for your CCNA exam, you need to solidly understand how ACLs work more than anything else in this chapter! Cisco’s IOS Firewall Here’s where we’re going to find out how to mitigate some of the more common security threats on the list I gave you earlier by using these Cisco IOS Firewall features: Stateful IOS Firewall inspection engine This is your perimeter protection feature because it gives your internal users secure access control on a per-application basis. People often call it Context-Based Access Control (CBAC). Intrusion detection A deep packet inspection tool that lets you monitor, intercept, and respond to abuse in real time by referencing 102 of the most common attack and intrusion detection signatures. Firewall voice traversal An application-level feature based on the protocol’s understanding of call flow as well as the relevant open channels. It supports both the H.323v2 and Session Initiation Protocol (SIP) voice protocols. ICMP   inspection Basically permits responses to ICMP packets like ping and traceroute that come from inside your firewall while denying other ICMP traffic. Authentication   proxy A feature that makes users authenticate any time they want to access the network’s resources through HTTP, HTTPS, FTP, and Telnet. It keeps personal network access profiles for users and automatically gets them for you from a RADIUS or TACACS+ server and applies them as well. Destination   URL   policy   management A buffet of features that’s commonly referred to as URL Filtering. Per-user firewalls These are basically personalized, user-specific, downloadable firewalls obtained through service providers. You can also get personalized ACLs and other settings via AAA server profile storage.

 

10089.book  Page 614  Monday, July 23, 2007  3:17 PM




 Introduction to Access Lists 615 Cisco IOS router and firewall provisioning Allows for no-touch router provisioning, version updates, and security policies. Denial of service (DoS) detection and prevention A feature that checks packet headers and drops any packets it finds suspicious. Dynamic port mapping A sort of adapter that permits applications supported by firewalls on nonstandard ports. Java applet blocking Protects you from any strange, unrecognized Java applets. Basic and Advanced Traffic Filtering You can use standard, extended, even dynamic ACLs like Lock-and-Key traffic filtering with Cisco’s IOS Firewall. And you get to apply access controls to any network segment you want. Plus, you can specify the exact kind of traffic you want to allow to pass through any segment. Policy-based, multi-interface support Allows you to control user access by IP address and interface depending on your security policy. Network Address Translation (NAT) Conceals the internal network from the outside, increasing security. (I’ll talk a lot about NAT in Chapter 11.) Time-based access lists Determine security policies based upon the exact time of day and the particular day of the week. Peer router authentication Guarantees that routers are getting dependable routing infor-mation from actual, trusted sources. (For this to work, you need a routing protocol that supports authentication, like RIPv2, EIGRP, or OSPF.)Now that you’ve been briefed on security threats, relevant features of the Cisco IOS Fire-wall, and how to use that software to your advantage, let’s dive deep into the world of access lists and learn how to use ACLs to mitigate security threats. They really are powerful tools, so pay attention! Introduction to Access Lists An  access list  is essentially a list of conditions that categorize packets. They can be really helpful when you need to exercise control over network traffic. An access list would be your tool of choice for decision making in these situations.One of the most common and easiest to understand uses of access lists is filtering unwanted packets when implementing security policies. For example, you can set them up to make very specific decisions about regulating traffic patterns so that they’ll allow only certain hosts to access web resources on the Internet while restricting others. With the right combination of access lists, network managers arm themselves with the power to enforce nearly any security policy they can invent.

 

10089.book  Page 615  Monday, July 23, 2007  3:17 PM




 616 Chapter10  Security Access lists can even be used in situations that don’t necessarily involve blocking packets. For example, you can use them to control which networks will or won’t be advertised by dynamic routing protocols. How you configure the access list is the same. The difference here is simply how you apply it—to a routing protocol instead of an interface. When you apply an access list in this way, it’s called a distribute list, and it doesn’t stop routing advertisements, it just controls their content. You can also use access lists to categorize packets for queuing or QoS-type services and for controlling which types of traffic can activate a pricey ISDN link.Creating access lists is really a lot like programming a series of  if-then  statements—if a given condition is met, then a given action is taken. If the specific condition isn’t met, nothing happens and the next statement is evaluated. Access-list statements are basically packet filters that packets are compared against, categorized by, and acted upon accordingly. Once the lists are built, they can be applied to either inbound or outbound traffic on any interface. Applying an access list causes the router to analyze every packet crossing that interface in the specified direction and take the appropriate action.There are a few important rules that a packet follows when it’s being compared with an access list:  It’s always compared with each line of the access list in sequential order—that is, it’ll always start with the first line of the access list, then go to line 2, then line 3, and so on.  It’s compared with lines of the access list only until a match is made. Once the packet matches the condition on a line of the access list, the packet is acted upon and no further comparisons take place.  There is an implicit “deny” at the end of each access list—this means that if a packet doesn’t match the condition on any of the lines in the access list, the packet will be discarded.Each of these rules has some powerful implications when filtering IP packets with access lists, so keep in mind that creating effective access lists truly takes some practice.There are two main types of access lists: Standard access lists These use only the source IP address in an IP packet as the condition test. All decisions are made based on the source IP address. This means that standard access lists basically permit or deny an entire suite of protocols. They don’t distinguish between any of the many types of IP traffic such as web, Telnet, UDP, and so on. Extended access lists Extended access lists can evaluate many of the other fields in the layer 3 and layer 4 headers of an IP packet. They can evaluate source and destination IP addresses, the protocol field in the Network layer header, and the port number at the Trans-port layer header. This gives extended access lists the ability to make much more granular deci-sions when controlling traffic. Named access lists Hey, wait a minute—I said there were two types of access lists but listed three! Well, technically there really are only two since  named access lists  are either standard or extended and not actually a new type. I’m just distinguishing them because they’re created and referred to differently than standard and extended access lists, but they’re functionally the same.

 We will look at these types of access lists in more depth later in the chapter.

 

10089.book  Page 616  Monday, July 23, 2007  3:17 PM




 Introduction to Access Lists 617 Once you create an access list, it’s not really going to do anything until you apply it. Yes, they’re there on the router, but they’re inactive until you tell that router what to do with them. To use an access list as a packet filter, you need to apply it to an interface on the router where you want the traffic filtered. And you’ve got to specify which direction of traffic you want the access list applied to. There’s a good reason for this—you may want different controls in place for traffic leaving your enterprise destined for the Internet than you’d want for traffic coming into your enterprise from the Internet. So, by specifying the direction of traffic, you can—and frequently you’ll need to—use different access lists for inbound and outbound traffic on a single interface: Inbound access lists When an access list is applied to inbound packets on an interface, those packets are processed through the access list before being routed to the outbound interface. Any packets that are denied won’t be routed because they’re discarded before the routing process is invoked. Outbound access lists When an access list is applied to outbound packets on an interface, those packets are routed to the outbound interface and then processed through the access list before being queued.There are some general access-list guidelines that should be followed when you’re creating and implementing access lists on a router:  You can assign only one access list per interface per protocol per direction. This means that when creating IP access lists, you can have only one inbound access list and one out-bound access list per interface.

 When you consider the implications of the implicit deny at the end of any access list, it makes sense that you can’t have multiple access lists applied on the same interface in the same direction for the same protocol. That’s because any packets that don’t match some condition in the first access list would be denied and there wouldn’t be any packets left over to compare  against a second access list.  Organize your access lists so that the more specific tests are at the top of the access list.  Any time a new entry is added to the access list, it will be placed at the bottom of the list. Using a text editor for access lists is highly suggested. You cannot remove one line from an access list. If you try to do this, you will remove the entire list. It is best to copy the access list to a text editor before trying to edit the list. The only exception is when using named access lists.

You can delete a single line from a named access list. I’ll show this to you shortly.

10089.book  Page 617  Monday, July 23, 2007  3:17 PM




618Chapter10 Security Unless your access list ends with a permit any command, all packets will be discarded if they do not meet any of the list’s tests. Every list should have at least one permit state-ment or it will deny all traffic. Create access lists and then apply them to an interface. Any access list applied to an inter-face without an access list present will not filter traffic. Access lists are designed to filter traffic going through the router. They will not filter traffic that has originated from the router. Place IP standard access lists as close to the destination as possible. This is the reason we don’t really want to use standard access lists in our networks. You cannot put a standard access list close to the source host or network because you can only filter based on source address and nothing would be forwarded. Place IP extended access lists as close to the source as possible. Since extended access lists can filter on very specific addresses and protocols, you don’t want your traffic to traverse the entire network and then be denied. By placing this list as close to the source address as possible, you can filter traffic before it uses up your precious bandwidth.Before I move on to how to configure basic and advanced access lists, let’s discuss how ACLs can be used to mitigate the security threats I discussed earlier in this chapter.Mitigating Security Issues with ACLsHere’s a list of the many security threats you can mitigate with ACLs: IP address spoofing, inbound IP address spoofing, outbound Denial of service (DoS) TCP SYN attacks, blocking external attacks DoS TCP SYN attacks, using TCP Intercept DoS smurf attacks Filtering ICMP messages, inbound Filtering ICMP messages, outbound Filtering tracerouteIt’s generally wise not to allow into a private network any IP packets that contain the source address of any internal hosts or networks—just don’t do it!Here’s a list of rules to live by when configuring ACLs from the Internet to your production network to mitigate security problems: Deny any addresses from your internal networks. Deny any local host addresses (127.0.0.0/8). Deny any reserved private addresses. Deny any addresses in the IP multicast address range (224.0.0.0/4).None of the above addresses should be allowed to enter your internetwork. Okay, finally, let’s get to work on configuring some basic and advanced access lists.

10089.book  Page 618  Monday, July 23, 2007  3:17 PM




Standard Access Lists619Standard Access ListsStandard IP access lists filter network traffic by examining the source IP address in a packet. You create a standard IP access list by using the access-list numbers 1–99 or 1300–1999 (expanded range). Access-list types are generally differentiated using a number. Based on the number used when the access list is created, the router knows which type of syntax to expect as the list is entered. By using numbers 1–99 or 1300–1999, you’re telling the router that you want to create a standard IP access list, so the router will expect syntax specifying only the source IP address in the test lines.The following is an example of the many access-list number ranges that you can use to filter traffic on your network (the protocols for which you can specify access lists depend on your IOS version):

Corp(config)#access-list ?  <1-99>            IP standard access list  <100-199>         IP extended access list  <1100-1199>       Extended 48-bit MAC address access list  <1300-1999>       IP standard access list (expanded range)  <200-299>         Protocol type-code access list  <2000-2699>       IP extended access list (expanded range)  <700-799>         48-bit MAC address access list  compiled          Enable IP access-list compilation  dynamic-extended  Extend the dynamic ACL absolute timer

  rate-limit        Simple rate-limit specific access listLet’s take a look at the syntax used when creating a standard access list:

Corp(config)#access-list 10 ?  deny    Specify packets to reject  permit  Specify packets to forward

  remark  Access list entry commentAs I said, by using the access-list numbers 1–99 or 1300–1999, you’re telling the router that you want to create a standard IP access list.After you choose the access-list number, you need to decide whether you’re creating a permit or deny statement. For this example, you will create a deny statement:

Corp(config)#access-list 10 deny ?            Hostname or A.B.C.D  Address to match  any                  Any source host

  host                 A single host addressThe next step requires a more detailed explanation. There are three options available. You can use the any parameter to permit or deny any host or network, you can use an IP address 

10089.book  Page 619  Monday, July 23, 2007  3:17 PM




620Chapter10 Securityto specify either a single host or a range of them, or you can use the host command to specify a specific host only. The any command is pretty obvious—any source address matches the statement, so every packet compared against this line will match. The host command is rel-atively simple. Here’s an example using it:

Corp(config)#access-list 10 deny host ?  Hostname or A.B.C.D  Host address

Corp(config)#access-list 10 deny host 172.16.30.2This tells the list to deny any packets from host 172.16.30.2. The default parameter is host. In other words, if you type access-list 10 deny 172.16.30.2, the router assumes you mean host 172.16.30.2.But there’s another way to specify either a particular host or a range of hosts—you can use wildcard masking. In fact, to specify any range of hosts, you have to use wildcard masking in the access list.What’s wildcard masking? You’ll learn all about it using a standard access list example, as well as how to control access to a virtual terminal, in the following sections.Wildcard MaskingWildcards are used with access lists to specify an individual host, a network, or a certain range of a network or networks. To understand a wildcard, you need to understand what a block size is; it’s used to specify a range of addresses. Some of the different block sizes available are 64, 32, 16, 8, and 4.When you need to specify a range of addresses, you choose the next-largest block size for your needs. For example, if you need to specify 34 networks, you need a block size of 64. If you want to specify 18 hosts, you need a block size of 32. If you only specify 2 networks, then a block size of 4 would work.Wildcards are used with the host or network address to tell the router a range of available addresses to filter. To specify a host, the address would look like this:

172.16.30.5 0.0.0.0The four zeros represent each octet of the address. Whenever a zero is present, it means that octet in the address must match exactly. To specify that an octet can be any value, the value of 255 is used. As an example, here’s how a /24 subnet is specified with a wildcard:

172.16.30.0 0.0.0.255This tells the router to match up the first three octets exactly, but the fourth octet can be any value.Now, that was the easy part. What if you want to specify only a small range of subnets? This is where the block sizes come in. You have to specify the range of values in a block size. In other words, you can’t choose to specify 20 networks. You can only specify the exact amount as the block size value. For example, the range would have to be either 16 or 32, but not 20.Let’s say that you want to block access to part of the network that is in the range from 172.16.8.0 through 172.16.15.0. That is a block size of 8. Your network number would be 

10089.book  Page 620  Monday, July 23, 2007  3:17 PM




Standard Access Lists621172.16.8.0, and the wildcard would be 0.0.7.255. Whoa! What is that? The 7.255 is what the router uses to determine the block size. The network and wildcard tell the router to start at 172.16.8.0 and go up a block size of eight addresses to network 172.16.15.0.Seriously—it really is easier than it looks—really! I could certainly go through the binary math for you, but no one needs that. Actually, all you have to do is remember that the wildcard is always one number less than the block size. So, in our example, the wildcard would be 7 since our block size is 8. If you used a block size of 16, the wildcard would be 15. Easy, huh?But just in case, we’ll go through some examples to help you nail it. The following example tells the router to match the first three octets exactly but that the fourth octet can be anything:

Corp(config)#access-list 10 deny 172.16.10.0 0.0.0.255The next example tells the router to match the first two octets and that the last two octets can be any value:

Corp(config)#access-list 10 deny 172.16.0.0

  0.0.255.255Try to figure out this next line:

Corp(config)#access-list 10 deny 172.16.16.0 0.0.3.255This configuration tells the router to start at network 172.16.16.0 and use a block size of 4. The range would then be 172.16.16.0 through 172.16.19.0.The following example shows an access list starting at 172.16.16.0 and going up a block size of 8 to 172.16.23.0:

Corp(config)#access-list 10 deny 172.16.16.0 0.0.7.255The next example starts at network 172.16.32.0 and goes up a block size of 16 to 172.16.47.0:

Corp(config)#access-list 10 deny 172.16.32.0 0.0.15.255The next example starts at network 172.16.64.0 and goes up a block size of 64 to 172.16.127.0:

Corp(config)#access-list 10 deny 172.16.64.0 0.0.63.255The last example starts at network 192.168.160.0 and goes up a block size of 32 to 192.168.191.255:

Corp(config)#access-list 10 deny 192.168.160.0 0.0.31.255Here are two more things to keep in mind when working with block sizes and wildcards: Each block size must start at 0 or a multiple of the block size. For example, you can’t say that you want a block size of 8 and then start at 12. You must use 0–7, 8–15, 16–23, etc. For a block size of 32, the ranges are 0–31, 32–63, 64–95, etc. The command any is the same thing as writing out the wildcard 0.0.0.0 255.255.255.255.

10089.book  Page 621  Monday, July 23, 2007  3:17 PM




622Chapter10 Security

Wildcard masking is a crucial skill to master when creating IP access lists. It’s used identically when creating standard and extended IP access lists.Standard Access List ExampleIn this section, you’ll learn how to use a standard access list to stop specific users from gaining access to the Finance department LAN.In Figure 10.2, a router has three LAN connections and one WAN connection to the Inter-net. Users on the Sales LAN should not have access to the Finance LAN, but they should be able to access the Internet and the marketing department. The Marketing LAN needs to access the Finance LAN for application services.FIGURE10.2IP access list example with three LANs and a WAN connectionOn the router in the figure, the following standard IP access list is configured:

Lab_A#config tLab_A(config)#access-list 10 deny 172.16.40.0 0.0.0.255

Lab_A(config)#access-list 10 permit anyIt’s very important to know that the any command is the same thing as saying the following using wildcard masking:

Lab_A(config)#access-list 10 permit 0.0.0.0

  255.255.255.255Since the wildcard mask says that none of the octets are to be evaluated, every address matches the test condition. So this is functionally the same as using the any keyword.

Lab_A

InternetS0/0E0SalesE2MarketingE1Finance

10089.book  Page 622  Monday, July 23, 2007  3:17 PM




Standard Access Lists623At this point, the access list is configured to deny source addresses from the Sales LAN access to the Finance LAN and allow everyone else. But remember, no action will be taken until the access list is applied on an interface in a specific direction. But where should this access list be placed? If you place it as an incoming access list on E0, you might as well shut down the Ethernet interface because all of the Sales LAN devices will be denied access to all networks attached to the router. The best place to apply this access list is on the E1 interface as an outbound list:

Lab_A(config)#int e1

Lab_A(config-if)#ip access-group 10 outThis completely stops traffic from 172.16.40.0 from getting out Ethernet 1. It has no effect on the hosts from the Sales LAN accessing the Marketing LAN and the Internet since traffic to those destinations doesn’t go through interface E1. Any packet trying to exit out E1 will have to go through the access list first. If there were an inbound list placed on E0, then any packet trying to enter interface E0 would have to go through the access list before being routed to an exit interface.Let’s take a look at another example of a standard access list. Figure 10.3 shows an inter-network of two routers with three LANs and one serial WAN connection.FIGURE10.3IP standard access list example 2You want to stop the Accounting users from accessing the Human Resources server attached to the Lab_B router but allow all other users access to that LAN. What standard access list would you create and where would you place it?The real answer is that you should use an extended access list and place it closest to the source, but the question specifies that you should use a standard access list. Standard access lists, by rule of thumb, are placed closest to the destination—in this example, Ethernet 0 out-bound on the Lab_B router. Here is the access list that should be placed on the Lab_B router:

Lab_B#config tLab_B(config)#access-list 10 deny 192.168.10.128 0.0.0.31Lab_B(config)#access-list 10 permit anyLab_B(config)#interface Ethernet 0

Lab_B(config-if)#ip access-group 10 out

Human Resources server192.168.10.222/27Human Resources

Accounting

Lab_A

Lab_B192.168.10.161/27E0E1 192.168.10.129/27E0

10089.book  Page 623  Monday, July 23, 2007  3:17 PM




624Chapter10 SecurityBefore we move on to restricting Telnet access on a router, let’s take a look at one more standard access list example, but it will require some thought. In Figure 10.4 you have a router with four LAN connections and one WAN connection to the Internet.You need to write an access list that will stop access from each of the four LANs shown in the diagram to the Internet. Each of the LANs shows a single host’s IP address, and from that you need to determine the subnet and use wildcards to configure the access list.FIGURE10.4IP standard access list example 3Here is an example of what your answer should look like (starting with the network on E0 and working through to E3):

Router(config)#access-list 1 deny 172.16.128.0 0.0.31.255Router(config)#access-list 1 deny 172.16.48.0 0.0.15.255Router(config)#access-list 1 deny 172.16.192.0 0.0.63.255Router(config)#access-list 1 deny 172.16.88.0 0.0.7.255Router(config)#access-list 1 permit anyRouter(config)#interface serial 0

Router(config-if)#ip access-group 1 outOkay, what would be the purpose of creating this list? If you actually applied this access list on the router, you’d effectively shut down access to the Internet, so what’s the purpose of even having an Internet connection? I wrote this exercise so you can practice how to use block sizes with access lists—which is critical for your success when studying the CCNA objectives.

172.16.144.17/19

172.16.198.94/18

172.16.50.173/20

172.16.92.10/21

S0E3E0E1E2

10089.book  Page 624  Monday, July 23, 2007  3:17 PM




Standard Access Lists625Controlling VTY (Telnet) AccessYou’ll probably have a difficult time trying to stop users from telnetting to a large router because any active interface on a router is fair game for VTY access. You could try to create an extended IP access list that limits Telnet access to every IP address on the router. But if you did that, you’d have to apply it inbound on every interface, and that really wouldn’t scale well to a large router with dozens, even hundreds, of interfaces, would it? Here’s a much better solution: Use a standard IP access list to control access to the VTY lines themselves.Why does this work? Because when you apply an access list to the VTY lines, you don’t need to specify the Telnet protocol since access to the VTY implies terminal access. You also don’t need to specify a destination address since it really doesn’t matter which interface address the user used as a target for the Telnet session. You really only need to control where the user is com-ing from—their source IP address.To perform this function, follow these steps:1.Create a standard IP access list that permits only the host or hosts you want to be able to telnet into the routers.2.Apply the access list to the VTY line with the access-class command.Here is an example of allowing only host 172.16.10.3 to telnet into a router:

Lab_A(config)#access-list 50 permit 172.16.10.3Lab_A(config)#line vty 0 4

Lab_A(config-line)#access-class 50 in

Should You Secure Your Telnet Lines on a Router?You’re monitoring your network and notice that someone has telnetted into your core router by using the show users command. You use the disconnect command and they are discon-nected from the router, but you notice they are back into the router a few minutes later. You are thinking about putting an access list on the router interfaces, but you don’t want to add a lot of latency on each interface since your router is already pushing a lot of packets. You are considering putting an access list on the VTY lines themselves, but not having done this before, you are not sure if this is a safe alternative to putting an access list on each interface. Is putting an access list on the VTY lines a good idea for this network?Yes, absolutely, and the access-class command illustrated in this section is the best way to do this. Why? Because it doesn’t use an access list that just sits on an interface looking at every packet that is coming and going. This can cause overhead on the packets trying to be routed.When you put the access-class command on the VTY lines, only packets trying to telnet into the router will be looked at and compared. This provides nice, easy-to-configure security for your router.

10089.book  Page 625  Monday, July 23, 2007  3:17 PM




626Chapter10 SecurityBecause of the implied deny any at the end of the list, the access list stops any host from telnetting into the router except the host 172.16.10.3, regardless of which individual IP address on the router is used as a target.

Cisco recommends that you use Secure Shell (SSH) instead of Telnet on the VTY lines of a router. See Chapter 4 for more information on SSH and how to configure SSH on your routers and switches.Extended Access ListsIn the standard IP access list example earlier, notice how you had to block all access from the Sales LAN to the finance department. What if you needed Sales to gain access to a certain server on the Finance LAN but not to other network services, for security reasons? With a standard IP access list, you can’t allow users to get to one network service and not another. Said another way, when you need to make decisions based on both source and destination addresses, a standard access list won’t allow you to do that since it only makes decisions based on source address.But an extended access list will hook you up. That’s because extended access lists allow you to specify source and destination address as well as the protocol and port number that identify the upper-layer protocol or application. By using extended access lists, you can effectively allow users access to a physical LAN and stop them from accessing specific hosts—or even specific services on those hosts.Here’s an example of an extended IP access list:

Corp(config)#access-list ?    <1-99>            IP standard access list  <100-199>         IP extended access list  <1100-1199>       Extended 48-bit MAC address access list  <1300-1999>       IP standard access list (expanded range)  <200-299>         Protocol type-code access list  <2000-2699>       IP extended access list (expanded range)  <700-799>         48-bit MAC address access list  compiled          Enable IP access-list compilation  dynamic-extended  Extend the dynamic ACL absolute timer

  rate-limit        Simple rate-limit specific access listThe first command shows the access-list numbers available. You’ll use the extended access-list range from 100 to 199. Be sure to notice that the range 2000–2699 is also available for extended IP access lists.

10089.book  Page 626  Monday, July 23, 2007  3:17 PM




Extended Access Lists627At this point, you need to decide what type of list entry you are making. For this example, you’ll choose a deny list entry.

Corp(config)#access-list 110 ?  deny     Specify packets to reject  dynamic  Specify a DYNAMIC list of PERMITs or DENYs  permit   Specify packets to forward

  remark   Access list entry commentOnce you choose the access-list type, you then need to select a protocol field entry.

Corp(config)#access-list 110 deny ?  <0-255>  An IP protocol number  ahp      Authentication Header Protocol  eigrp    Cisco's EIGRP routing protocol  esp      Encapsulation Security Payload  gre      Cisco's GRE tunneling  icmp     Internet Control Message Protocol  igmp     Internet Gateway Message Protocol  ip       Any Internet Protocol  ipinip   IP in IP tunneling  nos      KA9Q NOS compatible IP over IP tunneling  ospf     OSPF routing protocol  pcp      Payload Compression Protocol  pim      Protocol Independent Multicast  tcp      Transmission Control Protocol

  udp      User Datagram Protocol

If you want to filter by Application layer protocol, you have to choose the appropriate layer 4 transport protocol after the permit or deny statement. For example, to filter Telnet or FTP, you choose TCP since both Telnet and FTP use TCP at the Transport layer. If you were to choose IP, you wouldn’t be allowed to specify a specific application protocol later.Here, you’ll choose to filter an Application layer protocol that uses TCP by selecting TCP as the protocol. You’ll specify the specific TCP port later. Next, you will be prompted for the source IP address of the host or network (you can choose the any command to allow any source address):

Corp(config)#access-list 110 deny tcp ?  A.B.C.D  Source address  any      Any source host

  host     A single source host

10089.book  Page 627  Monday, July 23, 2007  3:17 PM




628Chapter10 SecurityAfter the source address is selected, the destination address is chosen:

Corp(config)#access-list 110 deny tcp any ?  A.B.C.D  Destination address  any      Any destination host  eq       Match only packets on a given port number  gt       Match only packets with a greater port number  host     A single destination host  lt       Match only packets with a lower port number  neq      Match only packets not on a given port number

  range    Match only packets in the range of port numbersIn the following example, any source IP address that has a destination IP address of 172.16.30.2 has been denied.

Corp(config)#access-list 110 deny tcp any host 172.16.30.2 ?  ack          Match on the ACK bit  dscp         Match packets with given dscp value  eq           Match only packets on a given port number  established  Match established connections  fin          Match on the FIN bit  fragments    Check non-initial fragments  gt           Match only packets with a greater port number  log          Log matches against this entry  log-input    Log matches against this entry, including input interface  lt           Match only packets with a lower port number  neq          Match only packets not on a given port number  precedence   Match packets with given precedence value  psh          Match on the PSH bit  range        Match only packets in the range of port numbers  rst          Match on the RST bit  syn          Match on the SYN bit  time-range   Specify a time-range  tos          Match packets with given TOS value  urg          Match on the URG bit

  <cr> You can press Enter here and leave the access list as is. But if you do that, all TCP traffic to host 172.16.30.2 will be denied, regardless of destination port. You can be even more spe-cific: once you have the host addresses in place, just specify the type of service you are denying. 

10089.book  Page 628  Monday, July 23, 2007  3:17 PM




Extended Access Lists629The following help screen shows you the available options. You can choose a port number or use the application or protocol name:

Corp(config)#access-list 110 deny tcp any host 172.16.30.2 eq ?  <0-65535>    Port number  bgp          Border Gateway Protocol (179)  chargen      Character generator (19)  cmd          Remote commands (rcmd, 514)  daytime      Daytime (13)  discard      Discard (9)  domain       Domain Name Service (53)  drip         Dynamic Routing Information Protocol (3949)  echo         Echo (7)  exec         Exec (rsh, 512)  finger       Finger (79)  ftp          File Transfer Protocol (21)  ftp-data     FTP data connections (20)  gopher       Gopher (70)  hostname     NIC hostname server (101)  ident        Ident Protocol (113)  irc          Internet Relay Chat (194)  klogin       Kerberos login (543)  kshell       Kerberos shell (544)  login        Login (rlogin, 513)  lpd          Printer service (515)  nntp         Network News Transport Protocol (119)  pim-auto-rp  PIM Auto-RP (496)  pop2         Post Office Protocol v2 (109)  pop3         Post Office Protocol v3 (110)  smtp         Simple Mail Transport Protocol (25)  sunrpc       Sun Remote Procedure Call (111)  syslog       Syslog (514)  tacacs       TAC Access Control System (49)  talk         Talk (517)  telnet       Telnet (23)  time         Time (37)  uucp         Unix-to-Unix Copy Program (540)  whois        Nicname (43)

  www          World Wide Web (HTTP, 80)

10089.book  Page 629  Monday, July 23, 2007  3:17 PM




630Chapter10 SecurityAt this point, let’s block Telnet (port 23) to host 172.16.30.2 only. If the users want to FTP, fine—that’s allowed. The log command is used to log messages every time the access list is hit. This can be an extremely cool way to monitor inappropriate access attempts. Here is how to do this:

Corp(config)#access-list 110 deny tcp any host 172.16.30.2 eq 23 logYou need to keep in mind that the next line is an implicit deny any by default. If you apply this access list to an interface, you might as well just shut the interface down since by default there is an implicit deny all at the end of every access list. You’ve got to follow up the access list with the following command:

Corp(config)#access-list 110 permit ip any anyRemember, the 0.0.0.0 255.255.255.255 is the same command as any, so the command could look like this:

Corp(config)#access-list 110 permit ip 0.0.0.0 255.255.255.255

0.0.0.0 255.255.255.255Once the access list is created, you need to apply it to an interface (it’s the same command as the IP standard list):

Corp(config-if)#ip access-group 110 inOr this:

Corp(config-if)#ip access-group 110 outIn the following section, we’ll look at an example of how to use an extended access list.Extended Access List Example 1Using Figure 10.2 from the IP standard access list example earlier, let’s use the same network and deny access to a host at 172.16.30.5 on the Finance department LAN for both Telnet and FTP services. All other services on this and all other hosts are acceptable for the sales and mar-keting departments to access.The following access list should be created:

Lab_A#config tLab_A(config)#access-list 110 deny tcp any host  172.16.30.5 eq 21Lab_A(config)#access-list 110 deny tcp any host  172.16.30.5 eq 23

Lab_A(config)#access-list 110 permit ip any any

10089.book  Page 630  Monday, July 23, 2007  3:17 PM




Extended Access Lists631The access-list 110 tells the router you are creating an extended IP access list. The tcp is the protocol field in the Network layer header. If the list doesn’t say tcp here, you cannot filter by port numbers 21 and 23 as shown in the example. (These are FTP and Telnet, and they both use TCP for connection-oriented services.) The any command is the source, which means any IP address, and the host is the destination IP address.

Remember that instead of the host 172.16.30.5 command when we created the extended access list, we could have entered 172.16.30.5 0.0.0.0 instead and there would be no difference in the result—other than the router would change the command to host 172.16.30.5 in the running-config.After the list is created, it needs to be applied to the Ethernet 1 interface outbound. This applies the policy we created to all hosts and effectively blocks all FTP and Telnet access to 172.16.30.5 from outside the local LAN. If this list were created to only block access from the Sales LAN, then we’d have put this list closer to the source, or on Ethernet interface 0. So, in this situation, we’d apply the list to inbound traffic.Let’s go ahead and apply the list to interface E1 and block all outside FTP and Telnet access to the host:

Lab_A(config-if)#ip access-group 110 outExtended Access List Example 2In this example, we’ll again use Figure 10.4, which has four LANs and a serial connection. What we need to do is stop Telnet access to the networks attached to the Ethernet 1 and Ether-net 2 interfaces. If we only used one access list, it would not be a very effective one because of the latency that will be caused on the Ethernet 1 and 2 interfaces (since every packet going out these interfaces must be looked at), but if we used two lists, the latency could be less on each interface if configured correctly. However, since we’re studying the CCNA objectives, we’re going to look at this with only one access list.The configuration on the router would look something like this, although the answer can vary:

Router(config)#access-list 110 deny tcp any 172.16.48.0 0.0.15.255eq 23Router(config)#access-list 110 deny tcp any 172.16.192.0 0.0.63.255eq 23Router(config)#access-list 110 permit ip any anyRouter(config)#interface Ethernet 1Router(config-if)#ip access-group 110 outRouter(config-if)#interface Ethernet 2

Router(config-if)#ip access-group 110 out

10089.book  Page 631  Monday, July 23, 2007  3:17 PM




632Chapter10 SecurityThe important information that you need to understand from this list is as follows: First, you need to verify that the number range is correct for the type of access list you are creating—in this example it’s extended, so the range must be 100–199. Second, you need to verify that the pro-tocol field matches the upper-layer process or application—in this example, port 23 (Telnet).The protocol parameter must be TCP since Telnet uses TCP. If the question stated to use TFTP, for example, then the protocol parameter would have to be UDP since TFTP uses UDP. Third, verify that the destination port number matches the application you are filtering for—in this case, port 23 matches Telnet, which is correct. Finally, the test statement permit ip any any is important to have at the end of the list to enable all packets other than Telnet pack-ets destined for the LANs connected to Ethernet 1 and Ethernet 2.Advanced Access ListsIn this section, I am going to show you some more advanced ways to use access lists. Most of the advanced access-list topics are beyond the objectives of this book, so I’ll just discuss them briefly and you can find more information on the Cisco website if you are interested.However, with that said, there are some important access-list options that you need to know, and the first one is named access lists.Named ACLsAs I said earlier, named access lists are just another way to create standard and extended access lists. In medium to large enterprises, management of access lists can become, well, a real hassle over time. For example, when you need to make a change to an access list, a frequent practice is to copy the access list to a text editor, change the number, edit the list, then paste the new list back into the router. With this done, you can simply change the access-list number on the interface from the old one to the new one and there will never be a time on the network where an access list isn’t in place.This would work pretty well if it weren’t for what I call “packrat” mentality. The question becomes, What do I do with the old access list? Delete it? Or should I save it in case I find a problem with the new list and need to back out of the change? What happens is that over time—through this and countless other scenarios—you can end up with a whole bunch of unapplied access lists building up on a router. What were they for? Are they important? Do I need them? All good questions, and named access lists could be your answer.This can also apply to access lists that are up and running. Let’s say you come into an existing network and are looking at access lists on a router. Suppose you find an access list 177 (which is an extended access list) that is 33 lines long. This could cause you much needless existential questioning—What is it for? Why is it here? Instead, wouldn’t an access list called, say, finance LAN be more descriptive than one that’s named 177?Named access lists allow you to use names to both create and apply either standard or extended access lists. There is nothing new or different about these access lists aside from being able to refer to them in a way that makes sense to humans. But there are some subtle changes 

10089.book  Page 632  Monday, July 23, 2007  3:17 PM




Advanced Access Lists633to the syntax, so let’s re-create the standard access list we created earlier for our test network in Figure 10.2 using a named access list:

Lab_A#config tEnter configuration commands, one per line.  End with CNTL/Z.Lab_A(config)#ip access-list ?  extended  Extended Acc  logging   Control access list logging

  standard  Standard Access ListNotice that I started by typing ip access-list, not access-list. This allows me to enter a named access list. Next, I’ll need to specify that it’s to be a standard access list:

Lab_A(config)#ip access-list standard ?  <1-99>  Standard IP access-list number  WORD    Access-list nameLab_A(config)#ip access-list standard BlockSales

Lab_A(config-std-nacl)#I’ve specified a standard access list, then added a name: BlockSales. Notice that I could’ve used a number for a standard access list, but instead, I chose to use a descriptive name. Also, notice that after entering the name, I hit Enter and the router prompt changed. I’m now in named access list configuration mode and entering the named access list:

Lab_A(config-std-nacl)#?Standard Access List configuration commands:  default  Set a command to its defaults  deny     Specify packets to reject  exit     Exit from access-list configuration mode  no       Negate a command or set its defaults  permit   Specify packets to forwardLab_A(config-std-nacl)#deny 172.16.40.0 0.0.0.255Lab_A(config-std-nacl)#permit anyLab_A(config-std-nacl)#exitLab_A(config)#^Z

Lab_A#I enter the access list, and then exit out of configuration mode. Next, I’ll take a look at the running configuration to verify that the access list is indeed in the router:

Lab_A#show running-config

10089.book  Page 633  Monday, July 23, 2007  3:17 PM




634Chapter10 Security!ip access-list standard BlockSales deny   172.16.40.0 0.0.0.255 permit any

!The BlockSales access list has truly been created and is in the running-config of the router. Next, I’ll need to apply the access list to an interface:

Lab_A#config tEnter configuration commands, one per line.  End with CNTL/Z.Lab_A(config)#int e1Lab_A(config-if)#ip access-group BlockSales outLab_A(config-if)#^Z

Lab_A#All right! At this point, we’ve re-created the work done earlier using a named access list.Switch Port ACLsOkay, now remember this: You can only apply port ACLs to layer 2 interfaces on your switches. Why? Because they’re only supported on physical layer 2 interfaces. Another good thing to keep in mind is that you can apply them as only inbound lists on your interfaces, and you can use only named lists as well.Here’s a short list of supported access lists: As I mentioned, standard IP access lists use only source addresses to filter traffic. On the other hand, extended IP access lists use both source and destination addresses as well as optional protocol information and port numbers. There are also MAC extended access lists that use source and destination MAC addresses and optional protocol type information.Switches scrutinize all inbound ACLs applied to a certain interface and decide to allow traffic through depending on whether the traffic is a good match to the ACL or not. So I’m sure you can see how important ACLs are to security—they’re gatekeepers with the power to permit or deny access to a special segment of your network, or all of it—period!ACLs can also be used to control traffic on VLANs. To make this happen, you just need to apply a port ACL to a trunk port. But do beware—if you do this on a port that has a voice VLAN, that ACL will actually filter your data VLANs too. So tread carefully here!Port ACLs control IP traffic via IP access lists. Any non-IP traffic is filtered through the use of MAC addresses. And even though you can apply both types of filter to a single interface, you only get to apply one of each. If you try to put an additional ACL of either type on an inter-face that’s already got them in place, the new one will override the one you had there before. So it’s a good idea to be careful here—look before you leap.

10089.book  Page 634  Monday, July 23, 2007  3:17 PM




Advanced Access Lists635Let’s check out the access list:

S1#config tS1(config)#mac access-list ?  extended  Extended Access ListS1(config)#mac access-list extended ?  WORD  access-list nameS1(config)#mac access-list extended Todd_MAC_ListS1(config-ext-macl)#deny ?  H.H.H  48-bit source MAC address  any    any source MAC address  host   A single source hostS1(config-ext-macl)#deny any ?  H.H.H  48-bit destination MAC address  any    any destination MAC address  host   A single destination hostS1(config-ext-macl)#deny any host ?  H.H.H  48-bit destination MAC addressS1(config-ext-macl)#deny any host 000d.29bd.4b85S1(config-ext-macl)#permit ?  H.H.H  48-bit source MAC address  any    any source MAC address  host   A single source hostS1(config-ext-macl)#permit any anyS1(config-ext-macl)#do show access-listExtended MAC access list Todd_MAC_List    deny   any host 000d.29bd.4b85    permit any any

S1(config-ext-macl)#All right—you can see that you can only create an extended named access list. You have no other options. And don’t forget to add the permit any any at the end!Here is how you would apply the list to a switch port:

S1(config-ext-macl)#int f0/6    

S1(config-if)#mac access-group Todd_MAC_List inThis is pretty much the same as it is with an IP list, except that you start with the command mac.But do you really want to deny MAC addresses? Doesn’t this sound like a bad hangover before you even start? While it’s true there are special circumstances where you would, there 

10089.book  Page 635  Monday, July 23, 2007  3:17 PM




636Chapter10 Securityis another option, and I think it’s usually the better one: Just deny access based on the ether-type field in the Ethernet frame header instead. Take a look:

S1(config-ext-macl)#deny any any ?  <0-65535>     An arbitrary EtherType in decimal, hex, or octal  aarp          EtherType: AppleTalk ARP  amber         EtherType: DEC-Amber  appletalk     EtherType: AppleTalk/EtherTalk  cos           CoS value  dec-spanning  EtherType: DEC-Spanning-Tree  decnet-iv     EtherType: DECnet Phase IV  diagnostic    EtherType: DEC-Diagnostic  dsm           EtherType: DEC-DSM  etype-6000    EtherType: 0x6000  etype-8042    EtherType: 0x8042  lat           EtherType: DEC-LAT  lavc-sca      EtherType: DEC-LAVC-SCA  lsap          LSAP value  mop-console   EtherType: DEC-MOP Remote Console  mop-dump      EtherType: DEC-MOP Dump  msdos         EtherType: DEC-MSDOS  mumps         EtherType: DEC-MUMPS  netbios       EtherType: DEC-NETBIOS  vines-echo    EtherType: VINES Echo  vines-ip      EtherType: VINES IP  xns-idp       EtherType: XNS IDP

  <cr>  It’s cool—I know. But don’t go nuts here and start denying all ether-type numbers available or you’ll end up with some issues that will make you regret it. But who actually uses DecNet and AppleTalk? They definitely deserve to be denied access to your beautiful, well-oiled net-work, don’t they?If you read Chapter 1, you would know that if you blocked 0x800, you’d block all of IP, right? This could come in handy in the future if you decided you wanted to force everyone to run IPv6. But for now, just don’t go there!Lock and Key (Dynamic ACLs)This flavor of ACL depends on either remote or local Telnet authentication in combination with extended ACLs.Before you can configure a dynamic ACL, you need to apply an extended ACL on your router to stop the flow of traffic through it. The only way anyone can get through the blockade 

10089.book  Page 636  Monday, July 23, 2007  3:17 PM




Advanced Access Lists637is if they telnet to the router and gain authentication. It works like this: The Telnet connection the user initiated gets dropped and is replaced with a single-entry dynamic ACL that’s appended to the extended ACL already in place. This causes traffic to be allowed through for a specific amount of time, and as you may have guessed, time-outs can and do happen.Reflexive ACLsThese ACLs filter IP packets depending upon upper-layer session information, and they often permit outbound traffic to pass but place limitations on inbound traffic. You can’t define reflex-ive ACLs with numbered or standard IP ACLs, or any other protocol ACLs for that matter. They can be used along with other standard or static extended ACLs, but they’re only defined with extended named IP ACLs—that’s it.Time-Based ACLsTime-based ACLs work a lot like extended ACLs do, but their type of access control is totally time oriented. Basically, you specify a certain time of day and week and then identify that par-ticular period by giving it a name referenced by a task. So, by necessity, the reference function will fall under whatever time constraints you’ve dictated. The time period is based upon the router’s clock, but I highly recommend using it in conjunction with Network Time Protocol (NTP) synchronization.Here’s an example:

Corp#config tCorp(config)#time-range no-httpCorp(config-time-range)#periodic we?Wednesday  weekdays  weekend Corp(config-time-range)#periodic weekend ?  hh:mm  Starting timeCorp(config-time-range)#periodic weekend 06:00 to 12:00Corp(config-time-range)#exit             Corp(config)#time-range tcp-yesCorp(config-time-range)#periodic weekend 06:00 to 12:00Corp(config-time-range)#exitCorp(config)#ip access-list extended TimeCorp(config-ext-nacl)#deny tcp any any eq www time-range no-httpCorp(config-ext-nacl)#permit tcp any any time-range tcp-yesCorp(config-ext-nacl)#interface f0/0Corp(config-if)#ip access-group Time inCorp(config-if)#do show time-rangetime-range entry: no-http (inactive)   periodic weekdays 8:00 to 15:00   used in: IP ACL entry

10089.book  Page 637  Monday, July 23, 2007  3:17 PM




638Chapter10 Securitytime-range entry: tcp-yes (inactive)   periodic weekend 8:00 to 13:00   used in: IP ACL entry

Corp(config-if)#The time-range command is pretty flexible and will drive users crazy if you deny them basic network access or access to the Internet during off-hours. Be careful with the preceding commands—make sure you test your list on a nonproduction network before you implement the lists on your production network.RemarksThis is the tool you grab to use the remark keyword, and it’s really important because it arms you with the ability to include comments, or rather remarks, regarding the entries you’ve made in both your IP standard and extended ACLs. Remarks are very cool because they efficiently increase your ability to examine and understand your ACLs to the super-hero level. Without them, you’d be caught in a quagmire of meaningless numbers without anything to help you recall what those numbers mean.Even though you have the option of placing your remarks either before or after a permit or deny statement, I totally recommend that you chose to position them consistently so you don’t get confused about which remark is relevant to which one of your permit or deny statements.To get this going for both standard and extended ACLs, just use the access-list access-list number remark remark global configuration command. And if you want to get rid of a remark, just use the command’s no form.Let’s take a look at an example of how to use the remark command:

R2#config tR2(config)#access-list 110 remark Permit Bob from Sales Only To FinanceR2(config)#access-list 110 permit ip host 172.16.10.1 172.16.20.0 0.0.0.255 R2(config)#access-list 110 deny ip 172.16.10.0 0.0.0.255172.16.20.0 0.0.0.255        R2(config)#ip access-list extended No_TelnetR2(config-ext-nacl)#remark Deny all of Sales from Telnettingto Marketing             R2(config-ext-nacl)#deny tcp 172.16.30.0 0.0.0.255172.16.40.0 0.0.0.255 eq 23        R2(config-ext-nacl)#permit ip any anyR2(config-ext-nacl)#do show run[output cut]!ip access-list extended No_Telnet remark Stop all of Sales from Telnetting to Marketing deny   tcp 172.16.30.0 0.0.0.255 172.16.40.0 0.0.0.255 eq telnet permit ip any any

10089.book  Page 638  Monday, July 23, 2007  3:17 PM




Advanced Access Lists639!access-list 110 remark Permit Bob from Sales Only To Financeaccess-list 110 permit ip host 172.16.10.1 172.16.20.0 0.0.0.255access-list 110 deny   ip 172.16.10.0 0.0.0.255 172.16.20.0 0.0.0.255

!I was able to add a remark to both an extended list and a named access list. However, you cannot see these remarks in the output of the show access-list command, only in the running-config. I’ll show you this command again when we configure ACLs using the SDM.Context-Based Access Control (Cisco IOS Firewall)You’ve got to have the Cisco IOS Firewall set in the IOS to make use of Context-Based Access Control (CBAC), and the funny thing is, it’s rare to hear someone—even Cisco—differentiate between the two. People usually just refer to the Cisco IOS Firewall and leave it at that. But what is it? Well, the CBAC’s job is to scrutinize any and all traffic that’s attempting to come through the firewall so it can find out about and control the state information for TCP and UDP sessions. And it uses that very information it’s gathered to determine whether to create a temporary pathway into the firewall’s access lists.To make this happen, just configure ip inspect lists in the same direction the traffic is flow-ing. If you don’t do this, any return traffic won’t be able to get back through, which will nega-tively impact any session connections originating from inside the internal network in a big way.Take a look at Figure 10.5, which illustrates in a very simple way how the Cisco IOS Fire-wall (CBAC) works.FIGURE10.5Cisco IOS Firewall (CBACe) example

Inside interface  Outside interface  Inside ACL, permitsinside trusted traffic.Outside ACL, by default,denies all traffic inbound tothe interface.Traffic flow direction 

1 

2 

3 4 

Inspect process 

10089.book  Page 639  Monday, July 23, 2007  3:17 PM




640Chapter10 SecurityA router that’s configured with the Cisco IOS Firewall will process traffic in the fol-lowing manner:1.First, if the inside ACL approves, the router will get all inside packets sent to it.2.Next, the approved traffic is subjected to the firewall’s ip inspect process, which adds the approved connection’s state information into the state table.3.Finally, the traffic passes through the IP inspect process, which then creates a dynamic ACL entry and puts it into the outside ACL so that the return traffic will be allowed to pass back through the router.4.I’ll demonstrate this in a bit when I create a firewall using the SDM.Authentication ProxyI have this set on all of my routers, but to be able to do that you must also have the Cisco IOS Firewall feature set up. I have the configuration set up this way because the authentication proxy is a good thing to have on your side. This is true because it authenticates inbound users, outbound users, or both. Those who would normally be blocked by an ACL can just bring up a browser to get through the firewall and then authenticate on a TACACS+ or RADIUS server.Monitoring Access ListsAgain, it’s always good to be able to verify a router’s configuration. Table 10.1 lists the commands that can be used to verify the configuration.TABLE10.1Commands Used to Verify Access List ConfigurationCommandEffectshow access-listDisplays all access lists and their parameters configured on the router. This command does not show you which interface the list is set on.show access-list 110Shows only the parameters for the access list 110. This command does not show you the interface the list is set on.show ip access-listShows only the IP access lists configured on the router.show ip interfaceShows which interfaces have access lists set.show running-configShows the access lists and which interfaces have access lists set.Show mac access-groupDisplays MAC access lists applied to all layer 2 interfaces or the specified layer 2 interface (used on layer 2 switches only).

10089.book  Page 640  Monday, July 23, 2007  3:17 PM




Monitoring Access Lists641We’ve already used the show running-config command to verify that a named access list was in the router as well as a MAC access list on a layer 2 switch. So now let’s take a look at the output from some of the other commands.The show access-list command will list all access lists on the router, whether they’re applied to an interface or not:

Lab_A#show access-listStandard IP access list 10    deny   172.16.40.0, wildcard bits 0.0.0.255    permit anyStandard IP access list BlockSales    deny   172.16.40.0, wildcard bits 0.0.0.255    permit anyExtended IP access list 110    deny tcp any host 172.16.30.5 eq ftp    deny tcp any host 172.16.30.5 eq telnet    permit ip any any

Lab_A#First, notice that both access list 10 and our named access list appear on this list. Second, notice that even though I entered actual numbers for TCP ports in access list 110, the show command gives us the protocol names rather than TCP ports for readability. (Hey, not every-one has them all memorized!)Here’s the output of the show ip interface command:

Lab_A#show ip interface e1Ethernet1 is up, line protocol is up  Internet address is 172.16.30.1/24  Broadcast address is 255.255.255.255  Address determined by non-volatile memory  MTU is 1500 bytes  Helper address is not set  Directed broadcast forwarding is disabled  Outgoing access list is BlockSales  Inbound  access list is not set  Proxy ARP is enabled  Security level is default  Split horizon is enabled  ICMP redirects are always sent  ICMP unreachables are always sent  ICMP mask replies are never sent  IP fast switching is disabled

10089.book  Page 641  Monday, July 23, 2007  3:17 PM




642Chapter10 Security  IP fast switching on the same interface is disabled  IP Null turbo vector  IP multicast fast switching is disabled  IP multicast distributed fast switching is disabled  Router Discovery is disabled  IP output packet accounting is disabled  IP access violation accounting is disabled  TCP/IP header compression is disabled  RTP/IP header compression is disabled  Probe proxy name replies are disabled  Policy routing is disabled  Network address translation is disabled  Web Cache Redirect is disabled  BGP Policy Mapping is disabled

Lab_A#Be sure to notice the bold line indicating that the outgoing list on this interface is BlockSales but the inbound access list isn’t set. One more verification command and then we’ll move on to using the SDM to configure firewall security.As I’ve already mentioned, you can use the show running-config command to see any and all access lists. However, on a layer 2 switch, you can verify your interface configurations with the show mac access-group command:

S1#sh mac access-groupInterface FastEthernet0/1:   Inbound access-list is not set   Outbound access-list is not setInterface FastEthernet0/2:   Inbound access-list is not set   Outbound access-list is not set

S1#Depending on how many interfaces you set your MAC access lists on, you can use the interface command to view individual interfaces:

S1#sh mac access-group interface f0/6Interface FastEthernet0/6:   Inbound access-list is Todd_MAC_List

   Outbound access-list is not setLet’s discuss how SDM can provide security on our networks.

10089.book  Page 642  Monday, July 23, 2007  3:17 PM




Configuring Access Lists Using SDM643Configuring Access Lists Using SDMI’ll start this section by showing you how to create an access list using the SDM, and then I’ll use the firewall wizards to add the Cisco IOS Firewall, which creates ACLs (which access lists are huge part of, as you’ll see). It’s much easier to just use the wizards. I’ll show you both ways, but by using the wizards, you really don’t need to do much at all but click a few Next buttons to make a good secure router.Creating ACLs with SDMLet’s begin by simply creating an ACL using the SDM. You can only create named access lists. Let’s take a look.Obviously, your first step in using SDM is to open it. Next, just click Configure   Firewall and ACL and you’ll get the Create Firewall screen.

10089.book  Page 643  Monday, July 23, 2007  3:17 PM




644Chapter10 SecurityNext, click on the Edit Firewall Policy/ACL tab.On the top, choose the From and To interfaces from the drop-down menus. I already have cho-sen s0/0/0 as my From interface and s0/2/0 as my To interface. Then in the middle of the page, click +Add, which provides a pull-down menu. Choose Add New and the following screen appears.

10089.book  Page 644  Monday, July 23, 2007  3:17 PM




Configuring Access Lists Using SDM645I’ve already configured the list that’s denying telnet (23) to Wireless Host C (WHC) from any host coming in the s0/0/0 interface, and I’ve also chosen to log matches. I’m going to click OK and then create a permit statement so I don’t shut my router’s interface down.A very cool thing about creating lists through the SDM is that the +Add menu asks if you want to create a new test statement and place it before or after the line you’ve already got in the list. This is great because by using the SDM, you can quickly and efficiently edit your ACLs!

10089.book  Page 645  Monday, July 23, 2007  3:17 PM




646Chapter10 SecurityNext, I’m going to create a simple permit ip any statement.After clicking OK, I received the main screen again, which shows me my list. I can easily add, delete, and manage my ACL form here. Very sweet indeed!

10089.book  Page 646  Monday, July 23, 2007  3:17 PM




Configuring Access Lists Using SDM647Let’s see what the router has in the running-config now:

!ip access-list extended sdm_serial0/0/0_in remark SDM_ACL Category=1 remark Deny Telnet to WHC deny   tcp any host 10.1.12.2 eq telnet log permit ip any any log!!interface Serial0/0/0 description 1st Connection to R1$FW_INSIDE$ ip address 10.1.2.1 255.255.255.0

 ip access-group sdm_serial0/0/0_in inLooks good—let’s try to telnet to host 10.1.12.2 and see what shows up on the Corp console:

Corp#*May 14 17:34:36.503: %SEC-6-IPACCESSLOGP: list sdm_serial0/0/0_in denied tcp

 10.1.2.2(30491) -> 10.1.12.2(23), 1 packetOkay—now I’m going to telnet to host 10.1.12.1:

*May 14 17:34:53.023: %SEC-6-IPACCESSLOGP: list sdm_serial0/0/0_in permitted  tcp 10.1.2.2(16774) -> 10.1.12.1(23), 1 packet

Corp#Nice! Packets entering s0/0/0 with a destination IP address of host 10.1.12.2 and with a destination of port 23 are being denied, but notice that when I telnetted to host 10.1.12.1, it was allowed.With all that behind us, I’m going to show you how easy all of this is using the SDM, and make my router even more secure in the process.Creating Firewalls with SDMThis section will show you how to use both the Basic and Advanced Firewall Wizards to set up the Cisco IOS Firewall software. It really is your best option when configuring security on your routers.Click Configure   Firewall and ACL, and you’ll get a wizard to lead you along the way to creating a Firewall rule. But as I mentioned, there are two wizards to choose from:Basic FirewallIf your goal is to connect your network that’s composed only of host machines, no servers, or servers that you don’t want to communicate with the outside world (e.g., the Internet), this is your wizard. To illustrate this fact further, after you’ve chosen Basic Firewall, a little diagram will come into view at the right. Next, just click Launch and voilà!

10089.book  Page 647  Monday, July 23, 2007  3:17 PM




648Chapter10 SecurityAdvanced FirewallThis is your go-to wizard for connecting a network that’s populated with both host machines and servers in which your servers need to access outside hosts like those found on the Internet. Again, after you’ve chosen this wizard, you’ll see the little diagram at the right depicting your network. So basically, with any network that includes web, email, or other servers that need to communicate via the Internet, choose Advanced Firewall, then just click Launch.The first screen is the Create Firewall screen.From here, I’m choosing to connect up and create a basic firewall. Then I clicked the Launch the Selected Task button. The next screen tells me what the Basic Firewall Configu-ration Wizard is going to do.

10089.book  Page 648  Monday, July 23, 2007  3:17 PM




Configuring Access Lists Using SDM649Wow—it looks like it is going to do a whole lot, and you know what? It will! I then just click Next, and from this next screen I am able to choose my inside and outside addresses—trusted and untrusted interfaces.Once I finish choosing my inside and outside interfaces, I click Next and a ton of access-list test statements are applied to my router—nice!

10089.book  Page 649  Monday, July 23, 2007  3:17 PM




650Chapter10 SecurityThen I click Finish, and it asks me if I want my configured routing protocol through the out-side interface.I click OK. (Trust me, you may not want to do this in a production network.) I then receive a “You’ve successfully configured firewall on your router” screen. Am I dreaming or did this really work? Well, let’s see. After clicking OK, I get this screen.Wow, looks that way. It took just a few clicks to set up a firewall on my router! But I really want to see what it put in my running-config. Let’s check it out:

Corp#sh runBuilding configuration...[output cut]!

10089.book  Page 650  Monday, July 23, 2007  3:17 PM




Configuring Access Lists Using SDM651ip inspect name SDM_LOW cuseemeip inspect name SDM_LOW dnsip inspect name SDM_LOW ftpip inspect name SDM_LOW h323ip inspect name SDM_LOW httpsip inspect name SDM_LOW icmpip inspect name SDM_LOW imapip inspect name SDM_LOW pop3ip inspect name SDM_LOW netshowip inspect name SDM_LOW rcmdip inspect name SDM_LOW realaudioip inspect name SDM_LOW rtspip inspect name SDM_LOW esmtpip inspect name SDM_LOW sqlnetip inspect name SDM_LOW streamworksip inspect name SDM_LOW tftpip inspect name SDM_LOW tcpip inspect name SDM_LOW udpip inspect name SDM_LOW vdolive!

[output cut]So what the Basic Firewall Configuration Wizard added was the IOS firewall that’s also called CBAC, or Context-Based Access Control. Cisco just calls it the IOS Firewall now—something I told you earlier.The ip inspect command turned on basic application inspection for mitigating attacks to each individual application.Notice that each of these inspect statements adds a separate protocol to the list. Without the wizard, you get to manually pick the protocols you want to deal with, but using the SDM wizard makes it so much easier, it’s still the better way to go. The Basic Firewall Configuration Wizard lim-its you to two interfaces. Check out the interface it was applied to:

!interface Serial0/2/0 description Connection to R3$FW_OUTSIDE$ ip address 64.1.1.5 255.255.255.252 ip access-group 103 in ip verify unicast reverse-path ip nat outside ip inspect SDM_LOW out  ip virtual-reassembly clock rate 2000000

!

10089.book  Page 651  Monday, July 23, 2007  3:17 PM




652Chapter10 SecurityThe ip inspect SDM_LOW out is where the inspect process was applied. Let’s take a look at rest of the configuration that SDM threw into the mix:

!access-list 100 remark auto generated by SDM firewall configurationaccess-list 100 remark SDM_ACL Category=1access-list 100 deny   ip 10.1.3.0 0.0.0.255 anyaccess-list 100 deny   ip 64.1.1.4 0.0.0.3 anyaccess-list 100 deny   ip 10.1.4.0 0.0.0.255 anyaccess-list 100 deny   ip host 255.255.255.255 anyaccess-list 100 deny   ip 127.0.0.0 0.255.255.255 anyaccess-list 100 permit ip any anyaccess-list 101 remark auto generated by SDM firewall configurationaccess-list 101 remark SDM_ACL Category=1access-list 101 deny   ip 10.1.2.0 0.0.0.255 anyaccess-list 101 deny   ip 64.1.1.4 0.0.0.3 anyaccess-list 101 deny   ip 10.1.4.0 0.0.0.255 anyaccess-list 101 deny   ip host 255.255.255.255 anyaccess-list 101 deny   ip 127.0.0.0 0.255.255.255 anyaccess-list 101 permit ip any anyaccess-list 102 remark auto generated by SDM firewall configurationaccess-list 102 remark SDM_ACL Category=1access-list 102 deny   ip 10.1.3.0 0.0.0.255 anyaccess-list 102 deny   ip 10.1.2.0 0.0.0.255 anyaccess-list 102 deny   ip 64.1.1.4 0.0.0.3 anyaccess-list 102 deny   ip host 255.255.255.255 anyaccess-list 102 deny   ip 127.0.0.0 0.255.255.255 any

access-list 102 permit ip any anyNotice that ACLs 100–102 are inbound on an inside interface. These ACLs define the traffic that’s allowed out and also what’s allowed to pass through the inspect firewall. Each ACL denies all the networks that the router knows about as well as the loopback address, leaving only the net-work that’s attached to this interface with permission to be a source address coming into the inter-face. Let’s take a look at the last list the SDM created:

access-list 103 remark auto generated by SDM firewall configurationaccess-list 103 remark SDM_ACL Category=1access-list 103 deny   ip 10.1.3.0 0.0.0.255 anyaccess-list 103 deny   ip 10.1.2.0 0.0.0.255 anyaccess-list 103 deny   ip 10.1.4.0 0.0.0.255 anyaccess-list 103 permit icmp any host 64.1.1.5 echo-replyaccess-list 103 permit icmp any host 64.1.1.5 time-exceeded

10089.book  Page 652  Monday, July 23, 2007  3:17 PM




Configuring Access Lists Using SDM653access-list 103 permit icmp any host 64.1.1.5 unreachableaccess-list 103 permit ospf any anyaccess-list 103 deny   ip 10.0.0.0 0.255.255.255 anyaccess-list 103 deny   ip 172.16.0.0 0.15.255.255 anyaccess-list 103 deny   ip 192.168.0.0 0.0.255.255 anyaccess-list 103 deny   ip 127.0.0.0 0.255.255.255 anyaccess-list 103 deny   ip host 255.255.255.255 anyaccess-list 103 deny   ip host 0.0.0.0 anyaccess-list 103 deny   ip any any log

!        You can see that test statement 103 is denying a bit more stuff. First, notice this is your out-side and that every network located on the inside, plus all the private ranges, are denied from entering. List 103 is allowing some ICMP and the OSPF traffic but that’s it. Right now, even if traffic from the trusted interface went out, the return traffic would be denied. But this is what the firewall process is for—when trusted traffic goes out, the state info is put in the table and a dynamic ACL entry is written for this ACL so that the return traffic gets allowed back in instead of locked out!Well, it appears that sometimes dreams really do come true—all I did was click a couple of little buttons, and Shazam! Kind of seems like I could have skipped this chapter, huh. Sorry, but you really need to understand the concepts of access lists, and this chapter gave you some major goods on that stuff!But we still need to take a quick look at the Advanced Firewall Configuration Wizard. After going back to the Firewall and ACL configure button and clicking Advanced Firewall, I received this screen.

10089.book  Page 653  Monday, July 23, 2007  3:17 PM




654Chapter10 SecurityThere are a couple differences between the Advanced Firewall Wizard and the Basic Fire-wall Wizard. The advanced wizard applies the access rules to the inside, outside, and DMZ interfaces, but the Basic Firewall Wizard applies the access rules only to the inside and out-side interfaces—DMZ interfaces are not configured using the basic wizard. Another differ-ence is that the advanced wizard applies inspection rules to all inside, outside, and DMZ interfaces, whereas the basic one applies the inspection rules only to the outside interfaces.The next screen tells me to pick my inside, outside, and DMZ interfaces.Once you do this, you don’t get to access SDM using any of the router’s outside interfaces. The rest of the wizard works pretty much the same way as the basic one does, only you end up with a more advanced configuration on your router—one that’s beyond the objectives of this book. I highly recommend downloading a copy of the SDM (discussed in Chapter 4). Play around with and familiarize yourself with this GUI as much as possible!As I’ve mentioned, the SDM is useful for advanced configurations like ACLs, NAT, and VPNs, and the previous output shows that by just understanding which interfaces are inside your network and which interfaces are outside of it, you can configure ACLs and firewall security like a seasoned pro.SummaryIn this chapter I covered how to configure standard access lists to properly filter IP traffic. You learned what a standard access list is and how to apply it to a Cisco router to add secu-rity to your network. You also learned how to configure extended access lists to further filter IP traffic. I also discussed the differences between standard and extended access lists, as well as how to apply them to Cisco routers.I then moved on to show you how to configure named access lists and apply them to interfaces on the router. Named access lists offer the advantage of being readily identifiable and, therefore, a whole lot easier to manage than access lists that are simply referred to by obscure numbers.

10089.book  Page 654  Monday, July 23, 2007  3:17 PM




Written Lab 10.1655We then covered how to monitor and verify selected access-list operations on both the router and a switch. And we went over some basic monitoring commands to verify both IP and MAC access lists.Last, I showed you how easy it really is to create both ACLs and firewall policies for a router using the SDM. And my friends, I tell you no lie when I say that this is where SDM truly shines!Exam EssentialsRemember the standard and extended IP access-list number ranges.The number ranges you can use to configure a standard IP access list are 1–99 and 1300–1999. The number ranges for an extended IP access list are 100–199 and 2000–2699.Understand the term implicit deny.At the end of every access list is an implicit deny. What this means is that if a packet does not match any of the lines in the access list, then it will be discarded. Also, if you have nothing but deny statements in your list, the list will not permit any packets.Understand the standard IP access-list configuration command.To configure a standard IP access list, use the access-list numbers 1–99 or 1300–1999 in global configuration mode. Choose permit or deny, then choose the source IP address you want to filter on using one of the three techniques covered in this chapter.Understand the extended IP access-list configuration command.To configure an extended IP access list, use the access-list numbers 100–199 or 2000–2699 in global configuration mode. Choose permit or deny, the Network layer protocol field, the source IP address you want to filter on, the destination address you want to filter on, and finally the Transport layer port number (if selected).Remember the command to verify an access list on a router interface.To see whether an access list is set on an interface and in which direction it is filtering, use the show ip interface command. This command will not show you the contents of the access list, merely which access lists are applied on the interface.Remember the command to verify the access-list configuration.To see the configured access lists on your router, use the show access-list command. This command will not show you which interfaces have an access list set.Written Lab 10.1In this section, write the answers to the following questions:1.What command would you use to configure a standard IP access list to prevent all machines on network 172.16.0.0 from accessing your Ethernet network?2.What command would you use to apply the access list you created in question 1 to an Ethernet interface?

10089.book  Page 655  Monday, July 23, 2007  3:17 PM




656Chapter10 Security3.What command would you use to create an access list that denies host 192.168.15.5 access to an Ethernet network?4.Which command verifies that you’ve entered the access list correctly?5.Which two commands verify that the access list was properly applied to the Ethernet interface?6.What command would you use to create an extended access list that stops host 172.16.10.1 from telnetting to host 172.16.30.5?7.What command would you use to set an access list on a VTY line?8.From question 1, write the same standard IP access list as a named access list.9.From question 2, write the command to apply the named access list you created to an interface.10.Which command verifies the placement and direction of an access list?(The answers to Written Lab 10 can be found following the answers to the review questions for this chapter.)Hands-on LabsIn this section, you will complete two labs. To complete these labs, you will need at least three routers. If you are using the RouterSim or Sybex software programs, please use the labs found in those programs.Lab 10.1: Standard IP Access ListsLab 10.2: Extended IP Access ListsAll of the labs will use the following diagram for configuring the routers.

1900

Lab_AF0/27F0/26

F0/0S0/0S0/0DCES0/1DCE

2950

Lab_BF0/3F0/2F0/1

F0/0F0/4F0/5S0/0

2950

Lab_CF0/3F0/2F0/1

F0/0F0/4F0/5

Host_AHost_BHost_C172.16.10.0/24172.16.30.0/24172.16.50.0/24172.16.20.0/24172.16.40.0/24

10089.book  Page 656  Monday, July 23, 2007  3:17 PM




Hands-on Labs657Hands-on Lab 10.1: Standard IP Access ListsIn this lab, you will allow only packets from Host_B from network 172.16.30.0 to enter network 172.16.10.0.1.Go to Lab_A and enter global configuration mode by typing config t.2.From global configuration mode, type access-list ? to get a list of all the different access lists available.3.Choose an access-list number that will allow you to create an IP standard access list. This is a number between 1 and 99 or 1300 and 1399.4.Choose to permit host 172.16.30.2, which is Host_B’s address:Lab_A(config)#access-list 10 permit 172.16.30.2 ?  A.B.C.D  Wildcard bits

  <cr>To specify only host 172.16.30.2, use the wildcards 0.0.0.0:Lab_A(config)#access-list 10 permit 172.16.30.2

  0.0.0.05.Now that the access list is created, you must apply it to an interface to make it work:Lab_A(config)#int f0/0

Lab_A(config-if)#ip access-group 10 out6.Verify your access list with the following commands:Lab_A#sh access-listStandard IP access list 10    permit 172.16.30.2Lab_A#sh run[output cut]interface FastEthernet0/0 ip address 172.16.10.1 255.255.255.0

 ip access-group 10 out7.Test your access list by pinging from Host_B (172.16.30.2) to Host_A (172.16.10.2).8.Ping from Lab_B and Lab_C to Host_A (172.16.10.2); this should fail if your access list is correct.Hands-on Lab 10.2: Extended IP Access ListsIn this lab, you will use an extended IP access list to stop host 172.16.10.2 from creating a Tel-net session to router Lab_B (172.16.20.2). However, the host still should be able to ping the 

10089.book  Page 657  Monday, July 23, 2007  3:17 PM




658Chapter10 SecurityLab_B router. IP extended lists should be placed close to the source, so add the extended list on router Lab_A.1.Remove any access lists on Lab_A and add an extended list to Lab_A.2.Choose a number to create an extended IP list. The IP extended lists use 100–199 or 2000–2699.3.Use a deny statement. (You’ll add a permit statement in step 7 to allow other traffic to still work.)Lab_A(config)#access-list 110 deny ?  <0-255>  An IP protocol number  ahp      Authentication Header Protocol  eigrp    Cisco's EIGRP routing protocol  esp      Encapsulation Security Payload  gre      Cisco's GRE tunneling  icmp     Internet Control Message Protocol  igmp     Internet Gateway Message Protocol  igrp     Cisco's IGRP routing protocol  ip       Any Internet Protocol  ipinip   IP in IP tunneling  nos      KA9Q NOS compatible IP over IP tunneling  ospf     OSPF routing protocol  pcp      Payload Compression Protocol  tcp      Transmission Control Protocol

  udp      User Datagram Protocol4.Since you are going to deny Telnet, you must choose TCP as a Transport layer protocol:Lab_A(config)#access-list 110 deny tcp ?  A.B.C.D  Source address  any      Any source host

  host     A single source host5.Add the source IP address you want to filter on, then add the destination host IP address. Use the host command instead of wildcard bits.Lab_A(config)#access-list 110 deny tcp host  172.16.10.2 host 172.16.20.2 ?  ack          Match on the ACK bit  eq           Match only packets on a given port               number  established  Match established connections  fin          Match on the FIN bit

10089.book  Page 658  Monday, July 23, 2007  3:17 PM




Hands-on Labs659  fragments    Check fragments  gt           Match only packets with a greater               port number  log          Log matches against this entry  log-input    Log matches against this entry,               including input interface  lt           Match only packets with a lower port               number  neq          Match only packets not on a given               port number  precedence   Match packets with given precedence               value  psh          Match on the PSH bit  range        Match only packets in the range of               port numbers  rst          Match on the RST bit  syn          Match on the SYN bit  tos          Match packets with given TOS value  urg          Match on the URG bit

  <cr>6.At this point, you can add the eq telnet command to filter host 172.16.10.2 from tel-netting to 172.16.20.2. The log command can also be used at the end of the command so that whenever the access-list line is hit, a log will be generated on the console.Lab_A(config)#access-list 110 deny tcp host

  172.16.10.2 host 172.16.20.2 eq telnet log7.It is important to add this line next to create a permit statement. (Remember that 0.0.0.0 255.255.255.255 is the same as the any command.)Lab_A(config)#access-list 110 permit ip any 0.0.0.0

  255.255.255.255You must create a permit statement; if you just add a deny statement, nothing will be permitted at all. Please see the sections earlier in this chapter for more detailed informa-tion on the permit command.8.Apply the access list to the FastEthernet0/0 on Lab_A to stop the Telnet traffic as soon as it hits the first router interface.Lab_A(config)#int f0/0Lab_A(config-if)#ip access-group 110 in

Lab_A(config-if)#^Z

10089.book  Page 659  Monday, July 23, 2007  3:17 PM




660Chapter10 Security9.Try telnetting from host 172.16.10.2 to Lab_A using the destination IP address of 172.16.20.2. The following messages should be generated on Lab_A’s console; however, the ping command should work:

From host 172.16.10.2: C:\>telnet 172.16.20.2On Lab_A’s console, this should appear as follows:01:11:48: %SEC-6-IPACCESSLOGP: list 110 denied tcp  172.16.10.2(1030) -> 172.16.20.2(23), 1 packet01:13:04: %SEC-6-IPACCESSLOGP: list 110 denied tcp

  172.16.10.2(1030) -> 172.16.20.2(23), 3 packets

10089.book  Page 660  Monday, July 23, 2007  3:17 PM




Review Questions661Review Questions

The following questions are designed to test your understanding of this chapter's material. For more information on how to get additional ques-tions, please see this book's Introduction.1.Which of the following is an example of a standard IP access list?A.access-list 110 permit host 1.1.1.1B.access-list 1 deny 172.16.10.1 0.0.0.0C.access-list 1 permit 172.16.10.1 255.255.0.0D.access-list standard 1.1.1.12.You need to create an access list that will prevent hosts in the network range of 192.168.160.0 to 192.168.191.0. Which of the following lists will you use?A.access-list 10 deny 192.168.160.0 255.255.224.0B.access-list 10 deny 192.168.160.0 0.0.191.255C.access-list 10 deny 192.168.160.0 0.0.31.255D.access-list 10 deny 192.168.0.0 0.0.31.2553.You have created a named access list called Blocksales. Which of the following is a valid command for applying this to packets trying to enter interface s0 of your router?A.(config)#ip access-group 110 inB.(config-if)#ip access-group 110 inC.(config-if)#ip access-group Blocksales inD.(config-if)#blocksales ip access-list in4.Which of the following are valid ways to refer only to host 172.16.30.55 in an IP access list? (Choose two.)A.172.16.30.55 0.0.0.255B.172.16.30.55 0.0.0.0C.any 172.16.30.55D.host 172.16.30.55E.0.0.0.0 172.16.30.55F.ip any 172.16.30.55

10089.book  Page 661  Monday, July 23, 2007  3:17 PM




662Chapter10 Security5.Which of the following access lists will allow only HTTP traffic into network 196.15.7.0?A.access-list 100 permit tcp any 196.15.7.0 0.0.0.255 eq wwwB.access-list 10 deny tcp any 196.15.7.0 eq wwwC.access-list 100 permit 196.15.7.0 0.0.0.255 eq wwwD.access-list 110 permit ip any 196.15.7.0 0.0.0.255E.access-list 110 permit www 196.15.7.0 0.0.0.2556.What router command allows you to determine whether an IP access list is enabled on a par-ticular interface?A.show ip portB.show access-listsC.show ip interfaceD.show access-lists interface7.Which router command allows you to view the entire contents of all access lists?A.Router#show interfaceB.Router>show ip interfaceC.Router#show access-listsD.Router>show all access-lists8.If you wanted to deny all Telnet connections to only network 192.168.10.0, which command could you use?A.access-list 100 deny tcp 192.168.10.0 255.255.255.0 eq telnetB.access-list 100 deny tcp 192.168.10.0 0.255.255.255 eq telnetC.access-list 100 deny tcp any 192.168.10.0 0.0.0.255 eq 23D.access-list 100 deny 192.168.10.0 0.0.0.255 any eq 239.If you wanted to deny FTP access from network 200.200.10.0 to network 200.199.11.0 but allow everything else, which of the following command strings is valid?A.access-list 110 deny 200.200.10.0 to network 200.199.11.0 eq ftpaccess-list 111 permit ip any 0.0.0.0 255.255.255.255B.access-list 1 deny ftp 200.200.10.0 200.199.11.0 any anyC.access-list 100 deny tcp 200.200.10.0 0.0.0.255 200.199.11.0 0.0.0.255 eq ftpD.access-list 198 deny tcp 200.200.10.0 0.0.0.255 200.199.11.0 0.0.0.255 eq ftpaccess-list 198 permit ip any 0.0.0.0 255.255.255.25510.You want to create a standard access list that denies the subnet of the following host: 172.16.50.172/20. Which of the following would you start your list with?A.access-list 10 deny 172.16.48.0 255.255.240.0B.access-list 10 deny 172.16.0.0 0.0.255.255C.access-list 10 deny 172.16.64.0 0.0.31.255D.access-list 10 deny 172.16.48.0 0.0.15.255

10089.book  Page 662  Monday, July 23, 2007  3:17 PM




Review Questions66311.Which command would you use to apply an access list to a router interface?A.ip access-list 101 outB.access-list ip 101 inC.ip access-group 101 inD.access-group ip 101 in12.You want to create a standard access list that denies the subnet of the following host: 172.16.198.94/19. Which of the following would you start your list with?A.access-list 10 deny 172.16.192.0 0.0.31.255B.access-list 10 deny 172.16.0.0 0.0.255.255C.access-list 10 deny 172.16.172.0 0.0.31.255D.access-list 10 deny 172.16.188.0 0.0.15.25513.You want to create a standard access list that denies the subnet of the following host: 172.16.144.17/21. Which of the following would you start your list with?A.access-list 10 deny 172.16.48.0 255.255.240.0B.access-list 10 deny 172.16.144.0 0.0.7.255C.access-list 10 deny 172.16.64.0 0.0.31.255D.access-list 10 deny 172.16.136.0 0.0.15.25514.Which of the following commands connect access list 110 inbound to interface ethernet0?A.Router(config)#ip access-group 110 inB.Router(config)#ip access-list 110 inC.Router(config-if)#ip access-group 110 inD.Router(config-if)#ip access-list 110 in15.What command will permit SMTP mail to only host 1.1.1.1?A.access-list 10 permit smtp host 1.1.1.1B.access-list 110 permit ip smtp host 1.1.1.1C.access-list 10 permit tcp any host 1.1.1.1 eq smtpD.access-list 110 permit tcp any host 1.1.1.1 eq smtp16.You configure the following access list:access-list 110 deny tcp 10.1.1.128 0.0.0.63 any eq smtpaccess-list 110 deny tcp any eq 23int ethernet 0

ip access-group 110 outWhat will the result of this access list be?A.Email and Telnet will be allowed out E0.B.Email and Telnet will be allowed in E0.C.Everything but email and Telnet will be allowed out E0.D.No IP traffic will be allowed out E0.

10089.book  Page 663  Monday, July 23, 2007  3:17 PM




664Chapter10 Security17.Which of the following series of commands will restrict Telnet access to the router?A.Lab_A(config)#access-list 10 permit 172.16.1.1Lab_A(config)#line con 0Lab_A(config-line)#ip access-group 10 inB.Lab_A(config)#access-list 10 permit 172.16.1.1Lab_A(config)#line vty 0 4Lab_A(config-line)#access-class 10 outC.Lab_A(config)#access-list 10 permit 172.16.1.1Lab_A(config)#line vty 0 4Lab_A(config-line)#access-class 10 inD.Lab_A(config)#access-list 10 permit 172.16.1.1Lab_A(config)#line vty 0 4Lab_A(config-line)#ip access-group 10 in18.Which of the following is true regarding access lists applied to an interface?A.You can place as many access lists as you want on any interface until you run out of memory.B.You can apply only one access list on any interface.C.One access list may be configured, per direction, for each layer 3 protocol configured on an interface.D.You can apply two access lists to any interface.19.You are working on a router that has established privilege levels that restrict access to certain functions. You discover that you are not able to execute the command show running-configuration. How can you view and confirm the access lists that have been applied to the Ethernet 0 interface on your router?A.show access-listsB.show interface Ethernet 0C.show ip access-listsD.show ip interface Ethernet 0

10089.book  Page 664  Monday, July 23, 2007  3:17 PM




Review Questions66520.You want users from the accounting LAN to not have access to the Human Resources server. The following access list has been created:Access-list 10 deny 192.168.10.128 0.0.0.31

Access-list 10 permit anyAccording to the following diagram, which interface of which router, and in which direction, should the access list be placed to prevent accounting users from accessing the network attached to the E0 interface of Lab_B?A.Lab_A, S0 outB.Lab_A, E1 inC.Lab_A, E1 outD.Lab_B, S1 inE.Lab_B, E0 outF.Lab_B, E0 in

Human Resources server192.168.10.222/27Human Resources

Accounting

Lab_A

Lab_B192.168.10.161/27E0E1 192.168.10.129/27E0

10089.book  Page 665  Monday, July 23, 2007  3:17 PM




666Chapter10 SecurityAnswers to Review Questions1.B. Standard IP access lists use the numbers 1–99 and 1300–1999 and filter based on source IP address only. Option C is incorrect because the mask must be in wildcard format.2.C. The range of 192.168.160.0 to 192.168.191.0 is a block size of 32. The network address is 192.168.160.0 and the mask would be 255.255.224.0, which for an access list must be a wildcard format of 0.0.31.255. The 31 is used for a block size of 32. The wildcard is always one less than the block size.3.C. Using a named access list just replaces the number used when applying the list to the router’s interface. ip access-group Blocksales in is correct.4.B, D. The wildcard 0.0.0.0 tells the router to match all four octets. This wildcard format alone can be replaced with the host command.5.A. The first thing to check in a question like this is the access-list number. Right away, you can see that the second option is wrong because it is using a standard IP access-list number. The second thing to check is the protocol. If you are filtering by upper-layer protocol, then you must be using either UDP or TCP; this eliminates the fourth option. The third and last answers have the wrong syntax.6.C. Only the show ip interface command will tell you which interfaces have access lists applied. show access-lists will not show you which interfaces have an access list applied.7.C. The show access-lists command will allow you to view the entire contents of all access lists, but it will not show you the interfaces to which the access lists are applied.8.C. The extended access list ranges are 100–199 and 2000–2699, so the access-list number of 100 is valid. Telnet uses TCP, so the protocol TCP is valid. Now you just need to look for the source and destination address. Only the third option has the correct sequence of parameters. Answer B may work, but the question specifically states “only” to network 192.168.10.0, and the wildcard in answer B is too broad.9.D. Extended IP access lists use numbers 100–199 and 2000–2699 and filter based on source and destination IP address, protocol number, and port number. The last option is correct because of the second line that specifies permit ip any any. (I used 0.0.0.0 255.255.255.255, which is the same as the any option.) The third option does not have this, so it would deny access but not allow everything else.10.D. First, you must know that a /20 is 255.255.240.0, which is a block size of 16 in the third octet. Counting by 16s, this makes our subnet 48 in the third octet, and the wildcard for the third octet would be 15 since the wildcard is always one less than the block size.11.C. To apply an access list, the proper command is ip access-group 101 in.12.A. First, you must know that a /19 is 255.255.224.0, which is a block size of 32 in the third octet. Counting by 32, this makes our subnet 192 in the third octet, and the wildcard for the third octet would be 31 since the wildcard is always one less than the block size.

10089.book  Page 666  Monday, July 23, 2007  3:17 PM




Answers to Review Questions66713.B. First, you must know that a /21 is 255.255.248.0, which is a block size of 8 in the third octet. Counting by eight, this makes our subnet 144 in the third octet, and the wildcard for the third octet would be 7 since the wildcard is always one less than the block size.14.C. To place an access list on an interface, use the ip access-group command in interface configuration mode.15.D. When trying to find the best answer to an access-list question, always check the access-list number and then the protocol. When filtering to an upper-layer protocol, you must use an extended list, numbers 100–199 and 2000–2699. Also, when you filter to an upper-layer pro-tocol, you must use either tcp or udp in the protocol field. If it says ip in the protocol field, you cannot filter to an upper-layer protocol. SMTP uses TCP.16.D. If you add an access list to an interface and you do not have at least one permit statement, then you will effectively shut down the interface because of the implicit deny any at the end of every list.17.C. Telnet access to the router is restricted by using either a standard or extended IP access list inbound on the VTY lines of the router. The command access-class is used to apply the access list to the VTY lines.18.C. A Cisco router has rules regarding the placement of access lists on a router interface. You can place one access list per direction for each layer 3 protocol configured on an interface.19.D. The only command that shows which access lists have been applied to an interface is show ip interface Ethernet 0. The command show access-lists displays all configured access lists, and show ip access-lists displays all configured IP access lists, but neither command indicates whether the displayed access lists have been applied to an interface.20.E. On a standard access list, you should place the access list as close to the destination as possible. In this example, that is “out, Ethernet 0” of the Lab_B router.

10089.book  Page 667  Monday, July 23, 2007  3:17 PM




668Chapter10 SecurityAnswers to Written Lab 10.11.access-list 10 deny 172.16.0.0 0.0.255.255access-list 10 permit any2.ip access-group 10 out3.access-list 10 deny host 192.168.15.5access-list 10 permit any4.show access-lists5.show running-configsh ip interface6.access-list 110 deny tcp host172.16.10.1 host  172.16.30.5 eq 23access-list 110 permit ip any any7.line vty 0 4access-class 110 in8.ip access-list standard No172Netdeny 172.16.0.0 0.0.255.255permit any9.ip access-group No172Net out10.show ip interfaces

10089c10.fm  Page 668  Thursday, August 30, 2007  12:47 PM




 

Chapter 11 Network Address Translation (NAT)

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Implement, verify, and troubleshoot NAT and ACLs in a medium-size Enterprise branch office network  Explain the basic operation of NAT   Configure NAT for given network requirements using (including: CLI/SDM)   Troubleshoot NAT issues 

 

10089c11.fm  Page 669  Thursday, August 30, 2007  12:51 PM




 In this chapter, I am going to give you the skinny on Network Address Translation (NAT), Dynamic NAT, and Port Address Translation (PAT), also known as NAT Overload. Of course, I’ll demonstrate NAT on the internetwork I’ve been using throughout the book, and then I’m going to finish this chapter by using SDM so you can see how NAT can be configured the easy way.It will be helpful for you to read Chapter 10 before reading this chapter since we need to use access lists in our NAT configurations. 

 For up-to-the minute updates for this chapter, please see  www.lammle.com  and/or  www.sybex.com . When Do We Use NAT? Similar to Classless Inter-Domain Routing (CIDR), the original intention for NAT was to slow the depletion of available IP address space by allowing many private IP addresses to be repre-sented by some smaller number of public IP addresses. Since then, it’s been discovered that NAT is also a useful tool for network migrations and mergers, server load sharing, and creating “virtual servers.” So in this chapter, I’m going to describe the basics of NAT functionality and the terminology common to NAT.At times, NAT really decreases the overwhelming amount of public IP addresses required in your networking environment. And NAT comes in really handy when two companies that have duplicate internal addressing schemes merge. NAT is also great to have around when an organization changes its Internet service provider (ISP) and the networking manager doesn’t want the hassle of changing the internal address scheme.Here’s a list of situations when it’s best to have NAT on your side:  You need to connect to the Internet and your hosts don’t have globally unique IP addresses.  You change to a new ISP that requires you to renumber your network.  You need to merge two intranets with duplicate addresses. You typically use NAT on a border router. For an illustration of this, see Figure 11.1.Now you may be thinking, “NAT’s totally cool. It’s the grooviest greatest network gadget and I just gotta have it.” Well, hang on a minute. There are truly some serious snags related to NAT use. Oh—don’t get me wrong: It really can save you sometimes, but there’s a dark side you need to know about, too. For a visual of the pros and cons linked to using NAT, check out Table 11.1.

 

10089c11.fm  Page 670  Thursday, August 30, 2007  12:51 PM




 Types of Network Address Translation 671 FIGURE11.1 Where to configure NAT

 The most obvious advantage associated with NAT is that it allows you to con-serve your legally registered address scheme. This is why we haven’t run out  of IPv4 addresses—think about it. Types of Network Address Translation In this section, I’m going to go over the three types of NAT with you: Static NAT This type of NAT is designed to allow one-to-one mapping between local and global addresses. Keep in mind that the static version requires you to have one real Internet IP address for every host on your network. TABLE11.1 Advantages and Disadvantages of Implementing NAT AdvantagesDisadvantages Conserves legally registered addresses.Translation introduces switching path delays.Reduces address overlap occurrence.Loss of end-to-end IP traceability.Increases flexibility when connecting to Internet.Certain applications will not function with NAT enabled.Eliminates address renumbering as network changes. 

 

10089c11.fm  Page 671  Thursday, August 30, 2007  12:51 PM




 672 Chapter11  Network Address Translation (NAT) Dynamic NAT This version gives you the ability to map an unregistered IP address to a reg-istered IP address from out of a pool of registered IP addresses. You don’t have to statically configure your router to map an inside to an outside address as you would using static NAT, but you do have to have enough real, bona-fide IP addresses for everyone who’s going to be sending packets to and receiving them from the Internet. Overloading This is the most popular type of NAT configuration. Understand that overload-ing really is a form of dynamic NAT that maps multiple unregistered IP addresses to a single reg-istered IP address—many-to-one—by using different ports. Now, why is this so special? Well, because it’s also known as  Port Address Translation (PAT) . And by using PAT (NAT Overload), you get to have thousands of users connect to the Internet using only one real global IP address—pretty slick, yeah? Seriously, NAT Overload is the real reason we haven’t run out of valid IP address on the Internet. Really—I’m not joking.

 Don’t worry, I’ll show you how to configure all three types of NAT toward the  end of this chapter.  NAT Names The names we use to describe the addresses used with NAT are pretty simple. Addresses used after NAT translations are called  global  addresses. These are usually the public addresses used on the Internet, but remember, you don’t need public addresses if you aren’t going on the Internet.  Local  addresses are the ones we use before NAT translation. So, the inside local address is actually the private address of the sending host that’s trying to get to the Internet, while the outside local address is the address of the destination host. The latter is usually a public address (web address, mail server, etc.) and is how the packet begins its journey. After translation, the inside local address is then called the  inside global address  and the outside global address then becomes the name of the destination host. Check out Table 11.2, which lists all this terminology, for a clear picture of the various names used with NAT. TABLE11.2 NAT Terms NamesMeaning Inside local Name of inside source address before translationOutside localName of destination host before translationInside globalName of inside host after translationOutside globalName of outside destination host after translation

 

10089c11.fm  Page 672  Thursday, August 30, 2007  12:51 PM




 How NAT Works 673 How NAT Works Okay, now it’s time to look at how this whole NAT thing works. I’m going to start by using Figure 11.2 to describe the basic translation of NAT. FIGURE11.2 Basic NAT translation In the example shown in Figure 11.2, host 10.1.1.1 sends an outbound packet to the border router configured with NAT. The router identifies the IP address as an inside local IP address destined for an outside network, translates the address, and documents the translation in the NAT table.The packet is sent to the outside interface with the new translated source address. The external host returns the packet to the destination host and the NAT router translates the inside global IP address back to the inside local IP address using the NAT table. This is as simple as it gets.Let’s take a look at a more complex configuration using overloading, or what is also referred to as Port Address Translation (PAT). I’ll use Figure 11.3 to demonstrate how PAT works.With overloading, all inside hosts get translated to one single IP address, hence the term  overloading . Again, the reason we have not run out of available IP addresses on the Internet is because of overloading (PAT).Take a look at the NAT table in Figure 11.3 again. In addition to the inside local IP address and outside global IP address, we now have port numbers. These port numbers help the router identify which host should receive the return traffic.

 

10089c11.fm  Page 673  Thursday, August 30, 2007  12:51 PM




 674 Chapter11  Network Address Translation (NAT) FIGURE11.3 NAT overloading example (PAT) Port numbers are used at the Transport layer to identify the local host in this example. If we had to use IP addresses to identify the source hosts, that would be called  static NAT  and we would run out of addresses. PAT allows us to use the Transport layer to identify the hosts, which in turn allows us to use (theoretically) up to 65,000 hosts with one real IP address. Static NAT Configuration Let’s take a look at a simple basic static NAT configuration:

 ip nat inside source static 10.1.1.1 170.46.2.2!interface Ethernet0 ip address 10.1.1.10 255.255.255.0 ip nat inside!interface Serial0 ip address 170.46.2.1 255.255.255.0 ip nat outside

 ! In the preceding router output, the  ip nat inside source  command identifies which IP addresses will be translated. In this configuration example, the  ip nat inside source   com-mand configures a static   translation between the inside local IP address 10.1.1.1 to the outside global IP address 170.46.2.2.

 

10089c11.fm  Page 674  Thursday, August 30, 2007  12:51 PM




 How NAT Works 675 If we look farther down in the configuration, we see that we have an  ip nat  command under each interface. The  ip nat inside  command identifies that interface as the inside inter-face. The  ip nat outside   command identifies that interface as the outside interface. When you look back at the ip nat inside source command, you see that the command is referencing the inside interface as the source or starting point of the translation. The command could also be used like this— ip nat outside source —which is referencing the interface you desig-nated as the outside interface to be the source or starting point for the translation. Dynamic NAT Configuration Dynamic NAT means that we have a pool of addresses that we will use to provide real IP addresses to a group of users on the inside. We do not use port numbers, so we have to have real IP addresses for every user trying to get outside the local network.Here is a sample output of a dynamic NAT configuration:

 ip nat pool todd 170.168.2.2 170.168.2.254    netmask 255.255.255.0ip nat inside source list 1 pool todd!interface Ethernet0 ip address 10.1.1.10 255.255.255.0 ip nat inside!interface Serial0 ip address 170.168.2.1 255.255.255.0 ip nat outside!access-list 1 permit 10.1.1.0 0.0.0.255

 ! The  ip nat inside source list 1 pool todd  command tells the router to translate IP addresses that match  access-list 1  to an address found in the IP NAT pool named  todd . The access list in this case is not being used to permit or deny traffic as we would use it for secu-rity reasons to filter traffic. It is being used in this case to select or designate what we often call interesting traffic. When interesting traffic has been matched with the access list, it is pulled into the NAT process to be translated. This is a common use for access lists; they don’t always have the dull job of just blocking traffic at an interface.The  ip nat pool todd 170.168.2.2 170.168.2.254  command creates a pool of addresses that will be distributed to those hosts that require NAT. PAT (Overloading) Configuration This last example shows how to configure inside global address overloading. This is the typical NAT that we would use today. It is rare that we would use static or dynamic NAT unless we were statically mapping a server, for example.

 

10089c11.fm  Page 675  Friday, November 7, 2008  11:10 PM




 676 Chapter11  Network Address Translation (NAT) Here is a sample output of a PAT configuration:

 ip nat pool globalnet 170.168.2.1 170.168.2.1   netmask 255.255.255.0ip nat inside source list 1 pool globalnet overload!interface Ethernet0/0 ip address 10.1.1.10 255.255.255.0 ip nat inside!interface Serial0/0 ip address 170.168.2.1 255.255.255.0 ip nat outside!

 access-list 1 permit 10.1.1.0 0.0.0.255 The nice thing about PAT is that the only differences between this configuration and the previ-ous dynamic NAT configuration is that our pool of addresses has shrunk to only one IP address and at the end of our  ip nat inside source   command we included the  overload   command.Notice in the example that the one IP address that is in the pool for us to use is the IP address of the outside interface. This is perfect if you are configuring NAT Overload for your-self at home or for a small office that only has one IP from your ISP. You could, however, use an additional address such as 170.168.2.2 if you had the address available to you. This could be helpful in a very large implementation where you may have so many internal users that you have to have more than one overloaded IP address on the outside. Simple Verification of NAT Once you have configured the type of NAT you are going to use, typically overload (PAT), you need to be able to verify the configuration.To see basic IP address translation information, use the following command:

 Router# show ip nat translation When looking at the IP NAT translations, you may see many translations from the same host to the same host at the destination. This is typical of many connections to the Web.In addition, you can verify your NAT configuration with the  debug ip nat  command. This output will show the sending address, the translation, and the destination address on each debug line:

 Router# debug ip nat How do you clear your NAT entries from the translation table? Use the  clear ip nat translation  command. To clear all entries from the NAT table, use an asterisk (*) at the end of the command.

 

10089c11.fm  Page 676  Thursday, August 30, 2007  12:51 PM




 Testing and Troubleshooting NAT 677 Testing and Troubleshooting NAT Cisco’s NAT gives you some serious power—and without too much effort, because the con-figurations are really pretty simple. But we all know nothing’s perfect, so in case something goes wrong, you can figure out some of the more common causes by going through this list of possible snags:  Check the dynamic pools—are they composed of the right scope of addresses?  Check to see if any dynamic pools overlap.  Check to see if the addresses used for static mapping and those in the dynamic pools overlap.  Ensure that your access lists specify the correct addresses for translation.  Make sure there aren’t any addresses left out that need to be there, and ensure none are included that shouldn’t be.  Check to make sure that you’ve got both the inside and outside interfaces delimited properly.One thing to keep in mind is that one of the most common problems with a new NAT con-figuration isn’t specific to NAT at all—it usually involves a routing blooper. So, make sure that because you’re changing a source or destination address in a packet, your router knows what to do with the new address after the translation!Supposedly the sky’s the limit regarding the number of mappings the NAT table can hold. In reality, however, it comes down to things like memory and CPU or the boundaries set in place by the scope of available addresses or ports that do, in fact, cause there to be limitations placed on the number of entries possible. You see, each NAT mapping devours about 160 bytes of memory. And sometimes—but not very often—the amount of entries has to be limited for the sake of performance or because of policy restrictions. In situations like these, just use the  ip nat translation max-entries  command for help.Another handy command for troubleshooting is  show ip nat statistics . Deploying this gives you a summary of the NAT configuration, and it will count the number of active trans-lation types. Also counted are hits to an existing mapping, as well any misses—the latter will result in an attempt to create a mapping. This command will also reveal expired translations. If you want to check into dynamic pools, their types, the total available addresses, how many addresses have been allocated and how many failed, plus the number of translations that have occurred, just use the  pool (refcount)  command.And did you know you can manually clear dynamic NAT entries from the NAT table? Doing this can come in pretty handy if you need to get rid of a specific rotten entry without sitting around waiting for the time-out to expire. A manual clear also is really useful when you want to clear the whole NAT table to reconfigure a pool of addresses.You also need to know that the Cisco IOS software just won’t allow you to change or delete address pools if any of that pool’s addresses are mapped in the NAT table. The  clear ip nat translations command clears entries—you can indicate a single entry via the global and local address and through TCP and UDP translations (including ports), or you can just type in an asterisk (*) to wipe out the entire table. But know that if you do that, only dynamic entries will be cleared because this command does not remove static entries.

10089c11.fm  Page 677  Thursday, August 30, 2007  12:51 PM




678Chapter11 Network Address Translation (NAT)Oh, and there’s more—any outside device’s packet destination address that happens to be responding to any inside device is known as the IG address. This means that the initial map-ping has to be held in the NAT table so that all packets arriving from a specific connection get translated consistently. Holding entries in the NAT table also cuts down on repeated lookups happening each time the same machine sends packets to the same outside destinations on a regular basis.Here’s what I mean: When an entry is placed into the NAT table the first time, a timer begins ticking; the duration of that timer is known as the translation timeout. Each time a packet for a given entry translates through the router, the timer gets reset. If the timer expires, the entry will be unceremoniously removed from the NAT table and the dynamically assigned address will then be returned to the pool. Cisco’s default translation timeout is 86,400 seconds (24 hours), but you can change that with the command ip nat translation timeout.Before we move on to the configuration section and actually use the commands I just talked about, let’s go through a couple of NAT examples and see if you can figure out the configu-ration that needs to be used. To start, look at Figure 11.4 and ask yourself two things: Where would you implement NAT in this design, and what type of NAT would you configure?FIGURE11.4NAT exampleIn Figure 11.4, the NAT configuration would be placed on the corporate router and the configuration would be dynamic NAT with overload (PAT). In this NAT example, what type of NAT is being used?

ip nat pool todd-nat 170.168.10.10 170.168.10.20 netmask 255.255.255.0The above command uses dynamic NAT. The pool in the command gives the answer away, plus there is more than one address in the pool, which means we probably are not using PAT. In the next NAT example, we’ll use Figure 11.5 to see if we can figure out the configuration needed.The example in Figure 11.5 shows a border router that needs to be configured with NAT and will allow the use of six public IP addresses, 192.1.2.109 through 114. However, on the inside network, you have 63 hosts that use the private addresses of 192.168.10.65 through 126. What would your NAT configuration be on the border router?

10089c11.fm  Page 678  Thursday, August 30, 2007  12:51 PM




 Configuring NAT on Our Internetwork 679 FIGURE11.5 Another NAT example Two different answers would work here, but the following would be my first choice:

 ip nat pool Todd 192.1.2.109 192.1.2.109 netmask 255.255.255.248access-list 1 permit 192.168.10.64 0.0.0.63

 ip nat inside source list 1 pool Todd overload The command  ip nat pool Todd 192.1.2.109 192.1.2.109 netmask 255.255.255.248  sets the pool name as Todd and creates a dynamic pool of addresses for the NAT to use address 192.1.2.109. Instead of the  netmask  command, you can use the  prefix-length 29  statement. (And I know what you’re thinking, but no, you cannot do this on router interfaces as well.) The second answer would end up with the exact same result of having only 192.1.2.109 as your inside global, but you can type this in and have it work too:  ip nat pool Todd 192.1.2.105 192.1.2.110 netmask 255.255.255.248 . This is a waste because the second through sixth addresses would only be used if there was a conflict with a TCP port number.If you do not understand the second line where the  access-list  is set, please see Chapter 10, “Security.”The command  ip nat inside source list 1 pool Todd overload  sets the dynamic pool to use Port Address Translation (PAT) by using the  overload  command.Be sure to add the  ip nat inside  and  ip nat outside  statements on the appropriate interfaces. Configuring NAT on Our Internetwork Okay, now I’m going to go ahead and connect the link between our Corp router and the R3 router using a 64.1.1.4/30 network and the LAN F0/0 link on the R3 router using the 64.1.1.8/30 net-work. After NAT is working, I’ll then walk you through the verification commands I’ve been talk-ing about throughout this chapter.Our internetwork is shown in Figure 11.6, and the inside local addresses that I’ve been using throughout this book are shown in Table 11.3.

 

10089c11.fm  Page 679  Friday, November 7, 2008  11:11 PM




680Chapter11 Network Address Translation (NAT)FIGURE11.6Our internetwork with new addressingI know—Figure 11.6 shows the same network we’ve been using, but there’s a difference here. The connection between the Corp router and the R3 router is now using global PAT addresses. They can’t talk because the other Corp connections have private IP addresses. (In the real world, the ISP would block these, right? So let’s make this work!) Remember, we call them “inside locals” when using NAT, meaning before translation, and our ISP is blocking the private IP address ranges. What do we do? Well first we need to configure NAT on the Corp router, so let’s get to work!TABLE11.3Network Addressing for the IP Network RouterNetwork AddressInterfaceAddressCorp   Corp10.1.1.0F0/110.1.1.1Corp10.1.2.0S0/0/010.1.2.1

10089c11.fm  Page 680  Thursday, August 30, 2007  12:51 PM




Configuring NAT on Our Internetwork681Corp10.1.3.0S0/0/1(DCE)10.1.3.1Corp10.1.4.0S0/1/0 10.1.4.1Corp64.1.1.4/30S0/2/064.1.1.5/30R1   R110.1.2.0S0/0/0 (DCE)10.1.2.2R110.1.3.0S0/0/110.1.3.2R110.1.6.0F0/010.1.6.1R110.1.7.0F0/110.1.7.1R2   R210.1.4.0S0/2/0 (DCE)10.1.4.2R210.1.8.0D0/3/010.1.8.1R210.1.9.0F0/010.1.9.1R3   R364.1.1.4/30S0/0/1 (DCE)64.1.1.6/30R364.1.1.8/30F0/064.1.1.9/30R310.1.11.0F0/110.1.11.1871W   871W10.1.11.0Vlan 110.1.11.2871W10.1.12.0Dot11radio010.1.12.11242 AP   1242 AP10.1.1.0BVI 110.1.1.2TABLE11.3Network Addressing for the IP Network(continued)RouterNetwork AddressInterfaceAddress

10089c11.fm  Page 681  Thursday, August 30, 2007  12:51 PM




682Chapter11 Network Address Translation (NAT)Now we all know we need to be able to communicate from all the networks connected to the Corp router out to all the networks connected to the R3 router using the new global address of 64.1.1.5/30. Right? You’re nodding your head yes—good! Here we go:

Corp#config tCorp(config)#ip nat pool Todd 64.1.1.5 64.1.1.5 net 255.255.255.252Corp(config)#access-list 1 permit 10.1.0.0 0.0.255.255

Corp(config)#ip nat inside source list 1 pool Todd overloadBefore I add the interface configurations, notice I used the IP address of the Corp’s outside interface 64.1.1.5 as both a start and finish address of the pool. I did that because it works just fine when using PAT.Anyway, it’s important not to forget to configure NAT on all interfaces:

Corp(config)#int s0/2/0Corp(config-if)#ip nat outsideCorp(config-if)#int f0/1Corp(config-if)#ip nat insideCorp(config-if)#int s0/0/0Corp(config-if)#ip nat insideCorp(config-if)#int s0/0/1  Corp(config-if)#ip nat insideCorp(config-if)#int s0/1/0Corp(config-if)#ip nat inside

Corp(config-if)#Now that PAT is configured and our interfaces are all set, let’s telnet from HostC to HostD—wait, first I’ll ping from host to host, then I’ll telnet:

Corp#sh ip nat transPro Inside global      Inside local       Outside local      Outside globalicmp 64.1.1.5:271      10.1.9.2:271       64.1.1.10:271      64.1.1.10:271tcp 64.1.1.5:11000     10.1.9.2:11000     64.1.1.10:23       64.1.1.10:23

Corp#Now I’m going to turn on debug ip nat on the Corp router, then telnet from HostB to HostD. Let’s take a look at the output on the Corp router:

Corp#debug ip nat*May  9 22:57:47.679: NAT*: TCP s=11000->1024, d=23*May  9 22:57:47.679: NAT*: s=10.1.6.2->64.1.1.5, d=64.1.1.10 [0]*May  9 22:57:47.683: NAT*: TCP s=23, d=1024->11000

10089c11.fm  Page 682  Thursday, August 30, 2007  12:51 PM




Configuring NAT on Our Internetwork683*May  9 22:57:47.683: NAT*: s=64.1.1.10, d=64.1.1.5->10.1.6.2 [0]*May  9 22:57:47.699: NAT*: TCP s=11000->1024, d=23*May  9 22:57:47.699: NAT*: s=10.1.6.2->64.1.1.5, d=64.1.1.10 [1]*May  9 22:57:47.703: NAT*: TCP s=23, d=1024->11000*May  9 22:57:47.703: NAT*: s=64.1.1.10, d=64.1.1.5->10.1.6.2 [1]*May  9 22:57:47.707: NAT*: TCP s=11000->1024, d=23*May  9 22:57:47.707: NAT*: s=10.1.6.2->64.1.1.5, d=64.1.1.10 [2]*May  9 22:57:47.711: NAT*: TCP s=11000->1024, d=23*May  9 22:57:47.711: NAT*: s=10.1.6.2->64.1.1.5, d=64.1.1.10 [3]*May  9 22:57:47.719: NAT*: TCP s=23, d=1024->11000*May  9 22:57:47.719: NAT*: s=64.1.1.10, d=64.1.1.5->10.1.6.2 [2]*May  9 22:57:47.723: NAT*: TCP s=23, d=1024->11000*May  9 22:57:47.723: NAT*: s=64.1.1.10, d=64.1.1.5->10.1.6.2 [3]*May  9 22:57:47.723: NAT*: TCP s=11000->1024, d=23*May  9 22:57:47.723: NAT*: s=10.1.6.2->64.1.1.5, d=64.1.1.10 [4]*May  9 22:57:47.731: NAT*: TCP s=11000->1024, d=23*May  9 22:57:47.731: NAT*: s=10.1.6.2->64.1.1.5, d=64.1.1.10 [5]*May  9 22:57:47.735: NAT*: TCP s=23, d=1024->11000*May  9 22:57:47.735: NAT*: s=64.1.1.10, d=64.1.1.5->10.1.6.2 [4]*May  9 22:57:47.735: NAT*: TCP s=11000->1024, d=23*May  9 22:57:47.735: NAT*: s=10.1.6.2->64.1.1.5, d=64.1.1.10 [6]*May  9 22:57:47.747: NAT*: TCP s=11000->1024, d=23*May  9 22:57:47.747: NAT*: s=10.1.6.2->64.1.1.5, d=64.1.1.10 [7]*May  9 22:57:47.951: NAT*: TCP s=11000->1024, d=23*May  9 22:57:47.951: NAT*: s=10.1.6.2->64.1.1.5, d=64.1.1.10 [8]*May  9 22:57:48.103: NAT*: TCP s=23, d=1024->11000*May  9 22:57:48.103: NAT*: s=64.1.1.10, d=64.1.1.5->10.1.6.2 [5]

Corp#Well, well—this is some pretty interesting output to say the least! You can see that the first line shows our source and destination port numbers that are being used on HostB. The second line shows our inside source being translated to our inside global with the outside local/global address listed last—and then, from the outside host back to our HostB. Let’s verify all this with the show ip nat translation command:

Corp#sh ip nat transPro Inside global      Inside local       Outside local      Outside globaltcp 64.1.1.5:11000     10.1.9.2:11000     64.1.1.10:23       64.1.1.10:23

Corp#

10089c11.fm  Page 683  Thursday, August 30, 2007  12:51 PM




684Chapter11 Network Address Translation (NAT)Now, let’s use the command show ip nat statistics on the Corp router:

Corp#sh ip nat statTotal active translations: 2 (0 static, 2 dynamic; 2 extended)Outside interfaces:  Serial0/2/0Inside interfaces:  FastEthernet0/1, Serial0/0/0, Serial0/0/1, Serial0/1/0Hits: 269  Misses: 13CEF Translated packets: 227, CEF Punted packets: 0Expired translations: 27Dynamic mappings:-- Inside Source[Id: 1] access-list 1 pool Todd refcount 2 pool Todd: netmask 255.255.255.252        start 64.1.1.5 end 64.1.1.5        type generic, total addresses 1, allocated 1 (100%), misses 0Queued Packets: 0

Corp#What we can see here is a summary of our configuration, our two active translations, as well as the inside and outside interfaces that are being used. The pool is listed right there toward the bottom of the output. And it all looks good, so it’s time to move on to configure NAT using SDM.Configuring NAT Using SDMConfiguring NAT using the SDM is really much easier that anyone would think—except for you of course, because you’ve already been through Chapter 10! Anyway, all you have to do is click Configure   NAT and you get a handy wizard that does a lot more that just hold your hand to create a NAT rule. It’s a lot like the one we used to create our firewall in Chapter 10, and also just like in Chapter 10, there’s more than one wizard. Again you get to pick between basic and advanced:Basic NATUse this wizard if you have some basic PCs/hosts on your trusted network that need access to the Internet. This wizard will guide you through the process of creating a basic NAT configuration.Advanced NATIf you have a DMZ, or servers on your inside network that users from the outside need to access, you definitely want to opt for the Advanced NAT configuration.

10089c11.fm  Page 684  Thursday, August 30, 2007  12:51 PM




Configuring NAT Using SDM685The first screen is the Create NAT Configuration screen.From here, I’m just going to simply connect up and create a basic NAT. After that, I click Launch the Selected Task, and get the next screen, which tells me what the Basic NAT Wizard is going to do.

10089c11.fm  Page 685  Thursday, August 30, 2007  12:51 PM




686Chapter11 Network Address Translation (NAT)As you might guess, it rocks—all I have to do is to click Next to get to a screen from which I’m able to select all my inside and outside addresses. Seem familiar? Good—that means you’ve been paying attention!After choosing my inside and outside interfaces, I click Next. A NAT pool is created and all my interfaces are assigned inside or outside configurations, just like that!Finally, I click Finish. Let’s see what doing that did to my router. Here are the interfaces it configured:

!interface FastEthernet0/0 ip address 1.1.1.1 255.255.255.0 ip nat inside

10089c11.fm  Page 686  Thursday, August 30, 2007  12:51 PM




Configuring NAT Using SDM687 ip virtual-reassembly duplex auto speed auto!interface FastEthernet0/1 description Connection to 1242 AP ip address 10.1.1.1 255.255.255.0 ip nat inside ip virtual-reassembly duplex auto speed auto![output cut]!        interface Serial0/2/0 description Connection to R3$FW_OUTSIDE$ ip address 64.1.1.5 255.255.255.252 ip access-group 103 in ip verify unicast reverse-path ip nat outside ip inspect SDM_LOW out ip virtual-reassembly clock rate 2000000!

[output cut]Here is the ip nat inside source list it created:

ip nat inside source list 2 interface Serial0/2/0 overload!

[output cut]And last, here is the access list created for each interface I chose as in inside network:

access-list 2 remark SDM_ACL Category=2access-list 2 permit 1.1.1.0 0.0.0.255access-list 2 permit 10.1.4.0 0.0.0.255access-list 2 permit 10.1.1.0 0.0.0.255access-list 2 permit 10.1.2.0 0.0.0.255

access-list 2 permit 10.1.3.0 0.0.0.255I know I’ve said this over and over in the book, but I like to repeat this because I want to assure you that SDM really is an incredibly useful tool for creating advanced configurations like ACLs, VPNs, and NAT. This is one thing I think I’ve nailed down for you, and the last two chapters have really proven that!

10089c11.fm  Page 687  Thursday, August 30, 2007  12:51 PM




688Chapter11 Network Address Translation (NAT)SummaryNow this really was a fun chapter. Come on—admit it! You learned a lot about Network Address Translation (NAT) and how it’s configured with static, dynamic, and Port Address Translation (PAT)—also called NAT Overload.I also described how each flavor of NAT is used in a network, as well as how each type is configured on a network. Plus, for your ease and comfort, I used the same intranetwork we’ve been configuring throughout the book and simply added NAT Overload (PAT) to it.I also went through some verification and troubleshooting commands and then finished the chapter up by showing you how to use SDM to configure NAT quickly and easily.Exam EssentialsUnderstand the term NAT.This may come as news to you, because I didn’t—okay, failed to—mention it earlier, but NAT has a few nicknames. In the industry, it’s referred to as net-work masquerading, IP-masquerading, and for those who are besieged with OCD and com-pelled to spell everything out, Native Address Translation. Whatever you want to dub it, basically, they all refer to the process of rewriting the source/destination addresses of IP pack-ets when they go through a router or firewall. Just focus on the process that’s occurring and your understanding of it (i.e., the important part), and you’re on it for sure!Remember the three methods of NAT.The three methods are static, dynamic, and over-loading, which is also called Port Address Translation (PAT).Understand static NAT.This type of NAT is designed to allow one-to-one mapping between local and global addresses.Understand dynamic NAT.This version gives you the ability to map an unregistered IP address to a registered IP address from out of a pool of registered IP addresses.Understand overloading.Overloading really is a form of dynamic NAT that maps multiple unregistered IP addresses to a single registered IP address—many-to-one—by using different ports. It’s also known as Port Address Translation (PAT).Written Lab 11In this section, write the answers to the following questions:1.What type of address translation can use only one address to allow thousands of hosts to be translated globally?2.What command can you use to show the NAT translations as they occur on your router?

10089c11.fm  Page 688  Thursday, August 30, 2007  12:51 PM




Hands-on Labs6893.What command will show you the translation table?4.What command will clear all your NAT entries from the translation table?5.How much memory does each NAT mapping use (approximately)?6.Why would you use the ip nat translation max-entries command?7.Which command can be used for troubleshooting and displays a summary of the NAT configuration as well as counts of active translation types and hits to an existing mapping.8.What commands must be used on your router interfaces before NAT will translate addresses?9.In the following output, what type of NAT is being used?

ip nat pool todd-nat 170.168.10.10 170.168.10.20 netmask 255.255.255.010.Instead of the netmask command, you can use the ____________ statement.Hands-on LabsI am going to use some basic routers for this lab, but really, almost any Cisco router will work.Here is a list of the labs in this chapter:Lab 11.1: Preparing for NATLab 11.2: Configuring Dynamic NATLab 11.3: Configuring PATI am going to use the network shown in Figure 11.7 for our hands-on lab. I highly recom-mend you connect up some routers and run through this lab. In this lab, you will configure NAT on router Lab_A to translate the private IP address of 192.168.10.0 to a public address of 171.16.10.0.Table 11.4 shows the commands we will use and the purpose of each command.TABLE11.4Command Summary for NAT/PAT Hands-on Lab  CommandPurposeip nat inside source list acl pool nameTranslates IPs that match the ACL from the poolip nat inside source static inside_addr outside_addrStatically maps an inside address to an out-side addressiP nat pool nameCreates an address pool

10089c11.fm  Page 689  Thursday, August 30, 2007  12:51 PM




690Chapter11 Network Address Translation (NAT)FIGURE11.7Chapter 11 hands-on lab networkiP nat insideSets an interface to be an inside interfaceiP nat outsideSets an interface to be an outside interfaceshow ip nat translationsShows current NAT translationsTABLE11.4Command Summary for NAT/PAT Hands-on Lab (continued)CommandPurpose

10089c11.fm  Page 690  Thursday, August 30, 2007  12:51 PM




Hands-on Labs691Lab 11.1: Preparing for NATIn this lab, you’ll set up your routers with IP addresses and RIP routing.1.Configure the routers with the IP addresses listed in Table 11.5.After you configure the routers, you should be able to ping from router to router, but since we do not have a routing protocol running until the next step, you can only verify from one router to another but not through the network until RIP is set up. You can use any routing protocol you wish; I am just using RIP for simplicity’s sake to get this up and running.2.On Lab_A, configure RIP routing, set a passive interface, and configure the default network:Lab_A#config tLab_A(config-router)#network 192.168.20.0Lab_A(config-router)#network 171.16.0.0Lab_A(config-router)#passive-interface s0/2Lab_A(config-router)#exit       

Lab_A(config)#ip default-network 171.16.10.1The passive-interface command stops RIP updates from being sent to the ISP and the ip default-network command advertises a default network to the other routers so they know how to get the Internet.3.On Lab_B, configure RIP routing:Lab_B#config tLab_B(config)#router ripLab_B(config-router)#network 192.168.30.0

Lab_B(config-router)#network 192.168.20.0TABLE11.5Router IP Address SchemeRouterInterfaceIP AddressISPS0171.16.10.1/24Lab_AS0/2171.16.10.2/24Lab_AS0/0192.168.20.1/24Lab_BS0192.168.20.2/24Lab_BE0192.168.30.1/24Lab_CE0192.168.30.2/24

10089c11.fm  Page 691  Thursday, August 30, 2007  12:51 PM




692Chapter11 Network Address Translation (NAT)4.On Lab_C, configure RIP routing, but also use the passive-interface command since there is no reason to send our routing table to the ISP:Lab_C#config tLab_C(config)#router rip

Lab_C(config-router)#network 192.168.30.05.On the ISP router, configure a default route to the corporate network:ISP#config t

ISP(config)#ip route 0.0.0.0 0.0.0.0 s06.Configure the ISP router so you can telnet into the router without being prompted for a password:ISP#config tISP(config)#line vty 0 4

ISP(config-line)#no login7.Verify that you can ping from the ISP router to the Lab_C router and from the Lab_C router to the ISP router. If you cannot, troubleshoot your network.Lab 11.2: Configuring Dynamic NATIn this lab, you’ll configure dynamic NAT on the Lab_A router.1.Create a pool of addresses called GlobalNet on the Lab_A router. The pool should con-tain a range of addresses of 171.16.10.50 through 171.16.10.55.Lab_A(config)#ip nat pool GlobalNet 171.16.10.50 171.16.10.55

net 255.255.255.02.Create access-list 1. This list permits traffic from the 192.168.20.0 and 192.168.30.0 net-work to be translated.Lab_A(config)#access-list 1 permit 192.168.20.0 0.0.0.255

Lab_A(config)#access-list 1 permit 192.168.30.0 0.0.0.2553.Map the access list to the pool that was created.

Lab_A(config)#ip nat inside source list 1 pool GlobalNet4.Configure serial 0/0 as an inside NAT interface.Lab_A(config)#int s0/0

Lab_A(config-if)#ip nat inside5.Configure serial 0/2 as an outside NAT interface.Lab_A(config-if)#int s0/2

Lab_A(config-if)#ip nat outside

10089c11.fm  Page 692  Thursday, August 30, 2007  12:51 PM




Hands-on Labs6936.Log in to the Lab_C router. Telnet from the Lab_C router to the ISP router.

Lab_C#telnet 171.16.10.17.Log in to the Lab_B router. Telnet from the Lab_B router to the ISP router.

Lab_B#telnet 171.16.10.18.Execute the command show users from the ISP router. (This shows who is accessing the VTY lines.)

ISP#show usersa.What does it show as your source IP address?________________b.What is your real source IP address?__________________The show users output should look something like this:ISP>sh users    Line       User       Host(s)              Idle       Location   0 con 0                idle                 00:03:32     2 vty 0                idle                 00:01:33 171.16.10.50*  3 vty 1                idle                 00:00:09 171.16.10.51  Interface  User      Mode                     Idle Peer Address

ISP>

Notice that there is a one-to-one translation. This means you must have a real IP address for every host that wants to get to the Internet, which is not always possible.9.Leave the session open on the ISP and connect to Lab_A. (Use Ctrl+Shift+6, let go, and then press X.)10.Log in to your Lab_A router and view your current translations by entering the show ip nat translation command. You should see something like this:Lab_A#sh ip nat translationsPro Inside global      Inside local       Outside local      Outside global--- 171.16.10.50       192.168.30.2       ---                ------ 171.16.10.51       192.168.20.2       ---                ---

Lab_A#11.If you turn on debug ip nat on the Lab_A router and then ping through the router, you will see the actual NAT process take place, which will look something like this:00:32:47: NAT*: s=192.168.30.2->171.16.10.50, d=171.16.10.1 [5]

00:32:47: NAT*: s=171.16.10.1, d=171.16.10.50->192.168.30.2

10089c11.fm  Page 693  Thursday, August 30, 2007  12:51 PM




 694 Chapter11  Network Address Translation (NAT) Lab 11.3: Configuring PAT In this lab, you’ll configure Port Address Translation (PAT) on the Lab_A router. We will use PAT because we don’t want a one-to-one translation, which uses just one IP address for every user on the network. 1. On the Lab_A router, delete the translation table and remove the dynamic NAT pool: Lab_A# clear ip nat translation * Lab_A# config t Lab_A(config)# no ip nat pool GlobalNet 171.16.10.50171.16.10.55 netmask 255.255.255.0

 Lab_A(config)# no ip nat inside source list 1 pool GlobalNet 2. On the Lab_A router, create a NAT pool with one address called Lammle. The pool should contain a single address 171.16.10.100. Enter the following command: Lab_A# config t Lab_A(config)# ip nat pool Lammle 171.16.10.l00 171.16.10.100

 net 255.255.255.0 3. Create access-list 2. It should permit networks 192.168.20.0 and 192.168.30.0 to be translated. Lab_A(config)# access-list 2 permit 192.168.20.0 0.0.0.255

 Lab_A(config)# access-list 2 permit 192.168.30.0 0.0.0.255 4. Map the access-list 2 to the new pool, allowing PAT to occur by using the  overload  command.

 Lab_A(config)# ip nat inside source list 2 pool Lammle overload 5. Log in to the Lab_C router and telnet to the ISP router; also, log in to the Lab_B router and telnet to the ISP router. 6. From the ISP router, use the  show users  command. The output should look like this: ISP> sh users     Line       User       Host(s)              Idle       Location*  0 con 0                idle                 00:00:00     2 vty 0                idle                 00:00:39 171.16.10.100   4 vty 2                idle                 00:00:37 171.16.10.100  Interface  User      Mode               Idle Peer Address

 ISP>

 

10089c11.fm  Page 694  Friday, November 7, 2008  11:12 PM




Hands-on Labs6957.From the Lab_A router, use the show ip nat translations command.Lab_A#sh ip nat translationsPro Inside global  Inside local  Outside local Outside globaltcp 171.16.10.100:11001 192.168.20.2:11001 171.16.10.1:23    171.16.10.1:23tcp 171.16.10.100:11002 192.168.30.2:11002 171.16.10.1:23    171.16.10.1:23

tcp 171.16.10.100:1024  192.168.20.2:11002 171.16.10.1:23    171.16.10.1:238.Also make sure that the debug ip nat command is on the Lab_A router. If you ping from the Lab_C router to the ISP router, the output will look like this:01:12:36: NAT: s=192.168.30.2->171.16.10.100, d=171.16.10.1 [35]01:12:36: NAT*: s=171.16.10.1, d=171.16.10.100->192.168.30.2 [35]01:12:36: NAT*: s=192.168.30.2->171.16.10.100, d=171.16.10.1 [36]01:12:36: NAT*: s=171.16.10.1, d=171.16.10.100->192.168.30.2 [36]01:12:36: NAT*: s=192.168.30.2->171.16.10.100, d=171.16.10.1 [37]01:12:36: NAT*: s=171.16.10.1, d=171.16.10.100->192.168.30.2 [37]01:12:36: NAT*: s=192.168.30.2->171.16.10.100, d=171.16.10.1 [38]01:12:36: NAT*: s=171.16.10.1, d=171.16.10.100->192.168.30.2 [38]01:12:37: NAT*: s=192.168.30.2->171.16.10.100, d=171.16.10.1 [39]

01:12:37: NAT*: s=171.16.10.1, d=171.16.10.100->192.168.30.2 [39]

10089c11.fm  Page 695  Thursday, August 30, 2007  12:51 PM




696Chapter11 Network Address Translation (NAT)Review Questions

The following questions are designed to test your understanding of this chapter’s material. For more information on how to get additional ques-tions, please see this book’s Introduction.1.Which of the following are disadvantages of using NAT? (Choose three.)A.Translation introduces switching path delays.B.Conserves legally registered addresses.C.Causes loss of end-to-end IP traceability.D.Increases flexibility when connecting to the Internet.E.Certain applications will not function with NAT enabled.F.Reduces address overlap occurrence.2.Which of the following are advantages of using NAT? (Choose three.)A.Translation introduces switching path delays.B.Conserves legally registered addresses.C.Causes loss of end-to-end IP traceability.D.Increases flexibility when connecting to the Internet.E.Certain applications will not function with NAT enabled.F.Reduces address overlap occurrence.3.Which command will allow you to see real-time translations on your router?A.show ip nat translationsB.show ip nat statisticsC.debug ip natD.clear ip nat translations *4.Which command will show you all the translations active on your router?A.show ip nat translationsB.show ip nat statisticsC.debug ip natD.clear ip nat translations *5.Which command will clear all the translations active on your router?A.show ip nat translationsB.show ip nat statisticsC.debug ip natD.clear ip nat translations *

10089c11.fm  Page 696  Thursday, August 30, 2007  12:51 PM




Review Questions6976.Which command will show you the summary of the NAT configuration?A.show ip nat translationsB.show ip nat statisticsC.debug ip natD.clear ip nat translations *7.Which command will create a dynamic pool named Todd that will provide you with 30 global addresses?A.ip nat pool Todd 171.16.10.65 171.16.10.94 net 255.255.255.240B.ip nat pool Todd 171.16.10.65 171.16.10.94 net 255.255.255.224C.ip nat pool todd 171.16.10.65 171.16.10.94 net 255.255.255.224D.ip nat pool Todd 171.16.10.1 171.16.10.254 net 255.255.255.08.Which are considered the three methods of NAT?A.StaticB.IP NAT poolC.DynamicD.NAT double-translationE.Overload9.When creating a pool of global addresses, which of the following can be used instead of the netmask command?A./ (slash notation)B.prefix-lengthC.no maskD.block-size10.Which of the following would be a good starting point for troubleshooting if your router is not translating?A.Reboot.B.Call Cisco.C.Check your interfaces for the correct configuration.D.Run the debug all command.11.Which of the following would be good reasons to run NAT? (Choose three.)A.You need to connect to the Internet and your hosts don’t have globally unique IP addresses.B.You change to a new ISP that requires you to renumber your network.C.You don’t want any hosts connecting to the Internet.D.You require two intranets with duplicate addresses to merge.

10089c11.fm  Page 697  Thursday, August 30, 2007  12:51 PM




698Chapter11 Network Address Translation (NAT)12.Which of the following is considered to be the address after translation?A.Inside localB.Outside localC.Inside globalD.Outside global13.Which of the following is considered to be the address before translation?A.Inside localB.Outside localC.Inside globalD.Outside global14.Which of the following is considered to be the destination host before translation?A.Inside localB.Outside localC.Inside globalD.Outside global15.Which of the following is considered to be the outside destination host after translation?A.Inside localB.Outside localC.Inside globalD.Outside global16.Which command would you place on interface on a private network?A.ip nat insideB.ip nat outsideC.ip outside globalD.ip inside local17.Which command would you place on interface connected to the Internet?A.ip nat insideB.ip nat outsideC.ip outside globalD.ip inside local18.Pat Address Translation is also termed what?A.NAT FastB.NAT StaticC.NAT OverloadD.Overloading Static

10089c11.fm  Page 698  Thursday, August 30, 2007  12:51 PM




Answers to Review Questions699Answers to Review Questions1.A, C, E. NAT is not perfect and can cause some issues in some networks, but most networks work just fine. NAT can cause delays and troubleshooting problems, and some applications just won’t work.2.B, D, F. NAT is not perfect, but there are some advantages. It conserves global addresses, which allow us to add millions of hosts to the Internet without “real” IP addresses. This pro-vides flexibility in our corporate networks. NAT can also allow you to use the same subnet more than once in the same network without overlapping networks.3.C. The command debug ip nat will show you in real time the translations occurring on your router.4.A. The command show ip nat translations will show you the translation table containing all the active NAT entries.5.D. The command clear ip nat translations * will clear all the active NAT entries in your translation table.6.B. The show ip nat statistics command displays a summary of the NAT configuration as well as counts of active translation types, hits to an existing mapping, misses (causing an attempt to create a mapping), and expired translations.7.B. The command ip nat pool name creates the pool that hosts can use to get onto the global Internet. What makes answer B correct is that the range 171.16.10.65 through 171.16.10.94 includes 30 hosts, but the mask has to match 30 hosts as well, and that mask is 255.255.255.224. Answer C is wrong because the pool name has a lower case “T” in the pool name. Pool name’s are case sensitive.8.A, C, E. You can configure NAT three ways on a Cisco router: static, dynamic, and NAT Overload (PAT).9.B. Instead of the netmask command, you can use the prefix-length length statement.10.C. In order for NAT to provide translation services, you must have ip nat inside and ip nat outside configured on your router’s interfaces.11.A, B, D. The most popular use of NAT is if you want to connect to the Internet and you don’t want hosts to have global (real) IP addresses, but answers B and D are correct as well.12.C. The host on the private network after translation is considered to be an inside global host.13.A. The host on the private network before translation is considered to be an inside local host.14.B. The host on the global network before translation is considered to be an outside local host.15.D. The host on the global network after translation is considered to be an outside global host.

10089c11.fm  Page 699  Thursday, August 30, 2007  12:51 PM




700Chapter11 Network Address Translation (NAT)16.A. As in access-lists, you must configure your interfaces before NAT will provide any transla-tions. On the inside networks you would use the command ip nat inside. On the outside interface, you will use the command ip nat outside.17.B. As in access-lists, you must configure your interfaces before NAT will provide any transla-tions. On the inside networks you would use the command ip nat inside. On the outside interface, you will use the command ip nat outside.18.C. Another term for port address translation is NAT Overload because that is the command used to enable port address translation.

10089c11.fm  Page 700  Thursday, August 30, 2007  12:51 PM




Answers to Written Lab 11701Answers to Written Lab 111.Port Address Translation (PAT), also called NAT Overload.2.debug ip nat3.show ip nat translations4.clear ip nat translations *5.160 bytes of memory6.In the rare case where the entries must be limited for either performance or policy reasons7.show ip nat statistics8.The ip nat inside and ip nat outside commands9.Dynamic NAT10.prefix-length

10089c11.fm  Page 701  Thursday, August 30, 2007  12:51 PM




10089c11.fm  Page 702  Thursday, August 30, 2007  12:51 PM




 

Chapter 12 Cisco’s Wireless Technologies

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Explain and select the appropriate administrative tasks required for a WLAN  Describe standards associated with wireless media (including: IEEE WI-FI Alliance, ITU/FCC)   Identify and describe the purpose of the components in a small wireless network (including: SSID, BSS, ESS)   Identify the basic parameters to configure on a wireless network to ensure that devices connect to the correct access point   Compare and contrast wireless security features and capabilities of WPA security (including: open, WEP, WPA-1/2)   Identify common issues with implementing wireless networks (including: Interface, Miss configuration) 

 

10089.book  Page 703  Monday, July 23, 2007  3:17 PM




 If you want to understand the basic wireless LANs, or WLANs, that are the most commonly used today, just think 10BaseT Ethernet with hubs. What this means is that our WLANs typi-cally run half-duplex communication—everyone is sharing the same bandwidth and only one user is communicating at a time. This isn’t necessarily bad—it’s just not good enough. Because most people rely upon wireless networks today, it’s critical that they evolve faster than greased lightning to keep up with our rapidly escalating needs. The good news is that this is actually happening—Cisco has reacted by coming up with an answer called the Cisco Unified Wireless Solution that works with all types of wireless connections. And it works securely too!My goal in this chapter isn’t so much to introduce you to wireless technologies in general, it’s to familiarize you with Cisco’s wireless technologies because as you’d probably guess, there are differences—however subtle. Yes, I will cover basic wireless LAN technologies and committees, but the main objective here is to ensure that you understand wireless through Cisco’s eyes and solidly grasp the solutions that Cisco provides.Cisco’s Unified Wireless Solution includes mobility and mesh, so these are the topics I’m going to focus upon. I’m also going to give you the skinny on the all-important topic of wire-less security.This chapter could easily be an extension of Chapter 14, “Wide Area Networks,” as wire-less cellular networks and other last-mile high-speed wireless technologies are emerging. But I’ve exalted this chapter’s topics by giving them their own chapter because I want you to realize just how important wireless is as an alternative to wide area networks. And in case you were wondering, yes, Cisco Unified Wireless Solution includes Wireless Metropolitan Area Networks (WMANs).

 For up-to-the-minute updates on the topics covered in this chapter, please see  www.lammle.com  and/or  www.sybex.com . Introduction to Wireless Technology Transmitting a signal using the typical 802.11 specifications works a lot like it does with a basic Ethernet hub: They’re both two-way forms of communication, and they both use the same frequency to both transmit and receive, often referred to as half-duplex and mentioned earlier in the chapter. Wireless LANs (WLANs) use radio frequencies (RFs) that are radiated into the air from an antenna that creates radio waves. These waves can be absorbed, refracted, 

 

10089.book  Page 704  Monday, July 23, 2007  3:17 PM




 Introduction to Wireless Technology 705 or reflected by walls, water, and metal surfaces, resulting in low signal strength. So because of this innate vulnerability to surrounding environmental factors, it’s pretty apparent that wire-less will never offer us the same robustness as a wired network can, but that still doesn’t mean we’re not going to run wireless. Believe me, we definitely will!We can increase the transmitting power and gain a greater transmitting distance, but doing so can create some nasty distortion, so it has to be done carefully. By using higher frequencies, we can attain higher data rates, but this is, unfortunately, at the cost of decreased transmitting distances. And if we use lower frequencies, we get to transmit greater distances but at lower data rates. This should make it pretty clear to you that understanding all the various types of WLANs you can implement is imperative to creating the LAN solution that best meets the specific requirements of the unique situation you’re dealing with.Also important to note is the fact that the 802.11 specifications were developed so that there would be no licensing required in most countries—to ensure the user the freedom to install and operate without any licensing or operating fees. This means that any manufacturer can create products and sell them at a local computer store or wherever. It also means that all our computers should be able to communicate wirelessly without configuring much, if anything at all.Various agencies have been around for a very long time to help govern the use of wireless devices, frequencies, standards, and how the frequency spectrums are used. Table 12.1 shows the current agencies that help create, maintain, and even enforce wireless standards worldwide.Because WLANs transmit over radio frequencies, they’re regulated by the same types of laws used to govern things like AM/FM radios. It’s the Federal Communications Commission (FCC) that regulates the use of wireless LAN devices, and the Institute of Electrical and Elec-tronics Engineers (IEEE) takes it from there and creates standards based on what frequencies the FCC releases for public use. TABLE12.1 Wireless Agencies and Standards AgencyPurposeWeb Site Institute of Electrical and Electronics Engineers (IEEE) Creates and maintains operational standards www.ieee.org Federal Communications Commission (FCC) Regulates the use of wireless devices in the U.S.  www.fcc.gov European Telecommunications Standards Institute (ETSi) Chartered to produce common standards in Europe  www.etsi.org Wi-Fi Alliance Promotes and tests for WLAN interoperability  www.wi-fi.com WLAN Association (WLANA) Educates and raises consumer awareness regarding WLANs  www.wlana.org

 

10089.book  Page 705  Monday, July 23, 2007  3:17 PM




 706 Chapter12  Cisco’s Wireless Technologies The FCC has released three unlicensed bands for public use: 900MHz, 2.4GHz, and 5.7GHz. The 900MHz and 2.4GHz bands are referred to as the Industrial, Scientific, and Medical (ISM) bands, and the 5-GHz band is known as the Unlicensed National Information Infrastructure (UNII) band. Figure 12.1 shows where the unlicensed bands sit within the RF spectrum. FIGURE12.1 Unlicensed frequencies So it follows that if you opt to deploy wireless in a range outside of the three public bands shown in Figure 12.1, you need to get a specific license from the FCC to do so. Once the FCC opened the three frequency ranges for public use, many manufacturers were able to start offer-ing myriad products that flooded the market, with 802.11b/g being the most widely used wire-less network found today.The Wi-Fi Alliance grants certification for interoperability among 802.11 products offered by various vendors. This certification provides a sort of comfort zone for the users purchasing the many types of products, although in my personal experience, it’s just a whole lot easier if you buy all your access points from the same manufacturer!In the current U.S. wireless LAN market, there are several accepted operational standards and drafts created and maintained by the Institute of Electrical and Electronics Engineers (IEEE). Let’s take a look at these standards and then talk about how the most commonly used standards work. The 802.11 Standards Taking off from what you learned in Chapter 1, “Internetworking,” wireless networking has its own 802 standards group—remember, Ethernet’s committee is 802.3. Wireless starts with 802.11, and there are various other up-and-coming standard groups as well, like 802.16 and 802.20. And there’s no doubt that cellular networks will become huge players in our wireless future. But for now, we’re going to concentrate on the 802.11 standards committee and subcommittees.IEEE 802.11 was the first, original standardized WLAN at 1 and 2Mbps. It runs in the 2.4GHz radio frequency and was ratified in 1997 even though we didn’t see many products pop up until around 1999 when 802.11b was introduced. All the committees listed in Table 12.2 are amendments to the original 802.11 standard except for 802.11F and 802.11T, which are both stand-alone documents.

AM Broadcast Cellular (840MHz) Visible light Sonar (extremely low)  FM Broadcast Infrared Wireless LAN X-rays 900MHz band 2.4GHz band  5GHz band 

 

10089.book  Page 706  Monday, July 23, 2007  3:17 PM




 Introduction to Wireless Technology 707 TABLE12.2 802.11 Committees and Subcommittees   CommitteePurpose IEEE 802.11a 54Mbps, 5GHz standard IEEE 802.11b Enhancements to 802.11 to support 5.5 and 11MbpsIEEE 802.11c Bridge operation procedures; included in the IEEE 802.1D standard IEEE 802.11d International roaming extensionsIEEE 802.11e Quality of serviceIEEE 802.11F Inter-Access Point Protocol IEEE 802.11g 54Mbps, 2.4GHz standard (backward compatible with 802.11b) IEEE 802.11h Dynamic Frequency Selection (DFS) and Transmit Power Control (TPC) at 5GhzIEEE 802.11i Enhanced securityIEEE 802.11j Extensions for Japan and U.S. public safetyIEEE 802.11k Radio resource measurement enhancements IEEE 802.11m Maintenance of the standard; odds and endsIEEE 802.11n Higher throughput improvements using MIMO (multiple input, multiple output antennas) IEEE 802.11p Wireless Access for the Vehicular Environment (WAVE) IEEE 802.11r Fast roaming IEEE 802.11s Extended Service Set (ESS) Mesh NetworkingIEEE 802.11T Wireless Performance Prediction (WPP) IEEE 802.11u Internetworking with non-802 networks (cellular, for example)IEEE 802.11v Wireless network management IEEE 802.11w Protected management frames IEEE 802.11y 3650–3700 operation in the U.S.

 

10089.book  Page 707  Monday, July 23, 2007  3:17 PM




 708 Chapter12  Cisco’s Wireless Technologies Okay, now let’s discuss some important specifics of the most popular 802.11 WLANs. 2.4GHz (802.11b) First on the menu is the 802.11b standard. It was the most widely deployed wireless standard, and it operates in the 2.4GHz unlicensed radio band that delivers a maximum data rate of 11Mbps. The 802.11b standard has been widely adopted by both vendors and customers who found that its 11Mbps data rate worked pretty well for most applications. But now that 802.11b has a big brother (802.11g), no one goes out and just buys an 802.11b card or access point anymore because why would you buy a 10Mbps Ethernet card when you can score a 10/100 Ethernet card for the same price?An interesting thing about all Cisco 802.11 WLAN products is that they have the ability to data-rate-shift while moving. This allows the person operating at 11Mbps to shift to 5.5Mbps, 2Mbps, and finally still communicate farthest from the access point at 1Mbps. And further-more, this rate shifting happens without losing connection and with no interaction from the user. Rate shifting also occurs on a transmission-by-transmission basis. This is important because it means that the access point can support multiple clients at varying speeds depending upon the location of each client.The problem with 802.11b lies in how the Data Link layer is dealt with. In order to solve prob-lems in the RF spectrum, a type of Ethernet collision detection was created called CSMA/CA, or Carrier Sense Multiple Access with Collision Avoidance. Check this out in Figure 12.2.CSMA/CA is also called a Request To Send, Clear To Send (RTS/CTS) because of the way that hosts must communicate to the access point (AP). For every packet sent, an RTS/CTS and acknowledgment must be received, and because of this rather cumbersome process, it’s kind of hard to believe it all actually works! 2.4GHz (802.11g) The 802.11g standard was ratified in June 2003 and is backward compatible with 802.11b. The 802.11g standard delivers the same 54Mbps maximum data rate as 802.11a but runs in the 2.4GHz range—the same as 802.11b. FIGURE12.2 802.11b CSMA/CA

ACK Source Destination RTS CTS Data 

 

10089.book  Page 708  Monday, July 23, 2007  3:17 PM




 Introduction to Wireless Technology 709 Because 802.11b/g operates in the same 2.4GHz unlicensed band, migrating to 802.11g is an affordable choice for organizations with existing 802.11b wireless infrastructures. Just keep in mind that 802.11b products can’t be “software upgraded” to 802.11g. This limitation is because 802.11g radios use a different chipset in order to deliver the higher data rate.But still, much like Ethernet and Fast Ethernet, 802.11g products can be commingled with 802.11b products in the same network. Yet, for example, completely unlike Ethernet, if you have four users running 802.11g cards and one user starts using an 802.11b card, everyone connected to the same access point is then forced to run the 802.11b CSMA/CA method—an ugly fact that really makes throughput suffer. So to optimize performance, it’s recommended that you disable the 802.11b-only modes on all your access points.To explain this further, 802.11b uses a modulation technique called Direct Sequence Spread Spectrum (DSSS) that’s just not as robust as the Orthogonal Frequency Division Multiplexing (OFDM) modulation used by both 802.11g and 802.11a. 802.11g clients using OFDM enjoy much better performance at the same ranges as 802.11b clients do, but—and remember this—when 802.11g clients are operating at the 802.11b rates (11, 5.5, 2, and 1Mbps), they’re actually using the same modulation 802.11b does.Figure 12.3 shows the 14 different channels (each 22Mhz wide) that the FCC released in the 2.4GHz range.In the U.S., only 11 channels are configurable, with channels 1, 6, and 11 being non-overlapping. This allows you to have three access points in the same area without expe-riencing interference. 5GHz (802.11a) The IEEE ratified the 802.11a standard in 1999, but the first 802.11a products didn’t begin appearing on the market until late 2001—and boy were they pricey! The 802.11a standard delivers a maximum data rate of 54Mbps with 12 non-overlapping frequency channels. Figure 12.4 shows the UNII bands. FIGURE12.3 ISM 2.4GHz channels

1

2

3

4

5

6

7 

8 

9 

10 

11

12 

13 

14 2.483GHz Channels 2.402GHz 22MHz 

 

10089.book  Page 709  Monday, July 23, 2007  3:17 PM




 710 Chapter12  Cisco’s Wireless Technologies FIGURE12.4 UNII 5GHz band has 12 non-overlapping channels (U.S.). Operating in the 5GHz radio band, 802.11a is also immune to interference from devices that operate in the 2.4GHz band, like microwave ovens, cordless phones, and Bluetooth devices. 802.11a isn’t backward compatible with 802.11b because they are different frequen-cies, so you don’t get to just “upgrade” part of your network and expect everything to work together in perfect harmony. But no worries—there are plenty of dual-radio devices that will work in both types of networks. A definite plus for 802.11a is that it can work in the same physical environment without interference from 802.11b users.Similar to the 802.11b radios, all 802.11a products also have the ability to data-rate-shift while moving. The 802.11a products allow the person operating at 54Mbps to shift to 48Mbps, 36Mbps, 24Mbps, 18Mbps, 12Mbps, 9Mbps, and finally still communicate farthest from the AP at 6Mbps.There’s also an extension of the 802.11a specifications called 802.11h. 5GHz (802.11h) The FCC added 11 new channels in February 2004, and in 2008, we finally get to begin using these channels based on manufacturers’ releases of more 802.11a 5GHz products. This means that soon, we’ll gain access to up to 23 non-overlapping channels! And there are two new features of the 5GHz radio that are part of the 802.11h specification: Transmit Power Control (TPC) and Dynamic Frequency Selection (DFS). DFS This cool feature continuously monitors a device’s operating range for any radar signals that are allowed to operate in portions of the 5GHz band as well as 802.11a before transmit-ting. If DFS discovers any radar signals, it’ll either abandon the occupied channel or mark it as unavailable to prevent interference from occurring on the WLAN. TPC Even though it’s been used by the mobile phone industry for a long time, this technol-ogy has some handy new uses. You can set the client machine’s adapter and the access point’s transmit power to cover various size ranges—a feature that’s useful for many reasons. For one, setting the access point’s transmit power to 5mW reduces cell range, which works great if you’ve got a compact area with high-density usage.Further advantages include the fact that TPC enables the client and the access point to com-municate. This means the client machine can fine-tune its transmit power dynamically so it uses just enough energy to preserve its connection to the access point, conserve its battery power, plus reduce interference on the neighboring WLAN cells—sweet!

161 5.15 Lower band 5.15–5.25 indoor 

Upper band 5.725–5.825 outdoor Middle band 5.25–5.35 indoor and outdoor 5.825 Channel center frequencies  5.180 5.200 5.220 5.240 5.805 5.260 5.280 5.300 5.320 5.745 5.765 5.785 Operating channels 36 40 44 48 52 56 60 64 149 153 157 

 

10089.book  Page 710  Monday, July 23, 2007  3:17 PM




 Introduction to Wireless Technology 711 Comparing 802.11 Before I move on to Cisco-specific products, take at look at Table 12.3, which lists the pros and cons of 802.11a, b, and g.Now let’s take a look at Figure 12.5, which delimits the range comparisons of each 802.11 standard and shows us the different ranges using an indoor open-office environment as a fac-tor. We’ll be using default power settings.You can see that to get the full 54Mbps benefit of both 802.11a and 802.11g, you need to be between 50 feet and 100 feet (at the farthest) away, and maybe even less if there are any obstructions between the client and the access point.All good, but there’s one more IEEE 802.11 standard I want to cover that we’ll use to get even higher speeds at greater distances. 2.4GHz/5GHz (802.11n) 802.11n builds upon previous 802.11 standards by adding Multiple-Input Multiple-Output (MIMO), which employs multiple transmitters and receiver antennas to increase data throughput. 802.11n can have up to eight antennas, but most of today’s access points use four. These are some-times referred to as smart antennas, and if you do have four of them, two would be used for trans-mitting simultaneously with the other two receiving simultaneously. This setup would allow for  TABLE12.3 802.11 Comparison 802.11b802.11g802.11a (h) 2.4GHz2.4GHz5GHzMost commonHigher throughputHighest throughputUp to 11MpbsUp to 54Mbps*Up to 54MbpsDSSSDSSS/OFDMOFDM3 non-overlapping channels3 non-overlapping channelsup to 23 non-overlapping channels**About 25 users per cellAbout 20 users per cellAbout 15 users per cellDistance limited by multipath Throughput degraded by 802.11b clientsLower market penetrations *Runs Direct Sequence Spread Spectrum when also running the 802.11b at speeds of 11Mbps and below.**This happens to be Cisco’s rule of thumb. Know that the actual number of users per cell varies based on many factors.

 

10089.book  Page 711  Monday, July 23, 2007  3:17 PM




 712 Chapter12  Cisco’s Wireless Technologies much higher data rates than 802.11a/b/g. In fact, the marketing people claim it will provide about 250Mbps, but personally, I’m not buying it. I just don’t believe that’s what our actual throughput levels can be, and even if what they’re saying is true, exactly how would that help if all you’ve got is a 1 or 2Mbps cable or DSL connection to the Internet? FIGURE12.5 Range comparisons of 802.11 standards Keep in mind that the 802.11n standard hasn’t yet been ratified and isn’t expected to be until sometime in 2008—maybe later. This means that the products on the shelf today are pro-prietary, and they are called “pre-N” products.With all this in mind, let’s move on and take a look at Cisco’s solution to the growing wire-less market. Cisco’s Unified Wireless Solution

 To be honest, if you’re brain is already full and you’re just cramming for your CCNA Composite exam, you can probably skip this section. But before you do, be sure and check  www.lammle.com  for the latest dirt on what the Cisco  CCNA Composite 640-802 exam objectives are. With a range of products that support IEEE 802.11a/b/g and soon “n” technologies, Cisco really does offer a pretty complete and impressive line of in-building and outdoor wireless LAN solutions. These products include access points, wireless controllers, wireless LAN client adapters, security and management servers, wireless management devices, wireless integrated switches and routers—even antennas and accessories. Did I say impressive or what?

350 ft 802.11b 802.11g 802.11a 

11Mbps 5.5Mbps 2Mbps 1Mbps54Mbps 48Mbps 36Mbps 24Mbps 18Mbps 12Mbps 9Mbps 6Mbps 54Mbps 48Mbps 36Mbps 24Mbps 18Mbps 12Mbps 9Mbps 6Mbps 50 ft. 100 ft. 150 ft. 200 ft. 250 ft. 300 ft. 

 

10089.book  Page 712  Monday, July 23, 2007  3:17 PM




 Cisco’s Unified Wireless Solution 713 Since about the year 2000, a lot of corporations have relied upon basic access points as their main wireless networks and connected them into an infrastructure, which allowed users to roam within their network. Figure 12.6 shows a typical infrastructure network, either with one access point or as an extended service set wherein you would have multiple access points—all using the same Service Set Identifier (SSID)   for roaming purposes. FIGURE12.6 Typical infrastructure network So what we see here is that each of the APs, in either configuration, is configured as a root AP. If you were to look back in Chapter 4 at the configuration of my two wireless routers (R2 and 871W) and the 1242AP, all three were configured as roots. Basically, this means that each router is essentially saying, “Yo, wireless client, connect to me and get your goods (wired resources).” If the APs weren’t root, they could only connect to a root device as a repeater. Nonroot devices include clients, bridges, repeater access points, and work group bridges.But wait a minute—that was like the IT Cretaceous period or something. Definitely not now when it’s almost 2008 and we have on the horizon the Cisco Unified Wireless Solution that provides a comprehensive, integrated WLAN solution! This new, tricked-out technology includes intelligent Cisco APs and Cisco WLAN controllers specifically designed to support APs. The solution is managed either through the controller web interface, from the controller itself, or from Cisco’s Wireless Control System (WCS).

  Infrastructure mode

- Basic Service  Set (BSS)     Mobile clients use a single AP     for connectivity to each other or     wired network resources.- Extended Service Set (ESS)     Two or more BSSs connected by     a common distribution system (DS

 

10089.book  Page 713  Monday, July 23, 2007  3:17 PM




 714 Chapter12  Cisco’s Wireless Technologies But the really sweet thing about this type of network is that after initial installation, it requires zero configuration. This means that you can connect an AP in an outdoor or indoor environment and the AP will automatically configure itself based on the controller’s informa-tion. It will even check for channel overlap and interference and assign itself a non-overlapping channel—how cool is that? And as I briefly mentioned earlier, if it happens to detect an over-lapping channel within its area, it’ll lower its transmitting level to limit interference. Cisco calls this “auto RF controls.”The news isn’t all sunshine—this product line isn’t necessarily for the poor because it can require that you to get your hands on quite a stash of goods. Yep—you’ll need more than just an AP for sure. Your minimum shopping list is a Cisco 1020 AP and a controller for an indoor solution, and for outdoors, a Cisco 1520 AP and a controller. And remember, these are min-imum requirements. Reality will probably require more stuff—I use both the 1020 and 1520 APs in my classes, and I’m using them to write my book along with two types of controllers—the absolute minimum number of devices I can use to still make this functional. The APs are reasonably priced, and as is usual for Cisco, their cost pretty much follows the product model number. It’s the controllers that’ll make your coinage disappear—they cost thousands! Hope-fully, by the time you read this book, they’ll have come down in price to somewhere below the stratosphere. There’s always hope!But for fun, let’s pretend you have a decent sized network with unlimited funds to spend on it. First, get a hold of at least two controllers (the good ones run about $20,000 a piece.) Why two? Because every packet from every AP must go to the controller in order to then be placed either on the wired network or back out to the wireless network. The controller decides the packet’s destiny based on the Lightweight Access Point Protocol (LWAPP) information that’s encapsulated on it. (I’ll tell you more about LWAPP in a minute.) Anyway, the reason you need at least two of these beauties is in the unfortunate event that one of them goes down. You’d be full-on nuts to create a design that has only a single point of failure, so, being com-pletely sane, you’ll get two and create the redundancy you’ll need for this type of network.Okay, now that you’ve got the single point of failure thing covered, you need to be able to manage your controllers as well. Cisco has the GUI Wireless Control System (WCS) to manage the entire WLAN from a single interface that also happens to provide you with some seriously detailed insight into the network coverage, the trending of network statistics, and the details on device location. (Remember—money is no object!)The thing is, you really don’t need to have the WCS because the Cisco WLAN controller analyzes the data collected by the APs, which can be managed either at the individual control-ler or by the comprehensive tools within WCS. But since you’re rolling in it, you are definitely going to hook yourself up with that WCS.

 You can actually download a 30-day demo of WCS from Cisco’s website. Oh, lest I forget, to make things just a little tougher on you, the controllers only come with giga-bit interfaces. This means you need a switch that has both 10/100 ports for your AP connections and a gigabit port to connect your controllers with. It’s probably best to get a minimum switch of 3560 (or better) with both types of ports so you can provide inter-VLAN routing as well.

 

10089.book  Page 714  Monday, July 23, 2007  3:17 PM




 Cisco’s Unified Wireless Solution 715 Split-MAC Architecture Okay—yes. It sounds weird, but this odd-sounding name is actually a pretty cool feature. We’re basically splitting the processing of the 802.11 protocol between two devices, the AP and a centralized Cisco WLAN controller. Figure 12.7 shows how the “splitting” of process-ing occurs at each location FIGURE12.7 Split-MAC architecture Although the 1520 AP and the 1020 AP appear to be directly connected to the controller in Figure 12.7, they can’t be—first, because they’ve go to connect with a switch to provide 10/100 to gigabit conversion, and second, because the controller only forwards LWAPP packets coming from an LWAPP-enabled port. This means you need a router if you want to take an LWAPP packet and forward it out as IP data to a non-LWAPP network. A high-end switch can handle the routing.The AP handles the portions of the protocol that have real-time requirements:  The frame exchange handshake between a client and AP when transferring a frame over the air  Transmitting beacon frames  Buffering and transmitting frames for clients in power save operations  Responding to probe request frames from clients  Forwarding notification of received probe requests to the controller  Providing real-time signal quality information to the controller with every received frame  Monitoring each of the radio channels for noise, interference, and other WLANs  Monitoring for the presence of other APs  Encryption and decryption except in the case of VPN/IPSec clients

Switched/routed network 

LWAPP 

Outdoor 1520 AP Indoor 1020 APLWAPP Controller

 

10089.book  Page 715  Monday, July 23, 2007  3:17 PM




 716 Chapter12  Cisco’s Wireless Technologies All remaining functionality is handled in the Cisco WLAN controller, so time sensitivity isn’t a concern but controller-wide visibility is certainly required. The following are some of the MAC-layer functions provided in the WLAN controller:  802.11 authentication  802.11 association and reassociation (mobility)  802.11 frame translation and bridgingAnd if a Cisco Wireless Controller in Appliance mode fails, its dropped Cisco APs will poll the network for another Cisco Wireless Controller. When an online Cisco Wireless Controller has any remaining AP ports, the management interface listens to the network for Cisco AP poll-ing messages to auto-discover, associate, and communicate with as many Cisco APs as it can.

 Basically, the split-MAC architecture allows the splitting of 802.11 protocol packets between the Cisco LWAPP-based AP that handles real-time portions of the protocol and the WLAN controller that handles any items that are not  time sensitive. MESH and LWAPP As more vendors migrate to a mesh hierarchical design, and as larger networks are built using lightweight access points, we really need a standardized protocol that governs how lightweight access points communicate with WLAN systems. This is exactly the role filled by one of the Internet Engineering Task Force’s (IETF’s) latest draft specification, Lightweight Access Point Protocol (LWAPP).With LWAPP, large multi-vendor wireless networks can be deployed with maximum capabil-ities and increased flexibility. Well…okay, this is mostly true. No one, and I do mean no one, has actually deployed a Cisco and Motorola network within the same company and is sitting back smugly saying, “Dude, this is really cool!” They’re saying something loud for sure, but it isn’t that! Cisco is Cisco and Motorola is well, not Cisco, and even though they supposedly run the same IETF protocols, they just don’t seem to see the standards exactly the same way. Basically, they don’t play well with each other.So, let’s say we’re using only Cisco. (Hey, we already have an unlimited budget here, so why not put in all Cisco too, I mean, this is a “Cisco” book, right?)Okay—so Cisco’s mesh networking infrastructure is decentralized and comparably inex-pensive for all the nice things it provides because each node only needs to transmit as far as the next node. Nodes act as repeaters to transmit data from nearby nodes to peers that are too far away for a manageable cabled connection, resulting in a network that can span a really large distance, especially over rough or difficult terrain. Figure 12.8 shows a large meshed environ-ment using Cisco 1520 APs to “umbrella” an area with wireless connectivity:Plus, mesh networks also happen to be extremely reliable—since each node can potentially be connected to several other nodes, if one of them drops out of the network because of hard-ware failure or something, its neighbors simply find another route. So you get extra capacity and fault tolerance by simply adding more nodes.

 

10089.book  Page 716  Monday, July 23, 2007  3:17 PM




 Cisco’s Unified Wireless Solution 717 FIGURE12.8 Typical Large meshed outdoor environment Wireless mesh connections between AP nodes are formed with a radio, providing many possible paths from a single node to other nodes. Paths through the mesh network can change in response to traffic loads, radio conditions, or traffic prioritization.Cisco LWAPP-enabled mesh access points are configured, monitored, and operated from and through any Cisco Wireless LAN Controller deployed in the Cisco Mesh Networking Solution—and they must go through a controller, which is why having redundant controllers is an absolute necessary.Let’s define a couple terms used in mesh networks: Root Access Points (RAPs) This access point is connected to the wired network and serves as the “root” or “gateway” to the wired network. RAPs have a wired connection back to a Cisco Wireless LAN Controller. They use the backhaul wireless interface to communicate with neighboring Mesh APs. Mesh Access Points (MAPs) The Mesh APs are remote APs that are typically located on rooftops or towers and can connect up to 32 MAPs over a 5GHz backhaul. During bootup, an access point will try to become a RAP if it’s connected to the wired network. Conversely, if a RAP loses its wired network connection, it will attempt to become a MAP and will search for a RAP.A typical mesh network would include the devices shown in Figure 12.9.In Figure 12.9, you can see that there’s one RAP connected to the infrastructure, and the MAPs connect to each other as well to the controller through the RAP.But we’re not quite done with this yet—I want to explain one more mesh term before we get into wireless security: AWPP.

Mesh controller Mesh controller Mesh is a network topology in which devices are connected with many redundant connections between nodes. 

 

10089.book  Page 717  Monday, July 23, 2007  3:17 PM




 718 Chapter12  Cisco’s Wireless Technologies FIGURE12.9 Typical devices found in a Cisco mesh network AWPP Each AP runs the Adaptive Wireless Path Protocol (AWPP)—a new protocol designed from the ground up by Cisco specifically for the wireless environment. This protocol allows RAPs to communicate with each other to determine the best path back to the wired network via the RAP. Once the optimal path is established, AWPP continues to run in the background to establish alternative routes back to the RAP just in case the topology changes or conditions cause the link strength to weaken.This protocol takes into consideration things like interference and characteristics of the specific radio so that the mesh can be self-configuring and self-healing. AWPP actually has the ability to consider all relevant elements of the wireless environment so that the mesh net-work’s functionality isn’t disrupted and can provide consistent coverage.This is pretty powerful considering how truly dynamic a wireless environment is. When there’s interference or if APs are added or removed, the Adaptive Wireless Path Protocol reconfigures the path back to the rooftop AP (RAP). Again, in response to the highly dynamic wireless environment, AWPP uses a “stickiness” factor to mitigate routes that ensure that an event, such as a large truck passing through the mesh causing a temporary disruption, doesn’t cause the mesh to change unnecessarily.

 For hands-on training with the Cisco Mesh and Mobility (1500/3200 Integra- tion) product line, see  www.globalnettraining.com.Wireless SecurityBy default, wireless security is nonexistent on access points and clients. The original 802.11 committee just didn’t imagine that wireless hosts would one day outnumber bounded media hosts, but that’s truly where we’re headed. Also, and unfortunately, just like with the IPv4 routed protocol, engineers and scientists didn’t add security standards that are robust enough 

MAP  MAP RAP Controller 

10089.book  Page 718  Monday, July 23, 2007  3:17 PM




Cisco’s Unified Wireless Solution719to work in a corporate environment. So we’re left with proprietary solution add-ons to aid us in our quest to create a secure wireless network. And no—I’m not just sitting here bashing the standards committees because the security problems we’re experiencing were also created by the U.S. government because of export issues with its own security standards. Our world is a complicated place, so it follows that our security solutions are going to be as well.A good place to start is by discussing the standard basic security that was added into the original 802.11 standards and why those standards are way too flimsy and incomplete to enable us to create a secure wireless network relevant to today’s challenges.Open AccessAll Wi-Fi Certified wireless LAN products are shipped in “open-access” mode, with their secu-rity features turned off. While open access or no security may be appropriate and acceptable for public hot spots such as coffee shops, college campuses, and maybe airports, it’s definitely not an option for an enterprise organization, and likely not even adequate for your private home network.Security needs to be enabled on wireless devices during their installation in enterprise environments. It may come as quite a shock, but some companies actually don’t enable any WLAN security features. Obviously, the companies that do this are exposing their networks to tremendous risk!The reason that the products are shipped with open access is so that any person who knows absolutely nothing about computers can just buy an access point, plug it into their cable or DSL modem, and voilà—they’re up and running. It’s marketing, plain and simple, and sim-plicity sells.SSIDs, WEP, and MAC Address AuthenticationWhat the original designers of 802.11 did to create basic security was include the use of Service Set Identifiers (SSIDs), open or shared-key authentication, static Wired Equivalency Protocol (WEP), and optional Media Access Control (MAC) authentication. Sounds like a lot, but none of these really offer any type of serious security solution—all they may be close to adequate for is use on a common home network. But we’ll go over them anyway…SSID is a common network name for the devices in a WLAN system that create the wireless LAN. An SSID prevents access by any client device that doesn’t have the SSID. The thing is, by default, an access point broadcasts its SSID in its beacon many times a second. And even if SSID broadcasting is turned off, a bad guy can discover the SSID by monitoring the network and just waiting for a client response to the access point. Why? Because, believe it or not, that informa-tion, as regulated in the original 802.11 specifications, must be sent in the clear—how secure!Two types of authentication were specified by the IEEE 802.11 committee: open and shared-key authentication. Open authentication involves little more than supplying the cor-rect SSID—but it’s the most common method in use today. With shared-key authentication, the access point sends the client device a challenge-text packet that the client must then encrypt with the correct Wired Equivalency Protocol (WEP) key and return to the access point. With-out the correct key, authentication will fail and the client won’t be allowed to associate with the access point. But shared-key authentication is still not considered secure because all an 

10089.book  Page 719  Monday, July 23, 2007  3:17 PM




720Chapter12 Cisco’s Wireless Technologiesintruder has to do to get around this is detect both the clear-text challenge and the same chal-lenge encrypted with a WEP key and then decipher the WEP key. Surprise—shared key isn’t used in today’s WLANs because of clear-text challenge.With open authentication, even if a client can complete authentication and associate with an access point, the use of WEP prevents the client from sending and receiving data from the access point unless the client has the correct WEP key. A WEP key is composed of either 40 or 128 bits and, in its basic form, is usually statically defined by the network administrator on the access point and all clients that communicate with that access point. When static WEP keys are used, a network administrator must perform the time-consuming task of entering the same keys on every device in the WLAN. Obviously, we now have fixes for this because this would be administratively impossible in today’s huge corporate wireless networks!Last, client MAC addresses can be statically typed into each access point, and any of them that show up without that MAC addresses in the filter table would be denied access. Sounds good, but of course all MAC layer information must be sent in the clear—anyone equipped with a free wireless sniffer can just read the client packets sent to the access point and spoof their MAC address.WEP can actually work if administered correctly. But basic static WEP keys are no longer a viable option in today’s corporate networks without some of the proprietary fixes that run on top of it. So let’s talk about some of these now.WPA or WPA 2 Pre-Shared KeyOkay, now we’re getting somewhere. Although this is another form of basic security that’s really just an add-on to the specifications, WPA or WPA2 Pre-Shared Key (PSK) is a better form of wireless security than any other basic wireless security method mentioned so far. I did say basic.The PSK verifies users via a password or identifying code (also called a passphrase) on both the client machine and the access point. A client only gains access to the network if its pass-word matches the access point’s password. The PSK also provides keying material that TKIP or AES uses to generate an encryption key for each packet of transmitted data. While more secure than static WEP, PSK still has a lot in common with static WEP in that the PSK is stored on the client station and can be compromised if the client station is lost or stolen even though finding this key isn’t all that easy to do. It’s a definite recommendation to use a strong PSK passphrase that includes a mixture of letters, numbers, and nonalphanumeric characters.Wi-Fi Protected Access (WPA) is a standard developed in 2003 by the Wi-Fi Alliance, formerly known as WECA. WPA provides a standard for authentication and encryption of WLANs that’s intended to solve known security problems existing up to and including the year 2003. This takes into account the well-publicized AirSnort and man-in-the-middle WLAN attacks.WPA is a step toward the IEEE 802.11i standard and uses many of the same components, with the exception of encryption—802.11i uses AES encryption. WPA’s mechanisms are designed to be implementable by current hardware vendors, meaning that users should be able to implement WPA on their systems with only a firmware/software modification.

The IEEE 802.11i standard has been sanctioned by WPA and is termed WPA version 2.

10089.book  Page 720  Monday, July 23, 2007  3:17 PM




Configuring Our Wireless Internetwork721Cisco Unified Wireless Network SecurityThe Cisco Unified Wireless Network delivers many innovative Cisco enhancements and sup-ports Wi-Fi Protected Access (WPA) and Wi-Fi Protected Access 2 (WPA2), which provide access control per user, per session via mutual authentication and data privacy and through strong dynamic encryption. Quality of service (QoS) and mobility are integrated into this solu-tion to enable a rich set of enterprise applications.The Cisco Unified Wireless Network provides the following:Secure Connectivity for WLANsStrong dynamic encryption keys that automatically change on a configurable basis to protect the privacy of transmitted data. WPA-TKIP includes encryption enhancements like MIC, per-packet keys via initializa-tion vector hashing, and broadcast key rotation. WPA2-AES is the “gold standard” for data encryption.Trust and Identity for WLANsA robust WLAN access control that helps to ensure that legitimate clients associate only with trusted access points rather than rogue, or unauthorized access points. It’s provided per user, per session via mutual authentication using IEEE 802.1X, a variety of Extensible Authentication Protocol (EAP) types, a Remote Authentication Dial-In User Service (RADIUS), and a Authentication, Authorization, and Accounting (AAA) server. It supports the following: The broadest range of 802.1X authentication types, client devices, and client operating systems on the market RADIUS accounting records for all authentication attemptsThreat Defense for WLANsDetection of unauthorized access, network attacks, and rogue access points via an Intrusion Prevention System (IPS), WLAN NAC, and advanced location services. Cisco’s IPS allows IT managers to continually scan the RF environment, detect rogue access points and unauthorized events, simultaneously track thousands of devices, and miti-gate network attacks. NAC has been specifically designed to help ensure that all wired and wireless endpoint devices like PCs, laptops, servers, and PDAs that are trying to access net-work resources are adequately protected from security threats. NAC allows organizations to analyze and control all devices coming into the network.Okay—let’s configure some wireless devices now!Configuring Our Wireless InternetworkConfiguring through the SDM is definitely the easiest way to go for wireless configurations, that is, if you’re using any type of security. And of course you should be! Basically, all you need to do to bring up an access point is to just turn it on. But if you do have a wireless card in your router, you’ll need to configure it just as I showed you back in Chapter 4.

10089.book  Page 721  Monday, July 23, 2007  3:17 PM




722Chapter12 Cisco’s Wireless TechnologiesHere’s a screen shot of my R2 router showing that I can configure the wireless card I have installed in slot 3.There really isn’t too much you can do from within SDM itself, but if I were to click on the Edit Interface/Connection tab and then click Summary, I could enable and disable the inter-face, as well as click the Edit button, which would allow me to add NAT, access lists, and so on to the interface.

10089.book  Page 722  Monday, July 23, 2007  3:17 PM




Configuring Our Wireless Internetwork723From either the Create Connection screen shown in the first screen of this section, or from the screen that appears when you click the Edit button of the second screen, you can click Launch Wireless Application. This will open up a new HTTP screen that your wireless device is configured from called Express Set-up.This is the same screen you would see if you just typed HTTP into an access point—one like our 1242AP. The SDM will be used with wireless interfaces for monitoring, to provide statis-tics, and for gaining access into the wireless configuration mode on a router that has wireless interfaces. This is so we don’t have to use the CLI for the hard configurations.

10089.book  Page 723  Monday, July 23, 2007  3:17 PM




724Chapter12 Cisco’s Wireless TechnologiesAgain, you can only configure some basic information from here. But from the next screen, Wireless Express Security, we can configure the wireless AP in either bridging mode or routing mode—a really cool feature!

10089.book  Page 724  Monday, July 23, 2007  3:17 PM




Configuring Our Wireless Internetwork725The next screen shows the wireless interfaces and the basic settings.

10089.book  Page 725  Monday, July 23, 2007  3:17 PM




726Chapter12 Cisco’s Wireless TechnologiesThe following screen shot is the second part of the Wireless Interfaces screen.

10089.book  Page 726  Monday, July 23, 2007  3:17 PM




Configuring Our Wireless Internetwork727Under the Wireless Security heading is really where HTTP management shines! You can configure encryption, add SSIDs and configure your Radius sever settings.

10089.book  Page 727  Monday, July 23, 2007  3:17 PM




728Chapter12 Cisco’s Wireless TechnologiesNow, if we just HTTP in to the 1242AG AP, we’ll see this screen.This looks amazingly like the APs we’ll find in our ISR routers, and we can configure the same devices and security too.

10089.book  Page 728  Monday, July 23, 2007  3:17 PM




Exam Essentials729SummaryLike rock ’n’ roll, wireless technologies are here to stay, and for those of us who have come to depend on wireless technologies, it’s actually pretty hard to imagine a world without wire-less networks—what did we do before cell phones?So we began this chapter by exploring the essentials and fundamentals of how wireless net-works function.Springing off that foundation, I then introduced you to the basics of wireless RF and the IEEE standards. We discussed 802.11 from its inception through its evolution to current and near future standards and talked about the subcommittees who create them.All of this lead into a discussion of wireless security—or rather, non-security for the most part, which logically directed us to Cisco’s proprietary answer to this dilemma: the Cisco Uni-fied Wireless Solution, which we went over in detail.We finished the chapter by actually using the SDM to configure our wireless network and its associated devices.Exam EssentialsUnderstand the IEEE 802.11a specification.802.11a runs in the 5GHz spectrum, and if you use the 802.11h extensions, you have 23 non-overlapping channels. 802.11a can run up to 54Mbps, but only if you are less than 50 feet from an access point.Understand the IEEE 802.11b specification.IEEE 802.11b runs in the 2.4GHz range and has three non-overlapping channels. It can handle long distances, but with a maximum data rate of up to 11Mpbs.Understand the IEEE 802.11g specification.IEEE 802.11g is 802.11b’s big brother and runs in the same 2.4GHz range, but it has a higher data rate of 54Mbps if you are less than 100 feet from an access point.

10089.book  Page 729  Monday, July 23, 2007  3:17 PM




730Chapter12 Cisco’s Wireless TechnologiesWritten Lab 12In this section, write the answers to the following wireless questions:1.What is the maximum data rate of IEEE 802.11b?2.What is the maximum data rate of IEEE 802.11g?3.What is the maximum data rate of IEEE 802.11a?4.What is the frequency range of IEEE 802.11b?5.What is the frequency range of IEEE 802.11g?6.What is the frequency range of IEEE 802.11a?7.In Cisco’s Unified Wireless Solution, what is the split-MAC architecture?8.The IEEE 802.11h specification adds what two extensions to IEEE 802.11a?9.Which IEEE committee has been sanctioned by WPA and is called WPA 2?10.The IEEE 802.11a basic standard has how many non-overlapping channels?(The answers to Written Lab 12 can be found following the answers to the review questions for this chapter.)

10089.book  Page 730  Monday, July 23, 2007  3:17 PM




Review Questions731Review Questions

The following questions are designed to test your understanding of this chap-ter’s material. For more information on how to get additional questions, please see this book’s Introduction.1.In Cisco’s Unified Wireless Solution, what is the split-MAC architecture?A.The split-MAC architecture uses MAC addresses to create a forward/filter table and break up collision domains.B.The split-MAC architecture allows the splitting of 802.11 protocol packets between the AP and the controller to allow processing by both devices.C.The split-MAC architecture uses MAC addresses on the wireless network and IP addresses on the wired network.D.The split-MAC architecture uses MAC addresses to create a forward/filter table and break up broadcast domains.2.What is the frequency range of the IEEE 802.11b standard?A.2.4GbpsB.5GbpsC.2.4GHzD.5GHz3.What is the frequency range of the IEEE 802.11a standard?A.2.4GbpsB.5GbpsC.2.4GHzD.5GHz4.What is the frequency range of the IEEE 802.11g standard?A.2.4GbpsB.5GbpsC.2.4GHzD.5GHz5.How many non-overlapping channels are available with 802.11h?A.3B.12C.23D.40

10089.book  Page 731  Monday, July 23, 2007  3:17 PM




732Chapter12 Cisco’s Wireless Technologies6.How many non-overlapping channels are available with 802.11g?A.3B.12C.23D.407.How many non-overlapping channels are available with 802.11b?A.3B.12C.23D.408.How many non-overlapping channels are available with 802.11a?A.3B.12C.23D.409.What is the maximum data rate for the 802.11a standard?A.6MbpsB.11MbpsC.22MbpsD.54Mbps10.What is the maximum data rate for the 802.11g standard?A.6MbpsB.11MbpsC.22MbpsD.54Mbps11.What is the maximum data rate for the 802.11b standard?A.6MbpsB.11MbpsC.22MbpsD.54Mbps12.What is the maximum distance with maximum data rate for 802.11a?A.About 65–75 feetB.About 90–100 feetC.About 150 feetD.Over 200 feet

10089.book  Page 732  Monday, July 23, 2007  3:17 PM




Review Questions73313.What is the maximum distance with maximum data rate for 802.11g?A.About 65–75 feetB.About 90–100 feetC.About 150 feetD.Over 200 feet14.What is the maximum distance with maximum data rate for 802.11b?A.About 65–75 feetB.About 90–100 feetC.About 150 feetD.Over 200 feet15.What is the maximum distance running the lowest data rate for 802.11b?A.About 100 feetB.About 175 feetC.About 300 feetD.About 350 feet16.What is the maximum distance running the lowest data rate for 802.11a?A.About 100 feetB.About 175 feetC.About 300 feetD.About 350 feet17.What is the maximum distance running the lowest data rate for 802.11g?A.About 100 feetB.About 175 feetC.About 300 feetD.About 350 feet18.Cisco’s Unified Wireless Solution provides a mesh solution. What devices do you absolutely need to purchase to run a Cisco solution? (Choose two.)A.WCSB.ControllerC.Access pointD.Bridge

10089.book  Page 733  Monday, July 23, 2007  3:17 PM




734Chapter12 Cisco’s Wireless Technologies19.You are connecting your access point and it is set to root. What does Extended Service Set ID mean?A.That you have more than one access point and they are in the same SSID connected by a distribution systemB.That you have more than one access point and they are in separate SSIDs connected by a distribution systemC.That you have multiple access points, but they are placed physically in different buildingsD.That you have multiple access points, but one is a repeater access point20.You have a Cisco mesh network. What protocol allows multiple APs to connect with many redundant connections between nodes?A.LWAPPB.AWPPC.STPD.IEEE

10089.book  Page 734  Monday, July 23, 2007  3:17 PM




Answers to Review Questions735Answers to Review Questions1.B. The split-MAC architecture allows the splitting of 802.11 protocol packets between the Cisco LWAPP-based AP, which handles real-time portions of the protocol, and the WLAN controller, which handles those items that are not time sensitive.2.C. The IEEE 802.11b and IEEE 802.11b standards both run in the 2.4GHz RF range.3.D. The IEEE 802.11a standard runs in the 5GHz RF range.4.C. The IEEE 802.11b and IEEE 802.11b standards both run in the 2.4GHz RF range.5.C. The IEEE 802.11h standard provides an addition 11 channels to the 802.11a standard’s 12 non-overlapping channel for a total of 23 non-overlapping channels.6.A. The IEEE 802.11g standard provides three non-overlapping channels.7.A. The IEEE 802.11b standard provides three non-overlapping channels.8.B. The IEEE 802.11a standard provides up to 12 non-overlapping channels.9.D. The IEEE 802.11a standard provides a maximum data rate of up to 54Mbps.10.D. The IEEE 802.11g standard provides a maximum data rate of up to 54Mbps.11.B. The IEEE 802.11b standard provides a maximum data rate of up to 11Mbps.12.A. The IEEE 802.11a standard provides a maximum data rate of up to 54Mbps, but you need to be close to the access point, somewhere around 65 to 75 feet.13.B. The IEEE 802.11g standard provides a maximum data rate of up to 54Mbps, but you need to be close to the access point, somewhere around 90 to 100 feet.14.C. The IEEE 802.11b standard provides a maximum data rate of up to only 11Mbps, and you can be around 150 feet, maybe farther, depending on conditions.15.D. The IEEE 802.11b standard provides the lowest data rate at 1Mbps, but it also has the long-est distance, which is about 350 feet.16.B. The IEEE 802.11a standard’s lowest data rate is 6Mbps, but it can run from a distance of about 175 feet.17.C. The IEEE 802.11g standard’s lowest data rate is 6Mbps, but it can run from a distance of about 300 feet.18.B, C. The Cisco Unified Wireless Solution is a great product, but you must purchase special-ized devices. Cisco managed access points and a controller are the devices you need to purchase to run the Unified Wireless Solution.

10089.book  Page 735  Monday, July 23, 2007  3:17 PM




736Chapter12 Cisco’s Wireless Technologies19.A. Extended Service Set ID means that you have more than one access point and they all are set to the same SSID and all are connected together in the same VLAN or distribution system so users can roam.20.B. Each AP in a mesh network runs the Adaptive Wireless Path Protocol (AWPP). This proto-col allows RAPs to communicate with each other to determine the best path back to the wired network via the RAP.

10089.book  Page 736  Monday, July 23, 2007  3:17 PM




Answers to Written Lab 12737Answers to Written Lab 121.11Mbps2.54Mbps3.54Mbps4.2.4GHz5.2.4GHz6.5GHz7.The split-MAC architecture allows the splitting of 802.11 protocol packets between the Cisco LWAPP-based AP, which handles real-time portions of the protocol, and the WLAN controller, which handles those items that are not time sensitive.8.There are two new features to the 5GHz radio that are part of the 802.11h specification: Transmit Power Control (TPC) and Dynamic Frequency Selection (DFS).9.The IEEE 802.11i standard has been sanctioned by WPA and is termed WPA version 2.10.12

10089.book  Page 737  Monday, July 23, 2007  3:17 PM




10089.book  Page 738  Monday, July 23, 2007  3:17 PM




 

Chapter 13 Internet Protocol Version 6 (IPv6)

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Implement an IP addressing scheme and IP Services to meet network requirements in a medium-size Enterprise branch office network  Describe the technological requirements for running IPv6 in conjunction with IPv4 (including: protocols, dual stack, tunneling, etc)   Describe IPv6 addresses 

 

10089.book  Page 739  Monday, July 23, 2007  3:17 PM




 I hope you’re ready to learn about the nuts and bolts of Internet Protocol version 6 (IPv6), because you’re going to get the rub on it   in this chapter!You should have a solid hold on IPv4 by now, but if you think you could use a refresher, just page back to Chapter 3, “Subnetting, Variable Length Subnet Masks (VLSMs), and Trouble-shooting TCP/IP.” And if you’re not crystal clear on the address problems inherent to IPv4, you really should review Chapter 11, “Network Address Translation (NAT).”People refer to IPv6 as “the next-generation Internet protocol,” and it was originally created as the answer to IPv4’s inevitable, looming address-exhaustion crisis. Though you’ve probably heard a thing or two about IPv6 already, it has been improved even further in the quest to bring us the flexibility, efficiency, capability, and optimized functionality that can truly meet our ever-increasing needs. The capacity of its predecessor, IPv4, pales in comparison—and that’s the reason it will eventually fade into history completely.The IPv6 header and address structure has been completely overhauled, and many of the features that were basically just afterthoughts and addendums in IPv4 are now included as full-blown standards in IPv6. It’s seriously well equipped, poised, and ready to manage the mind-blowing demands of the Internet to come.I promise—really—to make this chapter pretty painless. In fact, you might even find your-self actually enjoying it—I definitely did! Because IPv6 is so complex yet elegant, innovative and chock-full of features, it fascinates me like some weird combination of a brand-new Lam-borghini and a riveting futuristic novel. Hopefully you’ll experience this chapter as the cool ride that I did writing it!

 For up-to-the-minute updates for this chapter, please see  www.lammle.com  and/or  www.sybex.com . Why Do We Need IPv6? Well, the short answer is, because we need to communicate, and our current system isn’t really cutting it anymore—kind of like how the Pony Express can’t compete with airmail. Just look at how much time and effort we’ve invested in coming up with slick new ways to conserve band-width and IP addresses. We’ve even come up with Variable Length Subnet Masks (VLSMs) in our struggle to overcome the worsening address drought.

 

10089.book  Page 740  Monday, July 23, 2007  3:17 PM




 The Benefits and Uses of IPv6 741 It’s reality—the number of people and devices that connect to networks increases each and every day. That’s not a bad thing at all—we’re finding new and exciting ways to communicate to more people all the time, and that’s a good thing. In fact, it’s a basic human need. But the fore-cast isn’t exactly blue skies and sunshine because, as I alluded to in this chapter’s introduction, IPv4, upon which our ability to communicate is presently dependent, is going to run out of addresses for us to use. IPv4 has only about 4.3 billion addresses available—in theory, and we know that we don’t even get to use all of those. There really are only about 250 million addresses that can be assigned to devices. Sure, the use of Classless Inter-Domain Routing (CIDR) and Net-work Address Translation (NAT) has helped to extend the inevitable dearth of addresses, but we will run out of them, and it’s going to happen within a few years. China is barely online, and we know there’s a huge population of people and corporations there that surely want to be. There are a lot of reports that give us all kinds of numbers, but all you really need to think about to convince yourself that I’m not just being an alarmist is the fact that there are about 6.5 billion people in the world today, and it’s estimated that just over 10 percent of that population is con-nected to the Internet—wow!That statistic is basically screaming at us the ugly truth that based on IPv4’s capacity, every person can’t even have a computer—let alone all the other devices we use with them. I have more than one computer, and it’s pretty likely you do too. And I’m not even including in the mix phones, laptops, game consoles, fax machines, routers, switches, and a mother lode of other devices we use every day! So I think I’ve made it pretty clear that we’ve got to do some-thing before we run out of addresses and lose the ability to connect with each other as we know it. And that “something” just happens to be implementing IPv6. The Benefits and Uses of IPv6 So what’s so fabulous about IPv6? Is it really the answer to our coming dilemma? Is it really worth it to upgrade from IPv4? All good questions—you may even think of a few more. Of course, there’s going to be that group of people with the time-tested and well-known “resistance to change syndrome,” but don’t listen to them. If we had done that years ago, we’d still be wait-ing weeks, even months for our mail to arrive via horseback. Instead, just know that the answer is a resounding YES! Not only does IPv6 give us lots of addresses (3.4  ×  10^38 = definitely enough), but there are many other features built into this version that make it well worth the cost, time, and effort required to migrate to it. Later in the chapter I’ll talk about all that effort in the section called “Migrating to IPv6.” In it, I’ll cover some of the transition types required to move from version 4 to version 6, and I promise you’ll discover that the huge benefits of migrating will vastly outweigh any associated cons.Today’s networks, as well as the Internet, have a ton of unforeseen requirements that simply were not considerations when IPv4 was created. We’ve tried to compensate with a collection of add-ons that can actually make implementing them more difficult than they would be if they were required by a standard. By default, IPv6 has improved upon and included many of those features as standard and mandatory. One of these sweet new standards is IPSec—a feature that provides end-to-end security that I’ll cover in Chapter 14, “Wide Area Networks.” Another little 

 

10089.book  Page 741  Monday, July 23, 2007  3:17 PM




 742 Chapter13  Internet Protocol Version 6 (IPv6) beauty is known as mobility, and as its name suggests, it allows a device to roam from one net-work to another without dropping connections.But it’s the efficiency features that are really going to rock the house! For starters, the header in an IPv6 packet have half the fields, and they are aligned to 64 bits, which gives us some seriously souped-up processing speed—compared to IPv4, lookups happen at light speed! Most of the information that used to be bound into the IPv4 header was taken out, and now you can choose to put it, or parts of it, back into the header in the form of optional exten-sion headers that follow the basic header fields.And of course there’s that whole new universe of addresses (3.4 x 10^38) we talked about already. But where did we get them? Did that Criss Angel–Mindfreak dude just show up and, Blammo? I mean, that huge proliferation of addresses had to come from somewhere! Well it just so happens that IPv6 gives us a substantially larger address space, meaning the address is a whole lot bigger—four times bigger as a matter of fact! An IPv6 address is actually 128 bits in length, and no worries—I’m going to break down the address piece by piece and show you exactly what it looks like coming up in the section “IPv6 Addressing and Expressions.” For now, let me just say that all that additional room permits more levels of hierarchy inside the address space and a more flexible address architecture. It also makes routing much more effi-cient and scalable because the addresses can be aggregated a lot more effectively. And IPv6 also allows multiple addresses for hosts and networks. This is especially important for enter-prises jonesing for availability. Plus, the new version of IP now includes an expanded use of multicast communication (one device sending to many hosts or to a select group), which will also join in to boost efficiency on networks because communications will be more specific.IPv4 uses broadcasts very prolifically, causing a bunch of problems, the worst of which is of course the dreaded broadcast storm—an uncontrolled deluge of forwarded broadcast traffic that can bring an entire network to its knees and devour every last bit of bandwidth. Another nasty thing about broadcast traffic is that it interrupts each and every device on the network. When a broadcast is sent out, every machine has to stop what it’s doing and respond to the traffic whether the broadcast is meant for it or not.But smile everyone: There is no such thing as a broadcast in IPv6 because it uses multi-cast traffic instead. And there are two other types of communication as well: unicast, which is the same as it is in IPv4, and a new type called anycast. Anycast communication allows the same address to be placed on more than one device so that when traffic is sent to one device addressed in this way,  it is routed to the nearest host that shares the same address. This is just the beginning—we’ll get more into the various types of communica-tion in the section called “Address Types.” IPv6 Addressing and Expressions Just as understanding how IP addresses are structured and used is critical with IPv4 address-ing, it’s also vital when it comes to IPv6. You’ve already read about the fact that at 128 bits, an IPv6 address is much larger than an IPv4 address. Because of this, as well as the new ways the addresses can be used, you’ve probably guessed that IPv6 will be more complicated to manage. But no worries! As I said, I’ll break down the basics and show you what the address 

 

10089.book  Page 742  Monday, July 23, 2007  3:17 PM




 IPv6 Addressing and Expressions 743 looks like, how you can write it, and what many of its common uses are. It’s going to be a little weird at first, but before you know it, you’ll have it nailed!So let’s take a look at Figure 13.1, which has a sample IPv6 address broken down into sections. FIGURE13.1 IPv6 address example

 Remember where you can find this subnet ID because we’ll use it in the sec-tion “Configuring IPv6 on Our internetwork” later in the chapter when we  configure our routers. So as you can now see, the address is truly much larger—but what else is different? Well, first, notice that it has eight groups of numbers instead of four and also that those groups are separated by colons instead of periods. And hey wait a second… there are letters in that address! Yep, the address is expressed in hexadecimal just like a MAC address is, so you could say this address has eight 16-bit hexadecimal colon-delimited blocks. That’s already quite a mouthful, and you probably haven’t even tried to say the address out loud yet!One other thing I want to point out is for when you set up your test network to play with IPv6, because I know you’re going to want to do that. When you use a web browser to make an HTTP connection to an IPv6 device, you have to type the address into the browser with brackets around the literal address. Why? Well, a colon is already being used by the browser for specifying a port number. So basically, if you don’t enclose the address in brackets, the browser will have no way to identify the information.Here’s an example of how this looks:

 http://[2001:0db8:3c4d:0012:0000:0000:1234:56ab]/default.html Now obviously if you can, you would rather use names to specify a destination (like www.lammle.com), but even though it’s definitely going to be a pain in the rear, we just have to accept the fact that sometimes we have to bite the bullet and type in the address number. So it should be pretty clear that DNS is going to become extremely important when imple-menting IPv6. Shortened Expression The good news is there are a few tricks to help rescue us when writing these monster addresses. For one thing, you can actually leave out parts of the address to abbreviate it, but to get away with doing that you have to follow a couple of rules. First, you can drop any leading zeros in each of the individual blocks. After you do that, the sample address from earlier would then look like this:

 2001:db8:3c4d:12:0:0:1234:56ab

Interface ID2001:0db8:3c4d:0012:0000:0000:1234:56ab Global prefix Subnet 

 

10089.book  Page 743  Monday, July 23, 2007  3:17 PM




 744 Chapter13  Internet Protocol Version 6 (IPv6) Okay, that’s a definite improvement—at least we don’t have to write all of those extra zeros! But what about whole blocks that don’t have anything in them except zeros? Well, we can kind of lose those too—at least some of them. Again referring to our sample address, we can remove the two blocks of zeros by replacing them with double colons, like this:

 2001:db8:3c4d:12::1234:56ab Cool—we replaced the blocks of all zeros with double colons. The rule you have to follow to get away with this is that you can only replace one contiguous block of zeros in an address. So if my address has four blocks of zeros and each of them were separated, I just don't get to replace them all; remember the rule is that you can only replace one contiguous block with a double colon. Check out this example:

 2001:0000:0000:0012:0000:0000:1234:56ab And just know that you  can’t  do this:

 2001::12::1234:56ab Instead, this is the best that you can do:

 2001::12:0:0:1234:56ab   The reason why the above example is our best shot is that if we remove two sets of zeros, the device looking at the address will have no way of knowing where the zeros go back in. Basically, the router would look at the incorrect address and say, “Well, do I place two blocks into the first set of double colons and two into the second set, or do I place three blocks into the first set and one block into the second set?” And on and on it would go because the infor-mation the router needs just isn’t there. Address Types We’re all familiar with IPv4’s unicast, broadcast, and multicast addresses that basically define who or at least how many other devices we’re talking to. But as I mentioned, IPv6 adds to that trio and introduces the anycast. Broadcasts, as we know them, have been eliminated in IPv6 because of their cumbersome inefficiency.So let’s find out what each of these types of IPv6 addressing and communication methods do for us. Unicast Packets addressed to a unicast address are delivered to a single interface. For load balancing, multiple interfaces can use the same address. There are a few different types of uni-cast addresses, but we don’t need to get into that here. Global unicast addresses These are your typical publicly routable addresses, and they’re the same as they are in IPv4. Link-local addresses These are like the private addresses in IPv4 in that they’re not meant to be routed. Think of them as a handy tool that gives you the ability to throw a temporary LAN together for meetings or for creating a small LAN that’s not going to be routed but still needs to share and access files and services locally.

 

10089.book  Page 744  Monday, July 23, 2007  3:17 PM




 IPv6 Addressing and Expressions 745 Unique local addresses These addresses are also intended for non-routing purposes, but they are nearly globally unique, so it’s unlikely you’ll ever have one of them overlap. Unique local addresses were designed to replace site-local addresses, so they basically do almost exactly what IPv4 private addresses do—allow communication throughout a site while being routable to multiple local networks. Site-local addresses were denounced as of September 2004. Multicast Again, same as in IPv4, packets addressed to a multicast address are delivered to all interfaces identified by the multicast address. Sometimes people call them one-to-many addresses. It’s really easy to spot a multicast address in IPv6 because they always start with  FF . I’ll get into greater detail about multicast operation in the section “How IPv6 Works in an Internetwork.” Anycast Like multicast addresses, an anycast address identifies multiple interfaces, but there’s a big difference: the anycast packet is only delivered to one address—actually, to the first one it finds defined in terms of routing distance. And again, this address is special because you can apply a single address to more than one interface. You could call them one-to-one-of-many addresses, but just saying “anycast” is a lot easier.You’re probably wondering if there are any special, reserved addresses in IPv6 because you know they’re there in IPv4. Well there are—plenty of them! Let’s go over them now. Special Addresses I’m going to list some of the addresses and address ranges that you should definitely make a point to remember because you’ll eventually use them. They’re all special or reserved for specific use, but unlike IPv4, IPv6 gives us a galaxy of addresses, so reserving a few here and there doesn’t hurt a thing! 0:0:0:0:0:0:0:0   Equals ::. This is the equivalent of IPv4’s 0.0.0.0, and is typically the source address of a host when you’re using stateful configuration. 0:0:0:0:0:0:0:1    Equals ::1. The equivalent of 127.0.0.1 in IPv4. 0:0:0:0:0:0:192.168.100.1 This is how an IPv4 address would be written in a mixed IPv6/IPv4 network environment. 2000::/3 The global unicast address range. FC00::/7 The unique local unicast range. FE80::/10 The link-local unicast range. FF00::/8 The multicast range. 3FFF:FFFF::/32  Reserved for examples and documentation. 2001:0DB8::/32  Also reserved for examples and documentation. 2002::/16 Used with 6to4, which is the transition system—the structure that allows IPv6 packets to be transmitted over an IPv4 network without the need to configure explicit tunnels.We’ll get more into this later in “Migrating to IPv6,” but for now let me show you how IPv6 actually works in an internetwork. We all know how IPv4 works, so let’s see what’s new.

 

10089.book  Page 745  Monday, July 23, 2007  3:17 PM




 746 Chapter13  Internet Protocol Version 6 (IPv6) How IPv6 Works in an Internetwork It’s time to explore the finer points of IPv6. A great place to start is by showing you how to address a host and what gives it the ability to find other hosts and resources on a network. I’ll also demonstrate a device’s ability to automatically address itself—something called stateless autoconfiguration, plus another type of autoconfiguration known as stateful. Keep in mind that stateful autoconfiguration uses a DHCP server in a very similar way to how it’s used in an IPv4 configuration. I’ll also show you how Internet Control Message Protocol (ICMP) and multicast works for us on an IPv6 network. Autoconfiguration Autoconfiguration is an incredibly useful solution because it allows devices on a network to address themselves with a link-local unicast address. This process happens through first learn-ing the prefix information from the router and then appending the device’s own interface address as the interface ID. But where does it get that interface ID? Well, you know every device on an Ethernet network has a physical MAC address, and that’s exactly what’s used for the interface ID. But since the interface ID in an IPv6 address is 64 bits in length and a MAC address is only 48 bits, where do the extra 16 bits come from? The MAC address is padded in the middle with the extra bits—it’s padded with FFFE.For example, let’s say I have a device with a MAC address that looks like this: 0060.d673.1987.After it’s been padded, it would look like this: 0260.d6FF.FE73.1987.So where did that 2 in the beginning of the address come from? Another good question. You see, part of the process of padding (called modified eui-64 format) changes a bit to specify if the address is locally unique or globally unique. And the bit that gets changed is the seventh bit in the address. A bit value of 1 means globally unique, and a bit value of 0 means locally unique, so looking at this example, would you say that this address is globally or locally unique? If you answered that it’s a globally unique address, you’re right! Trust me, this is going to save you time in addressing your host machines because they communicate with the router to make this happen.To perform autoconfiguration, a host goes through a basic two-step process: 1. First, the host needs the prefix information (similar to the network portion of an IPv4 address) to configure its interface, so it sends a router solicitation (RS) request for it. This RS is then sent out as a multicast to each router’s multicast address. The actual informa-tion being sent is a type of ICMP message, and like everything in networking, this ICMP message has a number that identifies it. The RS message is ICMP type 133.  2. The router answers back with the required prefix information via a router advertisement (RA). An RA message also happens to be a multicast packet that’s sent to each node’s multicast address and is ICMP type 134. RA messages are sent on a periodic basis, but the host sends the RS for an immediate response so it doesn’t have to wait until the next scheduled RA to get what it needs.

 

10089.book  Page 746  Monday, July 23, 2007  3:17 PM




 How IPv6 Works in an Internetwork 747 These two steps are shown in Figure 13.2. FIGURE13.2 Two steps to IPv6 autoconfiguration  By the way, this type of autoconfiguration is also known as stateless autoconfiguration because it doesn’t contact or connect and receive any further information from the other device. We’ll get to stateful configuration when we talk about DHCPv6 in a minute.Now let’s take a look at how to configure Cisco routers with IPv6. Configuring Cisco Routers with IPv6 In order to enable IPv6 on a router, you have to use the  ipv6 unicast-routing  global con-figuration command:

 Corp(config)# ipv6 unicast-routing By default, IPv6 traffic forwarding is disabled, so using this command enables it. Also, as you’ve probably guessed, IPv6 isn’t enabled by default on any interfaces either, so we have to go to each interface individually and enable it.There are a few different ways to do this, but a really easy way is to just add an address to the interface. You use the interface configuration command  ipv6 address <ipv6prefix>/<prefix-length> [eui-64] to get this done. Here’s an example:

 Corp(config-if)# ipv6 address 2001:db8:3c4d:1:0260.d6FF.FE73.1987/64 You can specify the entire 128-bit global IPv6 address or you can use the eui-64 option. Remember, the eui-64 format allows the device to use its MAC address and pad it to make the interface ID. Check it out:

 Corp(config-if)# ipv6 address 2001:db8:3c4d:1::/64 eui-64 As an alternative to typing in an IPv6 address on a router, you can enable the interface instead to permit the application of an automatic link-local address.

Host receives the RA and included prefix, allowing it to autoconfigure its interface.  Step 1: Host sends RS message.  Step 2: Router sends RS message. 

 

10089.book  Page 747  Monday, July 23, 2007  3:17 PM




 748 Chapter13  Internet Protocol Version 6 (IPv6)

 Remember, if you only have a link-local address, you will only be able to com- municate on that local subnet. To configure a router so that it only uses link-local addresses, use the  ipv6 enable  inter-face configuration command:

 Corp(config-if)# ipv6 enable Okay, now let’s dive into stateful IPv6 by configuring a DHCP server for IPv6 use. DHCPv6 DHCPv6 works pretty much the same way DHCP does in v4, with the obvious difference that it supports the new addressing scheme for IPv6. And it might come as a surprise, but there are a couple of other options that DHCP still provides for us that autoconfiguration doesn’t. I’m serious—there’s absolutely no mention of DNS servers, domain names, or many of the other options that DHCP has always provided for us via IPv4 with autoconfiguration. This is a big reason why it’s likely we’ll still be using DHCP in IPv6 most of the time.Upon booting up in IPv4, a client sent out a DHCP discover message looking for a server to give it the information it needs. But remember, in IPv6, the RS and RA process happens first. If there’s a DHCPv6 server on the network, the RA that comes back to the client will tell it if DHCP is available for use. If a router isn’t found, the client will respond by sending out a DHCP solicit message—a solicit message that’s actually a multicast message addressed with a source of ff02::1:2, meaning all DHCP agents, both servers and relays.It’s good to know that there’s some support for DHCPv6 in the Cisco IOS. But it’s limited to a stateless DHCP server, meaning it doesn’t offer any address management of the pool, plus the options available for configuring that address pool are limited to the DNS, domain name, and SIP servers only.This means that you’re definitely going to need some other server around that can supply and dispense all the additional, required information, as well as manage the address assignment.Anyway, here’s how the configuration looks for the stateless DHCP server in the router IOS—it’s really close to what we’d configure with IPv4:

 Router1(config)# ipv6 dhcp pool ? WORD  DHCP pool nameRouter1(config)# ipv6 dhcp pool test Router1(config-dhcp)# ? IPv6 DHCP configuration commands:  default            Set a command to its defaults  dns-server         DNS servers  domain-name        Domain name to complete unqualified host names  exit               Exit from DHCPv6 configuration mode

 

10089.book  Page 748  Monday, July 23, 2007  3:17 PM




 How IPv6 Works in an Internetwork 749   no                 Negate a command or set its defaults  prefix-delegation  IPv6 prefix delegation  sip                SIP Servers optionsRouter1(config-dhcp)# dns-server ?   Hostname or X:X:X:X::X  Server’s name or IPv6 addressRouter1(config-dhcp)# domain-name lammle.com Router1(config-dhcp)# prefix-delegation ?   X:X:X:X::X/<0-128>  IPv6  x:x::y/<z>  aaa                 Acquire prefix from AAA  pool                IPv6 prefix poolRouter1(config-dhcp)# prefix-delegation pool ?   WORD  IPv6 prefix poolRouter1(config-dhcp)# prefix-delegation pool test ?   lifetime  Configure prefix lifetimes  <cr>Router1(config-dhcp)# prefix-delegation pool test lifetime ?  <60-4294967295>  Valid lifetime (seconds)  at               Expire prefix at a specific time/date  infinite         Infinite valid lifetimeRouter1(config-dhcp)#prefix-delegation pool test lifetime 3600 ?  <60-4294967295>  Preferred lifetime (seconds)  infinite         Infinite preferred lifetimeRouter1(config-dhcp)#prefix-delegation pool test lifetime 3600 3600 ?  <cr>

Router1(config-dhcp)#prefix-delegation pool test lifetime 3600 3600Notice that just like in DHCP with IPv4, you don’t need to set a lifetime.Okay—now that I’ve got the pool configured, I just have to assign it to an interface, a departure from IPv4:

Router1(config)#int fa 0/0      Router1(config-if)#ipv6 dhcp server ?  WORD  Name of IPv6 DHCP pool

Router1(config-if)#ipv6 dhcp server testSweet—we now have a fully configured DHCPv6 server applied to our interface fa0/0!ICMPv6IPv4 used ICMP for many things, such as error messages like destination unreachable, and troubleshooting functions like Ping and Traceroute. ICMPv6 still does those things for us, but unlike its predecessor, the v6 flavor isn’t implemented as a separate layer 4 protocol. It’s an 

10089.book  Page 749  Monday, July 23, 2007  3:17 PM




750Chapter13 Internet Protocol Version 6 (IPv6)integrated part of IPv6 and is carried after the basic IPv6 header information as an extension header. And ICMPv6 adds another cool feature—it prevents IPv6 from doing any fragmenta-tion through an ICMPv6 process called path MTU discovery. This is how it works: The source node of a connection will send a packet that’s equal to the MTU size of its local link’s MTU. As this packet traverses the path toward its destination, any link that has an MTU smaller than the size of the current packet will force the intermediate router to send a “packet too big” message back to the source machine. This message tells the source node what the maximum size is that the restrictive link will allow and asks the source to send a new scaled-down packet that can pass through. This process will continue until the des-tination is finally reached, with the source node now sporting the new path’s MTU. So now, when the rest of the data packets are transmitted, they’ll be protected from fragmentation.ICMPv6 now takes over the task of finding the address of other devices on the local link. Address Resolution Protocol used to perform this function for IPv4, but that’s been renamed Neighbor Discovery in ICMPv6. This process is accomplished by using a multicast address called the solicited node address, and all hosts join this multicast group when they connect to the network. Part of their IPv6 address (the 24 bits farthest to the right) is added to the end of the multicast address FF02:0:0:0:0:1:FF/104. When this address is queried, the correspond-ing host will send back its layer 2 address. Devices can find and keep track of other neighbor devices on the network in pretty much the same way. When I talked about RA and RS mes-sages earlier and told you that they use multicast traffic to request and send address informa-tion, that too was a function of ICMPv6—specifically, neighbor discovery.In IPv4, the protocol IGMP was used to allow a host device to tell its local router that it was joining a multicast group and would like to receive the traffic for that group. This IGMP function has been replaced by ICMPv6, and the process has been renamed multicast listener discovery.IPv6 Routing ProtocolsMost of the routing protocols we’ve already discussed have been upgraded for use in IPv6 net-works. Also, many of the functions and configurations that we’ve already learned will be used in almost the same way as they’re used now. Knowing that broadcasts have been eliminated in IPv6, it follows that any protocols that use entirely broadcast traffic will go the way of the dodo—but unlike the dodo, it’ll be good to say goodbye to these bandwidth-hogging, performance-annihilating little gremlins!The routing protocols that we’ll still use in v6 got a new name and a facelift. Let’s talk about a few of them now.First on the list is RIPng (next generation). Those of you who have been in IT for awhile know that RIP has worked very well for us on smaller networks, which happens to be the very reason it didn’t get whacked and will still be around in IPv6. And we still have EIGRPv6 because it already had protocol-dependent modules and all we had to do was add a new one to it for the IPv6 protocol. Rounding out our group of protocol survivors is OSPFv3—that’s not a typo, it really is v3. OSPF for IPv4 was actually v2, so when it got its upgrade to IPv6, it became OSPFv3.

10089.book  Page 750  Monday, July 23, 2007  3:17 PM




IPv6 Routing Protocols751RIPngTo be honest, the primary features of RIPng are the same as they were with RIPv2. It is still a distance-vector protocol, has a max hop count of 15, and uses split horizon, poison reverse, and other loop avoidance mechanisms, but it now uses UDP port 521.And it still uses multicast to send its updates too, but in IPv6, it uses FF02::9 for the trans-port address. This is actually kind of cool since in RIPv2, the multicast address was 224.0.0.9, so the address still has a 9 at the end in the new IPv6 multicast range. In fact, most routing pro-tocols got to keep a little bit of their IPv4 identities like that.But of course there are differences in the new version or it wouldn’t be a new version, would it? We know that routers keep the next-hop addresses of their neighbor routers for every des-tination network in their routing table. The difference is that with RIPng, the router keeps track of this next-hop address using the link-local address, not a global address.Probably one of the biggest changes with RIPng (and all of the IPv6 routing protocols for that matter) is the fact that you configure or enable the advertisement of a network from inter-face configuration mode instead of with a network command in router configuration mode. So in RIPng’s case, if you enable it directly on an interface without going into router config-uration mode and starting a RIPng process, a new RIPng process will simply be started for you. It will look something like this:

Router1(config-if)#ipv6 rip 1 enableThat 1 you see in this command is a tag that identifies the process of RIPng that’s running, and as I said, this will start a process of RIPng so you don’t have to go into router configuration mode.But if you need to go to router configuration mode to configure something else like redis-tribution, you still can. If you do that, it will look like this on your router:

Router1(config)#ipv6 router rip 1

Router1(config-rtr)#So just remember that RIPng will pretty much work the same way as with IPv4, with the biggest difference being that it uses the network itself instead of using the network command you used to use to enable the interface to route the connected network.EIGRPv6As with RIPng, EIGRPv6 works much the same as its IPv4 predecessor does—most of the features that EIGRP provided before EIGRPv6 will still be available. EIGRPv6 is still an advanced distance-vector protocol that has some link-state features. The neighbor discovery process using hellos still happens, and it still provides reliable com-munication with reliable transport protocol that gives us loop-free fast convergence using the Diffusing Update Algorithm (DUAL).Hello packets and updates are sent using multicast transmission, and as with RIPng, EIGRPv6’s multicast address stayed almost the same. In IPv4 it was 224.0.0.10; in IPv6, it’s FF02::A (A = 10 in hexadecimal notation).

10089.book  Page 751  Monday, July 23, 2007  3:17 PM




752Chapter13 Internet Protocol Version 6 (IPv6)But obviously, there are differences between the two versions. Most notably, and just as with RIPng, the use of the network command is gone, and the network and interface to be advertised must be enabled from interface configuration mode. But you still have to use the router configuration mode to enable the routing protocol in EIGRPv6 because the routing pro-cess must be literally turned on like an interface with the no shutdown command—interesting!The configuration for EIGRPv6 is going to look like this:

Router1(config)#ipv6 router eigrp 10The 10 in this case is still the autonomous system (AS) number. The prompt changes to (config-rtr), and from here you must perform a no shutdown:

Router1(config-rtr)#no shutdownOther options also can be configured in this mode, like redistribution.So now, let’s go to the interface and enable IPv6:

Router1(config-if)#ipv6 eigrp 10The 10 in the interface command again references the AS number that was enabled in the configuration mode.Last to check out in our group is what OSPF looks like in the IPv6 routing protocol.OSPFv3The new version of OSPF continues the trend of the routing protocols having many similarities with their IPv4 versions.The foundation of OSPF remains the same—it is still a link-state routing protocol that divides an entire internetwork or autonomous system into areas, making a hierarchy. And just trust me—be really thankful that multi-area OSPF is out of scope for the CCNA objectives—at least, for now! But a few of the options we discussed in Chapter 7, “Enhanced IGRP (EIGRP) and Open Shortest Path First (OSPF),” are going to be a bit different.In OSPF version 2, the router ID (RID) is determined by the highest IP addresses assigned to the router (or you could assign it). In version 3, you assign the RID, area ID, and link-state ID, which are all still 32-bit values but are not found using the IP address anymore because an IPv6 address is 128 bits. Changes regarding how these values are assigned, along with the removal of the IP address information from OSPF packet headers, makes the new version of OSPF capable of being routed over almost any Network layer protocol—cool!Adjacencies and next-hop attributes now use link-local addresses, and OSPFv3 still uses multicast traffic to send its updates and acknowledgments, with the addresses FF02::5 for OSPF routers and FF02::6 for OSPF-designated routers. These new addresses are the replace-ments for 224.0.0.5 and 224.0.0.6, respectively.Other, less flexible IPv4 protocols don’t give us the ability that OSPFv2 does to assign specific networks and interfaces into the OSPF process—however, this is something that is still configured under the router configuration process. And with OSPFv3, just as with the other IPv6 routing pro-tocols we’ve talked about, the interfaces and therefore the networks attached to them are config-ured directly on the interface in interface configuration mode.

10089.book  Page 752  Monday, July 23, 2007  3:17 PM




Migrating to IPv6753The configuration of OSPFv3 is going to look like this:

Router1(config)#ipv6 router osfp 10

Router1(config-rtr)#router-id 1.1.1.1You get to perform some configurations from router configuration mode like summariza-tion and redistribution, but we don’t even need to configure OSPFv3 from this prompt if we configure OSPFv3 from the interface.When the interface configuration is completed, the router configuration process is added automatically and the interface configuration looks like this:

Router1(config-if)#ipv6 ospf 10 area 0.0.0.0 So, if we just go to each interface and assign a process ID and area—poof, we’re done!With all that behind you, it’s now time to move on and learn about how to migrate to IPv6 from IPv4.Migrating to IPv6We certainly have talked a lot about how IPv6 works and how we can configure it to work on our networks, but what is doing that going to cost us? And how much work is it really going to take? Good questions for sure, but the answers to them won’t be the same for everyone. This is because how much you are going to end up having to pony up is highly dependent upon what you’ve got going on already in terms of your infrastructure. Obviously, if you’ve been making your really old routers and switches “last” and therefore have to upgrade every one of them so that they’re IPv6 compliant, that could very well turn out to be a good-sized chunk of change! Oh, and that sum doesn’t even include server and computer operating systems (OSs) and the blood, sweat, and maybe even tears spent on making all your applications com-pliant. So, my friend, it could cost you quite a bit! The good news is that unless you’ve really let things go, many OSs and network devices have been IPv6 compliant for a few years—we just haven’t been using all their features until now.Then there’s that other question about the amount of work and time. Straight up—this one could still be pretty intense. No matter what, it’s going to take you some time to get all of your systems moved over and make sure that things are working correctly. And if you’re talking about a huge network with tons of devices, well, it could take a really long time! But don’t panic—that’s why migration strategies have been created to allow for a slower integration. I’m going to show you three of the primary transition strategies available to us. The first is called dual stacking, which allows a device to have both the IPv4 and IPv6 protocol stack running so it’s capable of continuing on with its existing communications and simultaneously run newer IPv6 communications as they’re implemented. The next strategy is the 6to4 tunneling approach; this is your choice if you have an all IPv6 network that must communicate over an IPv4 network to reach another IPv6 network. I’ll surprise you with the third one just for fun!

10089.book  Page 753  Monday, July 23, 2007  3:17 PM




754Chapter13 Internet Protocol Version 6 (IPv6)Dual StackingThis is the most common type of migration strategy because, well, it’s the easiest on us—it allows our devices to communicate using either IPv4 or IPv6. Dual stacking lets you upgrade your devices and applications on the network one at a time. As more and more hosts and devices on the network are upgraded, more of your communication will happen over IPv6, and after you’ve arrived—everything’s running on IPv6, and you get to remove all the old IPv4 protocol stacks you no longer need.Plus, configuring dual stacking on a Cisco router is amazingly easy—all you have to do is enable IPv6 forwarding and apply an address to the interfaces already configured with IPv4. It’ll look something like this:

Corp(config)#ipv6 unicast-routingCorp(config)#interface fastethernet 0/0Corp(config-if)#ipv6 address 2001:db8:3c4d:1::/64 eui-64

Corp(config-if)#ip address 192.168.255.1 255.255.255.0But to be honest, it’s really a good idea to understand the various tunneling techniques because it’ll probably be awhile before we all start running IPv6 as a solo routed protocol.6to4 Tunneling6to4 tunneling is really useful for carrying IPv6 data over a network that’s still IPv4. It’s quite possible that you’ll have IPv6 subnets or other portions of your network that are all IPv6, and those networks will have to communicate with each other. Not so complicated, but when you consider that you might find this happening over a WAN or some other network that you don’t control, well, that could be a bit ugly. So what do we do about this if we don’t control the whole tamale? Create a tunnel that will carry the IPv6 traffic for us across the IPv4 net-work, that’s what. The whole idea of tunneling isn’t a difficult concept, and creating tunnels really isn’t as hard as you might think. All it really comes down to is snatching the IPv6 packet that’s happily traveling across the network and sticking an IPv4 header onto the front of it. Kind of like catch and release fishing, except for the fish doesn’t get something plastered on its face before being thrown back into the stream.To get a picture of this, take a look at Figure 13.3.Nice—but to make this happen we’re going to need a couple of dual-stacked routers, which I just demonstrated for you, so you should be good to go. Now we have to add a little con-figuration to place a tunnel between those routers. Tunnels are pretty simple—we just have to tell each router where the tunnel begins and where we want it to end up. Referring again to Figure 13.3, we’ll configure the tunnel on each router:

Router1(config)#int tunnel 0Router1(config-if)#ipv6 address 2001:db8:1:1::1/64Router1(config-if)#tunnel source 192.168.30.1

10089.book  Page 754  Monday, July 23, 2007  3:17 PM




Migrating to IPv6755Router1(config-if)#tunnel destination 192.168.40.1Router1(config-if)#tunnel mode ipv6ipRouter2(config)#int tunnel 0Router2(config-if)#ipv6 address 2001:db8:2:2::1/64Router2(config-if)#tunnel source 192.168.40.1Router2(config-if)#tunnel destination 192.168.30.1

Router2(config-if)#tunnel mode ipv6ipFIGURE13.3Creating a 6to4 tunnelWith this in place, our IPv6 networks can now communicate over the IPv4 network. Now, I’ve got to tell you that this is not meant to be a permanent configuration; your end goal should still be to run a total, complete IPv6 network end to end.One important note here—if the IPv4 network that you’re traversing in this situation has a NAT translation point, it would absolutely break the tunnel encapsulation we’ve just cre-ated! Over the years, NAT has been upgraded a lot so that it can handle specific protocols and dynamic connections, and without one of these upgrades, NAT likes to demolish most con-nections. And since this transition strategy isn’t present in most NAT implementations, that means trouble.But there is a way around this little problem and it’s called Teredo, which allows all your tunnel traffic to be placed in UDP packets. NAT doesn’t blast away at UDP packets, so they won’t get broken as other protocols packets do. So with Teredo in place and your packets dis-guised under their UDP cloak, the packets will easily slip by NAT alive and well!NAT-PTYou’ve probably heard that IPv6 doesn’t have any NAT in it, and you’ve heard correctly—sort of. By itself, IPv6 doesn’t have a NAT implementation. But that’s only a technicality because there is a transition strategy known as NAT protocol translation (NAT-PT). Just know that you really 

IPv4 network IPv6 packet encapsulated in an IPv4 packet  Dual stack Router1 Dual stack Router2 IPv6 host and network IPv6 host and network IPv4: 192.168.30.1 IPv6: 2001:db8:1:1::1 IPv4: 192.168.40.1 IPv6: 2001:db8:2:2::1 

IPv6 packet IPv4 

10089.book  Page 755  Monday, July 23, 2007  3:17 PM




756Chapter13 Internet Protocol Version 6 (IPv6)only use this approach as a last resort because it’s not that great of a solution. With it, your IPv4 hosts can only communicate with other IPv4 hosts, and those that are native IPv6, with other IPv6 hosts. What do I mean by that? Well, with a tunneling approach we took IPv6 packets and disguised them as IPv4 packets. With NAT-PT there is no encapsulation—the data of the source packet is removed from one IP type and repackaged as the new destination IP type. Even though being able to configure NAT-PT is beyond the scope of the CCNA objectives, I still want to explain it to you. And just as it is with NAT for IPv4, there are a couple of ways to implement it.Static NAT-PT provides a one-to-one mapping of a single IPv4 address to a single IPv6 address (sounds like static NAT). There is also Dynamic NAT-PT, which uses a pool of IPv4 addresses to provide a one-to-one mapping with an IPv6 address (sounding kind of familiar). Finally, there is Network Address Port Translation (NAPT-PT), which provides a many-to-one mapping of multiple IPv6 addresses to one IPv4 address and a port number (well, glad we have that cleared up from NAT).As you can see, we are not using NAT to translate a public and private IPv6 address as we did with IPv4, but rather between IPv4 and IPv6. Again, this should be used as an absolute last resort. In most cases a tunneling approach will work much better and without the headache of this configuration and system overhead.Configuring IPv6 on Our InternetworkIn this section, I’ll configure the internetwork I’ve been using throughout this book—five rout-ers connected together—but I’m not going to add IPv6 on the 871W router or to the LAN and WLAN networks connected to the R1, R2, and R3 routers, to keep things simple and easier to understand. So let’s get started by adding IPv6 to the Corp, R1, R2, and R3 routers. I’ll then add both the RIP and OSPF routing protocols and finish the chapter by running through some verification commands.As usual, I’ll start with the Corp router:

Corp#config tCorp(config)#ipv6 unicast-routingCorp(config)#int f0/1Corp(config-if)#ipv6 address 2001:db8:3c4d:11::/64 eui-64Corp(config-if)#int s0/0/0Corp(config-if)#ipv6 address 2001:db8:3c4d:12::/64 eui-64Corp(config-if)#int s0/0/1                             Corp(config-if)#ipv6 address 2001:db8:3c4d:13::/64 eui-64Corp(config-if)#int s0/1/0Corp(config-if)#ipv6 address 2001:db8:3c4d:14::/64 eui-64Corp(config-if)#int s0/2/0                             Corp(config-if)#ipv6 address 2001:db8:3c4d:15::/64 eui-64Corp(config-if)#^Z

10089.book  Page 756  Monday, July 23, 2007  3:17 PM




Configuring IPv6 on Our Internetwork757Corp#copy run startDestination filename [startup-config]?[enter]Building configuration...[OK]

Corp#In the preceding configuration, I changed the subnet address for each interface only slightly. Let’s take a look at the routing table:

Corp#sh ipv6 routeIPv6 Routing Table - 12 entriesCodes: C - Connected, L - Local, S - Static, R - RIP, B - BGP       U - Per-user Static route I1 - ISIS L1, I2 - ISIS L2, IA - ISIS       interarea, IS - ISIS summary O - OSPF intra, OI - OSPF inter,       OE1 - OSPF ext 1, OE2 - OSPF ext 2       ON1 - OSPF NSSA ext 1, ON2 - OSPF NSSA ext 2C   2001:DB8:3C4D:11::/64 [0/0]     via ::, FastEthernet0/1L   2001:DB8:3C4D:11:21A:2FFF:FE55:C9E9/128 [0/0]     via ::, FastEthernet0/1C   2001:DB8:3C4D:12::/64 [0/0]     via ::, Serial0/0/0L   2001:DB8:3C4D:12:21A:2FFF:FE55:C9E8/128 [0/0]     via ::, Serial0/0/0C   2001:DB8:3C4D:13::/64 [0/0]     via ::, Serial0/0/1L   2001:DB8:3C4D:13:21A:2FFF:FE55:C9E8/128 [0/0]     via ::, Serial0/0/1C   2001:DB8:3C4D:14::/64 [0/0]     via ::, Serial0/1/0L   2001:DB8:3C4D:14:21A:2FFF:FE55:C9E8/128 [0/0]     via ::, Serial0/1/0C   2001:DB8:3C4D:15::/64 [0/0]     via ::, Serial0/2/0L   2001:DB8:3C4D:15:21A:2FFF:FE55:C9E8/128 [0/0]     via ::, Serial0/2/0L   FE80::/10 [0/0]     via ::, Null0L   FF00::/8 [0/0]     via ::, Null0

Corp#

10089.book  Page 757  Monday, July 23, 2007  3:17 PM




758Chapter13 Internet Protocol Version 6 (IPv6)So what’s up with the two addresses for each interface? One shows C connected, and one shows L? Well, the connected address is the IPv6 address I configured on each interface, and the L is the automatically assigned link-local. Notice in the link-local address that the FF:FE is inserted into the address to create the eui-64 address.One more thing before we move on to the R1 router. Notice that when addressing the inter-faces, I added a different number to the subnet number for each one. Also note that they closely match my IPv4 private addresses. I did it this way to make administration easier. Okay—let’s configure the R1 router now:

R1#config tR1(config)#ipv6 unicast-routingR1(config)#int s0/0/0R1(config-if)#ipv6 address 2001:db8:3c4d:12::/64 eui-64R1(config-if)#int s0/0/1R1(config-if)#ipv6 address 2001:db8:3c4d:13::/64 eui-64R1(config-if)#^ZR1#show ipv6 routeIPv6 Routing Table - 6 entries[codes cut]C   2001:DB8:3C4D:12::/64 [0/0]     via ::, Serial0/0/0L   2001:DB8:3C4D:12:21A:6DFF:FE64:9B2/128 [0/0]     via ::, Serial0/0/0C   2001:DB8:3C4D:13::/64 [0/0]     via ::, Serial0/0/1L   2001:DB8:3C4D:13:21A:6DFF:FE64:9B2/128 [0/0]     via ::, Serial0/0/1L   FE80::/10 [0/0]     via ::, Null0L   FF00::/8 [0/0]     via ::, Null0

R1#Notice that I used the exact same IPv6 subnet addresses on each side of the link. Let’s configure the R2 and R3 routers, and then add RIPv6:

R2#config tR2(config)#ipv6 unicast-routingR2(config)#int s0/2/0R2(config-if)#ipv6 address 2001:db8:3c4d:14::/64 eui-64R2(config-if)#do show ipv6 routeIPv6 Routing Table - 4 entries

10089.book  Page 758  Monday, July 23, 2007  3:17 PM




Configuring IPv6 on Our Internetwork759C   2001:DB8:3C4D:14::/64 [0/0]     via ::, Serial0/2/0L   2001:DB8:3C4D:14:213:60FF:FE20:4E4C/128 [0/0]     via ::, Serial0/2/0L   FE80::/10 [0/0]     via ::, Null0L   FF00::/8 [0/0]     via ::, Null0

R2(config-if)#Looking good! Let’s go to R3:

R3#config tR3(config)#ipv6 unicast-routingR3(config)#int s0/0/1R3(config-if)#ipv6 address 2001:db8:3c4d:15::/64 eui-64R3(config-if)#do show ipv6 route IPv6 Routing Table - 4 entries      C   2001:DB8:3C4D:15::/64 [0/0]     via ::, Serial0/0/1L   2001:DB8:3C4D:15:21A:6DFF:FE37:A44E/128 [0/0]     via ::, Serial0/0/1L   FE80::/10 [0/0]     via ::, Null0L   FF00::/8 [0/0]     via ::, Null0

R3(config-if)#Again, notice that I used the exact same IPv6 subnet addresses on each side of the links from the Corp router to the R1, R2, and R3 routers. Now let’s start adding routing protocols!Configuring RIPngThis is really the easy part—all I need to do is to go to each interface on each of our routers and type in one command. Here we go:

Corp#config tCorp(config)#int f0/1Corp(config-if)#ipv6 rip ?       WORD  User selected string identifying this RIP processCorp(config-if)#ipv6 rip 1 enableCorp(config-if)#int s0/0/0

10089.book  Page 759  Monday, July 23, 2007  3:17 PM




760Chapter13 Internet Protocol Version 6 (IPv6)Corp(config-if)#ipv6 rip 1 enableCorp(config-if)#int s0/0/1Corp(config-if)#ipv6 rip 1 enableCorp(config-if)#int s0/1/0Corp(config-if)#ipv6 rip 1 enableCorp(config-if)#int s0/2/0

Corp(config-if)#ipv6 rip 1 enableLet’s configure the R1 router:

R1#config tR1(config)#int s0/0/0R1(config-if)#ipv6 rip 1 enableR1(config-if)#int s0/0/1

R1(config-if)#ipv6 rip 1 enableR2 config:

R2#config tR2(config)#int s0/2/0

R2(config-if)#ipv6 rip 1 enableR3 config:

R3#config tR3(config)#int s0/0/1

R3(config-if)#ipv6 rip 1 enableOkay, time to verify my IPv6 routing tables and configurations.Verifying RIPngI’m going to start with the usual show ip route command. Here’s the output from the R3 router:

R3#sh ipv6 routeR   2001:DB8:3C4D:11::/64 [120/2]     via FE80::21A:2FFF:FE55:C9E8, Serial0/0/1R   2001:DB8:3C4D:12::/64 [120/2]     via FE80::21A:2FFF:FE55:C9E8, Serial0/0/1R   2001:DB8:3C4D:13::/64 [120/2]     via FE80::21A:2FFF:FE55:C9E8, Serial0/0/1R   2001:DB8:3C4D:14::/64 [120/2]     via FE80::21A:2FFF:FE55:C9E8, Serial0/0/1C   2001:DB8:3C4D:15::/64 [0/0]     via ::, Serial0/0/1L   2001:DB8:3C4D:15:21A:6DFF:FE37:A44E/128 [0/0]

10089.book  Page 760  Monday, July 23, 2007  3:17 PM




Configuring IPv6 on Our Internetwork761     via ::, Serial0/0/1L   FE80::/10 [0/0]     via ::, Null0L   FF00::/8 [0/0]     via ::, Null0

R3#Wow, looks just like the regular IPv4 RIP table, including the administrative distance and hop count. I can see subnets 11, 12, 13, 14, and 15.Let’s take a look at a few more verification commands:

R3#sh ipv6 protocolsIPv6 Routing Protocol is “connected”IPv6 Routing Protocol is “static”IPv6 Routing Protocol is “rip 1”  Interfaces:    Serial0/0/1  Redistribution:    None

R3#Not too much information provided with the show ipv6 protocols command. Let’s try the show ipv6 rip command:

R3#sh ipv6 rip      RIP process “1”, port 521, multicast-group FF02::9, pid 60     Administrative distance is 120. Maximum paths is 16     Updates every 30 seconds, expire after 180     Holddown lasts 0 seconds, garbage collect after 120     Split horizon is on; poison reverse is off     Default routes are not generated     Periodic updates 44, trigger updates 19  Interfaces:    Serial0/0/1  Redistribution:

    NoneOkay—now we’re talking! We can see that the administrative distance is still 120, plus the multicast group, maximum paths, and timers. So let’s go ahead and try two more verification commands, beginning with the show ipv6 interface s0/0/1 command:

R3#sh ipv6 interface serial 0/0/1Serial0/0/1 is up, line protocol is up  IPv6 is enabled, link-local address is FE80::21A:6DFF:FE37:A44E

10089.book  Page 761  Monday, July 23, 2007  3:17 PM




762Chapter13 Internet Protocol Version 6 (IPv6)  Global unicast address(es):    2001:DB8:3C4D:1:21A:6DFF:FE37:A44E, subnet is 2001:DB8:3C4D:1::/64 [EUI]  Joined group address(es):    FF02::1    FF02::2    FF02::9    FF02::1:FF37:A44E  MTU is 1500 bytes  ICMP error messages limited to one every 100 milliseconds  ICMP redirects are enabled  ND DAD is enabled, number of DAD attempts: 1  ND reachable time is 30000 milliseconds

  Hosts use stateless autoconfig for addresses.This got us some pretty good information too. But wait, the best is yet to come: the debug ipv6 rip command—this should be good:

R3#debug ipv6 rip*May 24 18:31:11.959: RIPng: Sending multicast update on Serial0/0/1 for 1*May 24 18:31:11.959:        src=FE80::21A:6DFF:FE37:A44E*May 24 18:31:11.959:        dst=FF02::9 (Serial0/0/1)*May 24 18:31:11.959:        sport=521, dport=521, length=32*May 24 18:31:11.959:        command=2, version=1, mbz=0, #rte=1*May 24 18:31:11.959:        tag=0, metric=1, prefix=2001:DB8:3C4D:1::/64*May 24 18:40:44.079: %LINEPROTO-5-UPDOWN: Line protocol on Interface   Serial0/0/0, changed state to down*May 24 18:31:24.959: RIPng: response received from   FE80::21A:2FFF:FE55:C9E8 on Serial0/0/1 for 1*May 24 18:31:24.959:        src=FE80::21A:2FFF:FE55:C9E8 (Serial0/0/1)*May 24 18:31:24.959:        dst=FF02::9*May 24 18:31:24.959:        sport=521, dport=521, length=32*May 24 18:31:24.959:        command=2, version=1, mbz=0, #rte=1*May 24 18:31:24.959:        tag=0, metric=16,   prefix=2001:DB8:3C4D:12::/64*May 24 18:31:24.959: RIPng: 2001:DB8:3C4D:12::/64, path   FE80::21A:2FFF:FE55:C9E8/Serial0/0/1 unreachable*May 24 18:31:24.959: RIPng: 2001:DB8:3C4D:12::/64, expired, ttg is 120*May 24 18:31:24.959: RIPng: Triggered update requested*May 24 18:31:25.959: RIPng: generating triggered update for 1*May 24 18:31:25.959: RIPng: Suppressed null multicast update on

   Serial0/0/1 for 1

10089.book  Page 762  Monday, July 23, 2007  3:17 PM




Configuring IPv6 on Our Internetwork763Now this is interesting. We can see that the source and destination ports used are 521 (yes, we are still using UDP) and that network/subnet 12 is unreachable. This is because the s0/0/0 interface of my Corp router has just decided to go bad. (I swear, writing this book is just like being at work!) Either way, we can see that RIPng still has some basic IPv4 RIP characteristics. Let’s go ahead and add OSPFv3 to our routers.Configuring OSPFv3Just as with the RIPng configuration, all we have to do to enable OSPF on the internetwork is to go to each interface that we want to run it.Here is the Corp configuration:

Corp#config tCorp(config)#int f0/1Corp(config-if)#ipv6 ospf 1 ?  area  Set the OSPF area IDCorp(config-if)#ipv6 ospf 1 area 0Corp(config-if)#int s0/0/1Corp(config-if)#ipv6 ospf 1 area 0Corp(config-if)#int s0/1/0Corp(config-if)#ipv6 ospf 1 area 0Corp(config-if)#int s0/2/0Corp(config-if)#ipv6 ospf 1 area 0Corp(config-if)#^Z

Corp#That wasn’t so bad—actually somewhat easier than with IPv4. Let’s configure the other three routers:

R1#config tR1(config)#int s0/0/1     R1(config-if)#ipv6 ospf 1 area 0R1(config-if)#*May 24 19:24:55.279: %OSPFv3-5-ADJCHG: Process 1, Nbr 172.16.10.2 on

   Serial0/0/1 from LOADING to FULL, Loading DoneSweet! R1 has become adjacent to the Corp router. One interesting output line is that the IPv4 RID is being used in the OSPFv3 adjacent change.

R2#config tR2(config)#int s0/2/0R2(config-if)#ipv6 ospf 1 area 0R2(config-if)#*May 24 19:27:31.399: %OSPFv3-5-ADJCHG: Process 1, Nbr 172.16.10.3 on

   Serial0/1/0 from LOADING to FULL, Loading Done

10089.book  Page 763  Monday, July 23, 2007  3:17 PM




764Chapter13 Internet Protocol Version 6 (IPv6)Again, our adjacency popped up—this is great. One more router, then we’ll do some verification:

R3#config tR3(config)#int s0/0/1R3(config-if)#ipv6 ospf 1 area 0R3(config-if)#*May 24 19:29:07.231: %OSPFv3-5-ADJCHG: Process 1, Nbr 172.16.10.4 on

   Serial0/2/0 from LOADING to FULL, Loading DoneWithout even verifying our network, it seems to me that it’s up and running. But, we’ve still got to verify!Verifying OSPFv3I’ll start as usual with the show ipv6 route command:

R3#sh ipv6 routeIPv6 Routing Table - 7 entriesO   2001:DB8:3C4D:11::/64 [110/65]     via FE80::21A:2FFF:FE55:C9E8, Serial0/0/1O   2001:DB8:3C4D:13::/64 [110/128]     via FE80::21A:2FFF:FE55:C9E8, Serial0/0/1O   2001:DB8:3C4D:14::/64 [110/128]     via FE80::21A:2FFF:FE55:C9E8, Serial0/0/1C   2001:DB8:3C4D:15::/64 [0/0]     via ::, Serial0/0/1L   2001:DB8:3C4D:15:21A:6DFF:FE37:A44E/128 [0/0]     via ::, Serial0/0/1L   FE80::/10 [0/0]     via ::, Null0L   FF00::/8 [0/0]     via ::, Null0

R3#Perfect. I see all the subnets (except 12, which is down because of that bad interface). Let’s take a look at the show ipv6 protocols command:

R3#sh ipv6 protocolsIPv6 Routing Protocol is “connected”IPv6 Routing Protocol is “static”IPv6 Routing Protocol is “rip 1”  Interfaces:    Serial0/0/1

10089.book  Page 764  Monday, July 23, 2007  3:17 PM




Configuring IPv6 on Our Internetwork765  Redistribution:    NoneIPv6 Routing Protocol is “ospf 1”  Interfaces (Area 0):    Serial0/0/1  Redistribution:

    NoneFor the next command, I want to go back to the Corp router so can I see more connections: show ipv6 ospf neighbor.

Corp#sh ipv6 ospf neighborNeighbor ID     Pri   State   Dead Time  Interface ID  Interface172.16.10.4       1   FULL/  - 00:00:36    6        Serial0/2/0172.16.10.3       1   FULL/  - 00:00:33    16       Serial0/1/0172.16.10.2       1   FULL/  - 00:00:30    6        Serial0/0/1

Corp#Wait! We need to do our debugging commands. I’ll use two of them: debug ipv6 ospf packet and debug ipv6 ospf hello (almost the same commands I used with IPv4):

Corp#debug ipv6 ospf packet  OSPFv3 packet debugging is onCorp#*May 24 19:38:12.283: OSPFv3: rcv. v:3 t:1 l:40 rid:172.16.10.3      aid:0.0.0.0 chk:E1D2 inst:0 from Serial0/1/0Corp#*May 24 19:38:15.103: OSPFv3: rcv. v:3 t:1 l:40 rid:172.16.10.4      aid:0.0.0.0 chk:7EBB inst:0 from Serial0/2/0Corp#*May 24 19:38:18.875: OSPFv3: rcv. v:3 t:1 l:40 rid:172.16.10.2      aid:0.0.0.0 chk:192D inst:0 from Serial0/0/1Corp#*May 24 19:38:22.283: OSPFv3: rcv. v:3 t:1 l:40 rid:172.16.10.3      aid:0.0.0.0 chk:E1D2 inst:0 from Serial0/1/0Corp#un allAll possible debugging has been turned offCorp#debug ipv6 ospf hello  OSPFv3 hello events debugging is onCorp#*May 24 19:38:32.283: OSPFv3: Rcv hello from 172.16.10.3 area 0 from   Serial0/1/0 FE80::213:60FF:FE20:4E4C interface ID 16

10089.book  Page 765  Monday, July 23, 2007  3:17 PM




766Chapter13 Internet Protocol Version 6 (IPv6)*May 24 19:38:32.283: OSPFv3: End of hello processingCorp#*May 24 19:38:35.103: OSPFv3: Rcv hello from 172.16.10.4 area 0 from   Serial0/2/0 FE80::21A:6DFF:FE37:A44E interface ID 6*May 24 19:38:35.103: OSPFv3: End of hello processingCorp#*May 24 19:38:38.875: OSPFv3: Rcv hello from 172.16.10.2 area 0 from   Serial0/0/1 FE80::21A:6DFF:FE64:9B2 interface ID 6*May 24 19:38:38.875: OSPFv3: End of hello processingCorp#un allAll possible debugging has been turned off

Corp#Holy output! Now that is what I call a fun chapter. We even had an interface go bad, just like in the real world. I really hope you found this chapter as rewarding and interesting as I did. The best thing you can do to learn IPv6 is to nick some routers and just have a go at it!SummaryIn this chapter, I covered the very basics of IPv6 and how to make IPv6 work within a Cisco internetwork. As you now know by reading this chapter, even when discussing and configur-ing the basics, there is a lot to understand—and we just scratched the surface. But trust me when I say this—you now know more than you’ll need to meet the CCNA objectives.I began by talking about why we need IPv6 and the benefits associated with it. I followed that up by covering addressing with IPv6 as well as how to use the shortened expressions. And during the talk on addressing with IPv6, I showed you the different address types, plus the special addresses reserved in IPv6.IPv6 will mostly be deployed automatically, meaning hosts will use autoconfiguration, so I discussed how IPv6 uses autoconfiguration and how it comes into play when configuring a Cisco router. After that, I showed you how to add a DHCP server to the router so it can pro-vide options to hosts—not addresses, but options like a DNS server address.ICMP is extremely important with IPv6, and I discussed in detail how ICMP works with IPv6, followed by how to configure RIP, EIGRP, and OSPF with IPv6.Migrating to IPv6 is no small matter either, and I went over the pros and cons of doing this, and I told you about three migration strategies—dual stacking, tunneling using both IPv4 and IPv6, and a third approach, NAT-PT, to be used only as a last resort.Last, I showed you how to configure IPv6 on the internetwork that I’ve been using through-out this book and then demonstrated how to verify the configuration with the various show commands available with IPv6.

10089.book  Page 766  Monday, July 23, 2007  3:17 PM




Written Lab 13767Exam EssentialsUnderstand why we need IPv6.Without IPv6, the world would be depleted of IP addresses.Understand link-local.Link-local is like an IPv4 private IP address, but it can’t be routed at all, not even in your organization.Understand unique local.This, like link-local, is like private IP addresses in IPv4 and cannot be routed to the Internet. However, the difference between link-local and unique local is that unique local can be routed within your organization or company. Remember IPv6 addressing.IPv6 addressing is not like IPv4 addressing. IPv6 addressing has much more address space and is 128 bits long, represented in hexadecimal, unlike IPv4, which is only 32 bits long and represented in decimal.Written Lab 13In this section, write the answers to the following IPv6 questions:1.Which type of packet is addressed and delivered to only a single interface?2.Which type of address is used just like a regular public routable address in IPv4?3.Which type of address is not meant to be routed?4.Which type of address is not meant to be routed to the Internet but is still globally unique?5.Which type of address is meant to be delivered to multiple interfaces?6.Which type of address identifies multiple interfaces, but packets are delivered only to the first address it finds?7.Which routing protocol uses multicast address FF02::5?8.IPv4 had a loopback address of 127.0.0.1. What is the IPv6 loopback address?9.What does a link-local address always start with?10.What does a unique local unicast range start with?(The answers to Written Lab 13 can be found following the answers to the review questions for this chapter.)

10089.book  Page 767  Monday, July 23, 2007  3:17 PM




768Chapter13 Internet Protocol Version 6 (IPv6)Review Questions

The following questions are designed to test your understanding of this chapter’s material. For more information on how to get additional ques-tions, please see this book’s Introduction.1.Which of the following is true when describing a global unicast address?A.Packets addressed to a unicast address are delivered to a single interface.B.These are your typical publicly routable addresses, just like a regular publicly routable address in IPv4.C.These are like private addresses in IPv4 in that they are not meant to be routed.D.These addresses are meant for nonrouting purposes, but they are almost globally unique so it is unlikely they will have an address overlap.2.Which of the following is true when describing a unicast address?A.Packets addressed to a unicast address are delivered to a single interface.B.These are your typical publicly routable addresses, just like a regular publicly routable address in IPv4.C.These are like private addresses in IPv4 in that they are not meant to be routed.D.These addresses are meant for nonrouting purposes, but they are almost globally unique so it is unlikely they will have an address overlap.3.Which of the following is true when describing a link-local address?A.Packets addressed to a unicast address are delivered to a single interface.B.These are your typical publicly routable addresses, just like a regular publicly routable address in IPv4.C.These are like private addresses in IPv4 in that they are not meant to be routed.D.These addresses are meant for nonrouting purposes, but they are almost globally unique so it is unlikely they will have an address overlap.4.Which of the following is true when describing a unique local address?A.Packets addressed to a unicast address are delivered to a single interface.B.These are your typical publicly routable addresses, just like a regular publicly routable address in IPv4.C.These are like private addresses in IPv4 in that they are not meant to be routed.D.These addresses are meant for nonrouting purposes, but they are almost globally unique so it is unlikely they will have an address overlap.

10089.book  Page 768  Monday, July 23, 2007  3:17 PM




Review Questions7695.Which of the following is true when describing a multicast address?A.Packets addressed to a unicast address are delivered to a single interface.B.Packets are delivered to all interfaces identified by the address. This is also called a one-to-many address.C.Identifies multiple interfaces and is only delivered to one address. This address can also be called one-to-one-of-many.D.These addresses are meant for nonrouting purposes, but they are almost globally unique so it is unlikely they will have an address overlap.6.Which of the following is true when describing an anycast address?A.Packets addressed to a unicast address are delivered to a single interface.B.Packets are delivered to all interfaces identified by the address. This is also called one-to-many addresses.C.This address identifies multiple interfaces and the anycast packet is only delivered to one address. This address can also be called one-to-one-of-many.D.These addresses are meant for nonrouting purposes, but they are almost globally unique so it is unlikely they will have an address overlap.7.You want to ping the loopback address of your local host. What will you type?A.ping 127.0.0.1B.ping 0.0.0.0C.ping ::1D.trace 0.0.::18.What two multicast addresses does OSPFv3 use?A.FF02::AB.FF02::9C.FF02::5D.FF02::69.What multicast addresses does RIPng use?A.FF02::AB.FF02::9C.FF02::5D.FF02::610.What multicast addresses does EIGRPv6 use?A.FF02::AB.FF02::9C.FF02::5D.FF02::6

10089.book  Page 769  Monday, July 23, 2007  3:17 PM




 770 Chapter13  Internet Protocol Version 6 (IPv6) 11. To enable RIPng, which of the following would you use? A. Router1(config-if)# ipv6 ospf 10 area 0.0.0.0 B. Router1(config-if)#ipv6 rip 1 enable C. Router1(config)# ipv6 router eigrp 10 D. Router1(config-rtr)#no shutdown E. Router1(config-if)#ipv6 eigrp 10 12. To enable EIGRP, which three of the following would you use? A. Router1(config-if)# ipv6 ospf 10 area 0.0.0.0 B. Router1(config-if)#ipv6 router rip 1 C. Router1(config)# ipv6 router eigrp 10 D. Router1(config-rtr)#no shutdown E. Router1(config-if)#ipv6 eigrp 10 13. To enable OSPFv3, which of the following would you use? A. Router1(config-if)# ipv6 ospf 10 area 0.0.0.0 B. Router1(config-if)#ipv6 router rip 1 C. Router1(config)# ipv6 router eigrp 10 D. Router1(config-rtr)#no shutdown E. Router1(config-if)#ospf ipv6 10 area 0 14. What two statements about IPv6 addresses are true? (Choose two.) A. Leading zeros are required. B. Two colons (::) are used to represent successive hexadecimal fields of zeros. C. Two colons (::) are used to separate fields. D. A single interface will have multiple IPv6 addresses of different types. 15. What two statements about IPv4 and IPv6 addresses are true? A. An IPv6 address is 32 bits long, represented in hexidecimal. B. An IPv6 address is 128 bits long, represented in decimal. C. An IPv4 address is 32 bits long, represented in decimal. D. An IPv6 address is 128 bits long, represented in hexidecimal.

 

10089c13.fm  Page 770  Wednesday, February 27, 2008  5:24 PM




Answers to Review Questions771Answers to Review Questions1.B. Unlike unicast addresses, global unicast addresses are meant to be routed.2.A. Explanation:Packets addressed to a unicast address are delivered to a single interface. For load balancing, multiple interfaces can use the same address.3.C. Link-local addresses are meant for throwing together a temporary LAN for meetings or a small LAN that is not going to be routed but needs to share and access files and services locally.4.D. These addresses are meant for nonrouting purposes like link-local, but they are almost globally unique so it is unlikely they will have an address overlap. Unique local addresses were designed as a replacement for site-local addresses.5.B. Packets addressed to a multicast address are delivered to all interfaces identified by the multicast address, the same as in IPv4. It is also called a one-to-many address. You can always tell a multicast address in IPv6 because multicast addresses always start with FF.6.C. Anycast addresses identify multiple interfaces, which is the same as multicast; however, the big difference is that the anycast packet is only delivered to one address, the first one it finds defined in the terms of routing distance. This address can also be called one-to-one-of-many.7.C. The loopback address with IPv4 is 127.0.0.1. With IPv6, that address is ::1.8.C, D. Adjacencies and next-hop attributes now use link-local addresses, and OSPFv3 still uses multicast traffic to send its updates and acknowledgments with the addresses FF02::5 for OSPF routers and FF02::6 for OSPF designated routers. These are the replacements for 224.0.0.5 and 224.0.0.6, respectively.9.B. RIPng uses the multicast IPv6 address of FF02::9. If you remember the multicast addresses for IPv4, the numbers at the end of each IPv6 address are the same.10.A. EIGRPv6’s multicast address stayed very near the same. In IPv4 it was 224.0.0.10; now it is FF02::A (A=10 in hexadecimal notation).11.B. It’s pretty simple to enable RIPng for IPv6. You configure it right on the interface where you want RIP to run with the ipv6 router rip number command.12.C, D, E. Unlike RIPng and OSPFv3, you need to configure EIGRP both from global configu-ration mode and from interface mode, and you have to enable the command with the no shutdown command.13.A. To enable OSPFv3, you enable the protocol as with RIPng. The command string is ipv6 ospf process-id area area-id.14.B, D. In order to shorten the written length of an IPv6 address, successive fields of zeros may be replaced by double colons. In trying to shorten the address further, leading zeros may also be removed. Just as with IPv4, a single device’s interface can have more than one address; with IPv6 there are more types of addresses and the same rule applies. There can be link-local, global uni-cast, and multicast addresses all assigned to the same interface.15.C, D. IPv4 addresses are 32 bits long and are represented in decimal format. IPv6 addresses are 128 bits long and represented in hexadecimal format.

10089.book  Page 771  Monday, July 23, 2007  3:17 PM




772Chapter13 Internet Protocol Version 6 (IPv6)Answers to Written Lab 13.11.Unicast2.Global unicast3.Link-local4.Unique local (used to be called site-local)5.Multicast6.Anycast7.OSPF8.::19.FE80::10.FC00::

10089.book  Page 772  Monday, July 23, 2007  3:17 PM




 

Chapter 14 Wide Area Networks

 THE CCNA EXAM TOPICS COVERED IN THIS CHAPTER INCLUDE THE FOLLOWING:  Implement and verify WAN links  Describe different methods for connecting to a WAN   Configure and verify a basic WAN serial connection   Configure and verify Frame Relay on Cisco routers   Troubleshoot WAN implementation issues   Describe VPN technology (including: importance, benefits, role, impact, components)   Configure and verify a PPP connection between Cisco routers   Describe how a network works  Differentiate between LAN/WAN operation and features

 

10089.book  Page 773  Monday, July 23, 2007  3:17 PM




 The Cisco IOS supports a ton of different wide area network (WAN) protocols that help you extend your local LANs to other LANs at remote sites. And I don’t think I have to tell you how positively essential information exchange between disparate sites is these days—it’s vital! But even so, it wouldn’t exactly be cost effective or efficient to install your own cable and connect all of your company’s remote locations yourself, now would it? A much better way to go about doing this is to simply lease the existing installations that service providers already have in place, and save big time.So it follows that I’m going to discuss the various types of connections, technologies, and devices used in accordance with WANs in this chapter. We’ll also get into how to implement and configure High-Level Data-Link Control (HDLC), Point-to-Point Protocol (PPP), Point-to-Point Protocol over Ethernet (PPPoE), cable, DSL, and Frame Relay. I’ll also introduce you to WAN security concepts, tunneling, and virtual private network basics.

 Even though I could include discussion of wireless networks in this chapter as WAN solutions, I’m instead going to refer you to Chapter 12, which is chock full of detailed information about Cisco’s wireless solutions and how they can be  used both inside the corporate network and in the outside metropolitan areas. Just so you know, I’m not going to cover every type of Cisco WAN support here—again, the focus of this book is to equip you with everything you need to successfully meet the CCNA objectives. Because of that, I’m going to focus on cable, DSL, HDLC, PPP, PPPoE, and Frame Relay. Then I’m going to wrap the chapter up with a solid introduction to VPNs.But first things first—let’s begin with an exploration into WAN basics.

 For up-to-the-minute updates for this chapter, check out  www.lammle.com  and/ or  www.sybex.com . Introduction to Wide Area Networks So what, exactly, is it that makes something a  wide area network (WAN)  instead of a local area network (LAN)? Well, there’s obviously the distance thing, but these days, wireless 

 

10089.book  Page 774  Monday, July 23, 2007  3:17 PM




 Introduction to Wide Area Networks 775 LANs can cover some serious turf. What about bandwidth? Well, here again, some really big pipes can be had for a price in many places, so that’s not it either. So what the heck is it then? One of the main ways a WAN differs from a LAN is that while you generally own a LAN infrastructure, you usually lease WAN infrastructure from a service provider. To be honest, modern technologies even blur this definition, but it still fits neatly into the context of Cisco’s exam objectives.Anyway, I’ve already talked about the data link that you usually own (Ethernet), but now we’re going to find out about the kind you usually don’t own—the type most often leased from a service provider.The key to understanding WAN technologies is to be familiar with the different WAN terms and connection types commonly used by service providers to join your networks together. Defining WAN Terms Before you run out and order a WAN service type from a provider, it would be a really good idea to understand the following terms that service providers typically use: Customer premises equipment (CPE) Customer premises equipment (CPE)  is equipment that’s owned by the subscriber and located on the subscriber’s premises. Demarcation point The  demarcation point  is the precise spot where the service pro-vider’s responsibility ends and the CPE begins. It’s generally a device in a telecommuni-cations closet owned and installed by the telecommunications company (telco). It’s your responsibility to cable (extended demarc) from this box to the CPE, which is usually a con-nection to a CSU/DSU or ISDN interface. Local loop The  local loop  connects the demarc to the closest switching office, which is called a central office. Central office (CO) This point connects the customer’s network to the provider’s switch-ing network. Good to know is that a  central office (CO)  is sometimes referred to as a  point of presence (POP) . Toll network The  toll network  is a trunk line inside a WAN provider’s network. This net-work is a collection of switches and facilities owned by the ISP.Definitely familiarize yourself with these terms because they’re crucial to understanding WAN technologies. WAN Connection Types As you’re probably aware, a WAN can use a number of different connection types, and I’m going to introduce you to each of the various types of WAN connections you’ll find on the market today. Figure 14.1 shows the different WAN connection types that can be used to con-nect your LANs together (DTE) over a DCE network.

 

10089.book  Page 775  Monday, July 23, 2007  3:17 PM




 776 Chapter14  Wide Area Networks FIGURE14.1 WAN connection types Here’s a list explaining the different WAN connection types: Leased lines These are usually referred to as a  point-to-point  or dedicated connection. A  leased line  is a pre-established WAN communications path that goes from the CPE through the DCE switch, then over to the CPE of the remote site. The CPE enables DTE networks to communicate at any time with no cumbersome setup procedures to muddle through before transmitting data. When you’ve got plenty of cash, this is really the way to go because it uses synchronous serial lines up to 45Mbps. HDLC and PPP encapsulations are frequently used on leased lines; I’ll go over them with you in detail in a bit. Circuit switching When you hear the term  circuit switching , think phone call. The big advantage is cost—you only pay for the time you actually use. No data can transfer before an end-to-end connection is established. Circuit switching uses dial-up modems or ISDN and is used for low-bandwidth data transfers. Okay—I know what you’re thinking: “Modems? Did he say modems? Aren’t those only in museums by now?” After all, with all the wireless tech-nologies available, who would use a modem these days? Well, some people do have ISDN and it still is viable (and I do suppose someone does use a modem now and then), but circuit switching can be used in some of the newer WAN technologies as well. Packet switching This is a WAN switching method that allows you to share bandwidth with other companies to save money.  Packet switching  can be thought of as a network that’s designed to look like a leased line yet charges you more like circuit switching. But less cost isn’t always bet-ter—there’s definitely a downside: If you need to transfer data constantly, just forget about this option. Instead, get yourself a leased line. Packet switching will only work for you if your data transfers are the bursty type—not continuous. Frame Relay and X.25 are packet-switching tech-nologies with speeds that can range from 56Kbps up to T3 (45Mbps).

DedicatedCircuit-switchedPacket-switched

Synchronous serialAsynchronous serial, ISDNSynchronous serial

Telephonecompany

Serviceprovider

 

10089.book  Page 776  Monday, July 23, 2007  3:17 PM




 Introduction to Wide Area Networks 777

 MultiProtocol Label Switching (MPLS)  uses a combination of both circuit switching and packet switching, but it’s out of this book’s range. Even so, after you pass your CCNA exam, it would be well worth your time to look into  MPLS, so I’ll talk about MPLS briefly in a minute. WAN Support Basically, Cisco just supports HDLC, PPP, and Frame Relay on its serial interfaces, and you can see this with the  encapsulation ?  command from any serial interface (your output may vary depending on the IOS version you are running):

 Corp# config t Corp(config)# int s0/0/0 Corp(config-if)# encapsulation ?   atm-dxi      ATM-DXI encapsulation  frame-relay  Frame Relay networks  hdlc         Serial HDLC synchronous  lapb         LAPB (X.25 Level 2)  ppp          Point-to-Point protocol  smds         Switched Megabit Data Service (SMDS)

   x25          X.25 Understand that if I had other types of interfaces on my router, I would have other encap-sulation options, like ISDN or ADSL. And remember, you can’t configure Ethernet or Token Ring encapsulation on a serial interface.Next, I’m going to define the most prominently known WAN protocols used today: Frame Relay, ISDN, LAPB, LAPD, HDLC, PPP, PPPoE, Cable, DSL, MPLS, and ATM. Just so you know, the only WAN protocols you’ll usually find configured on a serial interface are HDLC, PPP, and Frame Relay, but who said we’re stuck with using only serial interfaces for wide area connections?

 The rest of the chapter is going to be dedicated to explaining, in depth, how cable, DSL, and basic WAN protocols work, plus how to configure them with Cisco routers. But since they’re important in the world beyond the latest CCNA exam objectives, I’m still going to briefly talk about ISDN, LAPB, LAPD, MPLS, and ATM in this next section. If any of them happen to pop up in the exam objectives, no worries, I promise you’ll immediately find an update  regarding the information at  www.lammle.com .

 

10089.book  Page 777  Monday, July 23, 2007  3:17 PM




 778 Chapter14  Wide Area Networks Frame Relay A packet-switched technology that made its debut in the early 1990s,  Frame Relay  is a high-performance Data Link and Physical layer specification. It’s pretty much a suc-cessor to X.25, except that much of the technology in X.25 used to compensate for physical errors (noisy lines) has been eliminated. An upside to Frame Relay is that it can be more cost effective than point-to-point links, plus it typically runs at speeds of 64Kbps up to 45Mbps (T3). Another Frame Relay benefit is that it provides features for dynamic bandwidth alloca-tion and congestion control. ISDN Integrated Services Digital Network (ISDN)  is a set of digital services that transmit voice and data over existing phone lines. ISDN offers a cost-effective solution for remote users who need a higher-speed connection than analog dial-up links can give them, and it’s also a good choice to use as a backup link for other types of links like Frame Relay or T1 connections. LAPB Link Access Procedure, Balanced (LAPB)  was created to be a connection-oriented protocol at the Data Link layer for use with X.25, but it can also be used as a simple data link transport. A not-so-good characteristic of LAPB is that it tends to create a tremendous amount of overhead due to its strict time-out and windowing techniques. LAPD Link Access Procedure, D-Channel (LAPD)  is used with ISDN at the Data Link layer (layer 2) as a protocol for the D (signaling) channel. LAPD was derived from the Link Access Procedure, Balanced (LAPB) protocol and is designed primarily to satisfy the signaling requirements of ISDN basic access. HDLC High-Level Data-Link Control (HDLC)  was derived from Synchronous Data Link Control (SDLC), which was created by IBM as a Data Link connection protocol. HDLC works at the Data Link layer and creates very little overhead compared to LAPB.It wasn’t intended to encapsulate multiple Network layer protocols across the same link—the HDLC header doesn’t contain any identification about the type of protocol being carried inside the HDLC encapsulation. Because of this, each vendor that uses HDLC has its own way of identifying the Network layer protocol, meaning each vendor’s HDLC is proprietary with regard to its specific equipment. PPP Point-to-Point Protocol (PPP)  is a pretty famous, industry-standard protocol. Because all multiprotocol versions of HDLC are proprietary, PPP can be used to create point-to-point links between different vendors’ equipment. It uses a Network Control Protocol field in the Data Link header to identify the Network layer protocol and allows authentication and multi-link connections to be run over asynchronous and synchronous links. PPPoE Point-to-Point Protocol over Ethernet encapsulates PPP frames in Ethernet frames and is usually used in conjunction with ADSL services. It gives you a lot of the familiar PPP features like authentication, encryption, and compression, but there’s a downside—it has a lower maximum transmission unit (MTU) than standard Ethernet does, and if your firewall isn’t solidly configured, this little attribute can really give you some grief!Still somewhat popular in the United States, PPPoE on Ethernet’s main feature is that it adds a direct connection to Ethernet interfaces while providing DSL support as well. It’s often used by many hosts on a shared Ethernet interface for opening PPP sessions to various destinations via at least one bridging modem.

 

10089.book  Page 778  Monday, July 23, 2007  3:17 PM




 Cable and DSL 779 Cable In a modern HFC network, typically 500 to 2,000 active data subscribers are connected to a certain cable network segment, all sharing the upstream and downstream bandwidth. ( Hybrid fibre-coaxial , or HFC, is a telecommunications industry term for a network that incorporates both optical fiber and coaxial cable to create a broadband network.) The actual bandwidth for Internet service over a cable TV (CATV) line can be up to about 27Mbps on the download path to the sub-scriber, with about 2.5Mbps of bandwidth on the upload path. Typically users get an access speed from 256Kbps to 6Mbps. This data rate varies greatly throughout the U.S. DSL Digital subscriber line is a technology used by traditional telephone companies to deliver advanced services (high-speed data and sometimes video) over twisted-pair copper tele-phone wires. It typically has lower data carrying capacity than HFC networks, and data speeds can be range limited by line lengths and quality. Digital subscriber line is not a complete end-to-end solution but rather a Physical layer transmission technology like dial-up, cable, or wire-less. DSL connections are deployed in the last mile of a local telephone network—the local loop. The connection is set up between a pair of modems on either end of a copper wire that is between the customer premises equipment (CPE) and the Digital Subscriber Line Access Multiplexer (DSLAM). A DSLAM is the device located at the provider’s central office (CO) and concentrates connections from multiple DSL subscribers. MPLS MultiProtocol Label Switching (MPLS)  is a data-carrying mechanism that emulates some properties of a circuit-switched network over a packet-switched network. MPLS is a switching mechanism that imposes labels (numbers) to packets and then uses those labels to forward packets. The labels are assigned on the edge of the MPLS of the network, and for-warding inside the MPLS network is done solely based on labels. Labels usually correspond to a path to layer 3 destination addresses (equal to IP destination-based routing). MPLS was designed to support forwarding of protocols other than TCP/IP. Because of this, label switch-ing within the network is performed the same regardless of the layer 3 protocol. In larger net-works, the result of MPLS labeling is that only the edge routers perform a routing lookup. All the core routers forward packets based on the labels, which makes forwarding the packets through the service provider network faster. (Most companies are replacing their Frame Relay networks with MPLS today). ATM Asynchronous Transfer Mode (ATM) was created for time-sensitive traffic, providing simultaneous transmission of voice, video, and data. ATM uses cells that are a fixed 53 bytes long instead of packets. It also can use isochronous clocking (external clocking) to help the data move faster. Typically, if you are running Frame Relay today, you will be running Frame Relay over ATM. Cable and DSL Okay, before I talk about the serial encapsulation connections used on Cisco routers (HDLC, PPP, and Frame Relay), I’m going to discuss cable modems and DSL (including ADSL and PPPoE) as solutions for connections to wide area networks because I think it will really help you understand the practical differences between DSL and cable modem networking.

 

10089.book  Page 779  Monday, July 23, 2007  3:17 PM




 780 Chapter14  Wide Area Networks DSL and cable Internet services truly do have a lot in common, but they still have some basic, essential differences that are important for you to understand: Speed Most would say that cable is faster than DSL Internet, but cable doesn’t always win the race in real-world use. Security DSL and cable are based on different network security models, and until recently, cable has been the reputed loser in this contest. But now, it’s pretty much a toss-up, and both offer adequate security that meets the needs of most users. And when I say adequate, I mean that there are still some very real security issues relating to both alternatives, no matter what your ISP says! Popularity Cable Internet is definitely “best in show” in the U.S., but DSL is beginning to catch up. Customer Satisfaction Here, the reverse is true—in the U.S., DSL is top dog. But still, do you know anyone that’s really totally satisfied with their ISP?Figure 14.2 shows how a connection can terminate from modems to either a PC directly or a router. Typically, your router would run DHCP on that interface, as well as PPPoE. Both DSL and cable high-speed Internet services are available to millions of residential and business consumers worldwide, but in some areas, only one (sometimes neither) service is available.Surprisingly, some of the differences between DSL and cable modem have nothing to do with the actual technologies—it comes down to the individual ISP. All other things being equal, issues like cost, reliability, and quality of customer support for both installation and maintenance issues vary significantly from one provider to the next. Cable Cable is a great cost-effective connection for a small office or home office, or SOHO—yes, there is an acronym for everything! And even in larger organizations, cable (or DSL for that matter) can be great to have as a backup link. FIGURE14.2 Broadband access using cable or DSL

Always-on Voice, Video, and Data Services

Underlying network is transparent to the user. 

Ethernet Cable or DSL Cable or DSL 

 

10089.book  Page 780  Monday, July 23, 2007  3:17 PM




 Cable and DSL 781 Here are a few cable network terms: Headend This is where all cable signals are received, processed, and formatted. The signals are then transmitted over the distribution network from the headend. Distribution network These are relatively small service areas that usually range in size from 100 to 2,000 customers. They’re typically composed of a mixed, fiber-coaxial, or HFC archi-tecture, with optical fiber substituting for the distribution network’s trunk portion. The fiber forms both the connection from the headend and an optical node that changes light to radio frequency (RF)   signals that are then distributed through a coaxial cable throughout the spe-cific area being serviced. DOCSIS (data over cable service interface specification) All cable modems and like devices have to measure up to this standard.Figure 14.3 shows where you would find the various types of networks and how the terms I just listed would be used in a network diagram. FIGURE14.3 Cable network and terms The problem is that ISPs often use a fiber-optic network that extends from the cable oper-ator’s master headend, sometimes even to regional headends, out to a neighborhood’s hubsite that then arrives at a fiber-optic node, which serves anywhere from 25 to 2,000 or more homes. (Don’t get me wrong, all links have problems—I’m not picking on cable—really!)And here’s another issue: If you have cable, open your PC’s command prompt, and type  ipconfig  and check out your subnet mask. It’s probably a /20 or /21 class B address. Oh my. You already know that’s either 4,094 or 2,046 hosts per cable network connection. Not good!When we say “cable,” we really mean using coax (coaxial) cable for transmission. And CATV, or community antenna television, is now used as a means to offer cost-effective broad-casting to subscribers. Cable is able to provide voice and data, plus analog and digital video, without requiring you to pony up your whole paycheck.Your average cable connection gives you a maximum download speed of 2Mbps. And remember—you have to share that bandwidth with all the other subscribers. As if that weren’t enough, there are other things like overloaded web servers and plain old Net congestion that factor in as well. But your email-checking neighbors really aren’t making that much of a dif-ference. So who or what is? Well, if you’re an online gamer, you would likely notice a bit more lag during peak periods (which could be a matter of virtual life and death!). And if somebody 

DOCSIS standard Headend HFC Fiber Coaxial cable serving area Node 

 

10089.book  Page 781  Monday, July 23, 2007  3:17 PM




 782 Chapter14  Wide Area Networks in your neighborhood is uploading a large amount of data—like, well, an entire collection of pirated  Star Wars movies—that could definitely max out the entire connection and bring everyone’s browser to a crawl.Cable modem access may or may not be faster or easier to install than DSL, and your mile-age will vary, depending on where you live plus a variety of other factors. But it’s usually more available and a tad less pricey, making it a winner by a nose. But no worries, if cable access isn’t available in your neighborhood, DSL is okay—anything is better than dial-up!Digital Subscriber Line (DSL)Coming in second in our subscriber-based popularity contest is DSL (digital subscriber line), a technology that uses your garden-variety copper phone wires to give you high-speed data transmission. DSL requires a phone line, a DSL modem (often included with service), either an Ethernet card or a router that has an Ethernet connection, and someone that can provide service wherever you happen to be located.The acronym DSL originally meant digital subscriber loop, but now its meaning has morphed to digital subscriber line. DSL group types fall into two categories based upon the upstream or downstream speed connections:Symmetrical DSLThe speed for both downstream and upstream connections are equal, or symmetrical.Asymmetrical DSLDifferent transmission speeds occur between two ends of a network—downstream speed is usually faster.Figure 14.4 shows an average home user with xDSL, which is a transmission technology that moves data over copper pairs.The term xDSL covers a number of DSL variations, such as ADSL, high-bit-rate DSL (HDSL), Rate Adaptive DSL (RADSL), Synchronous DSL (SDSL), ISDN DSL (IDSL), and very-high-data-rate DSL (VDSL)FIGURE14.4xDSL connection from home user to central office 

All types of DSL are layer 1 technologies. ATU-R = ADSL Transmission Unit - Remote ATU-C = ADSL Transmission Unit - Central 

Subscriber End user Ethernet ATU-R ATM xDSL Copper loop Local loop CO ATU-C 

10089.book  Page 782  Monday, July 23, 2007  3:17 PM




Cable and DSL783DSL flavors that don’t use the voice frequencies band, like ADSL and VDSL, allow DSL lines to carry both data and voice signals simultaneously. Others, like SDSL and IDSL, that occupy the complete frequency range, can only carry data. And by the way, the data service that the DSL connection gives you is always on.The speed that DSL service can offer depends on how far you are from the CO—the closer the better. In fact, you can blaze at rates up to around 6.1Mbps if you’re physically close enough!ADSLADSL supports both voice and data at the same time, but it was created to allot more band-width downstream than upstream because it’s best for residential subscribers that usually need more downstream bandwidth for doing things like downloading video, movies, and music; online gaming; surfing; and getting emails—some that include sizeable attachments. ADSL will give you a downstream rate from 256Kbps to 8Mbps, but anything going upstream is only going to reach around 1Mbps.POTS provides a channel for analog voice transmission and can transmit without a prob-lem with ADSL over the same twisted-pair telephone line. Actually, depending on the type of ADSL, not just two, but three information channels commonly utilize the same wiring at the same time. This is why people can use a phone line and an ADSL connection at the same time and not affect either service.ATM is the Data Link layer protocol typically used over the DSL layer 1 connection from the CPE that’s terminated at what’s known as the DSLAM—an ATM switch that contains DSL interface cards, or ATU-Cs. After ADSL connections meet their end at the DSLAM, it switches the data over an ATM network to something called an aggregation router—a layer 3 device where the subscriber’s IP connection then expires.You know by now how important encapsulation is, so as you’ve probably guessed, any IP packets over an ATM and DSL connection must have this done. This happens in one of three ways, depending on your interface type and the service provider’s switch:PPPoEThis will be discussed in more detail in the next section.RFC1483 RoutingRFC1483 describes two different methods for carrying connectionless network traffic over an ATM network: routed protocols and bridged protocols.PPPoAPoint-to-Point Protocol (PPP) over ATM is used to encapsulate PPP frames in ATM AAL5 (ATM Adaptation Layer 5). It is typically used with cable modems, DSL, and ADSL services and offers the usual PPP features of authentication, encryption, and compression and actually has less overhead in comparison to PPPoE.PPPoEUsed with ADSL services, PPPoE (Point-to-Point Protocol over Ethernet) encapsulates PPP frames in Ethernet frames and uses common PPP features like authentication, encryption, and compression. But as I said earlier, it’s trouble if you’ve got a badly configured firewall. This is a tunneling protocol that layers IP and other protocols that run over PPP with the attributes of a PPP link so they can then be used to contact other Ethernet devices and initiate a point-to-point connection to transport IP packets.

10089.book  Page 783  Monday, July 23, 2007  3:17 PM




784Chapter14 Wide Area NetworksFigure 14.5 displays typical usage of PPPoE over ADSL. As you can see, a PPP session is connected from the PC of the end user to the router and the subscriber PC IP address is assigned by the router via IPCP.FIGURE14.5PPPoE with ADSLPPPoE is used to equip custom PPP-based software with the ability to deal with a connec-tion that’s not using a serial line and to be at home in a packet-oriented network environment like Ethernet and to allow for a custom connection with login and password for Internet con-nection accounting. Another factor is that the opposite side of the link’s IP address is only given to it and available for the specific period that the PPPoE connection is open, so reusing IP addresses dynamically is permitted.PPPoE has a discovery stage and a PPP session stage (see RFC 2516) that works like this: First, a host begins a PPPoE session, during which it has to execute a discovery process so it can deter-mine the best server to meet the needs of the client machine’s request. After that, it has to discover the Ethernet MAC address of the peer device and create a PPPoE session ID. So even though PPP delimits a peer-to-peer relationship, the discovery part is innately a client-server relationship.Okay, before getting into serial connections, there’s one last thing I want to cover—Cisco LRE.Cisco Long Range Ethernet (LRE)The Cisco Long Range Ethernet solution employs something called VDSL (Very High Data Rate Digital Subscriber Line) technology to significantly expand Ethernet service capacity. And LRE can achieve these impressive results: speeds from 5 to 15Mbps (full duplex) at distances up to 5,000 feet traveling over existing twisted-pair wiring!

CPE (bridging) ATM DSLAM OC3 ATM Aggregation router AAA IP ISP/Corp router PADI PADO PADR PADS (Session ID)  LCP/IPCP 

10089.book  Page 784  Monday, July 23, 2007  3:17 PM




Cabling the Serial Wide Area Network785So basically, Cisco LRE technology can give us broadband service on POTS, digital tele-phone, and ISDN traffic lines, and it also can operate in modes that are compatible with ADSL technologies. This flexibility is important because it makes it possible for service providers to make LRE available in structures and/or buildings that have broadband services already in place but need it enhanced—very cool indeed.Cabling the Serial Wide Area NetworkAs you can imagine, there are a few things that you need to know before connecting your WAN in order to make sure everything goes well. For starters, you have to understand the kind of WAN Physical layer implementation that Cisco provides as well as ensure that you’re familiar with the various types of WAN serial connectors involved.The good news is that Cisco serial connections support almost any type of WAN service. Your typical WAN connection is a dedicated leased line using HDLC, PPP, and Frame Relay with speeds that can kick it up to 45Mbps (T3).HDLC, PPP, and Frame Relay can use the same Physical layer specifications, and I’ll go over the various types of connections and then move on to telling you all about the WAN pro-tocols specified in the CCNA objectives.Serial TransmissionWAN serial connectors use serial transmission, something that takes place 1 bit at a time over a single channel.

Parallel transmission can pass at least 8 bits at a time, but all WANs use serial transmission.Cisco routers use a proprietary 60-pin serial connector that you have to get from Cisco or a provider of Cisco equipment. Cisco also has a new, smaller proprietary serial connection that’s about one-tenth the size of the 60-pin basic serial cable called the “smart-serial”—I’m not sure why. But I do know that you have to make sure you have the right type of interface in your router before using this cable connector.The type of connector you have on the other end of the cable depends on your service provider and their particular end-device requirements. There are several different types of ends you’ll run into: EIA/TIA-232 EIA/TIA-449 V.35 (used to connect to a CSU/DSU) EIA-530

10089.book  Page 785  Monday, July 23, 2007  3:17 PM




786Chapter14 Wide Area NetworksMake sure you’re clear on these things: Serial links are described in frequency or cycles per second (hertz). The amount of data that can be carried within these frequencies is called band-width. Bandwidth is the amount of data in bits per second that the serial channel can carry.Data Terminal Equipment and Data Communication EquipmentBy default, router interfaces are data terminal equipment (DTE), and they connect into data communication equipment (DCE) like a channel service unit/data service unit (CSU/DSU). The CSU/DSU then plugs into a demarcation location (demarc) and is the service provider’s last responsibility. Most of the time, the demarc is a jack that has an RJ-45 (8-pin modular) female connector located in a telecommunications closet.Actually, you may already have heard of demarcs. If you’ve ever had the glorious experi-ence of reporting a problem to your service provider, they’ll usually tell you everything tests out fine up to the demarc, so the problem must be the CPE, or customer premises equipment. In other words, it’s your problem not theirs.Figure 14.6 shows a typical DTE-DCE-DTE connection and the devices used in the network.The idea behind a WAN is to be able to connect two DTE networks through a DCE net-work. The DCE network includes the CSU/DSU, through the provider’s wiring and switches, all the way to the CSU/DSU at the other end. The network’s DCE device (CSU/DSU) provides clocking to the DTE-connected interface (the router’s serial interface).As mentioned, the DCE network provides clocking to the router; this is the CSU/DSU. If you have a nonproduction network and you’re using a WAN crossover type of cable and do not have a CSU/DSU, then you need to provide clocking on the DCE end of the cable by using the clock rate command that I showed you in Chapter 4.

Terms such as EIA/TIA-232, V.35, X.21, and HSSI (High-Speed Serial Interface) describe the Physical layer between the DTE (router) and DCE device (CSU/DSU).FIGURE14.6DTE-DCE-DTE WAN connection

Clocking typically provided by DCE network to routers.In non-production environments, a DCE network is not always present.

DCE

DTEDTE

CSU/DSU

CSU/DSU

10089.book  Page 786  Monday, July 23, 2007  3:17 PM




High-Level Data-Link Control (HDLC) Protocol787High-Level Data-Link Control (HDLC) ProtocolThe High-Level Data-Link Control (HDLC) protocol is a popular ISO-standard, bit-oriented, Data Link layer protocol. It specifies an encapsulation method for data on synchronous serial data links using frame characters and checksums. HDLC is a point-to-point protocol used on leased lines. No authentication can be used with HDLC.In byte-oriented protocols, control information is encoded using entire bytes. On the other hand, bit-oriented protocols use single bits to represent the control information. Some com-mon bit-oriented protocols include SDLC, LLC, HDLC, TCP, and IP.HDLC is the default encapsulation used by Cisco routers over synchronous serial links. And Cisco’s HDLC is proprietary—it won’t communicate with any other vendor’s HDLC implementation. But don’t give Cisco grief for it—everyone’s HDLC implementation is pro-prietary. Figure 14.7 shows the Cisco HDLC format.FIGURE14.7Cisco HDLC frame formatAs shown in the figure, the reason that every vendor has a proprietary HDLC encapsulation method is that each vendor has a different way for the HDLC protocol to encapsulate multiple Network layer protocols. If the vendors didn’t have a way for HDLC to communicate the dif-ferent layer 3 protocols, then HDLC would only be able to carry one protocol. This propri-etary header is placed in the data field of the HDLC encapsulation.So let’s say you only have one Cisco router, and you need to connect to a non-Cisco router because your other Cisco router is on order. What would you do? You couldn’t use the default HDLC serial encapsulation because it wouldn’t work. Instead, you would use something like PPP, an ISO-standard way of identifying the upper-layer protocols. You can check out RFC 1661 for more information on the origins and standards of PPP. Let’s discuss PPP in more detail and how to connect to routers using the PPP encapsulation.

AddressFlag  Each vendor’s HDLC has a proprietary data field to support multiprotocol environments.  Supports only single-protocol environments.FlagAddressControlDataFCSFlagControlProprietaryDataFCSFlagCisco HDLCHDLC

10089.book  Page 787  Monday, July 23, 2007  3:17 PM




788Chapter14 Wide Area NetworksPoint-to-Point Protocol (PPP)Let’s spend a little time on Point-to-Point Protocol (PPP). Remember that it’s a Data Link layer protocol that can be used over either asynchronous serial (dial-up) or synchronous serial (ISDN) media. It uses Link Control Protocol (LCP) to build and maintain data-link connec-tions. Network Control Protocol (NCP) is used to allow multiple Network layer protocols (routed protocols) to be used on a point-to-point connection.Since HDLC is the default serial encapsulation on Cisco serial links and it works great, why and when would you choose to use PPP? Well, the basic purpose of PPP is to transport layer 3 packets across a Data Link layer point-to-point link, and it’s nonproprietary. So unless you have all Cisco routers, you need PPP on your serial interfaces—the HDLC encapsulation is Cisco proprietary, remember? Plus, since PPP can encapsulate several layer 3 routed protocols and provide authentication, dynamic addressing, and callback, PPP could be the best encap-sulation solution for you instead of HDLC.Figure 14.8 shows the protocol stack compared to the OSI reference model.FIGURE14.8Point-to-Point Protocol stackPPP contains four main components:EIA/TIA-232-C, V.24, V.35, and ISDNA Physical layer international standard for serial communication.HDLCA method for encapsulating datagrams over serial links.LCPA method of establishing, configuring, maintaining, and terminating the point-to-point connection.NCPA method of establishing and configuring different Network layer protocols. NCP is designed to allow the simultaneous use of multiple Network layer protocols. Some examples 

Upper-layer Protocols(such as IP, IPX, AppleTalk)Physical layer(such as EIA/TIA-232, V.24, V.35, ISDN)

Network Control Protocol (NCP)(specific to each Network-layer protocol)

Link Control Protocol (LCP)

High-Level Data Link Control Protocol (HDLC)OSI layer321

10089.book  Page 788  Monday, July 23, 2007  3:17 PM




Point-to-Point Protocol (PPP)789of protocols here are IPCP (Internet Protocol Control Protocol) and IPXCP (Internetwork Packet Exchange Control Protocol).Burn it into your mind that the PPP protocol stack is specified at the Physical and Data Link layers only. NCP is used to allow communication of multiple Network layer protocols by encapsulating the protocols across a PPP data link.

Remember that if you have a Cisco router and a non-Cisco router con-nected with a serial connection, you must configure PPP or another encap-sulation method, such as Frame Relay, because the HDLC default just won’t work!Next, I’ll cover the options for LCP and PPP session establishment.Link Control Protocol (LCP) Configuration OptionsLink Control Protocol (LCP) offers different PPP encapsulation options, including the following:AuthenticationThis option tells the calling side of the link to send information that can identify the user. The two methods are PAP and CHAP.CompressionThis is used to increase the throughput of PPP connections by compressing the data or payload prior to transmission. PPP decompresses the data frame on the receiving end.Error detectionPPP uses Quality and Magic Number options to ensure a reliable, loop-free data link.MultilinkStarting with IOS version 11.1, multilink is supported on PPP links with Cisco routers. This option makes several separate physical paths appear to be one logical path at layer 3. For example, two T1s running multilink PPP would show up as a single 3Mbps path to a layer 3 routing protocol.PPP callbackPPP can be configured to call back after successful authentication. PPP callback can be a good thing for you because you can keep track of usage based upon access charges, for accounting records, and a bunch of other reasons. With callback enabled, a calling router (client) will contact a remote router (server) and authenticate as I described earlier. (Know that both routers have to be configured for the callback feature for this to work.) Once authentication is completed, the remote router will terminate the connection and then re-initiate a connection to the calling router from the remote router.

If you have Microsoft devices in your PPP callback, be aware that Microsoft uses a proprietary callback known as Microsoft Callback Control Protocol (CBCP), which is supported in IOS release 11.3(2)T and later.

10089.book  Page 789  Monday, July 23, 2007  3:17 PM




790Chapter14 Wide Area NetworksPPP Session EstablishmentWhen PPP connections are started, the links go through three phases of session establishment, as shown in Figure 14.9.FIGURE14.9PPP session establishmentLink-establishment phaseLCP packets are sent by each PPP device to configure and test the link. These packets contain a field called the Configuration Option that allows each device to see the size of the data, compression, and authentication. If no Configuration Option field is present, then the default configurations will be used.Authentication phaseIf required, either CHAP or PAP can be used to authenticate a link. Authentication takes place before Network layer protocol information is read. And it’s pos-sible that link-quality determination will occur simultaneously.Network layer protocol phasePPP uses the Network Control Protocol (NCP) to allow multiple Network layer protocols to be encapsulated and sent over a PPP data link. Each Network layer protocol (e.g., IP, IPX, AppleTalk, which are routed protocols) establishes a service with NCP.PPP Authentication MethodsThere are two methods of authentication that can be used with PPP links:Password Authentication Protocol (PAP)The Password Authentication Protocol (PAP) is the less secure of the two methods. Passwords are sent in clear text, and PAP is only performed upon the initial link establishment. When the PPP link is first established, the remote node sends the username and password back to the originating router until authentication is acknowledged. Not exactly Fort Knox!Challenge Handshake Authentication Protocol (CHAP)The Challenge Handshake Authen-tication Protocol (CHAP) is used at the initial startup of a link and at periodic checkups on the link to make sure the router is still communicating with the same host.After PPP finishes its initial link-establishment phase, the local router sends a challenge request to the remote device. The remote device sends a value calculated using a one-way hash func-tion called MD5. The local router checks this hash value to make sure it matches. If the values don’t match, the link is immediately terminated.

PPP Session Establishment1. Link establishment phase2. Authentication phase (optional)3. Network layer protocol phase

Dial-up orcircuit-switchednetwork

10089.book  Page 790  Monday, July 23, 2007  3:17 PM




Point-to-Point Protocol (PPP)791Configuring PPP on Cisco RoutersConfiguring PPP encapsulation on an interface is really pretty straightforward. To configure it from the CLI, follow these simple router commands:

Router#config tEnter configuration commands, one per line. End with CNTL/Z.Router(config)#int s0Router(config-if)#encapsulation pppRouter(config-if)#^Z

Router#Of course, PPP encapsulation has to be enabled on both interfaces connected to a serial line in order to work, and there are several additional configuration options available to you via the help command.Configuring PPP AuthenticationAfter you configure your serial interface to support PPP encapsulation, you can configure authentication using PPP between routers. First, you need to set the hostname of the router, if it’s not already. Then you set the username and password for the remote router that will be connecting to your router:Here’s an example:

Router#config tEnter configuration commands, one per line. End with CNTL/Z.Router(config)#hostname RouterA

RouterA(config)#username RouterB password ciscoWhen using the hostname command, remember that the username is the hostname of the remote router that’s connecting to your router. And it’s case sensitive too. Also, the password on both routers must be the same. It’s a plain-text password that you can see with a show run com-mand; you can encrypt the password by using the command service password-encryption. You must have a username and password configured for each remote system you plan to connect to. The remote routers must also be configured with usernames and passwords.Now, after you’ve set the hostname, usernames, and passwords, choose the authentication type, either CHAP or PAP:

RouterA#config tEnter configuration commands, one per line. End with CNTL/Z.RouterA(config)#int s0RouterA(config-if)#ppp authentication chap papRouterA(config-if)#^Z

RouterA#

10089.book  Page 791  Monday, July 23, 2007  3:17 PM




792Chapter14 Wide Area NetworksIf both methods are configured on the same line, as shown here, then only the first method will be used during link negotiation—the second acts as a backup just in case the first method fails.

I’ll configure PPP with authentication using SDM later in this chapter, but honestly, the CLI way is easier.Verifying PPP EncapsulationOkay—now that PPP encapsulation is enabled, let me show you how to verify that it’s up and running. First, let’s take a look at a figure of a sample network. Figure 14.10 shows two routers connected with either a point-to-point serial or ISDN connection.FIGURE14.10PPP authentication exampleYou can start verifying the configuration with the show interface command:

Pod1R1#sh int s0/0Serial0/0 is up, line protocol is up  Hardware is PowerQUICC Serial  Internet address is 10.0.1.1/24  MTU 1500 bytes, BW 1544 Kbit, DLY 20000 usec,     reliability 239/255, txload 1/255, rxload 1/255  Encapsulation PPP  loopback not set  Keepalive set (10 sec)  LCP Open  Open: IPCP, CDPCP

[output cut]Notice that the sixth line lists encapsulation as PPP and the eighth line shows that the LCP is open. This means that it has negotiated the session establishment and all is good! The ninth line tells us that NCP is listening for the protocols IP and CDP.

hostname Pod1R1username Pod1R2 password cisco  interface serial 0   ip address 10.0.1.1 255.255.255.0  encapsulation ppp ppp authentication chaphostname Pod1R2username Pod1R1 password cisco  interface serial 0   ip address 10.0.1.2 255.255.255.0  encapsulation ppp ppp authentication chap

PSTN/ISDNPod1R1Pod1R2

10089.book  Page 792  Monday, July 23, 2007  3:17 PM




Point-to-Point Protocol (PPP)793But what will you see if everything isn’t perfect? I’m going to type in the configuration shown in Figure 14.11 and find out.FIGURE14.11Failed PPP authenticationOkay—what’s wrong here? Take a look at the usernames and passwords. Do you see the problem now? That’s right, the C is capitalized on the Pod1R2 username command found in the configuration of router Pod1R1. This is wrong because the usernames and passwords are case sensitive, remember? Let’s take a look at the show interface command and see what happens:

Pod1R1#sh int s0/0Serial0/0 is up, line protocol is down  Hardware is PowerQUICC Serial  Internet address is 10.0.1.1/24  MTU 1500 bytes, BW 1544 Kbit, DLY 20000 usec,     reliability 243/255, txload 1/255, rxload 1/255  Encapsulation PPP, loopback not set  Keepalive set (10 sec)  LCP Closed

  Closed: IPCP, CDPCPFirst, notice in the first line of output that Serial0/0 is up, line protocol is down. This is because there are no keepalives coming from the remote router. Next, notice that the LCP is closed because the authentication failed.Debugging PPP AuthenticationTo display the CHAP authentication process as it occurs between two routers in the network, just use the command debug ppp authentication.If your PPP encapsulation and authentication are set up correctly on both routers, and your usernames and passwords are all good, then the debug ppp authentication command will display an output that looks like this:

d16h: Se0/0 PPP: Using default call direction1d16h: Se0/0 PPP: Treating connection as a dedicated line

hostname Pod1R1username Pod1R2 password Cisco  interface serial 0   ip address 10.0.1.1 255.255.255.0  encapsulation ppp ppp authentication chaphostname Pod1R2username Pod1R1 password cisco  interface serial 0   ip address 10.0.1.2 255.255.255.0  encapsulation ppp ppp authentication chap

PSTN/ISDNPod1R1Pod1R2

10089.book  Page 793  Monday, July 23, 2007  3:17 PM




794Chapter14 Wide Area Networks1d16h: Se0/0 CHAP: O CHALLENGE id 219 len 27 from "Pod1R1"1d16h: Se0/0 CHAP: I CHALLENGE id 208 len 27 from "Pod1R2"1d16h: Se0/0 CHAP: O RESPONSE id 208 len 27 from "Pod1R1"1d16h: Se0/0 CHAP: I RESPONSE id 219 len 27 from "Pod1R2"1d16h: Se0/0 CHAP: O SUCCESS id 219 len 4

1d16h: Se0/0 CHAP: I SUCCESS id 208 len 4But if you have the username wrong, as we did previously in the PPP authentication failure example back in Figure 14.11, the output would look something like this:

1d16h: Se0/0 PPP: Using default call direction1d16h: Se0/0 PPP: Treating connection as a dedicated line1d16h: %SYS-5-CONFIG_I: Configured from console by console1d16h: Se0/0 CHAP: O CHALLENGE id 220 len 27 from "Pod1R1"1d16h: Se0/0 CHAP: I CHALLENGE id 209 len 27 from "Pod1R2"1d16h: Se0/0 CHAP: O RESPONSE id 209 len 27 from "Pod1R1"1d16h: Se0/0 CHAP: I RESPONSE id 220 len 27 from "Pod1R2"

1d16h: Se0/0 CHAP: O FAILURE id 220 len 25 msg is "MD/DES compare failed"PPP with CHAP authentication is a three-way authentication, and if the username and passwords are not configured exactly the way they should be, then the authentication will fail and the link will be down.Mismatched WAN EncapsulationsIf you have a point-to-point link but the encapsulations aren’t the same, the link will never come up. Figure 14.12 shows one link with PPP and one with HDLC.FIGURE14.12Mismatched WAN encapsulationsLook at router Pod1R1 in this output:

Pod1R1#sh int s0/0Serial0/0 is up, line protocol is down  Hardware is PowerQUICC Serial  Internet address is 10.0.1.1/24  MTU 1500 bytes, BW 1544 Kbit, DLY 20000 usec,

hostname Pod1R1username Pod1R2 password Cisco  interface serial 0   ip address 10.0.1.1 255.255.255.0   encapsulation ppphostname Pod1R2username Pod1R1 password cisco  interface serial 0   ip address 10.0.1.2 255.255.255.0   encapsulation HDLC

PSTN/ISDNPod1R1Pod1R2

10089.book  Page 794  Monday, July 23, 2007  3:17 PM




Point-to-Point Protocol (PPP)795     reliability 254/255, txload 1/255, rxload 1/255  Encapsulation PPP, loopback not set  Keepalive set (10 sec)  LCP REQsent

Closed: IPCP, CDPCPThe serial interface is down and LCP is sending requests but will never receive any responses because router Pod1R2 is using the HDLC encapsulation. To fix this problem, you would have to go to router Pod1R2 and configure the PPP encapsulation on the serial interface. One more thing—even though the usernames are configured and they’re wrong, it doesn’t matter because the com-mand ppp authentication chap isn’t used under the serial interface configuration and the user-name command isn’t relevant in this example.

Always remember that you just can’t have PPP on one side and HDLC on the other—they don’t get along!Mismatched IP AddressesA tricky problem to spot is if you have HDLC or PPP configured on your serial interface but your IP addresses are wrong. Things seem to be just fine because the interfaces will show that they are up. Take a look at Figure 14.13 and see if you can see what I mean—the two routers are connected with different subnets—router Pod1R1 with 10.0.1.1/24 and router Pod1R2 with 10.2.1.2/24.FIGURE14.13Mismatched IP addressesThis will never work. But as I said, take a look at the output:

Pod1R1#sh int s0/0Serial0/0 is up, line protocol is up  Hardware is PowerQUICC Serial  Internet address is 10.0.1.1/24  MTU 1500 bytes, BW 1544 Kbit, DLY 20000 usec,     reliability 255/255, txload 1/255, rxload 1/255

hostname Pod1R1username Pod1R2 password cisco  interface serial 0   ip address 10.0.1.1 255.255.255.0   encapsulation ppp   ppp authentication chaphostname Pod1R2username Pod1R1 password cisco  interface serial 0   ip address 10.2.1.2 255.255.255.0   encapsulation ppp   ppp authentication chap

PSTN/ISDNPod1R1Pod1R2

10089.book  Page 795  Monday, July 23, 2007  3:17 PM




796Chapter14 Wide Area Networks  Encapsulation PPP, loopback not set  Keepalive set (10 sec)  LCP Open

  Open: IPCP, CDPCPSee that? The IP addresses between the routers are wrong but the link looks like it’s working fine. This is because PPP, like HDLC and Frame Relay, is a layer 2 WAN encapsulation and doesn’t care about IP addresses at all. So yes, the link is up, but you can’t use IP across this link since it’s misconfigured.To find and fix this problem, you can use the show running-config or the show interfaces command on each router, or you can use what you learned in Chapter 5—the show cdp neighbors detail command:

Pod1R1#sh cdp neighbors detail-------------------------Device ID: Pod1R2Entry address(es):

  IP address: 10.2.1.2You can view and verify the directly connected neighbor’s IP address and then solve your problem.Okay—before we move onto Frame Relay, let’s take a look at PPPoE.PPPoE ConfigurationIf you have a router with an interface that supports PPPoE and the router is connected to a DSL modem, you can configure the router to be a PPPoE client—well, assuming your ISP has provided you with the authentication information, that is.Let’s take a look at configuring a PPPoE client on a router. Here’s what it looks like under the physical interface:

R1(config)#int f0/0R1(config-if)#p?pppoe  pppoe-client  priority-groupR1(config-if)#pppoe ?  enable        Enable pppoe  max-sessions  Maximum PPPOE sessionsR1(config-if)#pppoe enable ?  group  attach a BBA group  <cr>R1(config-if)#pppoe enable group ?  WORD    BBA Group name  global  Attach global PPPoE group

10089.book  Page 796  Monday, July 23, 2007  3:17 PM




Point-to-Point Protocol (PPP)797R1(config-if)#pppoe enable group globalR1(config-if)#pppoe-client dial-pool-number ?  <1-255>  Dialer pool numberR1(config-if)#pppoe-client dial-pool-number 1!interface FastEthernet4 description $ETH-WAN$ no ip address duplex auto speed auto pppoe enable group global pppoe-client dial-pool-number 1

!After all that, there really are only two commands needed under the physical interface—the pppoe enable command and the pppoe-client command. And both of them reference the logical interface we haven’t created yet.In order to add a PPPoE connection to your router, you need to also create a dialer inter-face. This is a logical interface, and under it, I’m going to add the ip address negotiated command so a DHCP address can be received and configured on the interface. And by the way, if you’re using private IP addresses between the DSL modem and your router, you can easily add a static IP address on this interface. Take a look:

!interface Dialer0 ip address negotiated ip mtu 1452 encapsulation ppp dialer pool 1 dialer-group 1 ppp authentication chap callin ppp chap hostname Todd ppp chap password 0 lammle

!Take special notice of how the logical interface associates itself to the physical interface with both the dial pool 1 command and the dialer-group 1 command.Last, under the dialer interface, the PPP authentication is set using the ppp authentication and ppp chap commands. Using the CLI, I provided these commands at global configuration mode, but in this setup, I’ll configure the command directly under the logical interface instead.Although this is a pretty simple configuration, it works really well! Still, I’ll show you how to configure PPPoE using the SDM in a bit.

10089.book  Page 797  Monday, July 23, 2007  3:17 PM




798Chapter14 Wide Area NetworksFrame RelayFrame Relay is still one of the most popular WAN services deployed over the past decade, and there’s a good reason for this—cost. And it’s a rare network design or designer that has the privilege to ignore that all-important cost factor!By default, Frame Relay is classified as a non-broadcast multi-access (NBMA) network, meaning it doesn’t send any broadcasts like RIP updates across the network. No worries—I’m not going leave you hanging. We’ll get into this more soon.Frame Relay has at its roots a technology called X.25, and it essentially incorporates the components of X.25 that are still relevant to today’s reliable and relatively “clean” telecom-munications networks while leaving out the no-longer-needed error-correction components. It’s substantially more complex than the simple leased-line networks you learned about when I discussed the HDLC and PPP protocols. The leased-line networks are easy to conceptualize—but not so much when it comes to Frame Relay. It can be significantly more complex and ver-satile, which is why it’s often represented as a “cloud” in networking graphics. I’ll get to that in a minute—for right now, I’m going to introduce Frame Relay in concept and show you how it differs from simpler leased-line technologies.Along with your introduction to this technology, you’ll get a virtual dictionary of all the new terminology you’ll need to solidly grasp the basics of Frame Relay. After that, I’ll guide you through some simple Frame Relay implementations.Introduction to Frame Relay TechnologyAs a CCNA, you’ll need to understand the basics of the Frame Relay technology and be able to configure it in simple scenarios. First, understand that Frame Relay is a packet-switched technology. From everything you’ve learned so far, just telling you this should make you immediately realize several things about it: You won’t be using the encapsulation hdlc or encapsulation ppp command to configure it. Frame Relay doesn’t work like a point-to-point leased line (although it can be made to look and act like one). Frame Relay is usually less expensive than leased lines are, but there are some sacrifices to make to get that savings.So, why would you even consider using Frame Relay? Take a look at Figure 14.14 to get an idea of what a network looked like before Frame Relay. Now check out Figure 14.15. You can see that there’s now only one connection between the Corporate router and the Frame Relay switch. That saves some major cash!If, for example, you had to add seven remote sites to the corporate office and had only one free serial port on your router—it’s Frame Relay to the rescue! Of course, I should probably mention that you now also have one single point of failure, which is not so good. But Frame Relay is used to save money, not to make a network more resilient.

10089.book  Page 798  Monday, July 23, 2007  3:17 PM




Frame Relay799FIGURE14.14Before Frame RelayFIGURE14.15After Frame RelayComing up, I’m going to cover the Frame Relay technology information you need to know about when studying the CCNA objectives.Committed Information Rate (CIR)Frame Relay provides a packet-switched network to many different customers at the same time. This is a really good thing because it spreads the cost of the switches among many cus-tomers. But remember, Frame Relay is based on the assumption that all customers won’t ever need to transmit data constantly, and all at the same time.Frame Relay works by providing a portion of dedicated bandwidth to each user, and it also allows the user to exceed their guaranteed bandwidth if resources on the telco network happen to be available. So basically, Frame Relay providers allow customers to buy a lower amount 

Dedicated/leased linesto each location$$$

Statistically multiplexingmultiple logical circuits over asingle physical connection

Frame RelayFrame Relay creates a cost-effective mesh network.

10089.book  Page 799  Monday, July 23, 2007  3:17 PM




800Chapter14 Wide Area Networksof bandwidth than what they really use. There are two separate bandwidth specifications with Frame Relay:Access rateThe maximum speed at which the Frame Relay interface can transmit.CIRThe maximum bandwidth of data guaranteed to be delivered. In reality, it’s the average amount that the service provider will allow you to transmit.If these two values are the same, the Frame Relay connection is pretty much just like a leased line. But they can also be set to different values. Here’s an example: Let’s say that you buy an access rate of T1 (1.544Mbps) and a CIR of 256Kbps. By doing this, the first 256Kbps of traffic you send is guaranteed to be delivered. Anything beyond that is called a “burst”—a transmission that exceeds your guaranteed 256Kbps rate, and can be any amount up to the T1 access rate (if that amount is in your contract). If your combined committed burst (the basis for your CIR) and excess burst sizes, known as the MBR or maximum burst rate when com-bined, exceed the access rate, you can pretty much say goodbye to your additional traffic. It will most likely be dropped, although this really depends on the subscription level of a partic-ular service provider.In a perfect world, this always works beautifully—but remember that little word guaran-tee? As in guaranteed rate—of 256Kbps, to be exact? This means that any burst of data you send that exceeds your guaranteed 256Kbps rate will be delivered on something called a “best effort” basis of delivery. Or maybe not—if your telco’s equipment doesn’t have the capacity to deliver it at the time you transmitted, then your frames will be discarded, and the DTE will be notified. Timing is everything—you can scream data out at six times your guaranteed rate of 256Kbps (T1) only if your telco has the capacity available on its equipment at that moment. Remember that “oversubscription” we talked about? Well, here it is in action!

The CIR is the rate, in bits per second, at which the Frame Relay switch agrees to transfer data.Frame Relay Encapsulation TypesWhen configuring Frame Relay on Cisco routers, you need to specify it as an encapsulation on serial interfaces. As I said earlier, you can’t use HDLC or PPP with Frame Relay. When you configure Frame Relay, you specify an encapsulation of Frame Relay (as shown in the follow-ing output).But unlike HDLC or PPP, with Frame Relay, there are two encapsulation types: Cisco and IETF (Internet Engineering Task Force). The following router output shows these two different encapsulation methods when Frame Relay is chosen on your Cisco router:

RouterA(config)#int s0RouterA(config-if)#encapsulation frame-relay ?  ietf  Use RFC1490 encapsulation

  <cr>

10089.book  Page 800  Monday, July 23, 2007  3:17 PM




Frame Relay801The default encapsulation is Cisco unless you manually type in ietf, and Cisco is the type to use when connecting two Cisco devices. You’d opt for the IETF-type encapsulation if you needed to connect a Cisco device to a non-Cisco device with Frame Relay. Whichever you choose, make sure that the Frame Relay encapsulation is the same on both ends.Virtual CircuitsFrame Relay operates using virtual circuits as opposed to the actual circuits that leased lines use. These virtual circuits are what link together the thousands of devices connected to the pro-vider’s “cloud.” Frame Relay provides a virtual circuit between your two DTE devices, mak-ing them appear to be connected via a circuit when in reality, they’re dumping their frames into a large, shared infrastructure. You never see the complexity of what’s actually happening inside the cloud because you only have a virtual circuit.And on top of all that, there are two types of virtual circuits—permanent and switched. Per-manent virtual circuits (PVCs) are by far the most common type in use today. What “perma-nent” means here is that the telco creates the mappings inside their gear and as long as you pay the bill, they’ll remain in place.Switched virtual circuits (SVCs) are more like a phone call. The virtual circuit is established when data needs to be transmitted, then it’s taken down when the data transfer is complete.

I have never seen a Frame Relay service using SVCs actually offered by a telco in North America. It’s used mainly in private FR networks.Data Link Connection Identifiers (DLCIs)Frame Relay PVCs are identified to DTE end devices by Data Link Connection Identifiers (DLCIs). A Frame Relay service provider typically assigns DLCI values, which are used on Frame Relay interfaces to distinguish between different virtual circuits. Because many virtual circuits can be terminated on one multipoint Frame Relay interface, many DLCIs are often affiliated with it.Let me explain—suppose you have a central HQ with three branch offices. If you were to connect each branch office to HQ using a T1, you would need three serial interfaces on your router at HQ, one for each T1. Simple, right? Well, suppose you use Frame Relay PVCs instead. You could have a T1 at each branch connected to a service provider and only a single T1 at HQ. There would be three PVCs on the single T1 at HQ, one going to each branch. And even though there’s only a single interface and a single CSU/DSU, the three PVCs function as three separate circuits. Remember what I said about saving money? How much for two addi-tional T1 interfaces and a pair of CSU/DSUs? Answer: A lot! So, why not just go ahead and ask for a percentage of the savings in your bonus?Okay, before we go on, I want to define Inverse ARP (IARP) and discuss how it’s used with DLCIs in a Frame Relay network. Yes, it is somewhat similar to ARP in the fact that it maps a DLCI to an IP address—kind of like ARP does with MAC addresses to IP addresses. And even though you can’t configure IARP, you can disable it. It runs on a Frame Relay router and 

10089.book  Page 801  Monday, July 23, 2007  3:17 PM




802Chapter14 Wide Area Networksmaps the DLCI to an IP address for Frame Relay so it knows how to get to the Frame Relay switch. You can see IP-to-DLCI mappings with the show frame-relay map command.But if you have a non-Cisco router living in your network and it doesn’t support IARP, then you’re stuck with having to statically provide IP-to-DLCI mappings with the frame-relay map command—something I’ll demonstrate in a bit.

Inverse ARP (IARP) is used to map a known DLCI to an IP address.Let’s talk about DLCIs a bit more. They’re locally significant—global significance requires the entire network to use the LMI extensions that offer global significance. This is why you’ll mostly find global DLCIs only in private networks.But the DLCI doesn’t have to be globally significant for it to be functional in getting a frame across the network. Let me explain: When RouterA wants to send a frame to RouterB, it looks up the IARP or manual mapping of the DLCI to the IP address it’s trying to get to. Equipped with the DLCI, it then sends the frame out with the DLCI value it found in the DLCI field of the FR header. The provider’s ingress switch gets this frame and does a lookup on the DLCI/physical-port com-bination it observes. Associated with that combination, it finds a new “locally significant” (mean-ing, between itself and the next-hop switch) DLCI to use in the header, and in the same entry in its table, it finds an outgoing physical port. This happens all the way to RouterB. So basically, you actually could say that the DLCI RouterA identifies the entire virtual circuit to RouterB, even though every DLCI between every pair of devices could be completely different. The big point here is that RouterA is unaware of these differences. That’s what makes the DLCI locally significant. So make a mental note that DLCIs really are used by the telco to “find” the other end of your PVC.To discover why DLCIs are considered locally significant, take a look at Figure 14.16. In the figure, DLCI 100 is considered locally significant to RouterA and identifies the cir-cuit between RouterA and its ingress Frame Relay switch. DLCI 200 would identify the circuit between RouterB and its ingress Frame Relay switch.FIGURE14.16DLCIs are local to your routerDLCI numbers that are used to identify a PVC are typically assigned by the provider and start at 16.You configure a DLCI number to be applied to an interface like this:

RouterA(config-if)#frame-relay interface-dlci ?  <16-1007> Define a DLCI as part of the current            subinterface

RouterA(config-if)#frame-relay interface-dlci 16

RouterARouterBDLCI 100DLCI 200

10089.book  Page 802  Monday, July 23, 2007  3:17 PM




Frame Relay803

DLCIs identify the logical circuit between the local router and a Frame Relay switch.Local Management Interface (LMI)Local Management Interface (LMI) is a signaling standard used between your router and the first Frame Relay switch it’s connected to. It allows for passing information about the operation and status of the virtual circuit between the provider’s network and the DTE (your router). It communicates information about the following:KeepalivesThese verify that data is flowing.MulticastingThis is an optional extension of the LMI specification that allows, for example, the efficient distribution of routing information and ARP requests over a Frame Relay net-work. Multicasting uses the reserved DLCIs from 1019 through 1022.Global addressingThis provides global significance to DLCIs, allowing the Frame Relay cloud to work exactly like a LAN.Status of virtual circuitsThis provides DLCI status. The status inquiries and messages are used as keepalives when there is no regular LMI traffic to send.But remember, LMI is not communication between your routers; it’s communication between your router and the nearest Frame Relay switch. So it’s entirely possible that the router on one end of a PVC is actively receiving LMI while the router on the other end of the PVC is not. And of course, PVCs won’t work with one end down. (I say this to clarify the local nature of LMI communications.)There are three different types of LMI message formats: Cisco, ANSI, and Q.933A. The dif-ferent kinds in use depend on both the type and configuration of the telco’s switching gear, so it’s imperative that you configure your router for the correct format, which should be provided by the telco.

Beginning with IOS version 11.2, the LMI type is autosensed. This enables the interface to determine the LMI type supported by the switch. If you’re not going to use the autosense feature, you’ll need to check with your Frame Relay provider to find out which type to use instead.On Cisco equipment, the default type is, surprise, Cisco, but you still might have to change to ANSI or Q.933A depending on what your service provider tells you. The three different LMI types are shown in the following router output:

RouterA(config-if)#frame-relay lmi-type ?  cisco  ansi

  q933a

10089.book  Page 803  Monday, July 23, 2007  3:17 PM




804Chapter14 Wide Area NetworksAs seen in the output, all three standard LMI signaling formats are supported. Here’s a description of each:CiscoLMI defined by the Gang of Four (default). The Local Management Interface (LMI) was developed in 1990 by Cisco Systems, StrataCom, Northern Telecom, and Digital Equip-ment Corporation and became known as the Gang-of-Four LMI, or Cisco LMI.ANSIAnnex D included with ANSI standard T1.617.ITU-T (Q.933A)Annex A included in the ITU-T standard and defined by using the Q.933a command keyword.Routers receive LMI information from the service provider’s Frame Relay switch on a frame-encapsulated interface and update the virtual circuit status to one of three different states:Active stateEverything is up, and routers can exchange information.Inactive stateThe router’s interface is up and working with a connection to the switching office, but the remote router isn’t up.Deleted stateNo LMI information is being received on the interface from the switch, which could be due to a mapping problem or a line failure.Frame Relay Congestion ControlRemember back to our talk about CIR? From that, it should be obvious that the lower your CIR is set, the greater the risk is that your data will become toast. This can be easily avoided if you have just one key piece of information—when and when not to transmit that huge burst! This begs the question, Is there any way for us to find out when our telco’s shared infrastructure is free and clear and when it’s crammed and jammed? Also, if there is a way to spy this out, how do you do it? Well, that’s exactly what I’m going to talk about next—how the Frame Relay switch notifies the DTE of congestion problems—and address those very important questions.Here are the three congestion bits and their meanings:Discard Eligibility (DE)As you know, when you burst (transmit packets beyond the CIR of a PVC), any packets exceeding the CIR are eligible to be discarded if the provider’s network is congested at the time. Because of this, the excessive bits are marked with a Discard Eligi-bility (DE) bit in the Frame Relay header. And if the provider’s network happens to be con-gested, the Frame Relay switch will discard the packets with the first DE bit set. So if your bandwidth is configured with a CIR of zero, the DE will always be on.Forward Explicit Congestion Notification (FECN)When the Frame Relay network recog-nizes congestion in the cloud, the switch will set the Forward Explicit Congestion Notification (FECN) bit to 1 in a Frame Relay packet header. This will indicate to the destination DTE that the path the frame just traversed is congested.Backward Explicit Congestion Notification (BECN)When the switch detects congestion in the Frame Relay network, it’ll set the Backward Explicit Congestion Notification (BECN) bit in a Frame Relay frame that’s destined for the source router. This notifies the router that con-gestion is ahead. But Cisco routers won’t take action on this congestion information unless you tell them to.

10089.book  Page 804  Monday, July 23, 2007  3:17 PM




Frame Relay805

To check into this further, search using “Frame Relay Traffic Shaping” on Cisco’s website.Troubleshooting Using Frame Relay Congestion ControlNow let’s say all your users are whining about the fact that their Frame Relay connection to the corporate site is super slow. Because you strongly suspect that the link is overloaded, you verify the Frame Relay congestion control information with the show frame-relay pvc com-mand and get this:

RouterA#sh frame-relay pvcPVC Statistics for interface Serial0/0 (Frame Relay DTE)              Active     Inactive      Deleted       Static  Local          1            0            0            0  Switched       0            0            0            0  Unused         0            0            0            0DLCI = 100, DLCI USAGE = LOCAL, PVC STATUS = ACTIVE, INTERFACE = Serial0/0  input pkts 1300          output pkts 1270       in bytes 21212000  out bytes 21802000       dropped pkts 4         in pkts dropped 147  out pkts dropped 0       out bytes dropped 0      in FECN pkts 147   in BECN pkts 192        out FECN pkts 147  out BECN pkts 259        in DE pkts 0             out DE pkts 214  out bcast pkts 0         out bcast bytes 0  pvc create time 00:00:06, last time pvc status changed 00:00:06

Pod1R1#What you want to look for is the in BECN pkts 192 output because this is what’s telling the local router that traffic sent to the corporate site is experiencing congestion. BECN means that the path that a frame took to “return” to you is congested.Frame Relay Implementation and MonitoringAs I’ve said, there are a ton of Frame Relay commands and configuration options, but I’m going to zero in on the ones you really need to know when studying for the CCNA exam objectives. I’m going to start with one of the simplest configuration options—two routers with a single PVC between them. Next, I’ll show you a more complex configuration using subinterfaces, and demonstrate some of the monitoring commands available to verify the configuration.

10089.book  Page 805  Monday, July 23, 2007  3:17 PM




806Chapter14 Wide Area NetworksSingle InterfaceLet’s get started by looking at a simple example. Say that we just want to connect two routers with a single PVC. Here’s how that configuration would look:

RouterA#config tEnter configuration commands, one per line.  End with CNTL/Z.RouterA(config)#int s0/0RouterA(config-if)#encapsulation frame-relayRouterA(config-if)#ip address 172.16.20.1 255.255.255.0RouterA(config-if)#frame-relay lmi-type ansiRouterA(config-if)#frame-relay interface-dlci 101RouterA(config-if)#^Z

RouterA#The first step is to specify the encapsulation as Frame Relay. Notice that since I didn’t specify a particular encapsulation type—either Cisco or IETF—the Cisco default type was used. If the other router were non-Cisco, I would’ve specified IETF. Next, I assigned an IP address to the interface, then specified the LMI type of ANSI (the default being Cisco) based on information provided by the telecommunications provider. Finally, I added the DLCI of 101, which indicates the PVC we want to use (again, given to me by my ISP) and assumes there’s only one PVC on this physical interface.That’s all there is to it—if both sides are configured correctly, the circuit will come up.

Check out Hands-on Lab 14.3 for a complete example of this type of configu-ration, including instructions on creating your own Frame Relay switch from a router.SubinterfacesYou probably know by now that we can have multiple virtual circuits on a single serial interface and yet treat each as a separate interface—I did mention this earlier. We can make this happen by creating subinterfaces. Think of a subinterface as a logical interface defined by the IOS software. Several subinterfaces will share a single hardware interface, yet for configuration purposes they operate as if they were separate physical interfaces, something known as multiplexing.To configure a router in a Frame Relay network so it will avoid split horizon issues by not permitting routing updates, just configure a separate subinterface for each PVC, with a unique DLCI and subnet assigned to the subinterface.You define subinterfaces using a command like int s0.subinterface number. First, you have to set the encapsulation on the physical serial interface, and then you can define the sub-interfaces—generally one subinterface per PVC. Here’s an example:

RouterA(config)#int s0RouterA(config-if)#encapsulation frame-relay

10089.book  Page 806  Monday, July 23, 2007  3:17 PM




Frame Relay807RouterA(config-if)#int s0.?  <0-4294967295>  Serial interface numberRouterA(config-if)#int s0.16 ?  multipoint      Treat as a multipoint link  point-to-point  Treat as a point-to-point link

RouterA(config-if)#int s0.16 point-to-point

Make sure that you don’t have an IP address under the physical interface if you have configured subinterfaces!You can define a serious amount of subinterfaces on any given physical interface, but keep in mind that there are only about a thousand available DLCIs. In the preceding example, I chose to use subinterface 16 because that represents the DLCI number assigned to that PVC by the carrier. There are two types of subinterfaces:Point-to-pointUsed when a single virtual circuit connects one router to another. Each point-to-point subinterface requires its own subnet.

A point-to-point subinterface maps a single IP subnet per DLCI and addresses and resolves NBMA split horizon issues.MultipointThis is when the router is the center of a star of virtual circuits that are using a single subnet for all routers’ serial interfaces connected to the frame switch. You’ll usually find this implemented with the hub router in this mode and the spoke routers in physical interface (always point-to-point) or point-to-point subinterface mode.Next, I’ll show you an example of a production router running multiple subinterfaces. In the following output, notice that the subinterface number matches the DLCI number—not a requirement, but it majorly helps you administer the interfaces:

interface Serial0 no ip address (notice there is no IP address on the physical interface!) no ip directed-broadcast encapsulation frame-relay!interface Serial0.102 point-to-point ip address 10.1.12.1 255.255.255.0 no ip directed-broadcastframe-relay interface-dlci 102!interface Serial0.103 point-to-point

10089.book  Page 807  Monday, July 23, 2007  3:17 PM




808Chapter14 Wide Area Networks ip address 10.1.13.1 255.255.255.0 no ip directed-broadcastframe-relay interface-dlci 103!interface Serial0.104 point-to-point ip address 10.1.14.1 255.255.255.0 no ip directed-broadcastframe-relay interface-dlci 104!interface Serial0.105 point-to-point ip address 10.1.15.1 255.255.255.0 no ip directed-broadcastframe-relay interface-dlci 105

!Notice that there’s no LMI type defined. This means that the routers are either running the Cisco default or they’re using autodetect (if running Cisco IOS version 11.2 or newer). I also want to point out that each interface maps to a single DLCI and is defined as a separate subnet. Remember—point-to-point subinterfaces solve split horizon issues as well.Monitoring Frame RelaySeveral commands are used frequently to check the status of your interfaces and PVCs once you have Frame Relay encapsulation set up and running. To list them, use the show frame ? command, as seen here:

RouterA>sho frame ?end-to-end     Frame-relay end-to-end VC informationfragment       show frame relay fragmentation informationip             show frame relay IP statisticslapf           show frame relay lapf status/statisticslmi            show frame relay lmi statisticsmap            Frame-Relay map tablepvc            show frame relay pvc statisticsqos-autosense  show frame relay qos-autosense informationroute          show frame relay routesvc            show frame relay SVC stufftraffic        Frame-Relay protocol statistics

vofr           Show frame-relay VoFR statisticsThe most common parameters that you view with the show frame-relay command are lmi, pvc, and map.Now, let’s take a look at the most frequently used commands and the information they provide.

10089.book  Page 808  Monday, July 23, 2007  3:17 PM




Frame Relay809The show frame-relay lmi CommandThe show frame-relay lmi command will give you the LMI traffic statistics exchanged between the local router and the Frame Relay switch. Here’s an example:

Router#sh frame lmiLMI Statistics for interface Serial0 (Frame Relay DTE)LMI TYPE = CISCO  Invalid Unnumbered info 0     Invalid Prot Disc 0  Invalid dummy Call Ref 0      Invalid Msg Type 0  Invalid Status Message 0      Invalid Lock Shift 0  Invalid Information ID 0      Invalid Report IE Len 0  Invalid Report Request 0      Invalid Keep IE Len 0  Num Status Enq. Sent 0        Num Status msgs Rcvd 0  Num Update Status Rcvd 0      Num Status Timeouts 0

Router#The router output from the show frame-relay lmi command shows you any LMI errors, plus the LMI type.The show frame pvc CommandThe show frame pvc command will present you with a list of all configured PVCs and DLCI numbers. It provides the status of each PVC connection and traffic statistics too. It will also give you the number of BECN and FECN packets received on the router per PVC.Here is an example:

RouterA#sho frame pvcPVC Statistics for interface Serial0 (Frame Relay DTE)DLCI = 16,DLCI USAGE = LOCAL,PVC STATUS =ACTIVE,INTERFACE = Serial0.1 input pkts 50977876    output pkts 41822892  in bytes 3137403144 out bytes 3408047602   dropped pkts 5  in FECN pkts 0 in BECN pkts 0      out FECN pkts 0     out BECN pkts 0 in DE pkts 9393     out DE pkts 0 pvc create time 7w3d, last time pvc status changed 7w3dDLCI = 18,DLCI USAGE =LOCAL,PVC STATUS =ACTIVE,INTERFACE = Serial0.3

10089.book  Page 809  Monday, July 23, 2007  3:17 PM




810Chapter14 Wide Area Networks input pkts 30572401   output pkts 31139837  in bytes 1797291100 out bytes 3227181474   dropped pkts 5  in FECN pkts 0 in BECN pkts 0      out FECN pkts 0     out BECN pkts 0 in DE pkts 28      out DE pkts 0

 pvc create time 7w3d, last time pvc status changed 7w3dIf you only want to see information about PVC 16, you can type the command show frame-relay pvc 16.The show interface CommandYou can use the show interface command to check for LMI traffic. The show interface command displays information about the encapsulation, as well as layer 2 and layer 3 infor-mation. It also displays line, protocol, DLCI, and LMI information. Check it out:

RouterA#sho int s0Serial0 is up, line protocol is up Hardware is HD64570 MTU 1500 bytes, BW 1544 Kbit, DLY 20000 usec, rely  255/255, load 2/255 Encapsulation FRAME-RELAY, loopback not set, keepalive  set (10 sec) LMI enq sent 451751,LMI stat recvd 451750,LMI upd recvd  164,DTE LMI up LMI enq recvd 0, LMI stat sent 0, LMI upd sent 0 LMI DLCI 1023 LMI type is CISCO frame relay DTE Broadcast queue 0/64, broadcasts sent/dropped 0/0,

  interface broadcasts 839294The LMI DLCI above is used to define the type of LMI being used. If it happens to be 1023, it’s the default LMI type of Cisco. If LMI DLCI is zero, then it’s the ANSI LMI type (Q.933A uses 0 as well). If LMI DLCI is anything other than 0 or 1023, it’s a 911—call your provider; they’ve got major issues!The show frame map CommandThe show frame map command displays the Network layer–to–DLCI mappings. Here’s how that looks:

RouterB#show frame mapSerial0 (up): ipx 20.0007.7842.3575 dlci 16(0x10,0x400),              dynamic, broadcast,, status defined, activeSerial0 (up): ip 172.16.20.1 dlci 16(0x10,0x400),              dynamic, broadcast,, status defined, active

10089.book  Page 810  Monday, July 23, 2007  3:17 PM




Frame Relay811Serial1 (up): ipx 40.0007.7842.153a dlci 17(0x11,0x410),              dynamic, broadcast,, status defined, activeSerial1 (up): ip 172.16.40.2 dlci 17(0x11,0x410),

              dynamic, broadcast,, status defined, activeNotice that the serial interfaces have two mappings—one for IP and one for IPX. Also impor-tant is that the Network layer addresses were resolved with the dynamic protocol Inverse ARP (IARP). After the DLCI number is listed, you can see some numbers in parentheses. The first one is 0x10, which is the hex equivalent for the DLCI number 16, used on serial 0. And the 0x11 is the hex for DLCI 17 used on serial 1. The second numbers, 0x400 and 0x410, are the DLCI numbers configured in the Frame Relay frame. They’re different because of the way the bits are spread out in the frame.The debug frame lmi CommandThe debug frame lmi command will show output on the router consoles by default (as with any debug command). The information this command gives you will enable you to verify and troubleshoot the Frame Relay connection by helping you determine whether the router and switch are exchanging the correct LMI information. Here’s an example:

Router#debug frame-relay lmiSerial3/1(in): Status, myseq 214RT IE 1, length 1, type 0KA IE 3, length 2, yourseq 214, myseq 214PVC IE 0x7 , length 0x6 , dlci 130, status 0x2 , bw 0Serial3/1(out): StEnq, myseq 215, yourseen 214, DTE updatagramstart = 0x1959DF4, datagramsize = 13FR encap = 0xFCF1030900 75 01 01 01 03 02 D7 D6Serial3/1(in): Status, myseq 215RT IE 1, length 1, type 1KA IE 3, length 2, yourseq 215, myseq 215Serial3/1(out): StEnq, myseq 216, yourseen 215, DTE updatagramstart = 0x1959DF4, datagramsize = 13FR encap = 0xFCF10309

00 75 01 01 01 03 02 D8 D7Troubleshooting Frame Relay NetworksTroubleshooting Frame Relay networks isn’t any harder than troubleshooting any other type of network as long as you know what to look for, which is what I’m going to cover now. We’ll go over some basic problems that commonly occur in Frame Relay configuration and how to solve them.

10089.book  Page 811  Monday, July 23, 2007  3:17 PM




812Chapter14 Wide Area NetworksFirst on the list are serial encapsulation problems. As you learned recently, there are two Frame Relay encapsulations: Cisco and IETF. Cisco is the default, and it means that you have a Cisco router on each end of the Frame Relay network. If you don’t have a Cisco router on the remote end of your Frame Relay network, then you need to run the IETF encapsulation as shown here:

RouterA(config)#int s0RouterA(config-if)#encapsulation frame-relay ?  ietf  Use RFC1490 encapsulation  <cr>

RouterA(config-if)#encapsulation frame-relay ietfOnce you verify that you’re using the correct encapsulation, you then need to check out your Frame Relay mappings. For example, take a look at Figure 14.17.FIGURE14.17Frame Relay mappingsSo why can’t RouterA talk to RouterB across the Frame Relay network? To find that out, take a close look at the frame-relay map statement. See the problem now? You cannot use a remote DLCI to communicate to the Frame Relay switch; you must use your DLCI number! The mapping should have included DLCI 100 instead of DLCI 200.Now that you know how to ensure that you have the correct Frame Relay encapsulation, and that DLCIs are only locally significant, let’s look into some routing protocol problems typically associated with Frame Relay. See if you can find a problem with the two configura-tions in Figure 14.18.FIGURE14.18Frame Relay routing problems

Frame Relay

RouterARouterBDLCI 100DLCI 200RouterA#show running-configinterface s0/0ip address 172.16.100.2 255.255.0.0encapsulation frame-relayframe-relay map ip 172.16.100.1 200 broadcast

Frame Relay

RouterARouterBDLCI 100DLCI 200RouterA#show running-configinterface s0/0ip address 172.16.100.2 255.255.0.0encapsulation frame-relayframe-relay map ip 172.16.100.1 100router rip network 172.16.0.0RouterB#show running-configinterface s0/0ip address 172.16.100.1 255.255.0.0encapsulation frame-relayframe-relay map ip 172.16.100.2 200router rip network 172.16.0.0

10089.book  Page 812  Monday, July 23, 2007  3:17 PM




Using SDM for WAN Connections813Hmmmm, well, the configs look pretty good. Actually, they look great, so what’s the prob-lem? Well, remember that Frame Relay is a non-broadcast multi-access (NBMA) network by default, meaning that it doesn’t send any broadcasts across the PVC. So, because the mapping statements do not have the broadcast argument at the end of the line, broadcasts, like RIP updates, won’t be sent across the PVC.Okay, now let’s use the SDM to configure our serial WAN connections—should be pretty simple!Using SDM for WAN ConnectionsI’m going to show you how to set up serial WAN connections using the SDM. My only options are HDLC (the default), PPP, and Frame Relay. Your router may have different options depending on the interfaces installed on it.Since HDLC is already running, and there’s not much to configure or show you with HDLC, I am going to configure PPP between two routers, use authentication, and then show you how to configure Frame Relay on a router interface with SDM.Configuring PPP with Authentication Using SDMFirst, I’m going to configure the serial WAN link between the Corp router and the R3 router using PPP with Authentication. The first thing I need to do is delete the interface from Inter-faces and Connections Tasks, then click on the Edit Interface Connection tab, and then click Delete. If I don’t do this, the interface won’t show up as available to configure through SDM. I could easily do it from the CLI instead, but that’s not where we’re going here.Okay—once I deleted the interface configuration, I clicked Create New Connection on the Create Connection tab.

10089.book  Page 813  Monday, July 23, 2007  3:17 PM




814Chapter14 Wide Area NetworksOnce I clicked Create New Connection, I received the first screen of the Serial WAN Con-figuration Wizard.Then, I just clicked Next, and the screen I got showed that HDLC is ready to rock. I would just have to click Next again to make it happen.

10089.book  Page 814  Monday, July 23, 2007  3:17 PM




Using SDM for WAN Connections815But instead, I clicked Point-to-Point Protocol, and then clicked Next, which brought me to the IP Address screen.I added the static IP address I wanted to add, and then I clicked Next and got the Authen-tication screen.

10089.book  Page 815  Monday, July 23, 2007  3:17 PM




816Chapter14 Wide Area NetworksYou don’t have to enter any authentication information here, but I went ahead and entered the information anyway. The Username field is for the name of the remote router (R3), or whatever information your ISP provided you with—same with the password. (Think back to the PPP configuration I showed you with the CLI earlier.) I then clicked Next. A screen appeared showing a summary of the configuration. I just clicked Finish.From the Interfaces and Connections task, on the Edit Interface/Connection tab, we can now see that my Serial0/2/0 is configured on the Corp router with PPP and CHAP authentication.

10089.book  Page 816  Monday, July 23, 2007  3:17 PM




Using SDM for WAN Connections817So now I’m going to go to the R3 router and implement the same configuration I just dem-onstrated on the Corp router. I’ll use Corp as the username and assign the same password (just as I showed you in the PPP CLI configuration earlier).Here’s the CLI output from the Corp router after both routers have been configured:

!interface Serial0/2/0 description Connection to R3$FW_OUTSIDE$ ip address 10.1.5.1 255.255.255.0 ip verify unicast reverse-path ip virtual-reassembly encapsulation ppp clock rate 2000000 ppp authentication chap callin ppp chap hostname R3 ppp chap password 0 cisco

!       You’d think I’d be done, right? After all, the links are configured and we’re up and running. But we’re not. We would be if we were connected to an ISP and don’t actually have a point-to-point connection like a T1 (which is what I am simulating). So in reality, the ISP would then provide the authentication commands. The thing is, we have a dedicated point-to-point serial connection, and the SDM PPP with authentication doesn’t work without some help from the CLI. Go figure. (I told you it was easier with the CLI!)Here’s how I know things aren’t working even though both routers were very easily configured:

Corp#sh int s0/2/0Serial0/2/0 is up, line protocol is down  Hardware is GT96K Serial  Description: Connection to R3$FW_OUTSIDE$  Internet address is 10.1.5.1/24  MTU 1500 bytes, BW 1544 Kbit, DLY 20000 usec,     reliability 255/255, txload 1/255, rxload 1/255  Encapsulation PPP, LCP Listen

[output cut]The first item listed (as you hopefully know) is carrier detect at the Physical layer, but the “line protocol is down” at the Data Link layer. This means we’re not getting keepalives from the R3 router. But why? I’m pretty sure I configured it correctly and it definitely wasn’t all that difficult! Let’s take a look at the authentication in action and see what we can find out:

 Corp#debug ppp auth*May 15 18:46:12.039: Se0/2/0 PPP: Authorization required*May 15 18:46:12.039: Se0/2/0 CHAP: O CHALLENGE id 33 len 23 from "R3"*May 15 18:46:12.039: Se0/2/0 CHAP: I CHALLENGE id 33 len 25 from "Corp"*May 15 18:46:12.043: Se0/2/0 CHAP: I RESPONSE id 33 len 25 from "Corp"

10089.book  Page 817  Monday, July 23, 2007  3:17 PM




818Chapter14 Wide Area Networks*May 15 18:46:12.043: Se0/2/0 CHAP: Using hostname from interface CHAP*May 15 18:46:12.043: Se0/2/0 CHAP: Using password from interface CHAP*May 15 18:46:12.043: Se0/2/0 CHAP: O RESPONSE id 33 len 23 from "R3"*May 15 18:46:12.043: Se0/2/0 PPP: Sent CHAP LOGIN Request*May 15 18:46:12.043: Se0/2/0 PPP: Received LOGIN Response FAIL*May 15 18:46:12.043: Se0/2/0 CHAP: O FAILURE id 33 len 25 msg is "Authentication failed"

Corp#un allActually, the authentication commands look like they are trying to work, but things fail at the end. This is where the CLI comes in if you are not connecting to a service provider that is config-uring your authentication. I now need to go to each router and add the username command. This is pretty simple, but I am surprised that the SDM didn’t at least prompt me for this. Okay, here it is:

Corp(config)#username R3 password ciscoAnd now for the R3 router:

R3(config)#username Corp password ciscoNow, finally, we should be up and running. Let’s check it out:

Corp#debug ppp authPPP authentication debugging is on*May 15 16:53:34.479: Se0/2/0 PPP: Authorization required*May 15 16:53:34.479: Se0/2/0 CHAP: O CHALLENGE id 1 len 25 from "Corp"*May 15 16:53:34.483: Se0/2/0 CHAP: I RESPONSE id 1 len 23 from "R3"*May 15 16:53:34.483: Se0/2/0 PPP: Sent CHAP LOGIN Request*May 15 16:53:34.483: Se0/2/0 PPP: Received LOGIN Response PASS*May 15 16:53:34.487: Se0/2/0 PPP: Sent LCP AUTHOR Request*May 15 16:53:34.487: Se0/2/0 PPP: Sent IPCP AUTHOR Request*May 15 16:53:34.487: Se0/2/0 LCP: Received AAA AUTHOR Response PASS*May 15 16:53:34.487: Se0/2/0 IPCP: Received AAA AUTHOR Response PASS*May 15 16:53:34.487: Se0/2/0 CHAP: O SUCCESS id 1 len 4*May 15 16:53:34.487: Se0/2/0 PPP: Sent CDPCP AUTHOR Request*May 15 16:53:34.491: Se0/2/0 PPP: Sent IPCP AUTHOR Request

*May 15 16:53:34.491: Se0/2/0 CDPCP: Received AAA AUTHOR Response PASSUnderstand that the SDM assumes that you are connecting to an ISP, and that the ISP will provide you with the authentication username and password. Ridiculous, yes, but absolutely true nonetheless!Configuring PPPoE with SDMTo configure PPPoE with SDM, we’ll first need a router connecting to a DSL modem, and then we’ll need to configure the interface as a client just as we did earlier. I’ll make it painless by using SDM on my 871W router.

10089.book  Page 818  Monday, July 23, 2007  3:17 PM




Using SDM for WAN Connections819After connecting to the router with SDM, I deleted the FastEthernet interface from SDM. Then, under Interfaces and Connections in the Tasks menu, I chose Create Connection.From here, I clicked Ethernet (PPPoE or Unencapsulated Routing), then clicked Create New Connection, and got the welcome screen of the Ethernet WAN Configuration Wizard.

10089.book  Page 819  Monday, July 23, 2007  3:17 PM




820Chapter14 Wide Area NetworksFrom here, I just clicked Next. From the Encapsulation screen, I checked Enable PPPoE Encapsulation and clicked Next.I was then asked for my IP information. I just selected Easy IP (IP Negotiated, meaning DHCP), and clicked Next.

10089.book  Page 820  Monday, July 23, 2007  3:17 PM




Using SDM for WAN Connections821The next screen asked for my authentication information.Normally, I would have to be provided with this information from my ISP to make this work. Here, I filled in some basic information and clicked Next.The next screen asked me for routing information.

10089.book  Page 821  Monday, July 23, 2007  3:17 PM




822Chapter14 Wide Area NetworksI choose to make this my gateway of last resort by checking Default Static Route and select-ing Use this Interface as Forwarding Interface. I could have added the next-hop address, but it’s possible it could change since I’m using DHCP. After clicking Next, I received a summary of my configuration.I clicked Next to have the configuration uploaded to my router.Now, let’s move on to configure Frame Relay with SDM.

For more information on configuring PPPoE with SDM, please use the SDM demo. You can install SDM on your computer and run a full demo program as described in Chapter 4, Hands-on Lab 4.6.Configuring Frame Relay with SDMNow that our serial connection between the Corp and R3 routers is up and running with PPP and you know how to configure PPPoE, I want to show you how easy it is to set up Frame Relay using SDM. After deleting the interface configuration through the SDM (by the way, I’m starting to get annoyed at this deleting the interface thing before I can reconfigure them!), I get to create a new serial connection.

10089.book  Page 822  Monday, July 23, 2007  3:17 PM




Using SDM for WAN Connections823I’ll open the Interface Wizard and choose Frame Relay for the Serial0/2/0 interface of the Corp router.I then got the same screen I got when I configured the PPP interface. I set the static IP address and clicked Next.

10089.book  Page 823  Monday, July 23, 2007  3:17 PM




824Chapter14 Wide Area NetworksIn the real world, this is where I would add the LMI and DLCI information given to me from my provider.Notice that check box at the bottom left that asks if you want to use IETF encapsulation? Remember, that means you don’t have a Cisco router on the other side of the Frame Relay cloud. After clicking Next, I got the Summary screen.

10089.book  Page 824  Monday, July 23, 2007  3:17 PM




Virtual Private Networks825This one shows a summary of my configuration. I clicked Finish to upload it to my router. Let’s take a look at the CLI to find out what, exactly, was uploaded to my router. Notice in the following output how the IP address from the physical interface has been moved to the subinterface the SDM created—nice!

!interface Serial0/2/0 description Connection to R3$FW_OUTSIDE$ no ip address ip verify unicast reverse-path ip virtual-reassembly encapsulation frame-relay clock rate 2000000 frame-relay lmi-type ansi!       interface Serial0/2/0.1 point-to-point ip address 10.1.5.1 255.255.255.0 frame-relay interface-dlci 17 CISCO 

!   If I had this router connected to an ISP and the ISP were correctly configured (that can truly be a big if ), then my PVC should have come up. Just remember—I used the LMI type Cisco, meaning that my ISP would have a Cisco Frame Relay switch. This is extremely unlikely. Most of the time you would use ANSI LMI.All right, so now let’s create a virtual private network between our Corp router and the R3 router.Virtual Private NetworksI’d be pretty willing to bet you’ve heard the term VPN more than once before. Maybe you even know what one is, but just in case, a virtual private network (VPN) allows the creation of pri-vate networks across the Internet, enabling privacy and tunneling of non-TCP/IP protocols. VPNs are used daily to give remote users and disjointed networks connectivity over a public medium like the Internet instead of using more expensive permanent means.Types of VPNs are named based upon the role they play in a business. There are three different categories of VPNs:Remote access VPNsRemote access VPNs allow remote users like telecommuters to securely access the corporate network wherever and whenever they need to.Site-to-site VPNsSite-to-site VPNs, or intranet VPNs, allow a company to connect its remote sites to the corporate backbone securely over a public medium like the Internet instead of requiring more expensive WAN connections like Frame Relay.

10089.book  Page 825  Monday, July 23, 2007  3:17 PM




826Chapter14 Wide Area NetworksExtranet VPNsExtranet VPNs allow an organization’s suppliers, partners, and customers to be connected to the corporate network in a limited way for business-to-business (B2B) communications.Now you’re interested, huh! And since VPNs are inexpensive and secure, I’m guessing you’re really jonesing to find out how VPNs are created right?. Well, there’s more than one way to bring a VPN into being. The first approach uses IPSec to create authentication and encryption services between endpoints on an IP network. The second way is via tunneling protocols, allowing you to establish a tunnel between endpoints on a network. And under-stand that the tunnel itself is a means for data or protocols to be encapsulated inside another protocol—pretty clean! I’m going to go over the first, IPSec way in a minute, but first, I really want to describe four of the most common tunneling protocols in use:Layer 2 Forwarding (L2F)Layer 2 Forwarding (L2F) is a Cisco-proprietary tunneling protocol, and it was their first tunneling protocol created for virtual private dial-up networks (VPDNs). VPDN allows a device to use a dial-up connection to create a secure connection to a corporate network. L2F was later replaced by L2TP, which is backward compatible with L2F.Point-to-Point Tunneling Protocol (PPTP)Point-to-Point Tunneling Protocol (PPTP) was created by Microsoft to allow the secure transfer of data from remote networks to the corporate network.Layer 2 Tunneling Protocol (L2TP)Layer 2 Tunneling Protocol (L2TP) was created by Cisco and Microsoft to replace L2F and PPTP. L2TP merged the capabilities of both L2F and PPTP into one tunneling protocol.Generic Routing Encapsulation (GRE)Generic Routing Encapsulation (GRE) is another Cisco-proprietary tunneling protocol. It forms virtual point-to-point links, allowing for a variety of protocols to be encapsulated in IP tunnels.Okay—now that you’re clear on both exactly what a VPN is and the various types of VPNs available, it’s time to dive into IPSec.Introduction to Cisco IOS IPSecSimply put, IPSec is an industry-wide standard suite of protocols and algorithms that allows for secure data transmission over an IP-based network that functions at the layer 3 network layer of the OSI model.Did you notice I said “IP-based network”? That’s really important because by itself, IPSec can’t be used to encrypt non-IP traffic. This means that if you run into a situation where you have to encrypt non-IP traffic, you’ll need to create a GRE tunnel for it and then use IPSec to encrypt that tunnel!IPSec TransformsAn IPSec transform specifies a single security protocol with its corresponding security algo-rithm; without these transforms, IPSec wouldn’t be able to give us its glory. It’s important to 

10089.book  Page 826  Monday, July 23, 2007  3:17 PM




Virtual Private Networks827be familiar with these technologies, so let me take a second to define the security protocols and briefly introduce the supporting encryption and hashing algorithms that IPSec relies upon.Security ProtocolsThe two primary security protocols used by IPSec are Authentication Header (AH) and Encap-sulating Security Payload (ESP).Authentication Header (AH)The AH protocol provides authentication for the data and the IP header of a packet using a one-way hash for packet authentication. It works like this: The sender generates a one-way hash; then the receiver generates the same one-way hash. If the packet has changed in any way, it won’t be authenticated and will be dropped. So basically, IPSec relies upon AH to guarantee authenticity. AH checks the entire packet, but it doesn’t offer any encryption services.This is unlike ESP, which only provides an integrity check on the data of a packet.Encapsulating Security Payload (ESP)It won’t tell you when or how the NASDAQ’s gonna bounce up and down like a superball, but ESP will provide confidentiality, data origin authentication, connectionless integrity, anti-replay service, and limited traffic-flow confidentiality by defeating traffic flow analysis. Which is almost as good! Anyway, there are four components of ESP:ConfidentialityConfidentiality is provided through the use of symmetric encryption algorithms like DES or 3DES. Confidentiality can be selected separately from all other services, but the confidentiality selected must be the same on all endpoints of your VPN.Data origin authentication and connectionless integrityData origin authentication and connectionless integrity are joint services offered as an option in conjunction with the likewise optional confidentiality.Anti-replay serviceYou can only use the anti-replay service if data origin authentication is selected. Anti-replay election is based upon the receiver, meaning the service is effective only if the receiver checks the sequence number. In case you were wondering, a replay attack is when a hacker nicks a copy of an authenticated packet and later transmits it to the intended destination. When the duplicate, authenticated IP packet gets to the destination, it can disrupt services and other ugly stuff. The Sequence Number field is designed to foil this type of attack.Traffic flowFor traffic flow confidentiality to work, you have to have tunnel mode selected. And it’s most effective if it’s implemented at a security gateway where tons of traffic amasses—a situation that can mask the true source-destination patterns of bad guys trying to breach your network’s security.

For more detailed information above and beyond the scope of the CCNA objectives, please see www.lammle.com.

10089.book  Page 827  Monday, July 23, 2007  3:17 PM




828Chapter14 Wide Area NetworksConfiguring VPNs/IPSec Using the SDMThere are a lot of different ways to configure VPNs on your router. What I am going to do here is add a straight VPN connection between the Corp and R3 routers.After clicking on VPN in the Tasks bar, I clicked Site-to-Site VPN and received the Create Site to Site VPN tab.I selected Create a Site to Site VPN and then clicked Launch the Selected Task to get the Site to Site VPN screen.

10089.book  Page 828  Monday, July 23, 2007  3:17 PM




Virtual Private Networks829I clicked View Defaults and took a peek at what the router was going to configure:After clicking Close, I clicked Next to receive the VPN Connection Information screen:

10089.book  Page 829  Monday, July 23, 2007  3:17 PM




830Chapter14 Wide Area NetworksI added the static IP address of my peer router (R3), added a pre-shared key, chose my source address of the Corp router, and the destination address, which happens to be the same address as my peer router (R3). I then clicked Next.I received a summary of the VPN configuration running IPSec. Whew…man, before SDM, I always had to configure VPNs with IPSec by default. This is so easy! I clicked Finish.OK, the best part is coming up! I received this next screen from the SDM.It’s asking if it’s OK to test the VPN connection. I clicked Yes, of course. SDM then returned another screen asking me for the source and destination addresses and also asking if I wanted to generate traffic or let SDM generate it. I chose to let SDM.

10089.book  Page 830  Monday, July 23, 2007  3:17 PM




Virtual Private Networks831I receive a response from SDM telling me there was a problem with the link and asked if SDM could fix it for me….umm…okay, sure! Once I did that I received this screen.So, it found a problem and fixed it for me. It’s like having my very own advanced tech sup-port elf—sweet! Man, SDM would be worth the money just for this feature alone! Let’s take a look at the Corp router’s running-config and see what was uploaded to the router’s config:

!crypto isakmp policy 1 encr 3des authentication pre-share group 2crypto isakmp key cisco address 10.1.5.2!!crypto ipsec transform-set ESP-3DES-SHA esp-3des esp-sha-hmac!crypto map SDM_CMAP_1 1 ipsec-isakmp description Tunnel to10.1.5.2 set peer 10.1.5.2 set transform-set ESP-3DES-SHA match address 104!interface Serial0/2/0[output cut] crypto map SDM_CMAP_1!access-list 104 remark SDM_ACL Category=4access-list 104 remark IPSec Ruleaccess-list 104 permit ip 10.1.5.0 0.0.0.255 10.1.5.0 0.0.0.255

!Yikes! Is this something you think you’d want to try and get working on your own without the help of our tech support elf-genius? The answer is a resounding no! I’ve done it for years, and it is no day at the beach.Since we can now do the hardest of hardest configurations easily using SDM elves, why stop now? Let’s add some QoS!

10089.book  Page 831  Monday, July 23, 2007  3:17 PM




832Chapter14 Wide Area NetworksConfiguring Quality of Service (QoS) across Our VPN TunnelNow we all get that the reason we have customer networks is to meet the needs of both applications and users effectively—well, hopefully effectively! And we also know that the blend of the Internet’s huge growth, corporate intranets that multiply like rabbits, legions of new applications that voraciously devour bandwidth, and the combined load of voice, data, plus video traffic traveling over our IP infrastructures is totally maxing them out. We basi-cally experience all this when our networks fail to perform well—as poor performance and unprecedented unreliability.This leads us to QoS. Believe it or not, deploying QoS is actually more secure, cost effective, even faster in today’s networking environment! This is why I’m going to show you how to con-figure QoS on our VPN serial link—a great place to configure QoS in a production network, by the way.This time, I’m going to start on the R3 router. After connecting with SDM, I’ll click the Configure button and then click Quality of Service under the Tasks bar.

10089.book  Page 832  Monday, July 23, 2007  3:17 PM




Virtual Private Networks833From here, I’ll click Launch QoS Wizard to get to the first screen of the wizard.After clicking Next, I’ll choose the interface that I want to use as my source or outgoing port and then click Next.

10089.book  Page 833  Monday, July 23, 2007  3:17 PM




834Chapter14 Wide Area NetworksFrom the QoS Policy Generation screen, I can create bandwidth allocation for various types of data. By default, SDM creates three QoS classes for a typical environment. You can click View Details to get more information. I clicked Next.If the SDM gets its way, it will go ahead and enable the NBAR protocol discovery feature for this interface.Network Based Application Recognition (NBAR) is very cool because it allows you to both accurately identify and classify your mission-critical and optimization applications—for example, ERP. And after you’ve got these applications classified, you then get to guar-antee them a minimum bandwidth to use. Basically they’re policy routed, and if these mission-critical applications are classified, they can be guaranteed a minimum amount of bandwidth, policy routed, and tagged to receive special treatment.

10089.book  Page 834  Monday, July 23, 2007  3:17 PM




Virtual Private Networks835I clicked Yes, and from the next screen, both the QoS class and value of real-time traffic and business-critical traffic are displayed.Click Close—the configuration’s summary is shown, but it’s super long, so I’m only show-ing you the top of the page.

10089.book  Page 835  Monday, July 23, 2007  3:17 PM




836Chapter14 Wide Area NetworksOnce I clicked Finish, it uploaded the configuration to the router. In fact, it uploaded a lot—way too much for me to copy into this book! SDM is a powerful tool, and as I’ve dem-onstrated, it can be used really effectively for some seriously tough configurations.SummaryIn this chapter, you learned the difference between the following WAN services: cable, DSL, HDLC, PPP, PPPoE, and Frame Relay. You also learned that you can use a VPN once any of those services are up and running.I have to tell you—you must understand High-Level Data-Link Control (HDLC) and how to verify with the show interface command that HDLC is enabled! You’ve been provided with some really important HDLC information, as well as how the Point-to-Point Protocol (PPP) is used if you need more features than HDLC offers or if you’re using two different brands of routers. You now know that this is because HDLC is proprietary and won’t work between two different vendors’ routers.When we went through the section on PPP, I discussed the various LCP options as well as the two types of authentication that can be used: PAP and CHAP.And we talked about Frame Relay and the two different encapsulation methods used with it in detail. We also discussed LMI options, Frame Relay maps, and subinterface configura-tions. In addition to the Frame Relay terms and features we covered, I demonstrated Frame Relay configuration and verification in depth.We finished up the chapter with how to use the SDM to configure a WAN link and then discussed VPNs and configured a VPN link with IPSec. We also touched upon using QoS on our WANs.Exam EssentialsRemember the default serial encapsulation on Cisco routers.Cisco routers use a propri-etary High-Level Data-Link Control (HDLC) encapsulation on all its serial links by default.Understand the different Frame Relay encapsulations.Cisco uses two different Frame Relay encapsulation methods on its routers. Cisco is the default and means that the router is connected to a Cisco Frame Relay switch; Internet Engineering Task Force (IETF) means that your router is connecting to anything but a Cisco Frame Relay switch.Remember what the CIR is in Frame Relay.The CIR is the average rate, in bits per second, at which the Frame Relay switch agrees to transfer data.Remember the commands for verifying Frame Relay.The show frame-relay lmi command will give you the LMI traffic statistics exchanged between the local router and the Frame Relay switch. The show frame pvc command will list all configured PVCs and DLCI numbers.Remember the PPP Data Link layer protocols.The three Data Link layer protocols are Network Control Protocol (NCP), which defines the Network layer protocols; Link Control 

10089.book  Page 836  Monday, July 23, 2007  3:17 PM




Hands-on Labs837Protocol (LCP), a method of establishing, configuring, maintaining, and terminating the point-to-point connection; and High-Level Data-Link Control (HDLC), the MAC layer protocol that encapsulates the packets.Remember the various type of serial WAN connections.The serial WAN connections that are most widely used are HDLC, PPP, and Frame RelayUnderstand the term virtual private network.You need to understand why and how to use a VPN between two sites.Written Lab 14Write the answers to the following WAN questions:1.Write the command to see the encapsulation method on serial 0 of a Cisco router.2.Write the commands to configure s0 to PPP encapsulation.3.Write the commands to configure a username of todd and password of cisco that is used on a Cisco router for PPP authentication.4.Write the commands to enable CHAP authentication on a Cisco serial interface. (Assume PPP is the encapsulation type.)5.Write the commands to configure the DLCI numbers for two serial interfaces, 0 and 1. Use 16 for s0 and 17 for s1.6.Write the commands to configure a remote office using a point-to-point subinterface. Use DLCI 16 and IP address 172.16.60.1/24.7.What protocol would you use if you were running xDSL and needed authentication?8.What are the three protocols specified in PPP?9.To provide security in your VPN tunnel, what protocol suite would you use?10.What are the typical three different categories of VPNs?(The answers to Written Lab 14 can be found following the answers to the review questions for this chapter.)Hands-on LabsIn this section, you will configure Cisco routers in three different WAN labs using the figure supplied in each lab. (These labs are included for use with real Cisco routers.)Lab 14.1: Configuring PPP Encapsulation and AuthenticationLab 14.2: Configuring and Monitoring HDLCLab 14.3: Configuring Frame Relay and Subinterfaces

10089.book  Page 837  Monday, July 23, 2007  3:17 PM




838Chapter14 Wide Area NetworksHands-on Lab 14.1: Configuring PPP Encapsulation and AuthenticationBy default, Cisco routers use High-Level Data-Link Control (HDLC) as a point-to-point encapsulation method on serial links. If you are connecting to non-Cisco equipment, then you can use the PPP encapsulation method to communicate.The lab you will configure is shown in the following diagram.1.Type sh int s0 on Routers A and B to see the encapsulation method.2.Make sure that each router has the hostname assigned:RouterA#config tRouterA(config)#hostname RouterARouterB#config t

RouterB(config)#hostname RouterB3.To change the default HDLC encapsulation method to PPP on both routers, use the encapsulation command at interface configuration. Both ends of the link must run the same encapsulation method.RouterA#Config tRouterA(config)#int s0

RouterA(config-if)#Encap ppp4.Now go to Router B and set serial 0 to PPP encapsulation.RouterB#config tRouterB(config)#int s0

RouterB(config-if)#encap ppp5.Verify the configuration by typing sh int s0 on both routers.6.Notice the IPCP, IPXCP, and CDPCP. This is the information used to transmit the upper-layer (Network layer) information across the HDLC at the MAC sublayer.7.Define a username and password on each router. Notice that the username is the name of the remote router. Also, the password must be the same.RouterA#config tRouterA(config)#username RouterB password toddRouterB#config t

RouterB(config)#username RouterA password todd

E0E0S0S0

RouterARouterB

10089.book  Page 838  Monday, July 23, 2007  3:17 PM




Hands-on Labs8398.Enable CHAP or PAP authentication on each interface.RouterA(config)#int s0RouterA(config-if)#ppp authentication chapRouterB(config)#int s0

RouterB(config-if)#ppp authentication chap9.Verify the PPP configuration on each router by using these two commands:sh int s0

debug ppp authenticationHands-on Lab 14.2: Configuring and Monitoring HDLCThere really is no configuration for HDLC, but if you completed Lab 14.1, then the PPP encapsulation would be set on both routers. This is why I put the PPP lab first. This lab allows you to actually configure HDLC encapsulation on a router.

This second lab will use the same configuration as Lab 14.1 used.1.Set the encapsulation for each serial interface by using the encapsulation hdlc command.RouterA#config tRouterA(config)#int s0RouterA(config-if)#encapsulation hdlcRouterB#config tRouterB(config)#int s0

RouterB(config-if)#encapsulation hdlc2.Verify the HDLC encapsulation by using the show interface s0 command on each router.

10089.book  Page 839  Monday, July 23, 2007  3:17 PM




 840 Chapter14  Wide Area Networks Hands-on Lab 14.3: Configuring Frame Relay and Subinterfaces In this lab, you will use the following diagram to configure Frame Relay.In this lab, you will configure the Lab _B router to be a Frame Relay switch. You will then configure the Lab_A and Lab_C routers to use the switch to bring up the PVC. 1. Set the hostname,  frame-relay switching  command, and the encapsulation of each serial interface on the Frame Relay switch. Router# config t Router(config)# hostname Lab_B Lab_B(config)# frame-relay switching  [makes the router anFR switch] Lab_B(config)# int s0/0 Lab_B(config-if)# encapsulation frame-relay Lab_B(config-if)# int s0/1

 Lab_B(config-if)# encapsulation frame-relay 2. Configure the Frame Relay mappings on each interface. You do not have to have IP addresses on these interfaces because they are only switching one interface to another with Frame Relay frames. Lab_B(config-if)# int s0/0 Lab_B(config-if)# frame intf-type dce [The above command makes this an FR DCE interface, whichis different than a router’s interface being DCE] Lab_B(config-if)# frame-relay route 102 interface  Serial0/1 201 Lab_B(config-if)# clock rate 64000 [The above command is used if you have this as DCE, whichis different than an FR DCE]

1900

Lab_AF0/27F0/26

F0/0S0/0S0/0DCES0/1DCE

2950

Lab_BF0/3F0/2F0/1

F0/0F0/4F0/5S0/0

2950

Lab_CF0/3F0/2F0/1

F0/0F0/4F0/5

 

10089c14.fm  Page 840  Monday, November 10, 2008  12:36 PM




 Hands-on Labs 841 Lab_B(config-if)# int s0/1 Lab_B(config-if)# frame intf-type dce Lab_B(config-if)# frame-relay route 201 interface  Serial0/0 102

 Lab_B(config-if)# clock rate 64000   [if you have this as DCE] This is not as hard as it looks. The  route  command just says that if you receive frames from PVC 102, send them out  int s0/1  using PVC 201. The second mapping on serial 0/1 is just the opposite. Anything that comes in  int s0/1  is routed out serial0/ 0 using PVC 102. 3. Configure Lab_A with a point-to-point subinterface. Router# config t Router(config)# hostname Lab_A Lab_A(config)# int s0/0 Lab_A(config-if)# encapsulation frame-relay Lab_A(config-if)# int s0.102 point-to-point Lab_A(config-if)# ip address 172.16.10.1  255.255.255.0

 Lab_A(config-if)# frame-relay interface-dlci 102 4. Configure Lab_C with a point-to-point subinterface. Router# config t Router(config)# hostname Lab_C Lab_C(config)# int s0/0 Lab_C(config-if)# encapsulation frame-relay Lab_C(config-if)# int s0.201 point-to-point Lab_C(config-if)# ip address 172.16.10.2  255.255.255.0

 Lab_C(config-if)# frame-relay interface-dlci 201 5. Verify your configurations with the following commands: Lab_A> sho frame ?  ip       show frame relay IP statistics lmi      show frame relay lmi statistics map      Frame-Relay map table pvc      show frame relay pvc statistics route    show frame relay route

  traffic  Frame-Relay protocol statistics 6. Also, use Ping and Telnet to verify connectivity.

 

10089c14.fm  Page 841  Monday, November 10, 2008  12:36 PM




842Chapter14 Wide Area NetworksReview Questions

The following questions are designed to test your understanding of this chapter’s material. For more information on how to get additional ques-tions, please see this book’s Introduction.1.Which command will display the CHAP authentication process as it occurs between two routers in the network?A.show chap authenticationB.show interface serial 0C.debug ppp authenticationD.debug chap authentication2.Which command is required for connectivity in a Frame Relay network if Inverse ARP is not operational?A.frame-relay arpB.frame-relay mapC.frame-relay interface-dciD.frame-relay lmi-type3.Suppose that you have a customer who has a central HQ and six branch offices. They antici-pate adding six more branches in the near future. They wish to implement a WAN technology that will allow the branches to economically connect to HQ and you have no free ports on the HQ router. Which of the following would you recommend?A.PPPB.HDLCC.Frame RelayD.ISDN4.Which of the following command options are displayed when you use the Router#show frame-relay ? command? (Choose three.)A.dlciB.neighborsC.lmiD.pvcE.map

10089.book  Page 842  Monday, July 23, 2007  3:17 PM




Review Questions8435.How should a router that is being used in a Frame Relay network be configured to keep split horizon issues from preventing routing updates?A.Configure a separate subinterface for each PVC with a unique DLCI and subnet assigned to the subinterface.B.Configure each Frame Relay circuit as a point-to-point line to support multicast and broadcast traffic.C.Configure many subinterfaces in the same subnet.D.Configure a single subinterface to establish multiple PVC connections to multiple remote router interfaces.6.Which encapsulations can be configured on a serial interface? (Choose three.)A.EthernetB.Token RingC.HDLCD.Frame RelayE.PPP7.When setting up Frame Relay for point-to-point subinterfaces, which of the following must not be configured?A.The Frame Relay encapsulation on the physical interfaceB.The local DLCI on each subinterfaceC.An IP address on the physical interfaceD.The subinterface type as point-to-point8.When a router is connected to a Frame Relay WAN link using a serial DTE interface, how is the clock rate determined?A.Supplied by the CSU/DSUB.By the far end routerC.By the clock rate commandD.By the Physical layer bit stream timing9.A default Frame Relay WAN is classified as what type of physical network?A.Point-to-pointB.Broadcast multi-accessC.Non-broadcast multi-accessD.Non-broadcast multipoint10.Which of the following encapsulates PPP frames in Ethernet frames and uses common PPP features like authentication, encryption, and compression?A.PPPB.PPPoAC.PPPoED.Token Ring

10089.book  Page 843  Monday, July 23, 2007  3:17 PM




844Chapter14 Wide Area Networks11.You need to configure a router for a Frame Relay connection to a non-Cisco router. Which of the following commands will prepare the WAN interface of the router for this connection?A.Router(config-if)#encapsulation frame-relay q933aB.Router(config-if)#encapsulation frame-relay ansiC.Router(config-if)#encapsulation frame-relay ietfD.Router(config-if)#encapsulation frame-relay cisco12.The Acme Corporation is implementing dial-up services to enable remote-office employees to connect to the local network. The company uses multiple routed protocols, needs authentica-tion of users connecting to the network, and since some calls will be long distance, needs call-back support. Which of the following protocols is the best choice for these remote services?A.802.1B.Frame RelayC.HDLCD.PPPE.PAP13.Which WAN encapsulations can be configured on an asynchronous serial connection? (Choose two.)A.PPPB.ATMC.HDLCD.SDLCE.Frame Relay14.Which of the following uses ATM as the Data Link layer protocol that’s terminated at what’s known as the DSLAM?A.DSLB.PPPoEC.Frame RelayD.Dedicated T1E.WirelessF.POTS15.Why won’t the serial link between the Corp router and the Remote router come up?Corp#sh int s0/0Serial0/0 is up, line protocol is down  Hardware is PowerQUICC Serial  Internet address is 10.0.1.1/24  MTU 1500 bytes, BW 1544 Kbit, DLY 20000 usec,     reliability 254/255, txload 1/255, rxload 1/255

10089.book  Page 844  Monday, July 23, 2007  3:17 PM




Review Questions845  Encapsulation PPP, loopback not setRemote#sh int s0/0Serial0/0 is up, line protocol is down  Hardware is PowerQUICC Serial  Internet address is 10.0.1.2/24  MTU 1500 bytes, BW 1544 Kbit, DLY 20000 usec,     reliability 254/255, txload 1/255, rxload 1/255

  Encapsulation HDLC, loopback not setA.The serial cable is faulty.B.The IP addresses are not in the same subnet.C.The subnet masks are not correct.D.The keepalive settings are not correct.E.The layer 2 frame types are not compatible.16.In which of the following technologies is the term HFC used?A.DSLB.PPPoEC.Frame RelayD.CableE.WirelessF.POTS17.A remote site has just been connected to the central office. However, remote users cannot access applications at the central office. The remote router can be pinged from the central office router. After reviewing the command output shown below, which do you think is the most likely reason for the problem?Central#show running-config!interface Serial0 ip address 10.0.8.1 255.255.248.0 encapsulation frame-relay frame-relay map ip 10.0.15.2 200!Router ripNetwork 10.0.0.0Remote#show running-config!interface Serial0

10089.book  Page 845  Monday, July 23, 2007  3:17 PM




846Chapter14 Wide Area Networks ip address 10.0.15.2 255.255.248.0 encapsulation frame-relay frame-relay map ip 10.0.8.1 100!Router rip

Network 10.0.0.0A.The Frame Relay PVC is down.B.The IP addressing on the Central/Remote router link is incorrect.C.RIP routing information is not being forwarded.D.Frame Relay Inverse ARP is not properly configured.18.Which of the following describes an industry-wide standard suite of protocols and algorithms that allows for secure data transmission over an IP-based network that functions at the layer 3 Network layer of the OSI model?A.HDLCB.CableC.VPND.IPSecE.xDSL19.Which of the following describes the creation of private networks across the Internet, enabling privacy and tunneling of non-TCP/IP protocols?A.HDLCB.CableC.VPND.IPSecE.xDSL20.Referring to the following diagram, what functions does the Frame Relay DLCI provide with respect to RouterA?A.Identifies the signaling standard between RouterA and the frame switchB.Identifies the circuit between RouterB and the frame switchC.Identifies the encapsulation used between RouterA and RouterBD.Defines the signaling standard between RouterB and the frame switch.

RouterARouterBDLCI 100DLCI 200

10089.book  Page 846  Monday, July 23, 2007  3:17 PM




Answers to Review Questions847Answers to Review Questions1.C. The command debug ppp authentication will show you the authentication process that PPP uses between point-to-point connections.2.B. If you have a router in your Frame Relay network that does not support IARP, you must create Frame Relay maps on your router, which provide known DLCI-to-IP address mappings.3.C. The key is “there are no free ports” on your router. Only Frame Relay can provide a con-nection to multiple locations with one interface, and in an economical manner no less.4.C, D, E. The show frame-relay ? command provides many options, but the options avail-able in this question are lmi, pvc, and map.5.A. If you have a serial port configured with multiple DLCIs connected to multiple remote sites, split horizon rules (discussed in Chapter 5) stop route updates received on an interface from being sent out the same interface. By creating subinterfaces for each PVC, you can avoid the split horizon issues when using Frame Relay.6.C, D, E. Ethernet and Token Ring are LAN technologies and cannot be configured on a serial interface. PPP, HDLC, and Frame Relay are layer 2 WAN technologies that are typically con-figured on a serial interface.7.C. It is very important to remember when studying the CCNA exam objectives, and when con-figuring Frame Relay with point-to-point subinterfaces, that you do not put an IP address on the physical interface.8.A. Clocking on a serial interface is always provided by the CSU/DSU (DCE device). However, if you do not have a CSU/DSU in your nonproduction test environment, then you need to sup-ply clocking with the clock rate command on the serial interface of the router with the DCE cable attached.9.C. Frame Relay, by default, is a non-broadcast multi-access (NBMA) network, which means that broadcasts, such as RIP updates, will not be forwarded across the link by default.10.C. PPPoE encapsulates PPP frames in Ethernet frames and uses common PPP features like authentication, encryption, and compression. PPPoA is used for ATM.11.C. If you have a Cisco router on one side of a Frame Relay network and a non-Cisco router on the other side, you would need to use the Frame Relay encapsulation type of IETF. The default is Cisco encapsulation, which means that a Cisco router must be on both sides of the Frame Relay network.12.D. PPP is your only option, as HDLC and Frame Relay do not support these types of business requirements. PPP provides dynamic addressing, authentication using PAP or CHAP, and call-back services.13.A, B. Please do not freak out because ATM is an answer to this question. ATM is not covered in depth on the CCNA exam. PPP is mostly used for dial-up (async) services, but ATM could be used as well, though it typically is not used anymore since PPP is so efficient.

10089.book  Page 847  Monday, July 23, 2007  3:17 PM




848Chapter14 Wide Area Networks14.A. ATM is the Data-Link layer protocol that’s typically used over the DSL layer 1 connection from the CPE and terminated at what’s known as the DSLAM—an ATM switch that contains DSL interface cards, or ATU-Cs.15.E. This is an easy question because the Remote router is using the default HDLC serial encap-sulation and the Corp router is using the PPP serial encapsulation. You should go to the Remote router and set that encapsulation to PPP or change the Corp router back to the default of HDLC.16.D. In a modern network, hybrid fibre-coaxial (HFC) is a telecommunications industry term for a network that incorporates both optical fiber and coaxial cable to create a broadband network.17.C. Even though the IP addresses don’t look correct, they are in the same subnet, so answer B is not correct. The question states that you can ping the other side, so the PVC must be up—answer A can’t be correct. You cannot configure IARP, so only answer C can be correct. Since a Frame Relay network is a non-broadcast multi-access network by default, broadcasts such as RIP updates cannot be sent across the PVC unless you use the broadcast statement at the end of the frame-relay map command.18.D. IPSec is an industry-wide standard suite of protocols and algorithms that allows for secure data transmission over an IP-based network that functions at the layer 3 Network layer of the OSI model.19.C. A  allows the creation of private networks across the Internet, enabling privacy and tunnel-ing of non-TCP/IP protocols. A VPN can be set up across any type of link.20.A. As I mentioned many times in this chapter, and you need to remember this: DLCIs are locally significant only and define the circuit from the router to the switch only. They do not reference a remote router or DLCI. RouterA would use DLCI 100 to get to the RouterB net-works. RouterB would use DLCI 200 to get to the RouterA networks.

10089.book  Page 848  Monday, July 23, 2007  3:17 PM




 Answers to Written Lab 14.1 849 Answers to Written Lab 14.1 1. sh int s0 2. config tint s0encap ppp 3. config tusername todd password cisco 4. config tint serial 0ppp authentication chap 5. config tint s0frame interface-dlci 16int s1frame interface-dlci 17 6. config tint s0encap frameint s0.16 point-to-pointip address 172.16.60.1 255.255.255.0frame interface-dlci 16 7. PPPoE 8. HDLC, LCP, and NCP 9. IPSec 10. Remote access VPNs, site-to-site VPNs, and extranet VPNs

 

10089c14.fm  Page 849  Friday, November 7, 2008  11:17 PM




10089.book  Page 850  Monday, July 23, 2007  3:17 PM




 

Glossary

 

10089.book  Page 851  Monday, July 23, 2007  3:17 PM




 852 Glossary 10BaseT Part of the original IEEE 802.3 standard, 10BaseT is the Ethernet specification of 10Mbps baseband that uses two pairs of twisted-pair, Category 3, 4, or 5 cabling—using one pair to send data and the other to receive. 10BaseT has a distance limit of about 100 meters per segment.  See also: Ethernet  and  IEEE 802.3. 100BaseT Based on the IEEE 802.3u standard, 100BaseT is the Fast Ethernet specification of 100Mbps baseband that uses UTP wiring. 100BaseT sends link pulses (containing more information than those used in 10BaseT) over the network when no traffic is present.  See also: 10BaseT, Fast Ethernet,  and  IEEE 802.3. 100BaseTX Based on the IEEE 802.3u standard, 100BaseTX is the 100Mbps baseband Fast Ethernet specification that uses two pairs of UTP or STP wiring. The first pair of wires receives data; the second pair sends data. To ensure correct signal timing, a 100BaseTX segment cannot be longer than 100 meters. A&B bit signaling Used in T1 transmission facilities and sometimes called “24th channel signaling.” Each of the 24 T1 subchannels in this procedure uses one bit of every sixth frame to send supervisory signaling information. AAA Authentication, Authorization, and Accounting: A system developed by Cisco to provide network security.  See also:   authentication,   authorization,  and  accounting . AAL ATM Adaptation Layer: A service-dependent sublayer of the Data Link layer, which accepts data from other applications and brings it to the ATM layer in 48-byte ATM payload segments. CS and SAR are the two sublayers that form AALs. Currently, the four types of AAL recommended by the ITU-T are AAL1, AAL2, AAL3/4, and AAL5. AALs are differentiated by the source-destination timing they use, whether they are CBR or VBR, and whether they are used for connection-oriented or connectionless mode data transmission.  See also: AAL1, AAL2, AAL3/4, AAL5, ATM,  and  ATM layer. AAL1 ATM Adaptation Layer 1: One of four AALs recommended by the ITU-T, it is used for connection-oriented, time-sensitive services that need constant bit rates, such as isochronous traffic and uncompressed video.  See also: AAL. AAL2 ATM Adaptation Layer 2: One of four AALs recommended by the ITU-T, it is used for connection-oriented services that support a variable bit rate, such as compressed voice traffic.  See also: AAL. AAL3/4 ATM Adaptation Layer 3/4: One of four AALs (a product of two initially distinct layers) recommended by the ITU-T, supporting both connectionless and connection-oriented links. Its primary use is in sending SMDS packets over ATM networks.  See also: AAL. AAL5 ATM Adaptation Layer 5: One of four AALs recommended by the ITU-T, it is used to support connection-oriented VBR services primarily to transfer classical IP over ATM and LANE traffic. This least complex of the AAL recommendations uses SEAL, offering lower bandwidth costs and simpler processing requirements but also providing reduced bandwidth and error-recovery capacities.  See also: AAL.

 

10089.book  Page 852  Monday, July 23, 2007  3:17 PM




 Glossary 853 AARP AppleTalk Address Resolution Protocol: The protocol in an AppleTalk stack that maps data-link addresses to network addresses. AARP probe packets Packets sent by the AARP to determine whether a given node ID is being used by another node in a nonextended AppleTalk network. If the node ID is not in use, the sending node appropriates that node’s ID. If the node ID is in use, the sending node will select a different ID and then send out more AARP probe packets.  See also: AARP. ABM Asynchronous Balanced Mode: When two stations can initiate a transmission, ABM is an HDLC (or one of its derived protocols) communication technology that supports peer-oriented, point-to-point communications between both stations. ABR Area Border Router: An OSPF router that is located on the border of one or more OSPF areas. ABRs are used to connect OSPF areas to the OSPF backbone area. access layer One of the layers in Cisco’s three-layer hierarchical model. The access layer provides users with access to the internetwork. access link A link used with switches that is part of only one virtual LAN (VLAN). Trunk links carry information from multiple VLANs. access list A set of test conditions kept by routers that determines “interesting traffic” to and from the router for various services on the network. access method The manner in which network devices approach gaining access to the network itself. access rate Defines the bandwidth rate of the circuit. For example, the access rate of a T1 circuit is 1.544Mbps. In Frame Relay and other technologies, there may be a fractional T1 connection—256Kbps, for example—however, the access rate and clock rate are still 1.544Mbps. access server Also known as a “network access server,” it is a communications process con-necting asynchronous devices to a LAN or WAN through network and terminal emulation software, providing synchronous or asynchronous routing of supported protocols. accounting One of the three components in AAA. Accounting provides auditing and logging functionalities to the security model. acknowledgment Verification sent from one network device to another signifying that an event has occurred. May be abbreviated as ACK.  Contrast with: NAK. ACR Allowed cell rate: A designation defined by the ATM Forum for managing ATM traffic. Dynamically controlled using congestion control measures, the ACR varies between the min-imum cell rate (MCR) and the peak cell rate (PCR).  See also: MCR  and  PCR. active monitor The mechanism used to manage a token ring. The network node with the highest MAC address on the ring becomes the active monitor and is responsible for manage-ment tasks such as preventing loops and ensuring that tokens are not lost.

 

10089.book  Page 853  Monday, July 23, 2007  3:17 PM




 854 Glossary active state In regard to an EIGRP routing table, a route will be in active state when a router is undergoing a route convergence. address learning Used with transparent bridges to learn the hardware addresses of all devices on a network. The switch then filters the network with the known hardware (MAC) addresses. address mapping By translating network addresses from one format to another, this meth-odology permits different protocols to operate interchangeably. address mask A bit combination descriptor identifying which portion of an address refers to the network or subnet and which part refers to the host. Sometimes simply called the mask.  See also: subnet mask. address resolution The process used for resolving differences between computer addressing schemes. Address resolution typically defines a method for tracing Network layer (layer 3) addresses to Data Link layer (layer 2) addresses.  See also: address mapping. adjacency The relationship made to exchange routing information between defined neigh-boring routers and end nodes using a common media segment. administrative distance (AD) A number between 0 and 255 that expresses the level of trust-worthiness of a routing information source. The lower the number, the higher the integrity rating. administrative weight A value designated by a network administrator to rate the preference given to a network link. It is one of four link metrics exchanged by PTSPs to test ATM network resource availability. ADSU ATM Data Service Unit: The terminal adapter used to connect to an ATM network through an HSSI-compatible mechanism.  See also: DSU. advertising The process whereby routing or service updates are transmitted at given inter-vals, allowing other routers on the network to maintain a record of viable routes. AEP AppleTalk Echo Protocol: A test for connectivity between two AppleTalk nodes where one node sends a packet to another and receives an echo, or copy, in response. AFI Authority and Format Identifier: The part of an NSAP ATM address that delineates the type and format of the IDI section of an ATM address. AFP AppleTalk Filing Protocol: A Presentation layer protocol, supporting AppleShare and Mac OS File Sharing, that permits users to share files and applications on a server. AIP ATM Interface Processor: Supporting AAL3/4 and AAL5, this interface for Cisco 7000 series routers minimizes performance bottlenecks at the UNI.  See also: AAL3/4  and  AAL5. algorithm A set of rules or processes used to solve a problem. In networking, algorithms are typically used for finding the best route for traffic from a source to its destination. alignment error An error occurring in Ethernet networks, in which a received frame has extra bits—that is, a number not divisible by eight. Alignment errors are generally the result of frame damage caused by collisions.

 

10089.book  Page 854  Monday, July 23, 2007  3:17 PM




 Glossary 855 all-routes explorer packet An explorer packet that can move across an entire SRB network, tracing all possible paths to a given destination. Also known as an all-rings explorer packet.  See also: explorer packet, local explorer packet,  and  spanning explorer packet. AM Amplitude modulation: A modulation method that represents information by varying the amplitude of the carrier signal.  See also: modulation. AMI Alternate Mark Inversion: A line-code type on T1 and E1 circuits that shows zeros as 01 during each bit cell and ones as 11 or 00, alternately, during each bit cell. The sending device must maintain ones density in AMI but not independently of the data stream. Also known as binary-coded, alternate mark inversion.  Contrast with: B8ZS. See also: ones density. amplitude An analog or digital waveform’s highest value. analog transmission Signal messaging whereby information is represented by various com-binations of signal amplitude, frequency, and phase. ANSI American National Standards Institute: The organization of corporate, government, and volunteer members that coordinates standards-related activities, approves U.S. national standards, and develops U.S. positions in international standards organizations. ANSI assists in the creation of international and U.S. standards in disciplines such as communications, net-working, and a variety of technical fields. It publishes over 13,000 standards for engineered products and technologies ranging from screw threads to networking protocols. ANSI is a member of the International Electrotechnical Commission (IEC) and International Organiza-tion for Standardization (ISO). anycast An ATM address that can be shared by more than one end system, allowing requests to be routed to a node that provides a particular service. AppleTalk Currently in two versions, the group of communication protocols designed by Apple Computer for use in Macintosh environments. The earlier Phase 1 protocols support one physical network with only one network number that resides in one zone. The later Phase 2 protocols support more than one logical network on a single physical network, allowing net-works to exist in more than one zone.  See also: zone. Application layer Layer 7 of the OSI reference network model, supplying services to appli-cation procedures (such as electronic mail and file transfer) that are outside the OSI model. This layer chooses and determines the availability of communicating partners along with the resources necessary to make the connection, coordinates partnering applications, and forms a consensus on procedures for controlling data integrity and error recovery.  See also: Data Link layer, Network layer, Physical layer, Presentation layer, Session layer,  and  Transport layer. ARA AppleTalk Remote Access: A protocol for Macintosh users establishing their access to resources and data from a remote AppleTalk location. area A logical, rather than physical, set of segments (based on CLNS, DECnet, or OSPF) along with their attached devices. Areas are commonly connected to others using routers to create a single autonomous system.  See also: autonomous system.

 

10089.book  Page 855  Monday, July 23, 2007  3:17 PM




 856 Glossary ARM Asynchronous Response Mode: An HDLC communication mode using one primary station and at least one additional station, in which transmission can be initiated from either the primary or one of the secondary units. ARP Address Resolution Protocol: Defined in RFC 826, the protocol that traces IP addresses to MAC addresses.  See also: RARP. AS autonomous system: A group of networks under mutual administration that share the same routing methodology. Autonomous systems are subdivided by areas and must be assigned an individual 16-bit number by the IANA.  See also: area. AS path prepending The use of route maps in BGP to lengthen the autonomous system path by adding false ASNs. ASBR Autonomous System Boundary Router: An Area Border Router placed between an OSPF autonomous system and a non-OSPF network that operates both OSPF and an additional routing protocol, such as RIP. ASBRs must be located in a non-stub OSPF area.  See also: ABR, non-stub area,  and  OSPF. ASCII American Standard Code for Information Interchange: An 8-bit code for representing characters, consisting of 7 data bits plus 1 parity bit. ASICs Application-specific integrated circuits: Used in layer 2 switches to make filtering decisions. The ASIC looks in the filter table of MAC addresses and determines which port the destination hardware address of a received hardware address is destined for. The frame will be allowed to traverse only that one segment. If the hardware address is unknown, the frame is forwarded out all ports. ASN.1 Abstract Syntax Notation One: An OSI language used to describe types of data that are independent of computer structures and depicting methods. Described by ISO Interna-tional Standard 8824. ASP AppleTalk Session Protocol: A protocol employing ATP to establish, maintain, and tear down sessions as well as sequence requests.  See also: ATP. AST Automatic Spanning Tree: A function that supplies one path for spanning explorer frames traveling from one node in the network to another, supporting the automatic resolu-tion of spanning trees in SRB networks. AST is based on the IEEE 802.1d standard.  See also: IEEE 802.1  and  SRB. asynchronous transmission Digital signals sent without precise timing, usually with different frequencies and phase relationships. Asynchronous transmissions generally enclose individual characters in control bits (called start and stop bits) that show the beginning and end of each character.  Contrast with: isochronous transmission  and  synchronous transmission. ATCP AppleTalk Control Program: The protocol for establishing and configuring AppleTalk over PPP, defined in RFC 1378.  See also: PPP. ATDM Asynchronous Time-Division Multiplexing: A technique for sending information, it differs from normal TDM in that the time slots are assigned when necessary rather than pre-assigned to certain transmitters.  Contrast with: FDM, statistical multiplexing,  and  TDM.

 

10089.book  Page 856  Monday, July 23, 2007  3:17 PM




 Glossary 857 ATG Address Translation Gateway: The mechanism within Cisco DECnet routing software that enables routers to route multiple, independent DECnet networks and to establish a user-designated address translation for chosen nodes between networks. ATM Asynchronous Transfer Mode: The international standard, identified by fixed-length 53-byte cells, for transmitting cells in multiple service systems, such as voice, video, or data. Transit delays are reduced because the fixed-length cells permit processing to occur in the hardware. ATM is designed to maximize the benefits of high-speed transmission media, such as SONET, E3, and T3. ATM ARP server A device that supplies logical subnets running classical IP over ATM with address-resolution services. ATM endpoint The initiating or terminating connection in an ATM network. ATM endpoints include servers, workstations, ATM-to-LAN switches, and ATM routers. ATM Forum The international organization founded jointly by Northern Telecom, Sprint, Cisco Systems, and NET/ADAPTIVE in 1991 to develop and promote standards-based implementation agreements for ATM technology. The ATM Forum broadens official standards developed by ANSI and ITU-T and creates implementation agreements before official standards are published. ATM layer A sublayer of the Data Link layer in an ATM network that is service independent. To create standard 53-byte ATM cells, the ATM layer receives 48-byte segments from the AAL and attaches a 5-byte header to each. These cells are then sent to the physical layer for transmission across the physical medium.  See also: AAL. ATMM ATM Management: A procedure that runs on ATM switches, managing rate enforce-ment and VCI translation.  See also: ATM. ATM user-user connection A connection made by the ATM layer to supply communication between at least two ATM service users, such as ATMM processes. These communications can be uni- or bidirectional, using one or two VCs, respectively.  See also: ATM layer  and  ATMM. ATP AppleTalk Transaction Protocol: A transport-level protocol that enables reliable trans-actions between two sockets; one requests the other to perform a given task and to report the results. ATP fastens the request and response together, assuring a loss-free exchange of request-response pairs. attenuation In communication, weakening or loss of signal energy, typically caused by distance. AURPAppleTalk Update-based Routing Protocol: A technique for encapsulating AppleTalk traffic in the header of a foreign protocol that allows the connection of at least two noncon-tiguous AppleTalk internetworks through a foreign network (such as TCP/IP) to create an AppleTalk WAN. The connection made is called an AURP tunnel. By exchanging routing information between exterior routers, the AURP maintains routing tables for the complete AppleTalk WAN. See also: AURP tunnel.

10089.book  Page 857  Monday, July 23, 2007  3:17 PM




858GlossaryAURP tunnelA connection made in an AURP WAN that acts as a single, virtual link between AppleTalk internetworks separated physically by a foreign network such as a TCP/IP network. See also: AURP.authenticationThe first component in the AAA model. Users are typically authenticated via a username and password, which are used to uniquely identify them.authority zoneA portion of the domain-name tree associated with DNS for which one name server is the authority. See also: DNS.authorizationThe act of permitting access to a resource based on authentication informa-tion in the AAA model.auto-detect mechanismUsed in Ethernet switch, hub, and interface cards to determine the duplex and speed that can be used.auto duplexA setting on layer 1 and layer 2 devices that sets the duplex of a switch or hub port automatically.automatic call reconnectA function that enables automatic call rerouting away from a failed trunk line.autonomous confederationA collection of self-governed systems that depend more on their own network accessibility and routing information than on information received from other systems or groups.autonomous switchingThe ability of Cisco routers to process packets more quickly by using the ciscoBus to switch packets independently of the system processor.autonomous systemSee: AS.autoreconfigurationA procedure executed by nodes within the failure domain of a token ring wherein nodes automatically perform diagnostics, trying to reconfigure the network around failed areas.auxiliary portThe console port on the back of Cisco routers that allows you to connect a modem and dial the router and make console configuration settings.B8ZSBinary 8-Zero Substitution: A line-code type, interpreted at the remote end of the con-nection, that uses a special code substitution whenever eight consecutive zeros are transmitted over the link on T1 and E1 circuits. This technique assures ones density independent of the data stream. Also known as bipolar 8-zero substitution. Contrast with: AMI. See also: ones density.backboneThe basic portion of the network that provides the primary path for traffic sent to and initiated from other networks.back endA node or software program supplying services to a front end. See also: server.bandwidthThe gap between the highest and lowest frequencies employed by network signals. More commonly, it refers to the rated throughput capacity of a network protocol or medium.

10089.book  Page 858  Monday, July 23, 2007  3:17 PM




Glossary859bandwidth on demand (BoD)This function allows an additional B channel to be used to increase the amount of bandwidth available for a particular connection.basebandA feature of a network technology that uses only one carrier frequency. Ethernet is an example. Also named “narrowband.” Compare with: broadband.baselineBaseline information includes historical data about the network and routine utilization information. This information can be used to determine whether there were recent changes made to the network that may contribute to the problem at hand.Basic Management SetupUsed with Cisco routers when in setup mode. Only provides enough management and configuration to get the router working so someone can telnet into the router and configure it.baudSynonymous with bits per second (bps), if each signal element represents 1 bit. It is a unit of signaling speed equivalent to the number of separate signal elements transmitted per second.B channelBearer channel: A full-duplex, 64Kbps channel in ISDN that transmits user data. Compare with: D channel, E channel, and H channel.BDRBackup designated router: This is used in an OSPF network to back up the designated router in case of failure.beaconAn FDDI frame or Token Ring frame that points to a serious problem with the ring, such as a broken cable. The beacon frame carries the address of the station thought to be down. See also: failure domain.BECNBackward Explicit Congestion Notification: BECN is the bit set by a Frame Relay network in frames moving away from frames headed into a congested path. A DTE that receives frames with the BECN may ask higher-level protocols to take necessary flow control measures. Compare with: FECN.BGP4BGP version 4: Version 4 of the interdomain routing protocol most commonly used on the Internet. BGP4 supports CIDR and uses route-counting mechanisms to decrease the size of routing tables. See also: CIDR.BGP IdentifierThis field contains a value that identifies the BGP speaker. This is a random value chosen by the BGP router when sending an OPEN message.BGP neighborsTwo routers running BGP that begin a communication process to exchange dynamic routing information; they use a TCP port at layer 4 of the OSI reference model. Specifically, TCP port 179 is used. Also known as “BGP peers.”BGP peersSee: BGP neighbors.BGP speakerA router that advertises its prefixes or routes.bidirectional shared treeA method of shared tree multicast forwarding. This method allows group members to receive data from the source or the RP, whichever is closer. See also: RP (rendezvous point).

10089.book  Page 859  Monday, July 23, 2007  3:17 PM




860GlossarybinaryA two-character numbering method that uses ones and zeros. The binary numbering system underlies all digital representation of information.bindingConfiguring a Network layer protocol to use a certain frame type on a LAN.BIPBit Interleaved Parity: A method used in ATM to monitor errors on a link, sending a check bit or word in the link overhead for the previous block or frame. This allows bit errors in transmissions to be found and delivered as maintenance information.BISDNBroadband ISDN: ITU-T standards created to manage high-bandwidth technologies such as video. BISDN presently employs ATM technology along SONET-based transmission circuits, supplying data rates typically between 155Mbps and 622Mbps and now even into the gigabyte range (if you have the big bucks). See also: BRI, ISDN, and PRI.bitOne binary digit; either a 1 or a 0. Eight bits make a byte.bit-oriented protocolRegardless of frame content, the class of Data Link layer communication protocols that transmits frames. Bit-oriented protocols, as compared with byte-oriented, supply more efficient and trustworthy full-duplex operation. Compare with: byte-oriented protocol.block sizeNumber of hosts that can be used in a subnet. Block sizes typically can be used in increments of 4, 8, 16, 32, 64, and 128.Boot ROMUsed in routers to put the router into bootstrap mode. Bootstrap mode then boots the device with an operating system. The ROM can also hold a small Cisco IOS.boot sequenceDefines how a router boots. The configuration register tells the router where to boot the IOS from as well as how to load the configuration.bootstrap protocolA protocol used to dynamically assign IP addresses and gateways to requesting clients.border gatewayA router that facilitates communication with routers in different autono-mous systems.border peerThe device in charge of a peer group; it exists at the edge of a hierarchical design. When any member of the peer group wants to locate a resource, it sends a single explorer to the border peer. The border peer then forwards this request on behalf of the requesting router, thus eliminating duplicate traffic.border routerTypically defined within Open Shortest Path First (OSPF) as a router that connected an area to the backbone area. However, a border router can be a router that con-nects a company to the Internet as well. See also: OSPF.BPDUBridge Protocol Data Unit: A Spanning Tree Protocol initializing packet that is sent at definable intervals for the purpose of exchanging information among bridges in networks.BRIBasic Rate Interface: The ISDN interface that facilitates circuit-switched communication between video, data, and voice; it is made up of two B channels (64Kbps each) and one D channel (16Kbps). Compare with: PRI. See also: BISDN.

10089.book  Page 860  Monday, July 23, 2007  3:17 PM




Glossary861bridgeA device for connecting two segments of a network and transmitting packets between them. Both segments must use identical protocols to communicate. Bridges function at the Data Link layer, layer 2 of the OSI reference model. The purpose of a bridge is to filter, send, or flood any incoming frame, based on the MAC address of that particular frame.bridge groupUsed in the router configuration of bridging, bridge groups are defined by a unique number. Network traffic is bridged between all interfaces that are members of the same bridge group.bridge identifierUsed to elect the root bridge in a layer 2 switched internetwork. The bridge ID is a combination of the bridge priority and base MAC address.bridge prioritySets the STP priority of the bridge. All bridge priorities are set to 32768 by default.bridging loopLoops occur in a bridged network if more than one link to a network exists and the STP protocol is not turned on.broadbandA transmission methodology for multiplexing several independent signals onto one cable. In telecommunications, broadband is classified as any channel with bandwidth greater than 4kHz (typical voice grade). In LAN terminology, it is classified as a coaxial cable on which analog signaling is employed. Also known as “wideband.”broadcastA data frame or packet that is transmitted to every node on the local network seg-ment (as defined by the broadcast domain). Broadcasts are known by their broadcast address, which is a destination network and host address with all the bits turned on. Also called “local broadcast.” Compare with: directed broadcast.broadcast addressUsed in both logical addressing and hardware addressing. In logical addressing, the host addresses will be all ones. With hardware addressing, the hardware address will be all ones in binary (all Fs in hex).broadcast domainA group of devices receiving broadcast frames initiating from any device within the group. Because routers do not forward broadcast frames, broadcast domains are not forwarded from one broadcast to another.broadcast (multi-access) networksBroadcast (multi-access) networks such as Ethernet allow multiple devices to connect to (or access) the same network, as well as provide a broad-cast ability in which a single packet is delivered to all nodes on the network.broadcast stormAn undesired event on the network caused by the simultaneous transmis-sion of any number of broadcasts across the network segment. Such an occurrence can over-whelm network bandwidth, resulting in time-outs.bufferA storage area dedicated to handling data while in transit. Buffers are used to receive/store sporadic deliveries of data bursts, usually received from faster devices, compensating for the variations in processing speed. Incoming information is stored until everything is received prior to sending data on. Also known as an “information buffer.”

10089.book  Page 861  Monday, July 23, 2007  3:17 PM




862GlossaryburstingSome technologies, including ATM and Frame Relay, are considered burstable. This means that user data can exceed the bandwidth normally reserved for the connection; however, it cannot exceed the port speed. An example of this would be a 128Kbps Frame Relay CIR on a T1—depending on the vendor, it may be possible to send more than 128Kbps for a short time.busAny common physical path, typically wires or copper, through which a digital signal can be used to send data from one part of a computer to another.BUSBroadcast and unknown servers: In LAN emulation, the hardware or software responsible for resolving all broadcasts and packets with unknown (unregistered) addresses into the point-to-point virtual circuits required by ATM. See also: LANE, LEC, LECS, and LES.bus topologyA linear LAN architecture in which transmissions from various stations on the network are reproduced over the length of the medium and are accepted by all other stations. Compare with: ring topology and star topology.BX.25AT&T’s use of X.25. See also: X.25.bypass modeAn FDDI and Token Ring network operation that deletes an interface.bypass relayA device that enables a particular interface in the token ring to be closed down and effectively taken off the ring.byteEight bits. See also: octet.byte-oriented protocolAny type of data-link communication protocol that, in order to mark the boundaries of frames, uses a specific character from the user character set. These protocols have generally been superseded by bit-oriented protocols. Compare with: bit-oriented protocol.cable rangeIn an extended AppleTalk network, the range of numbers allotted for use by existing nodes on the network. The value of the cable range can be anywhere from a single network number to a sequence of several touching network numbers. Node addresses are determined by their cable range value.CACConnection Admission Control: The sequence of actions executed by every ATM switch while connection setup is performed in order to determine if a request for connection is violating the guarantees of QoS for established connections. Also, CAC is used to route a connection request through an ATM network.call admission controlA device for managing traffic in ATM networks, determining the possibility of a path containing adequate bandwidth for a requested VCC.call establishmentUsed to reference an ISDN call setup scheme when the call is working.call priorityIn circuit-switched systems, the defining priority given to each originating port; it specifies in which order calls will be reconnected. Additionally, call priority identifies which calls are allowed during a bandwidth reservation.call setupHandshaking scheme that defines how a source and destination device will estab-lish a call to each other.

10089.book  Page 862  Monday, July 23, 2007  3:17 PM




Glossary863call setup timeThe length of time necessary to effect a switched call between DTE devices.CBRConstant bit rate: An ATM Forum QoS class created for use in ATM networks. CBR is used for connections that rely on precision clocking to guarantee trustworthy delivery. Compare with: ABR and VBR.CDCarrier detect: A signal indicating that an interface is active or that a connection gener-ated by a modem has been established.CDPCisco Discovery Protocol: Cisco’s proprietary protocol that is used to tell a neighbor Cisco device about the type of hardware, software version, and active interfaces the Cisco device is using. It uses a SNAP frame between devices and is not routable.CDP holdtimeThe amount of time a router will hold Cisco Discovery Protocol information received from a neighbor router before discarding it if the information is not updated by the neighbor. This timer is set to 180 seconds by default.CDP timerThe amount of time between Cisco Discovery Protocol advertisements trans-mitted out of all router interfaces, by default. The CDP timer is 90 seconds by default.CDVTCell Delay Variation Tolerance: A QoS parameter for traffic management in ATM networks specified when a connection is established. The allowable fluctuation levels for data samples taken by the PCR in CBR transmissions are determined by the CDVT. See also: CBR and PCR.cellIn ATM networking, the basic unit of data for switching and multiplexing. Cells have a defined length of 53 bytes, including a 5-byte header that identifies the cell’s data stream and 48 bytes of payload. See also: cell relay.cell payload scramblingThe method by which an ATM switch maintains framing on some medium-speed edge and trunk interfaces (T3 or E3 circuits). Cell payload scrambling rearranges the data portion of a cell to maintain the line synchronization with certain common bit patterns.cell relayA technology that uses small packets of fixed size, known as cells. Their fixed length enables cells to be processed and switched in hardware at high speeds, making this tech-nology the foundation for ATM and other high-speed network protocols. See also: cell.CentrexA local exchange carrier service providing local switching that resembles that of an on-site PBX. Centrex has no on-site switching capability. Therefore, all customer connections return to the central office (CO). See also: CO.CERCell error ratio: In ATM, the ratio of transmitted cells having errors to the total number of cells transmitted within a certain span of time.CGMPCisco Group Management Protocol: A proprietary protocol developed by Cisco. The router uses CGMP to send multicast membership commands to Catalyst switches.channelized E1Operating at 2.048Mpbs, an access link that is sectioned into 29 B channels and one D channel, supporting DDR, Frame Relay, and X.25. Compare with: channelized T1.

10089.book  Page 863  Monday, July 23, 2007  3:17 PM




864Glossarychannelized T1Operating at 1.544Mbps, an access link that is sectioned into 23 B channels and one D channel of 64Kbps each, where individual channels or groups of channels connect to various destinations, supporting DDR, Frame Relay, and X.25. Compare with: channelized E1.CHAPChallenge Handshake Authentication Protocol: Supported on lines using PPP encap-sulation, it is a security feature that identifies the remote end, helping keep out unauthorized users. After CHAP is performed, the router or access server determines whether a given user is permitted access. It is a newer, more secure protocol than PAP. Compare with: PAP.checksumA test for ensuring the integrity of sent data. It is a number calculated from a series of values taken through a sequence of mathematical functions, typically placed at the end of the data from which it is calculated, and then recalculated at the receiving end for ver-ification. Compare with: CRC.choke packetWhen congestion exists, it is a packet sent to inform a transmitter that it should decrease its sending rate.CIDRClassless Inter-Domain Routing: It allows a group of IP networks to appear to other networks as a unified, larger entity. In CIDR, IP addresses and their subnet masks are written as four dotted octets, followed by a forward slash and the number of masking bits (a form of subnet notation shorthand). See also: BGP4.CIPChannel Interface Processor: A channel attachment interface for use in Cisco 7000 series routers that connects a host mainframe to a control unit. This device eliminates the need for an FBP to attach channels.CIRCommitted information rate: Averaged over a minimum span of time and measured in bps, a Frame Relay network’s agreed-upon minimum rate of transferring information.circuit switchingUsed with dial-up networks such as PPP and ISDN. Passes data, but needs to set up the connection first—just like making a phone call.Cisco FRADCisco Frame Relay Access Device: A Cisco product that supports Cisco IPS Frame Relay SNA services, connecting SDLC devices to Frame Relay without requiring an existing LAN. May be upgraded to a fully functioning multiprotocol router. Can activate conversion from SDLC to Ethernet and Token Ring, but does not support attached LANs. See also: FRAD.CiscoFusionCisco’s name for the internetworking architecture under which its Cisco IOS operates. It is designed to “fuse” together the capabilities of its disparate collection of acquired routers and switches.Cisco IOSCisco Internet Operating System software. The kernel of the Cisco line of routers and switches that supplies shared functionality, scalability, and security for all products under its CiscoFusion architecture. See also: CiscoFusion.CiscoViewGUI-based management software for Cisco networking devices, enabling dynamic status, statistics, and comprehensive configuration information. Displays a physical view of the Cisco device chassis and provides device-monitoring functions and fundamental troubleshooting capabilities. May be integrated with a number of SNMP-based network management platforms.

10089.book  Page 864  Monday, July 23, 2007  3:17 PM




Glossary865Class A networkPart of the Internet Protocol hierarchical addressing scheme. Class A net-works have only 8 bits for defining networks and 24 bits for defining hosts and subnets on each network.Class B networkPart of the Internet Protocol hierarchical addressing scheme. Class B networks have 16 bits for defining networks and 16 bits for defining hosts and subnets on each network.Class C networkPart of the Internet Protocol hierarchical addressing scheme. Class C networks have 24 bits for defining networks and only 8 bits for defining hosts and subnets on each network.classful routingRouting protocols that do not send subnet mask information when a route update is sent out.classical IP over ATMDefined in RFC 1577, the specification for running IP over ATM that maximizes ATM features. Also known as “CIA.”classless routingRouting that sends subnet mask information in the routing updates. Class-less routing allows Variable-Length Subnet Masking (VLSM) and supernetting. Routing pro-tocols that support classless routing are RIP version 2, EIGRP, and OSPF.CLICommand-line interface: Allows you to configure Cisco routers and switches with maximum flexibility.CLPCell Loss Priority: The area in the ATM cell header that determines the likelihood of a cell being dropped during network congestion. Cells with CLP = 0 are considered insured traffic and are not apt to be dropped. Cells with CLP = 1 are considered best-effort traffic that may be dropped during congested episodes, delivering more resources to handle insured traffic.CLRCell Loss Ratio: The ratio of discarded cells to successfully delivered cells in ATM. CLR can be designated a QoS parameter when establishing a connection.COCentral office: The local telephone company office where all loops in a certain area connect and where circuit switching of subscriber lines occurs.collapsed backboneA nondistributed backbone where all network segments are connected to each other through an internetworking device. A collapsed backbone can be a virtual net-work segment at work in a device such as a router, hub, or switch.collisionThe effect of two nodes sending transmissions simultaneously in Ethernet. When they meet on the physical media, the frames from each node collide and are damaged. See also: collision domain.collision domainThe network area in Ethernet over which frames that have collided will be detected. Collisions are propagated by hubs and repeaters, but not by LAN switches, routers, or bridges. See also: collision.composite metricUsed with routing protocols, such as IGRP and EIGRP, that use more than one metric to find the best path to a remote network. IGRP and EIGRP both use band-width and delay of the line by default. However, maximum transmission unit (MTU), load, and reliability of a link can be used as well.

10089.book  Page 865  Monday, July 23, 2007  3:17 PM




866GlossarycompressionA technique to send more data across a link than would be normally permitted by representing repetitious strings of data with a single marker.configuration registerA 16-bit configurable value stored in hardware or software that determines how Cisco routers function during initialization. In hardware, the bit position is set using a jumper. In software, it is set by specifying specific bit patterns used to set startup options, configured using a hexadecimal value with configuration commands.congestionTraffic that exceeds the network’s ability to handle it.congestion avoidanceTo minimize delays, the method a network uses to control traffic entering the system. Lower-priority traffic is discarded at the edge of the network when indi-cators signal it cannot be delivered, thus using resources efficiently.congestion collapseThe situation that results from the retransmission of packets in ATM networks where little or no traffic successfully arrives at destination points. It usually happens in networks made of switches with ineffective or inadequate buffering capabilities combined with poor packet discard or ABR congestion feedback mechanisms.connection IDIdentifications given to each Telnet session into a router. The show sessions command will give you the connections a local router will have to a remote router. The show users command will show the connection IDs of users telnetted into your local router.connectionlessData transfer that occurs without the creation of a virtual circuit. It has low overhead, uses best-effort delivery, and is not reliable. Contrast with: connection-oriented. See also: virtual circuit.Connectionless Network Service (CLNS)See connectionless.connection-orientedData transfer method that sets up a virtual circuit before any data is transferred. Uses acknowledgments and flow control for reliable data transfer. Contrast with: connectionless. See also: virtual circuit.console portTypically an RJ-45 (8-pin modular) port on a Cisco router and switch that allows command-line interface capability.control direct VCCOne of two control connections defined by Phase I LAN emulation; a bidirectional virtual control connection (VCC) established in ATM by an LEC to an LES. See also: control distribute VCC.control distribute VCCOne of two control connections defined by Phase 1 LAN emulation; a unidirectional virtual control connection (VCC) set up in ATM from an LES to an LEC. Usually, the VCC is a point-to-multipoint connection. See also: control direct VCC.convergenceThe process required for all routers in an internetwork to update their routing tables and create a consistent view of the network using the best possible paths. No user data is passed during an STP convergence time.core layerTop layer in the Cisco three-layer hierarchical model, which helps you design, build, and maintain Cisco hierarchical networks. The core layer passes packets quickly to dis-tribution layer devices only. No packet filtering should take place at this layer.

10089.book  Page 866  Monday, July 23, 2007  3:17 PM




Glossary867costAlso known as path cost, an arbitrary value, based on hop count, bandwidth, or another calculation, that is typically assigned by a network administrator and used by the routing protocol to compare different routes through an internetwork. Routing protocols use cost values to select the best path to a certain destination: the lowest cost identifies the best path. Also known as “path cost.” See also: routing metric.count to infinityA problem occurring in routing algorithms that are slow to converge where routers keep increasing the hop count to particular networks. To avoid this problem, various solutions have been implemented into each of the different routing protocols. Some of those solutions include defining a maximum hop count (defining infinity), route poising, poison reverse, and split horizon.CPCSCommon Part Convergence Sublayer: One of two AAL sublayers that is service dependent, it is further segmented into the CS and SAR sublayers. The CPCS prepares data for transmission across the ATM network; it creates the 48-byte payload cells that are sent to the ATM layer. See also: AAL and ATM layer.CPECustomer premises equipment: Items such as telephones, modems, and terminals installed at customer locations and connected to the service provider network.crankbackIn ATM, a correction technique used when a node somewhere on a chosen path cannot accept a connection setup request, blocking the request. The path is rolled back to an inter-mediate node, which then uses GCAC to attempt to find an alternate path to the final destination.CRCCyclic redundancy check: A methodology that detects errors, whereby the frame recip-ient makes a calculation by dividing frame contents with a prime binary divisor and compares the remainder to a value stored in the frame by the sending node. Contrast with: checksum.crossover cableType of Ethernet cable that connects a switch to switch, host to host, hub to hub, or switch to hub.CSMA/CDCarrier Sense Multiple Access with Collision Detection: A technology defined by the Ethernet IEEE 802.3 committee. Each device senses the cable for a digital signal before transmitting. Also, CSMA/CD allows all devices on the network to share the same cable, but one at a time. If two devices transmit at the same time, a frame collision will occur and a jam-ming pattern will be sent; the devices will stop transmitting, wait a predetermined as well as a self-imposed random amount of time, and then try to transmit again.CSUChannel service unit: A digital mechanism that connects end-user equipment to the local digital telephone loop. Frequently referred to along with the data service unit as CSU/DSU. See also: DSU.CSU/DSUChannel service unit/data service unit: Physical layer device used in wide area net-works to convert the CPE digital signals to what is understood by the provider’s switch. A CSU/DSU is typically one device that plugs into a RJ-45 (8-pin modular) jack, known as the demarcation point.CTDCell Transfer Delay: For a given connection in ATM, the time period between a cell exit event at the source user-network interface (UNI) and the corresponding cell entry event at the 

10089.book  Page 867  Monday, July 23, 2007  3:17 PM




868Glossarydestination. The CTD between these points is the sum of the total inter-ATM transmission delay and the total ATM processing delay.cumulative interface delayThis is a Cisco term for delay of the line. The composite metric in IGRP and EIGRP is calculated by using the bandwidth and delay of the line by default.cut-through frame switchingA frame-switching technique that flows data through a switch so that the leading edge exits the switch at the output port before the packet finishes entering the input port. Frames will be read, processed, and forwarded by devices that use cut-through switching as soon as the destination address of the frame is confirmed and the outgoing port is identified.data circuit-terminating equipmentDCE is used to provide clocking to DTE equipment.data compressionSee: compression.data direct VCCA bidirectional point-to-point virtual control connection (VCC) set up between two LECs in ATM and one of three data connections defined by Phase 1 LAN emu-lation. Because data direct VCCs do not guarantee QoS, they are generally reserved for UBR and ABR connections. Compare with: control distribute VCC and control direct VCC.data encapsulationThe process in which the information in a protocol is wrapped, or con-tained, in the data section of another protocol. In the OSI reference model, each layer encap-sulates the layer immediately above it as the data flows down the protocol stack.data frameProtocol Data Unit encapsulation at the Data Link layer of the OSI reference model. Encapsulates packets from the Network layer and prepares the data for transmission on a network medium.datagramA logical collection of information transmitted as a Network layer unit over a medium without a previously established virtual circuit. IP datagrams have become the primary information unit of the Internet. At various layers of the OSI reference model, the terms cell, frame, message, packet, and segment also define these logical information groupings.Data Link Control layerLayer 2 of the SNA architectural model, it is responsible for the transmission of data over a given physical link and compares somewhat to the Data Link layer of the OSI model.Data Link layerLayer 2 of the OSI reference model, it ensures the trustworthy transmission of data across a physical link and is primarily concerned with physical addressing, line disci-pline, network topology, error notification, ordered delivery of frames, and flow control. The IEEE has further segmented this layer into the MAC sublayer and the LLC sublayer. Also known as the link layer. Can be compared somewhat to the data link control layer of the SNA model. See also: Application layer, LLC, MAC, Network layer, Physical layer, Presentation layer, Session layer, and Transport layer.data terminal equipmentSee: DTE.DCCData Country Code: Developed by the ATM Forum, one of two ATM address formats designed for use by private networks. Compare with: ICD.

10089.book  Page 868  Monday, July 23, 2007  3:17 PM




Glossary869DCEData communications equipment (as defined by the EIA) or data circuit-terminating equipment (as defined by the ITU-T): The mechanisms and links of a communications net-work that make up the network portion of the user-to-network interface, such as modems. The DCE supplies the physical connection to the network, forwards traffic, and provides a clocking signal to synchronize data transmission between DTE and DCE devices. Compare with: DTE.D channel(1) Data channel: A full-duplex, 16Kbps (BRI) or 64Kbps (PRI) ISDN channel. Compare with: B channel, E channel, and H channel. (2) In SNA, anything that provides a connection between the processor and main storage with any peripherals.DDPDatagram Delivery Protocol: Used in the AppleTalk suite of protocols as a connection-less protocol that is responsible for sending datagrams through an internetwork.DDRDial-on-demand routing: A technique that allows a router to automatically initiate and end a circuit-switched session per the requirements of the sending station. By mimicking keep-alives, the router fools the end station into treating the session as active. DDR permits routing over ISDN or telephone lines via a modem or external ISDN terminal adapter.DEDiscard Eligibility: Used in Frame Relay networks to tell a switch that a frame can be pref-erentially discarded if the switch is too busy. The DE is a field in the frame that is turned on by transmitting routers if the committed information rate (CIR) is oversubscribed or set to 0.dedicated linePoint-to-point connection that does not share any bandwidth.de-encapsulationThe technique used by layered protocols in which a layer removes header information from the Protocol Data Unit (PDU) from the layer below. See: encapsulation.default routeThe static routing table entry used to direct frames whose next hop is not oth-erwise spelled out in the routing table.delayThe time elapsed between a sender’s initiation of a transaction and the first response they receive. Also, the time needed to move a packet from its source to its destination over a path. See also: latency.demarcThe demarcation point between the customer premises equipment (CPE) and the telco’s carrier equipment.demodulationA series of steps that return a modulated signal to its original form. When receiving, a modem demodulates an analog signal to its original digital form (and, conversely, modulates the digital data it sends into an analog signal). See also: modulation.demultiplexingThe process of converting a multiplexed signal comprising more than one input stream back into separate output streams. See also: multiplexing.designated bridgeIn the process of forwarding a frame from a segment to the root bridge, the bridge with the lowest root path cost.designated portUsed with the Spanning Tree Protocol (STP) to designate forwarding ports. If there are multiple links to the same network, STP will shut a port down to stop network loops.

10089.book  Page 869  Monday, July 23, 2007  3:17 PM




870Glossarydesignated router (DR)An OSPF router that creates LSAs for a multi-access network and is required to perform other special tasks in OSPF operations. Multi-access OSPF networks that maintain a minimum of two attached routers identify one router that is chosen by the OSPF Hello protocol, which makes possible a decrease in the number of adjacencies necessary on a multi-access network. This in turn reduces the quantity of routing protocol traffic and the physical size of the database.desktop layerThe access layer is sometimes referred to as the desktop layer. The access layer controls user and workgroup access to internetwork resources.destination addressThe address for the network device(s) that will receive a packet.DHCPDynamic Host Configuration Protocol: DHCP is a superset of the BootP protocol. This means that it uses the same protocol structure as BootP, but it has enhancements added. Both of these protocols use servers that dynamically configure clients when requested. The two major enhancements are address pools and lease times.dial backupDial backup connections are typically used to provide redundancy to Frame Relay connections. The backup link is activated over an analog modem or ISDN.directed broadcastA data frame or packet that is transmitted to a specific group of nodes on a remote network segment. Directed broadcasts are known by their broadcast address, which is a destination subnet address with all the host bits turned on.discovery modeAlso known as dynamic configuration, this technique is used by an Apple-Talk interface to gain information from a working node about an attached network. The information is subsequently used by the interface for self-configuration.distance-vector protocolsThe distance-vector protocols find the best path to a remote net-work by judging distance. Each time a packet goes through a router, that’s called a hop. The route with the least number of hops to the network is determined to be the best route. How-ever, Cisco’s IGRP is considered distance vector and uses a composite metric of bandwidth and delay of the line to determine the best path to a remote network.distance-vector routing algorithmIn order to find the shortest path, this group of routing algorithms reports on the number of hops in a given route, requiring each router to send its complete routing table with each update, but only to its neighbors. Routing algorithms of this type tend to generate loops, but they are fundamentally simpler than their link-state counter-parts. See also: link-state routing algorithm and SPF.distribution layerMiddle layer of the Cisco three-layer hierarchical model, which helps you design, install, and maintain Cisco hierarchical networks. The distribution layer is the point where access layer devices connect. Routing is performed at this layer.DLCIData Link Connection Identifier: Used to identify virtual circuits in a Frame Relay network.DLSwData Link Switching: IBM developed Data Link Switching (DLSw) in 1992 to provide support for SNA (Systems Network Architecture) and NetBIOS protocols in router-based net-works. SNA and NetBIOS are nonroutable protocols that do not contain any logical layer 3 

10089.book  Page 870  Monday, July 23, 2007  3:17 PM




Glossary871network information. DLSw encapsulates these protocols into TCP/IP messages that can be routed and is an alternative to Remote Source-Route Bridging (RSRB).DLSw+Cisco’s implementation of DLSw. In addition to support for the RFC standards, Cisco added enhancements intended to increase scalability and to improve performance and availability.DNSDomain Name System: Used to resolve hostnames to IP addresses.DSAPDestination Service Access Point: The service access point of a network node, specified in the destination field of a packet. See also: SSAP and SAP.DSRData Set Ready: When a DCE is powered up and ready to run, this EIA/TIA-232 inter-face circuit is also engaged.DSUData service unit: This device is used to adapt the physical interface on a data terminal equipment (DTE) mechanism to a transmission facility such as T1 or E1 and is also responsible for signal timing. It is commonly grouped with the channel service unit and referred to as the CSU/DSU. See also: CSU.DTEData terminal equipment: Any device located at the user end of a user-network interface serving as a destination, a source, or both. DTE includes devices such as multiplexers, routers, protocol translators, and computers. The connection to a data network is made through data communication equipment (DCE) such as a modem, using the clocking signals generated by that device. See also: DCE.DTRData Terminal Ready: An activated EIA/TIA-232 circuit communicating to the DCE the state of preparedness of the DTE to transmit or receive data.DUALDiffusing Update Algorithm: Used in Enhanced IGRP, this convergence algorithm provides loop-free operation throughout an entire route’s computation. DUAL grants routers involved in a topology revision the ability to synchronize simultaneously, while routers unaf-fected by this change are not involved. See also: Enhanced IGRP.DVMRPDistance Vector Multicast Routing Protocol: Based primarily on the Routing Infor-mation Protocol (RIP), this Internet gateway protocol implements a common, condensed-mode IP multicast scheme, using IGMP to transfer routing datagrams between its neighbors. See also: IGMP.DXIData Exchange Interface: DXI defines the effectiveness of a network device such as a router, bridge, or hub to act as an FEP to an ATM network by using a special DSU that accom-plishes packet encapsulation.dynamic entriesUsed in layer 2 and layer 3 devices to dynamically create a table of either hardware addresses or logical addresses dynamically.dynamic routingAlso known as “adaptive routing,” this technique automatically adapts to traffic or physical network revisions.

10089.book  Page 871  Monday, July 23, 2007  3:17 PM




872Glossarydynamic VLANAn administrator will create an entry in a special server with the hardware addresses of all devices on the internetwork. The server will then report the associated VLAN to a switch that requests it based on the new device’s hardware address.E1Generally used in Europe, a wide-area digital transmission scheme carrying data at 2.048Mbps. E1 transmission lines are available for lease from common carriers for private use.E.164(1) Evolved from standard telephone numbering system, the standard recommended by ITU-T for international telecommunication numbering, particularly in ISDN, SMDS, and BISDN. (2) Label of field in an ATM address containing numbers in E.164 format.eBGPExternal Border Gateway Protocol: Used to exchange route information between different autonomous systems.E channelEcho channel: A 64Kbps ISDN control channel used for circuit switching. Specific description of this channel can be found in the 1984 ITU-T ISDN specification, but it was dropped from the 1988 version. See also: B channel, D channel, and H channel.edge deviceA device that enables packets to be forwarded between legacy interfaces (such as Ethernet and Token Ring) and ATM interfaces based on information in the Data Link and Network layers. An edge device does not take part in the running of any Network layer routing protocol; it merely uses the route description protocol in order to get the forwarding information required.EEPROMElectronically erasable programmable read-only memory: Programmed after their manufacture, these nonvolatile memory chips can be erased if necessary using electric power and reprogrammed. See also: EPROM and PROM.EFCIExplicit Forward Congestion Indication: A congestion feedback mode permitted by ABR service in an ATM network. The EFCI may be set by any network element that is in a state of immediate or certain congestion. The destination end system is able to carry out a protocol that adjusts and lowers the cell rate of the connection based on value of the EFCI. See also: ABR.EIGRPSee: Enhanced IGRP.EIPEthernet Interface Processor: A Cisco 7000 series router interface processor card, sup-plying 10Mbps AUI ports to support Ethernet Version 1 and Ethernet Version 2 or IEEE 802.3 interfaces with a high-speed data path to other interface processors.ELANEmulated LAN: An ATM network configured using a client/server model in order to emulate either an Ethernet or Token Ring LAN. Multiple ELANs can exist at the same time on a single ATM network and are made up of a LAN emulation client (LEC), a LAN emulation server (LES), a broadcast and unknown server (BUS), and a LAN emulation configuration server (LECS). ELANs are defined by the LANE specification. See also: LANE, LEC, LECS, and LES.ELAPEtherTalk Link Access Protocol: In an EtherTalk network, the link-access protocol constructed above the standard Ethernet Data Link layer.

10089.book  Page 872  Monday, July 23, 2007  3:17 PM




Glossary873encapsulationThe technique used by layered protocols in which a layer adds header infor-mation to the Protocol Data Unit (PDU) from the layer above. As an example, in Internet ter-minology, a packet would contain a header from the Data Link layer, followed by a header from the Network layer (IP), followed by a header from the Transport layer (TCP), followed by the application protocol data.encryptionThe conversion of information into a scrambled form that effectively disguises it to prevent unauthorized access. Every encryption scheme uses some well-defined algorithm, which is reversed at the receiving end by an opposite algorithm in a process known as decryption.endpointsSee: BGP neighbors.end-to-end VLANsVLANs that span the switch fabric from end to end; all switches in end-to-end VLANs understand about all configured VLANs. End-to-end VLANs are configured to allow membership based on function, project, department, and so on.Enhanced IGRP (EIGRP)Enhanced Interior Gateway Routing Protocol: An advanced routing protocol created by Cisco combining the advantages of link-state and distance-vector protocols. Enhanced IGRP has superior convergence attributes, including high operating efficiency. See also: IGP, OSPF, and RIP.enterprise networkA privately owned and operated network that joins most major locations in a large company or organization.EPROMErasable programmable read-only memory: Programmed after their manufacture, these nonvolatile memory chips can be erased if necessary using high-power light and repro-grammed. See also: EEPROM and PROM.ESFExtended Superframe: Made up of 24 frames with 192 bits each, with the 193rd bit pro-viding other functions including timing. This is an enhanced version of SF. See also: SF.EthernetA baseband LAN specification created by the Xerox Corporation and then improved through joint efforts of Xerox, Digital Equipment Corporation, and Intel. Ethernet is similar to the IEEE 802.3 series standard and, using CSMA/CD, operates over various types of cables at 10Mbps. Also called DIX (Digital/Intel/Xerox) Ethernet. See also: 10BaseT, Fast Ethernet, and IEEE.EtherTalkA data-link product from Apple Computer that permits AppleTalk networks to be connected by Ethernet.excess burst sizeThe amount of traffic by which the user may exceed the committed burst size.excess rateIn ATM networking, traffic exceeding a connection’s insured rate. The excess rate is the maximum rate less the insured rate. Depending on the availability of network resources, excess traffic can be discarded during congestion episodes. Compare with: maximum rate.EXEC sessionCisco term used to describe the command-line interface. The EXEC session exists in user mode and privileged mode.

10089.book  Page 873  Monday, July 23, 2007  3:17 PM




874GlossaryexpansionThe procedure of directing compressed data through an algorithm, restoring information to its original size.expedited deliverySpecified by one protocol layer communicating either with other layers or with the identical protocol layer in a different network device, an option that requires that identified data be processed faster.explorer frameUsed with source route bridging to find the route to the remote bridged net-work before a frame is transmitted.explorer packetAn SNA packet transmitted by a source Token Ring device to find the path through a source-route-bridged network.extended IP access listIP access list that filters the network by logical address, protocol field in the Network layer header, and even the port field in the Transport layer header.extended IPX access listIPX access list that filters the network by logical IPX address, pro-tocol field in the Network layer header, or even socket number in the Transport layer header.Extended SetupUsed in setup mode to configure the router with more detail than Basic Setup mode. Allows multiple-protocol support and interface configuration.external EIGRP routeNormally, the administrative distance of an EIGRP route is 90, but this is true only for what is known as an internal EIGRP route. These are routes originated within a specific autonomous system by EIGRP routers that are members of the same autonomous system. The other type of route is called an external EIGRP route and has an administrative dis-tance of 170, which is not so good. These routes appear within EIGRP route tables courtesy of either manual or automatic redistribution, and they represent networks that originated outside of the EIGRP autonomous system.failure domainThe region in which a failure has occurred in a token ring. When a station gains information that a serious problem, such as a cable break, has occurred with the net-work, it sends a beacon frame that includes the station reporting the failure, its NAUN and everything between. This defines the failure domain. Beaconing then initiates the procedure known as autoreconfiguration. See also: autoreconfiguration and beacon.fallbackIn ATM networks, this mechanism is used for scouting a path if it isn’t possible to locate one using customary methods. The device relaxes requirements for certain characteris-tics, such as delay, in an attempt to find a path that meets a certain set of the most important requirements.Fast EthernetAny Ethernet specification with a speed of 100Mbps. Fast Ethernet is 10 times faster than 10BaseT while retaining qualities such as MAC mechanisms, MTU, and frame format. These similarities make it possible for existing 10BaseT applications and management tools to be used on Fast Ethernet networks. Fast Ethernet is based on an extension of IEEE 802.3 specification (IEEE 802.3u). Compare with: Ethernet. See also: 100BaseT, 100BaseTX, and IEEE.fast switchingA Cisco feature that uses a route cache to speed packet switching through a router. Contrast with: process switching.

10089.book  Page 874  Monday, July 23, 2007  3:17 PM




Glossary875fault toleranceThe extent to which a network device or a communication link can fail without communication being interrupted. Fault tolerance can be provided by added sec-ondary routes to a remote network.FDDIFiber Distributed Data Interface: A LAN standard, defined by ANSI X3T9.5, that can run at speeds up to 200Mbps and uses token-passing media access on fiber-optic cable. For redundancy, FDDI can use a dual-ring architecture.FDMFrequency-Division Multiplexing: A technique that permits information from several channels to be assigned bandwidth on one wire based on frequency. See also: TDM, ATDM, and statistical multiplexing.FECNForward Explicit Congestion Notification: A bit set by a Frame Relay network that informs the DTE receptor that congestion was encountered along the path from source to des-tination. A device receiving frames with the FECN bit set can ask higher-priority protocols to take flow-control action as needed. See also: BECN.FEIPFast Ethernet Interface Processor: An interface processor employed on Cisco 7000 series routers, supporting up to two 100Mbps 100BaseT ports.filteringUsed to provide security on the network with access lists. LAN switches filter the network by MAC (hardware) address.firewallA barrier purposefully erected between any connected public networks and a private network—made up of a router or access server or several routers or access servers—that uses access lists and other methods to ensure the security of the private network.fixed configuration routerA router that cannot be upgraded with any new interfaces.flappingTerm used to describe a serial interface that is going up and down.FlashElectronically erasable programmable read-only memory (EEPROM). Used to hold the Cisco IOS in a router by default.flash memoryDeveloped by Intel and licensed to other semiconductor manufacturers, it is nonvolatile storage that can be erased electronically and reprogrammed, physically located on an EEPROM chip. Flash memory permits software images to be stored, booted, and rewritten as needed. Cisco routers and switches use flash memory to hold the IOS by default. See also: EPROM and EEPROM.flat networkNetwork that is one large collision domain and one large broadcast domain.floating routesUsed with dynamic routing to provide backup routes (static routes) in case of failure.floodingWhen traffic is received on an interface, it is then transmitted to every interface con-nected to that device except the interface from which the traffic originated. This technique can be used for traffic transfer by bridges and switches throughout the network.

10089.book  Page 875  Monday, July 23, 2007  3:17 PM




876Glossaryflow controlA methodology used to ensure that receiving units are not overwhelmed with data from sending devices. Pacing, as it is called in IBM networks, means that when buffers at a receiving unit are full, a message is transmitted to the sending unit to temporarily halt trans-missions until all the data in the receiving buffer has been processed and the buffer is again ready for action.forward/filter decisionsWhen a frame is received on an interface, the switch looks at the destination hardware address and finds the exit interface in the MAC database. The frame is only forwarded out the specified destination port.FQDNFully qualified domain name: Used within the DNS domain structure to provide name-to-IP-address resolution on the Internet. An example of an FQDN is bob.acme.com.FRADFrame Relay access device: Any device affording a connection between a LAN and a Frame Relay WAN. See also: Cisco FRAD and FRAS.fragmentAny portion of a larger packet that has been intentionally segmented into smaller pieces. A packet fragment does not necessarily indicate an error and can be intentional. See also: fragmentation.fragmentationThe process of intentionally segmenting a packet into smaller pieces when sending data over an intermediate network medium that cannot support the larger packet size.FragmentFreeLAN switch type that reads into the data section of a frame to make sure fragmentation did not occur. Sometimes called modified cut-through.frameA logical unit of information sent by the Data Link layer over a transmission medium. The term often refers to the header and trailer, employed for synchronization and error con-trol, that surround the data contained in the unit.frame filteringFrame filtering is used on a layer 2 switch to provide more bandwidth. A switch reads the destination hardware address of a frame and then looks for this address in the filter table, built by the switch. It then sends the frame out only the port where the hardware address is located, and the other ports do not see the frame.frame identification (frame tagging)VLANs can span multiple connected switches, which Cisco calls a switch fabric. Switches within this switch fabric must keep track of frames as they are received on the switch ports, and they must keep track of the VLAN they belong to as the frames traverse this switch fabric. Frame tagging performs this function. Switches can then direct frames to the appropriate port.Frame RelayA more efficient replacement of the X.25 protocol (an unrelated packet relay technology that guarantees data delivery). Frame Relay is an industry-standard, shared-access, best-effort, switched Data Link layer encapsulation that services multiple virtual circuits and protocols between connected mechanisms.Frame Relay bridgingDefined in RFC 1490, this bridging method uses the identical spanning-tree algorithm as other bridging operations but permits packets to be encapsu-lated for transmission across a Frame Relay network.

10089.book  Page 876  Monday, July 23, 2007  3:17 PM




Glossary877Frame Relay switchingPacket switching for Frame Relay packets that is provided by a service provider.frame taggingSee: frame identification.frame typesUsed in LANs to determine how a packet is put on the local network. Ethernet provides four different frame types. These are not compatible with each other, so for two hosts to communicate, they must use the same frame type.framingEncapsulation at the Data Link layer of the OSI model. It is called framing because the packet is encapsulated with both a header and a trailer.FRASFrame Relay Access Support: A feature of Cisco IOS software that enables SDLC, Ethernet, Token Ring, and Frame Relay-attached IBM devices to be linked with other IBM mechanisms on a Frame Relay network. See also: FRAD.frequencyThe number of cycles of an alternating current signal per time unit, measured in hertz (cycles per second).FSIPFast Serial Interface Processor: The Cisco 7000 routers’ default serial interface processor, it provides four or eight high-speed serial ports.FTPFile Transfer Protocol: The TCP/IP protocol used for transmitting files between network nodes, it supports a broad range of file types and is defined in RFC 959. See also: TFTP.full duplexThe capacity to transmit information between a sending station and a receiving unit at the same time. See also: half duplex.full meshA type of network topology where every node has either a physical or a virtual circuit linking it to every other network node. A full mesh supplies a great deal of redundancy but is typ-ically reserved for network backbones because of its expense. See also: partial mesh.global commandCisco term used to define commands that are used to change the router configuration and that affect the whole router. In contrast, an interface command only affects the interface on which it’s configured.GMIIGigabit MII: Media Independent Interface that provides 8 bits at a time of data transfer.GNSGet Nearest Server: On an IPX network, a request packet sent by a customer for deter-mining the location of the nearest active server of a given type. An IPX network client launches a GNS request to get either a direct answer from a connected server or a response from a router disclosing the location of the service on the internetwork to the GNS. GNS is part of IPX and SAP. See also: IPX and SAP.graftingA process that activates an interface that has been deactivated by the pruning process. It is initiated by an IGMP membership report sent to the router.GREGeneric Routing Encapsulation: A tunneling protocol created by Cisco with the capacity for encapsulating a wide variety of protocol packet types inside IP tunnels, thereby generating a virtual point-to-point connection to Cisco routers across an IP network at remote points. IP 

10089.book  Page 877  Monday, July 23, 2007  3:17 PM




878Glossarytunneling using GRE permits network expansion across a single-protocol backbone environ-ment by linking multiprotocol subnetworks in a single-protocol backbone environment.guardbandThe unused frequency area found between two communications channels, furnishing the space necessary to avoid interference between the two.half duplexThe capacity to transfer data in only one direction at a time between a sending unit and receiving unit. See also: full duplex.handshakeAny series of transmissions exchanged between two or more devices on a network to ensure synchronized operations.H channelHigh-speed channel: A full-duplex, ISDN primary rate channel operating at a speed of 384Kbps. See also: B channel, D channel, and E channel.HDLCHigh-Level Data-Link Control: Using frame characters, including checksums, HDLC designates a method for data encapsulation on synchronous serial links and is the default encapsulation for Cisco routers. HDLC is a bit-oriented synchronous Data Link layer protocol created by ISO and derived from SDLC. However, most HDLC vendor implementations (including Cisco’s) are proprietary. See also: SDLC.helper addressThe unicast address specified, which configures the Cisco router to change the client’s local broadcast request for a service into a directed unicast to the server.hierarchical addressingAny addressing plan employing a logical chain of commands to deter-mine location. IP addresses are made up of a hierarchy of network numbers, subnet numbers, and host numbers to direct packets to the appropriate destination.hierarchyTerm used in defining IP addressing; in hierarchical addressing, some bits are used for networking and some bits for host addressing. Also used in the DNS structure and the Cisco design model.HIPHSSI Interface Processor: An interface processor used on Cisco 7000 series routers, pro-viding one HSSI port that supports connections to ATM, SMDS, Frame Relay, or private lines at speeds up to T3 or E3.holddownThe state a route is placed in so that routers can neither advertise the route nor accept advertisements about it for a defined time period. Holddowns are used to avoid accepting bad information. The actual information might be good, but it is not trusted. A route is generally placed in holddown when one of its links fails.hopThe movement of a packet between any two network nodes. See also: hop count.hop countA routing metric that calculates the distance between a source and a destination based on the number of routers in the path. RIP employs hop count as its sole metric. See also: hop and RIP.host addressLogical address configured by an administrator or server on a device. Logically identifies this device on an internetwork.

10089.book  Page 878  Monday, July 23, 2007  3:17 PM




Glossary879Host-to-Host layerLayer in the Internet Protocol suite that is equal to the Transport layer of the OSI model.HSCIHigh-Speed Communication Interface: Developed by Cisco, a single-port interface that provides full-duplex synchronous serial communications capability at speeds up to 52Mbps.HSRPHot Standby Router Protocol: A protocol that provides high network availability and nearly instantaneous hardware fail-over without administrator intervention. It generates a Hot Standby router group, including a lead router that lends its services to any packet being transferred to the Hot Standby address. If the lead router fails, it will be replaced by any of the other routers—the standby routers—that monitor it.HSSIHigh-Speed Serial Interface: A network standard physical connector for high-speed serial linking over a WAN at speeds of up to 52Mbps.hubsPhysical layer devices that are really just multiple port repeaters. When an electronic digital signal is received on a port, the signal is reamplified or regenerated and forwarded out all segments except the segment from which the signal was received.hybrid routing protocolRouting protocol that uses the attributes of both distance-vector and link-state. Enhanced Interior Gateway Routing Protocol (Enhanced IGRP).ICDInternational Code Designator: Adapted from the subnetwork model of addressing, this assigns the mapping of Network layer addresses to ATM addresses. ICD is one of two ATM formats for addressing created by the ATM Forum to be utilized with private networks. See also: DCC.ICMPInternet Control Message Protocol: Documented in RFC 792, it is a Network layer Internet protocol for the purpose of reporting errors and providing information pertinent to IP packet procedures.IEEEInstitute of Electrical and Electronics Engineers: A professional organization that, among other activities, defines standards in a number of fields within computing and electronics, including networking and communications. IEEE standards are the predominant LAN standards used today throughout the industry. Many protocols are commonly known by the reference number of the corresponding IEEE standard.IEEE 802.1The IEEE committee specification that defines the bridging group. The specifica-tion for STP (Spanning Tree Protocol) is IEEE 802.1D. The STP uses STA (spanning-tree algo-rithm) to find and prevent network loops in bridged networks. The specification for VLAN trunking is IEEE 802.1Q.IEEE 802.3The IEEE committee specification that defines the Ethernet group, specifically the original 10Mbps standard. Ethernet is a LAN protocol that specifies physical layer and MAC sublayer media access. IEEE 802.3 uses CSMA/CD to provide access for many devices on the same network. Fast Ethernet is defined as 802.3U, and Gigabit Ethernet is defined as 802.3Q. See also: CSMA/CD.IEEE 802.5IEEE committee that defines Token Ring media access.

10089.book  Page 879  Monday, July 23, 2007  3:17 PM




880GlossaryIGMPInternet Group Management Protocol: Employed by IP hosts, the protocol that reports their multicast group memberships to an adjacent multicast router.IGPInterior gateway protocol: Any protocol used by an internetwork to exchange routing data within an independent system. Examples include RIP, IGRP, and OSPF.IGRPInterior Gateway Routing Protocol: Cisco proprietary distance-vector routing algorithm. Upgrade from the RIP protocol.ILMIIntegrated (or Interim) Local Management Interface. A specification created by the ATM Forum, designated for the incorporation of network-management capability into the ATM UNI. Integrated Local Management Interface cells provide for automatic configuration between ATM systems. In LAN emulation, ILMI can provide sufficient information for the ATM end station to find an LECS. In addition, ILMI provides the ATM NSAP (Network Service Access Point) prefix information to the end station.in-band managementIn-band management is the management of a network device “through” the network. Examples include using Simple Network Management Protocol (SNMP) and Telnet directly via the local LAN. Compare with: out-of-band management.in-band signalingIn-band signaling is the use of the bearer channel to deliver signaling, as call waiting in analog POTS lines. This is as opposed to out-of-band signaling, as in the case of the D channel being used to present a second active call in an ISDN circuit.inside networkIn NAT terminology, the inside network is the set of networks that are sub-ject to translation. The outside network refers to all other addresses—usually those located on the Internetinsured burstIn an ATM network, it is the largest, temporarily permitted data burst exceeding the insured rate on a PVC and not tagged by the traffic policing function for being dropped if network congestion occurs. This insured burst is designated in bytes or cells.interarea routingRouting between two or more logical areas. Contrast with: intra-area routing. See also: area.interface configuration modeMode that allows you to configure a Cisco router or switch port with specific information, such as an IP address and mask.interface processorAny of several processor modules used with Cisco 7000 series routers. See also: AIP, CIP, EIP, FEIP, HIP, MIP, and TRIP.Intermediate System to Intermediate System (IS-IS)Intermediate System–to–Intermediate System: An OSI link-state hierarchical routing protocol.internal EIGRP routeThese are routes originated within a specific autonomous system by EIGRP routers that are members of the same autonomous system.InternetThe global “network of networks,” whose popularity has exploded starting in the mid 1990s. Originally a tool for collaborative academic research, it has become a medium for exchanging and distributing information of all kinds. The Internet’s need to link disparate 

10089.book  Page 880  Monday, July 23, 2007  3:17 PM




Glossary881computer platforms and technologies has led to the development of uniform protocols and standards that have also found widespread use within corporate LANs. See also: TCP/IP and MBONE.internetBefore the rise of the Internet, this lowercase form was shorthand for “internet-work” in the generic sense. Now rarely used. See also: internetwork.Internet layerLayer in the Internet Protocol suite of protocols that provides network addressing and routing through an internetwork.Internet protocol (IP)Any protocol belonging to the TCP/IP protocol stack. See also: TCP/IP.internetworkAny group of networks interconnected by routers and other mechanisms, typically operating as a single entity.internetworkingBroadly, anything associated with the general task of linking networks to each other. The term encompasses technologies, procedures, and products. When you connect networks to a router, you are creating an internetwork.intra-area routingRouting that occurs within a logical area. Contrast with: interarea routing.Inverse ARPInverse Address Resolution Protocol: A technique by which dynamic mappings are constructed in a network, allowing a device such as a router to locate the logical network address and associate it with a permanent virtual circuit (PVC). Commonly used in Frame Relay to determine the far-end node’s TCP/IP address by sending the Inverse ARP request across the local DLCI.IPInternet Protocol: Defined in RFC 791, it is a Network layer protocol that is part of the TCP/IP stack and offers connectionless service. IP furnishes an array of features for addressing, type-of-service specification, fragmentation and reassembly, and security.IP addressOften called an Internet address, this is an address uniquely identifying any device (host) on the Internet (or any TCP/IP network). Each address consists of four octets (32 bits), represented as decimal numbers separated by periods (a format known as “dotted-decimal”). Every address is made up of a network number, an optional subnetwork number, and a host number. The network and subnetwork numbers together are used for routing, while the host number addresses an individual host within the network or subnetwork. The network and subnetwork information is extracted from the IP address using the subnet mask. There are five classes of IP addresses (A–E), in which classes A through C allocate different numbers of bits to the network, subnetwork, and host portions of the address. See also: CIDR, IP, and subnet mask.IPCPIP Control Program: The protocol used to establish and configure IP over PPP. See also: IP and PPP.IP multicastA technique for routing that enables IP traffic to be reproduced from one source to several endpoints or from multiple sources to many destinations. Instead of transmitting one packet to each individual point of destination, one packet is sent to a multicast group specified by only one IP endpoint address for the group.

10089.book  Page 881  Monday, July 23, 2007  3:17 PM




882GlossaryIPXInternetwork Packet eXchange: Network layer protocol (layer 3) used in Novell NetWare networks for transferring information from servers to workstations. Similar to IP and XNS.IPXCPIPX Control Protocol: The protocol used to establish and configure IPX over PPP. See also: IPX and PPP.IPXWANProtocol used for new WAN links to provide and negotiate line options on the link using IPX. After the link is up and the options have been agreed upon by the two end-to-end links, normal IPX transmission begins.ISDNIntegrated Services Digital Network: Offered as a service by telephone companies, a communication protocol that allows telephone networks to carry data, voice, and other digital traffic. See also: BISDN, BRI, and PRI.IS-ISSee: Intermediate System–to–Intermediate System (IS-IS)ISL routingInter-Switch Link routing: A Cisco proprietary method of frame tagging in a switched internetwork. Frame tagging is a way to identify the VLAN membership of a frame as it traverses a switched internetwork.isochronous transmissionAsynchronous data transfer over a synchronous data link, requiring a constant bit rate for reliable transport. Compare with: asynchronous transmission and synchronous transmission.ITU-TInternational Telecommunication Union-Telecommunication Standardization Sector: This is a group of engineers that develops worldwide standards for telecommuni-cations technologies.KerberosAn authentication and encryption method that can be used by Cisco routers to ensure that data cannot be “sniffed” off of the network. Kerberos was developed at MIT and was designed to provide strong security using the Data Encryption Standard (DES) crypto-graphic algorithm.LANLocal area network: Broadly, any network linking two or more computers and related devices within a limited geographical area (up to a few kilometers). LANs are typically high-speed, low-error networks within a company. Cabling and signaling at the Physical and Data Link layers of the OSI are dictated by LAN standards. Ethernet, FDDI, and Token Ring are among the most popular LAN technologies. Compare with: MAN.LANELAN emulation: The technology that allows an ATM network to operate as a LAN backbone. To do so, the ATM network is required to provide multicast and broadcast support, address mapping (MAC-to-ATM), and SVC management, in addition to an operable packet format. Additionally, LANE defines Ethernet and Token Ring ELANs. See also: ELAN.LAN switchA high-speed, multiple-interface transparent bridging mechanism, transmitting packets between segments of data links, usually referred to specifically as an Ethernet switch. LAN switches transfer traffic based on MAC addresses. See also: multilayer switch and store-and-forward packet switching.

10089.book  Page 882  Monday, July 23, 2007  3:17 PM




Glossary883LAPBLink Accessed Procedure, Balanced: A bit-oriented Data Link layer protocol that is part of the X.25 stack and has its origin in SDLC. See also: SDLC and X.25.LAPDLink Access Procedure on the D channel: The ISDN Data Link layer protocol used specif-ically for the D channel and defined by ITU-T Recommendations Q.920 and Q.921. LAPD evolved from LAPB and is created to comply with the signaling requirements of ISDN basic access.latencyBroadly, the time it takes a data packet to get from one location to another. In spe-cific networking contexts, it can mean either (1) the time elapsed (delay) between the execution of a request for access to a network by a device and the time the mechanism actually is per-mitted transmission, or (2) the time elapsed between when a mechanism receives a frame and the time that frame is forwarded out of the destination port.layerTerm used in networking to define how the OSI model works to encapsulate data for transmission on the network.layer 3 switchSee: multilayer switch.layered architectureIndustry standard way of creating applications to work on a network. Layered architecture allows the application developer to make changes in only one layer instead of the whole program.LCPLink Control Protocol: The protocol designed to establish, configure, and test data-link connections for use by PPP. See also: PPP.leaky bucketAn analogy for the generic cell rate algorithm (GCRA) used in ATM networks for checking the conformance of cell flows from a user or network. The bucket’s “hole” is understood to be the prolonged rate at which cells can be accommodated, and the “depth” is the tolerance for cell bursts over a certain time period.learning bridgeA bridge that transparently builds a dynamic database of MAC addresses and the interfaces associated with each address. Transparent bridges help to reduce traffic con-gestion on the network.LE ARPLAN Emulation Address Resolution Protocol: The protocol providing the ATM address that corresponds to a MAC address.leased linePermanent connection between two points leased from the telephone companies.LECLAN emulation client: Software providing the emulation of the link layer interface that allows the operation and communication of all higher-level protocols and applications to con-tinue. The LEC runs in all ATM devices, which include hosts, servers, bridges, and routers. See also: ELAN and LES.LECSLAN emulation configuration server: An important part of emulated LAN services, providing the configuration data that is furnished upon request from the LES. These services include address registration for Integrated Local Management Interface (ILMI) support, con-figuration support for the LES addresses and their corresponding emulated LAN identifiers, and an interface to the emulated LAN. See also: LES and ELAN.

10089.book  Page 883  Monday, July 23, 2007  3:17 PM




884GlossaryLESLAN emulation server: The central LANE component that provides the initial configura-tion data for each connecting LEC. The LES typically is located on either an ATM-integrated router or a switch. Responsibilities of the LES include configuration and support for the LEC, address registration for the LEC, database storage and response concerning ATM addresses, and interfacing to the emulated LAN. See also: ELAN, LEC, and LECS.linkA link is a network or router interface assigned to any given network. When an interface is added to the OSPF process, it’s considered by OSPF to be a link. This link, or interface, will have state information associated with it (up or down) as well as one or more IP addresses.link-state protocolsIn link-state protocols, also called shortest-path-first protocols, the routers each create three separate tables. One of these tables keeps track of directly attached neighbors, one determines the topology of the entire internetwork, and one is used as the routing table. Link-state routers know more about the internetwork than any distance-vector routing protocol.link-state routing algorithmA routing algorithm that allows each router to broadcast or multicast information regarding the cost of reaching all its neighbors to every node in the inter-network. Link-state algorithms provide a consistent view of the network and are therefore not vulnerable to routing loops. However, this loop-free network is achieved at the cost of some-what greater difficulty in computation and more widespread traffic (compared with distance-vector routing algorithms). See also: distance-vector routing algorithm.LLAPLocalTalk Link Access Protocol: In a LocalTalk environment, the data link–level pro-tocol that manages node-to-node delivery of data. This protocol provides node addressing and management of bus access, and it also controls data sending and receiving to ensure packet length and integrity.LLCLogical Link Control: Defined by the IEEE, the higher of two Data Link layer sub-layers. LLC is responsible for error detection (but not correction), flow control, framing, and software-sublayer addressing. The predominant LLC protocol, IEEE 802.2, defines both con-nectionless and connection-oriented operations. See also: Data Link layer and MAC.LMILocal Management Interface: An enhancement to the original Frame Relay specification. Among the features it provides are a keepalive mechanism, a multicast mechanism, global addressing, and a status mechanism.LNNILAN Emulation Network-to-Network Interface: In the Phase 2 LANE specification, an interface that supports communication between the server components within one ELAN.loadLike IGRP, EIGRP uses only bandwidth and delay of the line to determine the best path to a remote network by default. However, EIGRP can use a combination of bandwidth, delay, load, and reliability in its quest to find the best path to a remote network. Load refers to the amount of data on the link.load balancingThe act of balancing packet load over multiple links to the same remote network.local explorer packetIn a Token Ring SRB network, a packet generated by an end system to find a host linked to the local ring. If no local host can be found, the end system will produce one of two solutions: a spanning explorer packet or an all-routes explorer packet.

10089.book  Page 884  Monday, July 23, 2007  3:17 PM




Glossary885local loopConnection from a demarcation point to the closest switching office.LocalTalkUtilizing CSMA/CD, in addition to supporting data transmission at speeds of 230.4Kbps, LocalTalk is Apple Computer’s proprietary baseband protocol, operating at the Data Link and Physical layers of the OSI reference model.logical addressNetwork layer address that defines how data is sent from one network to another. Examples of logical addresses are IP and IPX.loop avoidanceIf multiple connections between switches are created for redundancy pur-poses, network loops can occur. Spanning Tree Protocol (STP) is used to stop network loops while still permitting redundancy.loopback addressThe IP address 127.0.0.1 is called the diagnostic or loopback address, and if you get a successful ping to this address, your IP stack is then considered to be initial-ized. If it fails, then you have an IP stack failure and need to reinstall TCP/IP on the host.loopback interfaceLoopback interfaces are logical interfaces, which means they are not real router interfaces. They can be used for diagnostic purposes as well as OSPF configuration.LPDLine Printer Daemon: Used in the Unix world to allow printing to an IP address.LSALink-State Advertisement: Contained inside of link-state packets (LSPs), these adver-tisements are usually multicast packets, containing information about neighbors and path costs, that are employed by link-state protocols. Receiving routers use LSAs to maintain their link-state databases and, ultimately, routing tables.LUNILAN Emulation User-to-Network Interface: Defining the interface between the LAN emulation client (LEC) and the LAN emulation server (LES), LUNI is the ATM Forum’s standard for LAN emulation on ATM networks. See also: LES and LECS.MACMedia Access Control: The lower sublayer in the Data Link layer, it is responsible for hardware addressing, media access, and error detection of frames. See also: Data Link layer and LLC.MAC addressA Data Link layer hardware address that every port or device needs in order to connect to a LAN segment. These addresses are used by various devices in the network for accurate location of logical addresses. MAC addresses are defined by the IEEE standard and their length is six characters, typically using the burned-in address (BIA) of the local LAN interface. Variously called hardware address, physical address, burned-in address, or MAC layer address.MacIPIn AppleTalk, the Network layer protocol encapsulating IP packets in Datagram Delivery Protocol (DDP) packets. MacIP also supplies substitute ARP services.MANMetropolitan area network: Any network that encompasses a metropolitan area; that is, an area typically larger than a LAN but smaller than a WAN. See also: LAN.Manchester encodingA method for digital coding in which a mid-bit-time transition is employed for clocking, and a 1 (one) is denoted by a high voltage level during the first half of the bit time. This scheme is used by Ethernet and IEEE 802.3.

10089.book  Page 885  Monday, July 23, 2007  3:17 PM




886Glossarymaximum burstSpecified in bytes or cells, the largest burst of information exceeding the insured rate that will be permitted on an ATM permanent virtual connection for a short time and will not be dropped even if it goes over the specified maximum rate. Compare with: insured burst. See also: maximum rate.maximum hop countNumber of routers a packet is allowed to pass before it is terminated. This is created to prevent a packet from circling a network forever.maximum rateThe maximum permitted data throughput on a particular virtual circuit, equal to the total of insured and uninsured traffic from the traffic source. Should traffic con-gestion occur, uninsured information may be deleted from the path. Measured in bits or cells per second, the maximum rate represents the highest throughput of data the virtual circuit is ever able to deliver and cannot exceed the media rate. Compare with: excess rate. See also: maximum burst.MBONEThe multicast backbone of the Internet, it is a virtual multicast network made up of multicast LANs, including point-to-point tunnels interconnecting them.MBSMaximum Burst Size: In an ATM signaling message, this metric, coded as a number of cells, is used to convey the burst tolerance.MCDVMaximum Cell Delay Variation: The maximum two-point CDV objective across a link or node for the identified service category in an ATM network.MCLRMaximum Cell Loss Ratio: The maximum ratio of cells in an ATM network that fail to transit a link or node compared with the total number of cells that arrive at the link or node. MCLR is one of four link metrics that are exchanged using PTSPs to verify the available resources of an ATM network. The MCLR applies to cells in VBR and CBR traffic classes whose CLP bit is set to zero. See also: CBR, CLP, and VBR.MCRMinimum cell rate: A parameter determined by the ATM Forum for traffic manage-ment of the ATM networks. MCR is specifically defined for ABR transmissions and specifies the minimum value for the allowed cell rate (ACR). See also: ACR and PCR.MCTDMaximum Cell Transfer Delay: In an ATM network, the total of the maximum cell delay variation and the fixed delay across the link or node. MCTD is one of four link metrics that are exchanged using PNNI topology state packets to verify the available resources of an ATM network. There is one MCTD value assigned to each traffic class. See also: MCDV.media translationA router property that allows two different types of LAN to communicate—for example, Ethernet to Token Ring.MIBManagement Information Base: Used with SNMP management software to gather information from remote devices. The management station can poll the remote device for information, or the MIB running on the remote station can be programmed to send informa-tion on a regular basis.MIIMedia Independent Interface: Used in Fast Ethernet and Gigabit Ethernet to provide faster bit transfer rates of 4 and 8 bits at a time. Contrast to AUI interface, which is 1 bit at a time.

10089.book  Page 886  Monday, July 23, 2007  3:17 PM




Glossary887MIPMultichannel Interface Processor: The resident interface processor on Cisco 7000 series routers, providing up to two channelized T1 or E1 connections by serial cables connected to a CSU. The two controllers are capable of providing 24 T1 or 30 E1 channel groups, with each group being introduced to the system as a serial interface that can be configured individually.mipsMillions of instructions per second: A measure of processor speed.MLPMultilink PPP: A technique used to split, recombine, and sequence datagrams across numerous logical data links.MMPMultichassis Multilink PPP: A protocol that supplies MLP support across multiple routers and access servers. MMP enables several routers and access servers to work as a single, large dial-up pool with one network address and ISDN access number. MMP successfully sup-ports packet fragmenting and reassembly when the user connection is split between two phys-ical access devices.modemModulator-demodulator: A device that converts digital signals to analog and vice versa so that digital information can be transmitted over analog communication facilities such as voice-grade telephone lines. This is achieved by converting digital signals at the source to analog for transmission and reconverting the analog signals back into digital form at the des-tination. See also: modulation and demodulation.modem eliminatorA mechanism that makes possible a connection between two DTE devices without modems by simulating the commands and physical signaling required.modulationThe process of modifying some characteristic of an electrical signal, such as amplitude (AM) or frequency (FM), in order to represent digital or analog information. See also: AM.MOSPFMulticast OSPF: An extension of the OSPF unicast protocol that enables IP multi-cast routing within the domain. See also: OSPF.MPOAMultiprotocol over ATM: An effort by the ATM Forum to standardize how existing and future Network layer protocols such as IP, IPv6, AppleTalk, and IPX run over an ATM network with directly attached hosts, routers, and multilayer LAN switches.MTUMaximum transmission unit: The largest packet size, measured in bytes, that an inter-face can handle.multicastBroadly, any communication between a single sender and multiple receivers. Unlike broadcast messages, which are sent to all addresses on a network, multicast messages are sent to a defined subset of the network addresses; this subset has a group multicast address, which is specified in the packet’s destination address field. See also: broadcast and directed broadcast.multicast addressA single address that points to more than one device on the network by specifying a special nonexistent MAC address transmitted in that particular multicast pro-tocol. Identical to group address. See also: multicast.multicast groupMulticast works by sending messages or data to IP multicast group addresses. The group is a defined set of users or hosts that are allowed to read or view the data sent via multicast.

10089.book  Page 887  Monday, July 23, 2007  3:17 PM




888Glossarymulticast send VCCA two-directional point-to-point virtual control connection (VCC) arranged by an LEC to a BUS, it is one of the three types of informational links specified by phase 1 LANE. See also: control distribute VCC and control direct VCC.multilayer switchA highly specialized, high-speed, hardware-based type of LAN router, the device filters and forwards packets based on their layer 2 MAC addresses and layer 3 network addresses. It’s possible that even layer 4 can be read. Sometimes called a layer 3 switch. See also: LAN switch.multilinkUsed to combine multiple async or ISDN links to provide combined bandwidth.multiplexingThe process of converting several logical signals into a single physical signal for transmission across one physical channel. Contrast with: demultiplexing.NAKNegative acknowledgment: A response sent from a receiver, telling the sender that the information was not received or contained errors. Compare with: acknowledgment.named access listUsed in both standard and extended lists to help with administration of access lists by allowing you to name the lists instead of using numbers. This also allows you to change a single line of an access list, which isn’t possible in regular, numbered access lists.NATNetwork Address Translation: An algorithm instrumental in minimizing the require-ment for globally unique IP addresses, permitting an organization whose addresses are not all globally unique to connect to the Internet nevertheless by translating those addresses into glo-bally routable address space.native VLANCisco switches all have a native VLAN called VLAN 1. This cannot be deleted or changed in any way. All switch ports are in VLAN 1 by default.NBPName Binding Protocol: In AppleTalk, the transport-level protocol that interprets a socket client’s name, entered as a character string, into the corresponding DDP address. NBP gives AppleTalk protocols the capacity to discern user-defined zones and names of mecha-nisms by showing and keeping translation tables that map names to their corresponding socket addresses.neighboring routersTwo routers in OSPF that have interfaces to a common network. On networks with multi-access, these neighboring routers are dynamically discovered using the Hello protocol of OSPF.neighborsEIGRP and OSPF routers become neighbors when each router sees the other’s Hello packets.neighborship tableIn OSPF and EIGRP routing protocols, each router keeps state informa-tion about adjacent neighbors. When newly discovered neighbors are learned, the address and interface of the neighbor is recorded. This information is stored in the neighbor data structure and the neighbor table holds these entries. Neighborship table can also be referred to as neighbor table or neighborship database.

10089.book  Page 888  Monday, July 23, 2007  3:17 PM




Glossary889NetBEUINetBIOS Extended User Interface: An improved version of the NetBIOS protocol used in a number of network operating systems including LAN Manager, Windows NT, LAN Server, and Windows for Workgroups, implementing the OSI LLC2 protocol. NetBEUI formal-izes the transport frame not standardized in NetBIOS and adds more functions. See also: OSI.NetBIOSNetwork Basic Input/Output System: The API employed by applications residing on an IBM LAN to ask for services, such as session termination or information transfer, from lower-level network processes.NetViewA mainframe network product from IBM used for monitoring SNA (Systems Network Architecture) networks. It runs as a VTAM (Virtual Telecommunications Access Method) application.NetWareA widely used NOS created by Novell, providing a number of distributed network services and remote file access.Network Access layerBottom layer in the Internet Protocol suite that provides media access to packets.network addressUsed with the logical network addresses to identify the network segment in an internetwork. Logical addresses are hierarchical in nature and have at least two parts: network and host. An example of a hierarchical address is 172.16.10.5, where 172.16 is the network and 10.5 is the host address.network control protocolA method of establishing and configuring different Network layer protocols. NCP is designed to allow the simultaneous use of multiple Network layer pro-tocols. Some examples of protocols here are IPCP (Internet Protocol Control Protocol) and IPXCP (Internetwork Packet Exchange Control Protocol).Network layerIn the OSI reference model, it is layer 3—the layer in which routing is imple-mented, enabling connections and path selection between two end systems. See also: Application layer, Data Link layer, Physical layer, Presentation layer, Session layer, and Transport layer.network segmentationBreaking up a large network into smaller networks. Routers, switches, and bridges are used to create network segmentation.NFSNetwork File System: One of the protocols in Sun Microsystems’s widely used file system protocol suite, allowing remote file access across a network. The name is loosely used to refer to the entire Sun protocol suite, which also includes RPC, XDR (External Data Rep-resentation), and other protocols.NHRPNext Hop Resolution Protocol: In a nonbroadcast multi-access (NBMA) network, the protocol employed by routers in order to dynamically locate MAC addresses of various hosts and routers. It enables systems to communicate directly without requiring an intermediate hop, thus facilitating increased performance in ATM, Frame Relay, X.25, and SMDS systems.NHSNext Hop Server: Defined by the NHRP protocol, this server maintains the next-hop resolution cache tables, listing IP-to-ATM address maps of related nodes and nodes that can be reached through routers served by the NHS.

10089.book  Page 889  Monday, July 23, 2007  3:17 PM




890GlossarynibbleFour bits.NICNetwork interface card: An electronic circuit board placed in a computer. The NIC pro-vides network communication to a LAN.NLSPNetWare Link Services Protocol: Novell’s link-state routing protocol, based on the IS-IS model.NMPNetwork Management Processor: A Catalyst 5000 switch processor module used to control and monitor the switch.node addressUsed to identify a specific device in an internetwork. Can be a hardware address, which is burned into the network interface card, or a logical network address, which an administrator or server assigns to the node.non-broadcast multi-access (NBMA) networksNon-broadcast multi-access (NBMA) net-works are types such as Frame Relay, X.25, and Asynchronous Transfer Mode (ATM). These networks allow for multi-access, but have no broadcast ability like Ethernet. So, NBMA net-works require special OSPF configuration to function properly and neighbor relationships must be defined.non-designated portA switch port that will not forward frames in order to prevent a switching loop. Spanning Tree Protocol (STP) is responsible for deciding whether a port is designated (forwarding) or non-designated (blocking).non-stub areaIn OSPF, a resource-consuming area carrying a default route, intra-area routes, interarea routes, static routes, and external routes. Non-stub areas are the only areas that can have virtual links configured across them and exclusively contain an autonomous system border router (ASBR). Compare with: stub area. See also: ASBR and OSPF.NRZNonreturn to zero: One of several encoding schemes for transmitting digital data. NRZ signals sustain constant levels of voltage with no signal shifting (no return to zero-voltage level) during a bit interval. If there is a series of bits with the same value (1 or 0), there will be no state change. The signal is not self-clocking. See also: NRZI.NRZINonreturn to zero inverted: One of several encoding schemes for transmitting digital data. A transition in voltage level (either from high to low or vice versa) at the beginning of a bit interval is interpreted as a value of 1; the absence of a transition is interpreted as a 0. Thus, the voltage assigned to each value is continually inverted. NRZI signals are not self-clocking. See also: NRZ.NTNetwork termination: A point in an ISDN network. See: NT1 and NT2.NT1NT1 is the device that converts the two-wire “U” interface to the four-wire “S/T.”NT2NT2 is an ISDN-compliant switching device, like a PBX, that splits the “S/T” bus into two separate, but electrically equivalent, interfaces. The “T” interface connects to the NT1, while the “S” interface connects to TE1 devices.NVRAMNonvolatile RAM: Random-access memory that keeps its contents intact while power is turned off.

10089.book  Page 890  Monday, July 23, 2007  3:17 PM




Glossary891OCOptical Carrier: A series of physical protocols, designated as OC-1, OC-2, OC-3, and so on, for SONET optical signal transmissions. OC signal levels place STS frames on a multi-mode fiber-optic line at various speeds, of which 51.84Mbps is the lowest (OC-1). Each sub-sequent protocol runs at a speed divisible by 51.84. See also: SONET.octetBase-8 numbering system used to identify a section of a dotted decimal IP address. Also referred to as a byte.ones densityAlso known as pulse density, this is a method of signal clocking. The CSU/DSU retrieves the clocking information from data that passes through it. For this scheme to work, the data needs to be encoded to contain at least one binary 1 for each 8 bits transmitted. See also: CSU and DSU.OSIOpen Systems Interconnection: International standardization program designed by ISO and ITU-T for the development of data networking standards that make multivendor equip-ment interoperability a reality.OSI reference modelOpen Systems Interconnection reference model: A conceptual model defined by the International Organization for Standardization (ISO), describing how any com-bination of devices can be connected for the purpose of communication. The OSI model divides the task into seven functional layers, forming a hierarchy with the applications at the top and the physical medium at the bottom, and it defines the functions each layer must pro-vide. See also: Application layer, Data Link layer, Network layer, Physical layer, Presentation layer, Session layer, and Transport layer.OSPFOpen Shortest Path First: A link-state, hierarchical routing algorithm derived from an earlier version of the IS-IS protocol, whose features include multipath routing, load balancing, and least-cost routing. OSPF is the suggested successor to RIP in the Internet environment. See also: Enhanced IGRP, IGP, and IP.OSPF areaAn OSPF area is a grouping of contiguous networks and routers. All routers in the same area share a common Area ID. Because a router can be a member of more than one area at a time, the Area ID is associated with specific interfaces on the router. This would allow some interfaces to belong to area 1, while the remaining interfaces can belong to area 0. All of the routers within the same area have the same topology table.OUIOrganizationally unique identifier: Code assigned by the IEEE to an organization that makes network interface cards. The organization then puts this OUI on each and every card it manufactures. The OUI is 3 bytes (24 bits) long. The manufacturer then adds a 3-byte iden-tifier to uniquely identify the host. The total length of the address is 48 bits (6 bytes) and is called a hardware address or MAC address.out-of-band managementManagement “outside” of the network’s physical channels—for example, using a console connection not directly interfaced through the local LAN or WAN or a dial-in modem. Compare to: in-band management.out-of-band signalingWithin a network, any transmission that uses physical channels or frequencies separate from those ordinarily used for data transfer.

10089.book  Page 891  Monday, July 23, 2007  3:17 PM




892Glossaryoutside networkIn NAT terminology, the inside network is the set of networks that are subject to translation. The outside network refers to all other addresses—usually those located on the Internet.packetIn data communications, the basic logical unit of information transferred. A packet consists of a certain number of data bytes, wrapped or encapsulated in headers and/or trailers that contain information about where the packet came from, where it’s going, and so on. The various protocols involved in sending a transmission add their own layers of header informa-tion, which the corresponding protocols in receiving devices then interpret.packet switchA physical device that makes it possible for a communication channel to share several connections; its functions include finding the most efficient transmission path for packets.packet switchingA networking technology based on the transmission of data in packets. Dividing a continuous stream of data into small units—packets—enables data from multiple devices on a network to share the same communication channel simultaneously but also requires the use of precise routing information.PAPPassword Authentication Protocol: In Point-to-Point Protocol (PPP) networks, a method of validating connection requests. The requesting (remote) device must send an authentication request, containing a password and ID, to the local router when attempting to connect. Unlike the more secure CHAP (Challenge Handshake Authentication Protocol), PAP sends the password unencrypted and does not attempt to verify whether the user is authorized to access the requested resource; it merely identifies the remote end. See also: CHAP.parity checkingA method of error checking in data transmissions. An extra bit (the parity bit) is added to each character or data word so that the sum of the bits will be either an odd number (in odd parity) or an even number (even parity).partial meshA type of network topology in which some network nodes form a full mesh (where every node has either a physical or a virtual circuit linking it to every other network node), but others are attached to only one or two nodes in the network. A typical use of partial-mesh topology is in peripheral networks linked to a fully meshed backbone. See also: full mesh.passive stateRegarding an EIGRP routing table, a route is considered to be in the passive state when a router is not performing a route convergence.PATPort Address Translation: This process allows a single IP address to represent multiple resources by altering the source TCP or UDP port number.PCMPulse code modulation: Process by which an analog signal is converted into digital information.PCRPeak cell rate: As defined by the ATM Forum, the parameter specifying, in cells per second, the maximum rate at which a source may transmit.PDNPublic data network: Generally for a fee, a PDN offers the public access to a computer communication network operated by private concerns or government agencies. Small organi-zations can take advantage of PDNs, aiding them to create WANs without investing in long-distance equipment and circuitry.

10089.book  Page 892  Monday, July 23, 2007  3:17 PM




Glossary893PDUProtocol Data Unit: The processes at each layer of the OSI model. PDUs at the Trans-port layer are called segments; PDUs at the Network layer are called packets or datagrams; and PDUs at the Data Link layer are called frames. The Physical layer uses bits.PGPPretty Good Privacy: A popular public-key/private-key encryption application offering protected transfer of files and messages.phantom routerUsed in a Hot Standby Routing Protocol (HSRP) network to provide an IP default gateway address to hosts.Physical layerThe lowest layer—layer 1—in the OSI reference model, it is responsible for converting data frames from the Data Link layer (layer 2) into electrical signals. Physical layer protocols and standards define, for example, the type of cable and connectors to be used, including their pin assignments and the encoding scheme for signaling 0 and 1 values. See also: Application layer, Data Link layer, Network layer, Presentation layer, Session layer, and Transport layer.PIMProtocol Independent Multicast: A multicast protocol that handles the IGMP requests as well as requests for multicast data forwarding.PIM-DMProtocol Independent Multicast Dense Mode: PIM-DM utilizes the unicast route table and relies on the source root distribution architecture for multicast data forwarding.PIM-SMProtocol Independent Multicast Sparse Mode: PIM-SM utilizes the unicast route table and relies on the shared root distribution architecture for multicast data forwarding.PingPacket Internet Groper: A Unix-based Internet diagnostic tool consisting of a message sent to test the accessibility of a particular device on the IP network. The term’s acronym reflects the underlying metaphor of submarine sonar. Just as the sonar operator sends out a signal and waits to hear it echo (“ping”) back from a submerged object, the network user can ping another node on the network and wait to see if it responds.pinhole congestionA problem associated with distance-vector routing protocols if more than one connection to a remote network is known, but they are different bandwidths.plesiochronousNearly synchronous, except that clocking comes from an outside source instead of being embedded within the signal as in synchronous transmissions.PLPPacket Level Protocol: Occasionally called X.25 level 3 or X.25 Protocol, a Network layer protocol that is part of the X.25 stack.PNNIPrivate Network-Network Interface: An ATM Forum specification for offering topology data used for the calculation of paths through the network, among switches and groups of switches. It is based on well-known link-state routing procedures and allows for automatic configuration in networks whose addressing scheme is determined by the topology.point-to-multipoint connectionIn ATM, a communication path going only one way, con-necting a single system at the starting point, called the “root node,” to systems at multiple points of destination, called “leaves.” See also: point-to-point connection.

10089.book  Page 893  Monday, July 23, 2007  3:17 PM




894Glossarypoint-to-point connectionIn ATM, a channel of communication that can be directed either one way or two ways between two ATM end systems. Also refers to a point-to-point WAN serial connection. See also: point-to-multipoint connection.poison reverse updatesThese update messages are transmitted by a router back to the orig-inator (thus ignoring the split-horizon rule) after route poisoning has occurred. Typically used with DV routing protocols in order to overcome large routing loops and offer explicit infor-mation when a subnet or network is not accessible (instead of merely suggesting that the net-work is unreachable by not including it in updates). See also: route poisoning.pollingThe procedure of orderly inquiry used by a primary network mechanism to deter-mine if secondary devices have data to transmit. A message is sent to each secondary, granting the secondary the right to transmit.POP(1) Point of presence: The physical location where an interexchange carrier has placed equipment to interconnect with a local exchange carrier. (2) Post Office Protocol: A protocol used by client e-mail applications for recovery of mail from a mail server.port securityUsed with layer 2 switches to provide some security. Not typically used in pro-duction because it is difficult to manage. Allows only certain frames to traverse administrator-assigned segments.port numbersUsed at the transport layer with TCP and UDP to keep track of host-to-host virtual circuits.positive acknowledgment with retransmissionA connection-oriented session that provides acknowledgment and retransmission of the data if it is not acknowledged by the receiving host within a certain time frame.POTSPlain old telephone service: This refers to the traditional analog phone service that is found in most installations.PPPPoint-to-Point Protocol: The protocol most commonly used for dial-up Internet access, superseding the earlier SLIP. Its features include address notification, authentication via CHAP or PAP, support for multiple protocols, and link monitoring. PPP has two layers: the Link Control Protocol (LCP) establishes, configures, and tests a link; and then any of various Network Control Protocols (NCPs) transport traffic for a specific protocol suite, such as IPX. See also: CHAP, PAP, and SLIP.prefix routingMethod of defining how many bits are used in a subnet and how this infor-mation is sent in a routing update. For example, RIP version 1 does not send subnet mask information in the route updates. However, RIP version 2 does. This means that RIP v2 updates will send /24, /25, /26, etc., with a route update, which RIP v1 will not.Presentation layerLayer 6 of the OSI reference model, it defines how data is formatted, pre-sented, encoded, and converted for use by software at the Application layer. See also: Applica-tion layer, Data Link layer, Network layer, Physical layer, Session layer, and Transport layer.PRIPrimary Rate Interface: A type of ISDN connection between a PBX and a long-distance carrier, which is made up of a single 64Kbps D channel in addition to 23 (T1) or 30 (E1) B channels. See also: ISDN.

10089.book  Page 894  Monday, July 23, 2007  3:17 PM




Glossary895priority queuingA routing function in which frames temporarily placed in an interface output queue are assigned priorities based on traits such as packet size or type of interface.privileged modeCommand-line EXEC mode used in Cisco routers and switches that provides both viewing and changing of configurations.Process/Application layerUpper layer in the Internet Protocol stack. Responsible for network services.process switchingAs a packet arrives on a router to be forwarded, it’s copied to the router’s process buffer, and the router performs a lookup on the layer 3 address. Using the route table, an exit interface is associated with the destination address. The processor forwards the packet with the added new information to the exit interface, while the router initializes the fast-switching cache. Subsequent packets bound for the same destination address follow the same path as the first packet.PROMProgrammable read-only memory: ROM that is programmable only once, using special equipment. Compare with: EPROM.propagation delayThe time it takes data to traverse a network from its source to its destination.protocolIn networking, the specification of a set of rules for a particular type of communi-cation. The term is also used to refer to the software that implements a protocol.protocol-dependent modulesThe protocol-dependent modules, used in the EIGRP routing protocol, are responsible for network layer, protocol-specific requirements that allow multiple protocol support for IP, IPX and AppleTalk.protocol stackA collection of related protocols.Proxy Address Resolution ProtocolProxy ARP: Used to allow redundancy in case of a failure with the configured default gateway on a host. Proxy ARP is a variation of the ARP protocol in which an intermediate device, such as a router, sends an ARP response on behalf of an end node to the requesting host.pruningThe act of trimming down the shortest-path tree. This deactivates interfaces that do not have group participants.PSEPacket switching exchange: The X.25 term for a switch.PSNPacket-switched network: Any network that uses packet-switching technology. Also known as packet-switched data network (PSDN). See also: packet switching.PSTNPublic switched telephone network: Colloquially referred to as “plain old telephone service” (POTS). A term that describes the assortment of telephone networks and services available globally.PVCPermanent virtual circuit: In a Frame Relay or ATM network, a logical connection, defined in software, that is maintained permanently. Compare with: SVC. See also: virtual circuit.PVPPermanent virtual path: A virtual path made up of PVCs. See also: PVC.

10089.book  Page 895  Monday, July 23, 2007  3:17 PM




896GlossaryPVP tunnelingPermanent virtual path tunneling: A technique that links two private ATM networks across a public network using a virtual path, wherein the public network transpar-ently trunks the complete collection of virtual channels in the virtual path between the two private networks.QoSQuality of service: A set of metrics used to measure the quality of transmission and service availability of any given transmission system.queueBroadly, any list of elements arranged in an orderly fashion and ready for processing, such as a line of people waiting to enter a movie theater. In routing, it refers to a backlog of information packets waiting in line to be transmitted over a router interface.R reference pointUsed with ISDN networks to identify the connection between an NT1 and an S/T device. The S/T device converts the four-wire network to the two-wire ISDN stan-dard network.RADIUSRemote Authentication Dial-In User Service: A protocol that is used to communi-cate between the remote access device and an authentication server. Sometimes an authenti-cation server running RADIUS will be called a RADIUS server.RAMRandom-access memory: Used by all computers to store information. Cisco routers use RAM to store packet buffers and routing tables, along with the hardware addresses cache.RARPReverse Address Resolution Protocol: The protocol within the TCP/IP stack that maps MAC addresses to IP addresses. See also: ARP.RARP serverA Reverse Address Resolution Protocol server is used to provide an IP address from a known MAC address.rate queueA value, assigned to one or more virtual circuits, that specifies the speed at which an individual virtual circuit will transmit data to the remote end. Every rate queue identifies a segment of the total bandwidth available on an ATM link. The sum of all rate queues should not exceed the total available bandwidth.RCPRemote Copy Protocol: A protocol for copying files to or from a file system that resides on a remote server on a network, using TCP to guarantee reliable data delivery.redundancyIn internetworking, the duplication of connections, devices, or services that can be used as a backup in the event that the primary connections, devices, or services fail.reference modelUsed by application developers to create applications that work on any type of network. The most popular reference model is the Open Systems Interconnection (OSI) model.reliabilityLike IGRP, EIGRP uses only bandwidth and delay of the line to determine the best path to a remote network by default. However, EIGRP can use a combination of bandwidth, delay, load and reliability in its quest to find the best path to a remote network. Reliability refers to the reliability of the link to each remote network.reliable multicastWhen EIGRP sends multicast traffic it uses the Class D address 224.0.0.10. As I said, each EIGRP router is aware of who its neighbors are, and for each multicast it sends out, it 

10089.book  Page 896  Monday, July 23, 2007  3:17 PM




Glossary897maintains a list of the neighbors who have replied. If EIGRP doesn’t get a reply from a neighbor, it will switch to using unicasts to resend the same data. If it still doesn’t get a reply after 16 unicast attempts, the neighbor is declared dead. People often refer to this process as reliable multicast.Reliable Transport Protocol (RTP)The reliable transport protocol, used in the EIGRP routing protocol, is responsible for guaranteed, ordered delivery of EIGRP packets to all neighbors.reloadAn event or command that causes Cisco routers to reboot.RIFRouting Information Field: In source-route bridging, a header field that defines the path direction of the frame or token. If the Route Information Indicator (RII) bit is not set, the RIF is read from source to destination (left to right). If the RII bit is set, the RIF is read from the destination back to the source, so the RIF is read right to left. It is defined as part of the token ring frame header for source-routed frames, which contains path information.ringTwo or more stations connected in a logical circular topology. In this topology, which is the basis for Token Ring, FDDI, and CDDI, information is transferred from station to station in sequence.ring topologyA network logical topology comprising a series of repeaters that form one closed loop by connecting unidirectional transmission links. Individual stations on the net-work are connected to the network at a repeater. Physically, ring topologies are generally organized in a closed-loop star. Compare with: bus topology and star topology.RIPRouting Information Protocol: The most commonly used interior gateway protocol in the Internet. RIP employs hop count as a routing metric. See also: Enhanced IGRP, IGP, OSPF, and hop count.RJ connectorRegistered jack connector: Used with twisted-pair wiring to connect the copper wire to network interface cards, switches, and hubs.rolled cableType of wiring cable that is used to connect a PC’s COM port to a router or switch console port.ROMRead-only memory: Chip used in computers to help boot the device. Cisco routers use a ROM chip to load the bootstrap, which runs a power-on self-test, and then find and load the IOS in flash memory by default.root bridgeUsed with Spanning Tree Protocol to stop network loops from occurring. The root bridge is elected by having the lowest bridge ID. The bridge ID is determined by the priority (32,768 by default on all bridges and switches) and the main hardware address of the device.route flapA route that is being announced in an up/down fashion.route poisoningUsed by various DV routing protocols in order to overcome large routing loops and offer explicit information about when a subnet or network is not accessible (instead of merely suggesting that the network is unreachable by not including it in updates). Typically, this is accomplished by setting the hop count to one more than maximum. See also: poison reverse updates.

10089.book  Page 897  Monday, July 23, 2007  3:17 PM




898Glossaryroute summarizationIn various routing protocols, such as OSPF, EIGRP, and IS-IS, the consolidation of publicized subnetwork addresses so that a single summary route is advertised to other areas by an area border router.routed protocolRouted protocols (such as IP and IPX) are used to transmit user data through an internetwork. By contrast, routing protocols (such as RIP, IGRP, and OSPF) are used to update routing tables between routers.routerA Network layer mechanism, either software or hardware, using one or more metrics to decide on the best path to use for transmission of network traffic. Sending packets between networks by routers is based on the information provided on Network layers. Historically, this device has sometimes been called a gateway.Router ID (RID)The Router ID (RID) is an IP address used to identify the router. Cisco chooses the Router ID by using the highest IP address of all configured loopback interfaces. If no loopback interfaces are configured with addresses, OSPF will choose the highest IP address of all active physical interfaces.routingThe process of forwarding logically addressed packets from their local subnetwork toward their ultimate destination. In large networks, the numerous intermediary destinations a packet might travel before reaching its destination can make routing very complex.routing domainAny collection of end systems and intermediate systems that operate under an identical set of administrative rules. Every routing domain contains one or several areas, all individually given a certain area address.routing metricAny value that is used by routing algorithms to determine whether one route is superior to another. Metrics include such information as bandwidth, delay, hop count, path cost, load, MTU, reliability, and communication cost. Only the best possible routes are stored in the routing table, while all other information may be stored in link-state or topological data-bases. See also: cost.routing protocolAny protocol that defines algorithms to be used for updating routing tables between routers. Examples include IGRP, RIP, and OSPF.routing tableA table kept in a router or other internetworking mechanism that maintains a record of only the best possible routes to certain network destinations and the metrics associ-ated with those routes.RPRoute processor: Also known as a supervisory processor; a module on Cisco 7000 series routers that holds the CPU, system software, and most of the memory components used in the router.RSPRoute/Switch Processor: A processor module combining the functions of RP and SP used in Cisco 7500 series routers. See also: RP and SP.RTSRequest To Send: An EIA/TIA-232 control signal requesting permission to transmit data on a communication line.

10089.book  Page 898  Monday, July 23, 2007  3:17 PM




Glossary899S reference pointISDN reference point that works with a T reference point to convert a four-wire ISDN network to the two-wire ISDN network needed to communicate with the ISDN switches at the network provider.sampling rateThe rate at which samples of a specific waveform amplitude are collected within a specified period of time.SAP(1) Service Access Point: A field specified by IEEE 802.2 that is part of an address specification. (2) Service Advertising Protocol: The Novell NetWare protocol that supplies a way to inform network clients of resources and services availability on network, using routers and servers. See also: IPX.SCRSustainable cell rate: An ATM Forum parameter used for traffic management, it is the long-term average cell rate for VBR connections that can be transmitted.SDHSynchronous Digital Hierarchy: One of the standards developed for Fiber Optics Transmission Systems (FOTS).SDLCSynchronous Data Link Control: A protocol used in SNA Data Link layer communi-cations. SDLC is a bit-oriented, full-duplex serial protocol that is the basis for several similar protocols, including HDLC and LAPB. See also: HDLC and LAPB.seed routerIn an AppleTalk network, the router that is equipped with the network number or cable range in its port descriptor. The seed router specifies the network number or cable range for other routers in that network section and answers to configuration requests from nonseed routers on its connected AppleTalk network, permitting those routers to affirm or modify their configurations accordingly. Every AppleTalk network needs at least one seed router physically connected to each network segment.sequencingUsed in virtual circuits and segmentation to number segments so they can be put back together again in the correct order.serial transmissionWAN serial connectors use serial transmission, which takes place one bit at a time, over a single channel.serverHardware and software that provide network services to clients.Session layerLayer 5 of the OSI reference model, responsible for creating, managing, and terminating sessions between applications and overseeing dataexchange between presentation layer entities. See also: Application layer, Data Link layer, Network layer, Physical layer, Presentation layer, and Transport layer.set-basedSet-based routers and switches use the set command to configure devices. Cisco is moving away from set-based commands and is using the command-line interface (CLI) on all new devices.setup modeMode that a router will enter if no configuration is found in nonvolatile RAM when the router boots. Allows the administrator to configure a router step-by-step. Not as robust or flexible as the command-line interface.

10089.book  Page 899  Monday, July 23, 2007  3:17 PM




900GlossarySFA super frame (also called a D4 frame) consists of 12 frames with 192 bits each, and the 193rd bit providing other functions including error checking. SF is frequently used on T1 circuits. A newer version of the technology is Extended Super Frame (ESF), which uses 24 frames. See also: ESF.shared treeA method of multicast data forwarding. Shared trees use an architecture in which multiple sources share a common rendezvous point.Shortest Path First (SPF)A type of routing algorithm. The only true SPF protocol is Open Shortest Path First (OSPF).signaling packetAn informational packet created by an ATM-connected mechanism that wants to establish connection with another such mechanism. The packet contains the QoS parameters needed for connection and the ATM NSAP address of the endpoint. The endpoint responds with a message of acceptance if it is able to support the desired QoS, and the con-nection is established. See also: QoS.silicon switchingA type of high-speed switching used in Cisco 7000 series routers, based on the use of a separate processor (the Silicon Switch Processor, or SSP). See also: SSE.simplexA mode at which data or a digital signal is transmitted. Simplex is a way of trans-mitting in only one direction. Half duplex transmits in two directions but only one direction at a time. Full duplex transmits both directions simultaneously.sliding windowThe method of flow control used by TCP, as well as several Data Link layer pro-tocols. This method places a buffer between the receiving application and the network data flow. The “window” available for accepting data is the size of the buffer minus the amount of data already there. This window increases in size as the application reads data from it and decreases as new data is sent. The receiver sends the transmitter announcements of the current window size, and it may stop accepting data until the window increases above a certain threshold.SLIPSerial Line Internet Protocol: An industry standard serial encapsulation for point-to-point connections that supports only a single routed protocol, TCP/IP. SLIP is the predecessor to PPP. See also: PPP.SMDSSwitched Multimegabit Data Service: A packet-switched, datagram-based WAN net-working technology offered by telephone companies that provides high speed.SMTPSimple Mail Transfer Protocol: A protocol used on the Internet to provide electronic mail services.SNASystem Network Architecture: A complex, feature-rich, network architecture similar to the OSI reference model but with several variations; created by IBM in the 1970s and essen-tially composed of seven layers.SNAPSubnetwork Access Protocol: SNAP is a frame used in Ethernet, Token Ring, and FDDI LANs. Data transfer, connection management, and QoS selection are three primary functions executed by the SNAP frame.

10089.book  Page 900  Monday, July 23, 2007  3:17 PM




Glossary901snapshot routingSnapshot routing takes a point-in-time capture of a dynamic routing table and maintains it even when the remote connection goes down. This allows the use of a dynamic routing protocol without requiring the link to remain active, which might incur per-minute usage charges.SNMPSimple Network Management Protocol: This protocol polls SNMP agents or devices for statistical and environmental data. This data can include device temperature, name, per-formance statistics, and much more. SNMP works with MIB objects that are present on the SNMP agent. This information is queried, then sent to the SNMP server.socket(1) A software structure that operates within a network device as a destination point for communications. (2) In AppleTalk networks, an entity at a specific location within a node; AppleTalk sockets are conceptually similar to TCP/IP ports.software addressAlso called a logical address. This is typically an IP address, but can also be an IPX address.SOHOSmall office/home office: A contemporary term for remote users.SONETSynchronous Optical Network: The ANSI standard for synchronous transmission on fiber-optic media, developed at Bell Labs. It specifies a base signal rate of 51.84Mbps and a set of multiples of that rate, known as Optical Carrier levels, up to 2.5Gbps.source treeA method of multicast data forwarding. Source trees use the architecture of the source of the multicast traffic as the root of the tree.SPSwitch processor: Also known as a ciscoBus controller, it is a Cisco 7000 series processor module acting as governing agent for all CxBus activities.spanA full-duplex digital transmission line connecting two facilities.SPANSwitched Port Analyzer: A feature of the Catalyst 5000 switch, offering freedom to manipulate within a switched Ethernet environment by extending the monitoring ability of the existing network analyzers into the environment. At one switched segment, the SPAN mirrors traffic onto a predetermined SPAN port, while a network analyzer connected to the SPAN port is able to monitor traffic from any other Catalyst switched port.spanning explorer packetSometimes called limited-route or single-route explorer packet, it pursues a statically configured spanning tree when searching for paths in a source-route bridging network. See also: all-routes explorer packet, explorer packet, and local explorer packet.spanning treeA subset of a network topology, within which no loops exist. When bridges are interconnected into a loop, the bridge, or switch, cannot identify a frame that has been for-warded previously, so there is no mechanism for removing a frame as it passes the interface numerous times. Without a method of removing these frames, the bridges continuously for-ward them—consuming bandwidth and adding overhead to the network. Spanning trees prune the network to provide only one path for any packet. See also: Spanning Tree Protocol and spanning-tree algorithm.

10089.book  Page 901  Monday, July 23, 2007  3:17 PM




902Glossaryspanning-tree algorithm (STA)An algorithm that creates a spanning tree using the Span-ning Tree Protocol (STP). See also: spanning tree and Spanning Tree Protocol.Spanning Tree Protocol (STP)The bridge protocol (IEEE 802.1D) that enables a learning bridge to dynamically avoid loops in the network topology by creating a spanning tree using the spanning-tree algorithm. Spanning-tree frames called Bridge Protocol Data Units (BPDUs) are sent and received by all switches in the network at regular intervals. The switches partic-ipating in the spanning tree don’t forward the frames; instead, they’re processed to determine the spanning-tree topology itself. Cisco Catalyst series switches use STP 802.1D to perform this function. See also: BPDU, learning bridge, MAC address, spanning tree, and spanning-tree algorithm.SPFShortest Path First algorithm: A routing algorithm used to decide on the shortest-path. Sometimes called Dijkstra’s algorithm and frequently used in link-state routing algorithms. See also: link-state routing algorithm.SPIDService Profile Identifier: A number assigned by service providers or local telephone companies and configured by administrators to a BRI port. SPIDs are used to determine sub-scription services of a device connected via ISDN. ISDN devices use SPID when accessing the telephone company switch that initializes the link to a service provider.split horizonUseful for preventing routing loops, a type of distance-vector routing rule where information about routes is prevented from leaving the router interface through which that information was received.spoofing(1) In dial-on-demand routing (DDR), where a circuit-switched link is taken down to save toll charges when there is no traffic to be sent, spoofing is a scheme used by routers that causes a host to treat an interface as if it were functioning and supporting a session. The router pretends to send “spoof” replies to keepalive messages from the host in an effort to convince the host that the session is up and running. See also: DDR. (2) The illegal act of sending a packet labeled with a false address, in order to deceive network security mechanisms such as filters and access lists.spoolerA management application that processes requests submitted to it for execution in a sequential fashion from a queue. A good example is a print spooler.SPXSequenced Packet Exchange: A Novell NetWare transport protocol that augments the datagram service provided by Network layer (layer 3) protocols, it was derived from the Switch-to-Switch Protocol of the XNS protocol suite.SQESignal Quality Error: In an Ethernet network, a message sent from a transceiver to an attached machine that the collision-detection circuitry is working.SRBSource-Route Bridging: Created by IBM, the bridging method used in Token Ring net-works. The source determines the entire route to a destination before sending the data and includes that information in routing information fields (RIF) within each packet. Contrast with: transparent bridging.

10089.book  Page 902  Monday, July 23, 2007  3:17 PM




Glossary903SRTSource-Route Transparent bridging: A bridging scheme developed by IBM, merging source-route and transparent bridging. SRT takes advantage of both technologies in one device, fulfilling the needs of all end nodes. Translation between bridging protocols is not necessary. Compare with: SR/TLB.SR/TLBSource-Route Translational Bridging: A bridging method that allows source-route stations to communicate with transparent bridge stations aided by an intermediate bridge that translates between the two bridge protocols. Used for bridging between Token Ring and Ethernet. Compare with: SRT.SSAPSource Service Access Point: The SAP of the network node identified in the Source field of the packet identifying the Network layer protocol. See also: DSAP and SAP.SSESilicon Switching Engine: The software component of Cisco’s silicon switching tech-nology, hard-coded into the Silicon Switch Processor (SSP). Silicon switching is available only on the Cisco 7000 with an SSP. Silicon-switched packets are compared to the silicon-switching cache on the SSE. The SSP is a dedicated switch processor that offloads the switching process from the route processor, providing a fast-switching solution, but packets must still traverse the backplane of the router to get to the SSP and then back to the exit interface.standard IP access listIP access list that uses only the source IP addresses to filter a network.standard IPX access listIPX access list that uses only the source and destination IPX address to filter a network.star topologyA LAN physical topology with endpoints on the network converging at a common central device (known as a hub) using point-to-point links. A logical ring topology can be configured as a physical star topology using a unidirectional closed-loop star rather than point-to-point links. That is, connections within the hub are arranged in an internal ring. See also: bus topology and ring topology.startup rangeIf an AppleTalk node does not have a number saved from the last time it was booted, then the node selects from the range of values from 65,280 to 65,534.state transitionsDigital signaling scheme that reads the “state” of the digital signal in the middle of the bit cell. If it is five volts, the cell is read as a one. If the state of the digital signal is zero volts, the bit cell is read as a zero.static routeA route whose information is purposefully entered into the routing table by an administrator and takes priority over those chosen by dynamic routing protocols.static VLANA VLAN that is manually configured port-by-port. This is the method typically used in production networks.statistical multiplexingMultiplexing in general is a technique that allows data from multiple logical channels to be sent across a single physical channel. Statistical multiplexing dynamically assigns bandwidth only to input channels that are active, optimizing available bandwidth so that more devices can be connected than with other multiplexing techniques. Also known as statistical time-division multiplexing or stat mux.

10089.book  Page 903  Monday, July 23, 2007  3:17 PM




904GlossarySTM-1Synchronous Transport Module Level 1. In the European SDH standard, one of many formats identifying the frame structure for the 155.52Mbps lines that are used to carry ATM cells.store-and-forward packet switchingA technique in which the switch first copies each packet into its buffer and performs a cyclic redundancy check (CRC). If the packet is error-free, the switch then looks up the destination address in its filter table, determines the appro-priate exit port, and sends the packet.STP(1) Shielded twisted-pair: A wiring scheme, used in many network implementations, that has a layer of shielded insulation to reduce EMI. (2) Spanning Tree Protocol.straight-through cableType of Ethernet cable that connects a host to a switch, host to a hub, or router to a switch or hub.stub areaAn OSPF area carrying a default route, intra-area routes, and interarea routes, but no external routes. Configuration of virtual links cannot be achieved across a stub area, and stub areas are not allowed to contain an ASBR. See also: non-stub area, ASBR, and OSPF.stub networkA network having only one connection to a router.STUNSerial Tunnel: A technology used to connect an HDLC link to an SDLC link over a serial link.subareaA portion of an SNA network made up of a subarea node and its attached links and peripheral nodes.subarea nodeAn SNA communications host or controller that handles entire network addresses.subchannelA frequency-based subdivision that creates a separate broadband communica-tions channel.subinterfaceOne of many virtual interfaces available on a single physical interface.subnetSee: subnetwork.subnet addressThe portion of an IP address that is specifically identified by the subnet mask as the subnetwork. See also: IP address, subnetwork, and subnet mask.subnet maskAlso simply known as mask, a 32-bit address mask used in IP to identify the bits of an IP address that are used for the subnet address. Using a mask, the router does not need to examine all 32 bits, only those indicated by the mask. See also: address mask and IP address.subnettingUsed in IP networks to break up larger networks into smaller subnetworks.subnetwork(1) Any network that is part of a larger IP network and is identified by a subnet address. A network administrator segments a network into subnetworks in order to provide a hierarchical, multilevel routing structure, and at the same time protect the subnetwork from the addressing complexity of networks that are attached. Also known as a subnet. See also: IP 

10089.book  Page 904  Monday, July 23, 2007  3:17 PM




Glossary905address, subnet mask, and subnet address. (2) In OSI networks, the term specifically refers to a collection of ESs and ISs controlled by only one administrative domain, using a solitary net-work connection protocol.summarizationTerm used to describe the process of summarizing multiple routing table entries into one entry.supernettingSee: summarization.SVCSwitched virtual circuit: A dynamically established virtual circuit created on demand and dissolved as soon as transmission is over and the circuit is no longer needed. In ATM ter-minology, it is referred to as a switched virtual connection. See also: PVC.switch(1) In networking, a device responsible for multiple functions such as filtering, flooding, and sending frames. It works using the destination address of individual frames. Switches operate at the Data Link layer of the OSI model. (2) Broadly, any electronic/mechanical device allowing connections to be established as needed and terminated if no longer necessary.switch blockA combination of layer 2 switches and layer 3 routers. The layer 2 switches connect users in the wiring closet into the access layer and provide 10 or 100Mbps dedicated connections. 1900/2820 and 2900 Catalyst switches can be used in the switch block.switch fabricTerm used to identify a layer 2 switched internetwork with many switches. More commonly, it is a term used to identify the inner workings of a switch itself. Thus, it is the matrix of pathways that any frame or cell might be able to traverse as it is switched from input port to output port.switched LANAny LAN implemented using LAN switches. See also: LAN switch.synchronous transmissionSignals transmitted digitally with precision clocking. These signals have identical frequencies and contain individual characters encapsulated in control bits (called start/stop bits) that designate the beginning and ending of each character. See also: asynchronous transmission and isochronous transmission.syslogA protocol used to monitor system log messages by a remote device.T reference pointUsed with an S reference point to change a 4-wire ISDN network to a two-wire ISDN network.T1Digital WAN that uses 24 DS0s at 64Kbps each to create a bandwidth of 1.536Mbps, minus clocking overhead, providing 1.544Mbps of usable bandwidth.T3Digital WAN that can provide bandwidth of 44.763Mbps.TACACS+Terminal Access Controller Access Control System Plus: An enhanced version of TACACS, this protocol is similar to RADIUS. See also: RADIUS.tagged trafficATM cells with their cell loss priority (CLP) bit set to 1. Also referred to as Discard Eligible (DE) traffic in Frame Relay networks. Tagged traffic can be eliminated in order to ensure trouble-free delivery of higher priority traffic, if the network is congested. See also: CLP.

10089.book  Page 905  Monday, July 23, 2007  3:17 PM




906GlossaryTCPTransmission Control Protocol: A connection-oriented protocol that is defined at the transport layer of the OSI reference model. Provides reliable delivery of data.TCP/IPTransmission Control Protocol/Internet Protocol. The suite of protocols underlying the Internet. TCP and IP are the most widely known protocols in that suite. See also: IP and TCP.TDMTime Division Multiplexing: A technique for assigning bandwidth on a single wire, based on preassigned time slots, to data from several channels. Bandwidth is allotted to each channel regardless of a station’s intent to send data. See also: ATDM, FDM, and multiplexing.TETerminal equipment: Any peripheral device that is ISDN-compatible and attached to a net-work, such as a telephone or computer. TE1s are devices that are ISDN-ready and understand ISDN signaling techniques. TE2s are devices that are not ISDN-ready and do not understand ISDN signaling techniques. A terminal adapter must be used with a TE2.TE1Terminal Equipment Type 1. A device with a four-wire, twisted-pair digital interface is referred to as terminal equipment type 1. Most modern ISDN devices are of this type.TE2Terminal Equipment Type 2. Devices known as terminal equipment type 2 do not under-stand ISDN signaling techniques, and a terminal adapter must be used to convert the signaling.telcoA common abbreviation for the telephone company.TelnetThe standard terminal emulation protocol within the TCP/IP protocol stack. Method of remote terminal connection, enabling users to log in on remote networks and use those resources as if they were locally connected. Telnet is defined in RFC 854.terminal adapter (TA)A hardware interface between a computer without a native ISDN interface and an ISDN line. In effect, a device to connect a standard async interface to a non-native ISDN device, emulating a modem.terminal emulationThe use of software, installed on a PC or LAN server, that allows the PC to function as if it were a “dumb” terminal directly attached to a particular type of mainframe.TFTPTrivial File Transfer Protocol: Conceptually, a stripped-down version of FTP; it’s the protocol of choice if you know exactly what you want and where it’s to be found. TFTP doesn’t provide the abundance of functions that FTP does. In particular, it has no directory browsing abilities; it can do nothing but send and receive files.TFTP host/serverA host or server on which Trivial File Transfer Protocol is used to send files using IP at the Network layer and UDP at the Transport layer, which makes it unreliable.thicknetAlso called 10Base5. Bus network that uses a thick coaxial cable and runs Ethernet up to 500 meters.thinnetAlso called 10Base2. Bus network that uses a thin coax cable and runs Ethernet media access up to 185 meters.three-way handshakeTerm used in a TCP session to define how a virtual circuit is set up. It is called a “three-way” handshake because it uses three data segments.

10089.book  Page 906  Monday, July 23, 2007  3:17 PM




Glossary907tokenA frame containing only control information. Possessing this control information gives a network device permission to transmit data onto the network. See also: token passing.token busLAN architecture that is the basis for the IEEE 802.4 LAN specification and employs token-passing access over a bus topology. See also: IEEE.token passingA method used by network devices to access the physical medium in a systematic way based on possession of a small frame called a token. See also: token.Token RingIBM’s token-passing LAN technology. It runs at 4Mbps or 16Mbps over a ring topology. Defined formally by IEEE 802.5. See also: ring topology and token passing.toll networkWAN network that uses the public switched telephone network (PSTN) to send packets.topology databaseA topology database (also called a topology table) contains all destina-tions advertised by neighboring routers. Associated with each entry is the destination address and a list of neighbors that have advertised the destination.TracerouteAlso Trace; IP command used to trace the path a packet takes through an internetwork.transparent bridgingThe bridging scheme used in Ethernet and IEEE 802.3 networks, it passes frames along one hop at a time, using bridging information stored in tables that asso-ciate end-node MAC addresses with bridge ports. This type of bridging is considered trans-parent because the source node does not know it has been bridged, because the destination frames are addressed directly to the end node. Contrast with: SRB.Transport layerLayer 4 of the OSI reference model, used for reliable communication between end nodes over the network. The transport layer provides mechanisms used for estab-lishing, maintaining, and terminating virtual circuits, transport fault detection and recovery, and controlling the flow of information. See also: Application layer, Data Link layer, Network layer, Physical layer, Presentation layer, and Session layer.trapUsed to send SNMP messages to SNMP managers.TRIPToken Ring Interface Processor: A high-speed interface processor used on Cisco 7000 series routers. The TRIP provides two or four ports for interconnection with IEEE 802.5 and IBM media with ports set to speeds of either 4Mbps or 16Mbps set independently of each other.trunk linkLink used between switches and from some servers to the switches. Trunk links carry traffic for many VLANs. Access links are used to connect host devices to a switch and carry only VLAN information that the device is a member of.TTLTime to live: A field in an IP header, indicating the length of time a packet is valid.TUDTrunk Up-Down: A protocol used in ATM networks for the monitoring of trunks. Should a trunk miss a given number of test messages being sent by ATM switches to ensure trunk line quality, TUD declares the trunk down. When a trunk reverses state and comes back up, TUD recognizes that the trunk is up and returns the trunk to service.

10089.book  Page 907  Monday, July 23, 2007  3:17 PM




908GlossarytunnelingA method of avoiding protocol restrictions by wrapping packets from one pro-tocol in another protocol’s frame and transmitting this encapsulated packet over a network that supports the wrapper protocol. See also: encapsulation.U reference pointReference point between a TE1 and an ISDN network. The U reference point understands ISDN signaling techniques and uses a 2-wire connection.UDPUser Datagram Protocol: A connectionless transport layer protocol in the TCP/IP protocol stack that simply allows datagrams to be exchanged without acknowledgments or delivery guarantees, requiring other protocols to handle error processing and retransmission. UDP is defined in RFC 768.unicastUsed for direct host-to-host communication. Communication is directed to only one destination and is originated only from one source.unidirectional shared treeA method of shared tree multicast forwarding. This method allows only multicast data to be forwarded from the RP.unnumbered framesHDLC frames used for control-management purposes, such as link startup and shutdown or mode specification.user modeCisco IOS EXEC mode that allows an administrator to perform very few com-mands. You can only verify statistics in user mode; you cannot see or change the router or switch configuration.UTPUnshielded twisted-pair: Copper wiring used in small-to-large networks to connect host devices to hubs and switches. Also used to connect switch to switch or hub to hub.VBRVariable bit rate: A QoS class, as defined by the ATM Forum, for use in ATM networks that is subdivided into real time (RT) class and non–real time (NRT) class. RT is employed when connections have a fixed-time relationship between samples. Conversely, NRT is employed when connections do not have a fixed-time relationship between samples, but still need an assured QoS.VCCVirtual channel connection: A logical circuit that is created by VCLs (virtual channel links). VCCs carry data between two endpoints in an ATM network. Sometimes called a virtual circuit connection.VIP(1) Versatile Interface Processor: An interface card for Cisco 7000 and 7500 series routers, providing multilayer switching and running the Cisco IOS software. The most recent version of VIP is VIP2. (2) Virtual IP: A function making it possible for logically separated switched IP workgroups to run Virtual Networking Services across the switch port.virtual circuit (VC)A logical circuit devised to assure reliable communication between two devices on a network. Defined by a virtual path identifier/virtual channel (really the only time “channel” is used) identifier (VPI/VCI) pair, a virtual circuit can be permanent (PVC) or switched (SVC). Virtual circuits are used in Frame Relay and X.25. Known as virtual channel in ATM. See also: PVC and SVC.virtual ringIn an SRB network, a logical connection between physical rings, either local or remote.

10089.book  Page 908  Monday, July 23, 2007  3:17 PM




Glossary909VLANVirtual LAN: A group of devices on one or more logically segmented LANs (configured by use of management software), enabling devices to communicate as if attached to the same physical medium, when they are actually located on numerous different LAN segments. VLANs are based on logical instead of physical connections and thus are tremendously flexible.VLAN IDSometimes referred to as VLAN color, the VLAN ID is tagged onto a frame to tell a receiving switch which VLAN the frame is a member of.VLSMVariable Length Subnet Mask: Helps optimize available address space and specify a different subnet mask for the same network number on various subnets. Also commonly referred to as “subnetting a subnet.”VMPSVLAN Management Policy Server: Used to dynamically assign VLANs to a switch port.VPNVirtual private network: A method of encrypting point-to-point logical connections across a public network, such as the Internet. This allows secure communications across a public network.VTPVLAN Trunking Protocol: Used to update switches in a switch fabric about VLANs configured on a VTP server. VTP devices can be a VTP server, client, or transparent device. Servers update clients. Transparent devices are only local devices and do not share information with VTP clients. VTP devices send VLAN information down trunked links only.VTP transparent modeSwitch mode that receives VLAN Trunking Protocol VLAN infor-mation and passes it on, but doesn’t read the information.WANWide area network: Is a designation used to connect LANs together across a DCE (data communications equipment) network. Typically, a WAN is a leased line or dial-up con-nection across a PSTN network. Examples of WAN protocols include Frame Relay, PPP, ISDN, and HDLC.wildcardUsed with access lists and OSPF configurations. Wildcards are designations used to identify a range of subnets.windowingFlow-control method used with TCP at the Transport layer of the OSI model.WINSWindows Internet Name Service: Name resolution database for NetBIOS names to TCP/IP address.WinSockWindows Socket Interface: A software interface that makes it possible for an assortment of applications to use and share an Internet connection. The WinSock software consists of a dynamic link library (DLL) with supporting programs such as a dialer program that initiates the connection.workgroup layerThe distribution layer is sometimes referred to as the workgroup layer and is the communication point between the access layer and the core. The primary functions of the distribution layer are to provide routing, filtering, and WAN access and to determine how packets can access the core, if needed.workgroup switchingA switching method that supplies high-speed (100Mbps) transparent bridging between Ethernet networks as well as high-speed translational bridging between Ethernet and CDDI or FDDI.

10089.book  Page 909  Monday, July 23, 2007  3:17 PM




910GlossaryX WindowA distributed multitasking windowing and graphics system originally developed by MIT for communication between X terminals and Unix workstations.X.25An ITU-T packet-relay standard that defines communication between DTE and DCE network devices. X.25 uses a reliable Data Link layer protocol called LAPB. X.25 also uses PLP at the Network layer. X.25 has mostly been replaced by Frame Relay.ZIPZone Information Protocol: A Session layer protocol used by AppleTalk to map network numbers to zone names. NBP uses ZIP in the determination of networks containing nodes that belong to a zone. See also: ZIP storm and zone.ZIP stormA broadcast storm occurring when a router running AppleTalk reproduces or transmits a route for which there is no corresponding zone name at the time of execution. The route is then forwarded by other routers downstream, thus causing a ZIP storm. See also: broadcast storm and ZIP.zoneA logical grouping of network devices in AppleTalk. Also used in DNS. See also: ZIP.

10089.book  Page 910  Monday, July 23, 2007  3:17 PM




 Index Note to the Reader:  Throughout this index  boldfaced  page numbers indicate primary discussions of a topic.  Italicized  page numbers indicate illustrations.

 A AAA (Authentication, Authorization, and Accounting)defined, 852wireless networks, 721AAL (ATM Adaptation Layer), 852AAL1 (ATM Adaptation Layer 1), 852AAL2 (ATM Adaptation Layer 2), 852AAL3/4 (ATM Adaptation Layer 3/4), 852AAL5 (ATM Adaptation Layer 5), 852AARP (AppleTalk Address Resolution Protocol), 853AARP probe packets, 853A&B bit signaling, 852ABM (Asynchronous Balanced Mode), 853ABRs (Area Border Routers)defined, 853with OSPF, 445,  445 absolute option with aging, 536Abstract Syntax Notation One (ASN.1), 856access-class command, 625access-enable command, 185access layer,  47 ,  49 , 853access linksdefined, 853VLANs, 560–561,  561 access-list command, 186, 619–620, 626access-list deny command, 619–624, 627access-list deny host command, 620access-list deny tcp command, 627access-list deny tcp any command, 628access-list deny tcp any host command, 628–630access-list permit command, 625access-list permit any command, 622access-list permit ip command, 630access-list permit ip any any command, 630access-list remark command, 638–639access lists,  615–618 authentication proxy,  640 Context-Based Access Control,  639–640 ,  639 defined, 853dynamic,  636–637 exam essentials,  655 hands-on lab,  656–660 ,  656 IPextended,  626–632 ,  657–660 , 874monitoring,  640–642 standard,  619–624 ,  622–624 ,  657 , 903for Telnet,  625–626 wildcards with,  620–622 IPXextended, 874standard, 903named,  632–634 reflexive,  637 remarks,  638–639 review questions,  661–667 SDM forcreating,  643–647 ,  643–646 firewalls,  647–654

 

10089bindex.fm  Page 911  Tuesday, July 24, 2007  9:38 AM




 912 access methods–aggregate rates security issues mitigated by,  618 summary,  654–655 switch port,  634–636 time-based,  637–638 written lab for,  655–656 ,  667 access methods, 853access ports in VLANs, 559–560access-profile command, 185access rates, 853access servers, 853access state, 854access-template command, 185accounting, 853Acknowledgment number field, 76acknowledgmentsdefined, 853Transport layer,  21 ,  21 ACR (Allowed Cell Rate), 853active monitors, 853active stateEIGRP, 441LMI, 804AD (administrative distances)defined, 854EIGRP, 422IP routing,  377–378 static routing, 363Adaptive Wireless Path Protocol (AWPP),  718 Add DHCP Pool dialog box, 230,  230 address masks, 854address resolution, 854Address Resolution Protocol (ARP)defined, 856IP routing process, 332–336, 339–340operation, 46,  90–92 ,  90 Address Translation Gateway (ATG), 857addressesARP for, 46,  90–92 ,  90 Ethernet networking,  34–35 ,  34 IP.  See  IP addressesIPv6 protocolspecial,  745 structure,  742–743 ,  743 types,  744–745 learningdefined, 854by layer 2 switching,  499–501 ,  500–501 MAC.  See  MAC (Media Access Control) addressesmapping, 854RARP for,  91 ,  92 adjacenciesdefined, 854OSPF, 446,  465–466 administrative distances (ADs)defined, 854EIGRP, 422IP routing,  377–378 static routing, 363administrative weight, 854ADSL (asymmetrical DSL),  782–785 ,  784 ADSU (ATM Data Service Unit), 854Advanced Firewall Configuration Wizard, 653–654,  653–654 Advanced NAT configuration, 684advertised distances in EIGRP, 420, 441advertising, 854AEP (AppleTalk Echo Protocol), 854AFI (Authority and Format Interface), 854AFP (AppleTalk Filing Protocol), 854agencies for wireless technologies, 705agents in SNMP, 73aggregate rates, 33

 

10089bindex.fm  Page 912  Tuesday, July 24, 2007  9:38 AM




 aging command–ASICs (Application-Specific Integrated Circuits) 913 aging command, 536AH (Authentication Header) for IPSec,  827 AIP (ATM Interface Processor), 854algorithms, 854alignment errors, 854all 1s broadcasts, 96all networks address, 96all nodes address, 96all-routes explorer packets, 855Allowed Cell Rate (ACR), 853Alternate Mark Inversion (AMI), 855AM (Amplitude Modulation), 855American National Standards Institute (ANSI), 855American Standard Code for Information Interchange (ASCII), 856AMI (Alternate Mark Inversion), 855amplitude, 855Amplitude Modulation (AM), 855analog transmissions, 855anonymous FTP users, 71ANSI (American National Standards Institute), 855ANSI format, 804anti-replay service, 827any command, 622anycastsdefined, 855IPv6, 742, 745applet blocking, 615AppleTalk Address Resolution Protocol (AARP), 853AppleTalk Control Program (ATCP), 856AppleTalk Echo Protocol (AEP), 854AppleTalk Filing Protocol (AFP), 854AppleTalk protocols, 855AppleTalk Remote Access (ARA), 855AppleTalk Session Protocol (ASP), 856AppleTalk Transaction Protocol (ATP), 857AppleTalk Update-based Routing Protocol (AURP), 857Application layerattacks, 612defined, 855tasks,  15–16 Application-Specific Integrated Circuits (ASICs), 25defined, 856filter tables, 497–498ARA (AppleTalk Remote Access), 855archive command, 185Area Border Routers (ABRs)defined, 853with OSPF, 445,  445 area IDs, 465areasdefined, 855OSPF, 447,  450–453 ,  452 ARM (Asynchronous Response Mode), 856ARP (Address Resolution Protocol)defined, 856IP routing process, 332–336, 339–340operation, 46,  90–92 ,  90 arp command, 152AS path prepending, 856ASBRs (Autonomous System Boundary Routers)defined, 856OSPF, 446ASCII (American Standard Code for Information Interchange), 856ASICs (Application-Specific Integrated Circuits), 25defined, 856filter tables, 497–498

 

10089bindex.fm  Page 913  Tuesday, July 24, 2007  9:38 AM




 914 ASN.1 (Abstract Syntax Notation One)–auto-summarization in EIGRP ASN.1 (Abstract Syntax Notation One), 856ASP (AppleTalk Session Protocol), 856ASs (autonomous systems)defined, 856EIGRP, 420,  422–423 IGRP, 377AST (Automatic Spanning Tree), 856asymmetrical DSL (ADSL),  782–785 ,  784 Asynchronous Balanced Mode (ABM), 853Asynchronous Response Mode (ARM), 856Asynchronous Time-Division Multiplexing (ATDM), 856Asynchronous Transfer Mode (ATM)for ADSL, 783defined, 857description, 779asynchronous transmissions, 856ATCP (AppleTalk Control Program), 856ATDM (Asynchronous Time-Division Multiplexing), 856ATG (Address Translation Gateway), 857ATM (Asynchronous Transfer Mode)for ADSL, 783defined, 857description, 779ATM Adaptation Layer (AAL), 852ATM Adaptation Layer 1 (AAL1), 852ATM Adaptation Layer 2 (AAL2), 852ATM Adaptation Layer 3/4 (AAL3/4), 852ATM Adaptation Layer 5 (AAL5), 852ATM ARP servers, 857ATM Data Service Unit (ADSU), 854ATM endpoints, 857ATM Forum, 857ATM Interface Processor (AIP), 854ATM layer, 857ATM Management (ATMM), 857ATM user-user connections, 857ATP (AppleTalk Transaction Protocol), 857attenuation, 857AUIs (Attachment Unit Interfaces), 38AURP (AppleTalk Update-based Routing Protocol), 857AURP tunnels, 858authenticationdefined, 858ESP for, 827LCP, 789OSPF, 465peer route, 615PPP,  790–794 ,  813–818 ,  813–816 RIPv1 vs. RIPv2, 391wireless networks,  719–720 Authentication, Authorization, and Accounting (AAA)defined, 852wireless networks, 721Authentication Header (AH) for IPSec,  827 authentication proxyaccess lists,  640 IOS firewall, 614Authentication screen, 815,  815 Authority and Format Interface (AFI), 854authority zones, 858authorization, 858auto-detect mechanisms, 34, 858auto duplex, 858auto RF controls, 714auto-summarization in EIGRP, 424,  424

 

10089bindex.fm  Page 914  Tuesday, July 24, 2007  9:38 AM




 autoconfiguration in IPv6–beacons 915 autoconfiguration in IPv6,  746–747 ,  747 automatic call reconnect, 858Automatic Spanning Tree (AST), 856autonomous confederation, 858autonomous switching, 858autonomous system (AS) number, 393Autonomous System Boundary Routers (ASBRs)defined, 856OSPF, 446autonomous systems (ASs)defined, 856EIGRP, 420,  422–423 IGRP, 377autoreconfiguration, 858autorooter attacks, 612aux command, 195auxiliary passwords,  195–196 auxiliary portsconnecting through, 174defined, 858AWPP (Adaptive Wireless Path Protocol),  718

 B B (Bearer) channels, 859B8ZS (Binary 8-Zero Substitution), 858back ends, 858BackboneFast feature,  513 ,  524–525 backbonescollapsed, 494–495,  495 defined, 858backdoor attacks, 612backoff algorithms, 32–33Backspace command, 187Backup Designated Routers (BDRs)defined, 859elections,  465–466 OSPF, 447backupsIOS,  264–265 ,  314 router,  274–276 hands-on lab,  314–315 SDM,  280–283 ,  280–283 Backward-Explicit Congestion Notification (BECN) bitdefined, 859Frame Relay, 804bandwidthdefault, 219defined, 858displaying, 218EIGRP, 425Frame Relay, 799–800IGRP, 393multimedia applications, 554OSPF, 448serial links, 786UDP, 77bandwidth command, 211–212bandwidth on demand (BoD), 859banner command, 192–193banners,  192–194 baseband technology, 38, 859baseline information, 73, 859Basic Firewall Configuration Wizard, 648–651,  648–649 Basic Management Setup mode, 859Basic NAT Wizard, 684–686,  685–686 Basic Rate Interface (BRI), 860basic router information, command-line interface for,  189–191 Basic Service Set (BSS), 713baud, 859BDRs (Backup Designated Routers), 859defined, 859elections,  465–466 OSPF, 447beacons, 859

 

10089bindex.fm  Page 915  Tuesday, July 24, 2007  9:38 AM




 916 Bearer (B) channels–broadcast domains Bearer (B) channels, 859BECN (Backward-Explicit Congestion Notification) bitdefined, 859Frame Relay, 804bfe command, 185BGP (Border Gateway Protocol), 377BGP Identifier field, 859BGP neighbors, 859BGP speakers, 859BGP4 protocol, 859bidirectional shared trees, 859Binary 8-Zero Substitution (B8ZS), 858binary numbering systemconversions with,  26–30, 53–55defined, 860for IP addresses, 93binding, 12BISDN (Broadband ISDN), 860Bit Interleaved Parity (BIP), 860bit-oriented protocolscontrol information in, 787defined, 860bits, 27–29defined, 860in IP, 93block sizessummarization, 147–150VLSMs, 140–142with wildcards, 620–621blocked ports in STP, 507blocking state in STP, 510BNC connectors, 38BoD (bandwidth on demand), 859Boot default ROM software bit, 255Boot field, 255Boot image from ROM field, 255Boot ROM, 860boot sequencedefined, 860routers, 253–254, 259boot system commands, 261–262bootstrap protocolsdefined, 860routers, 252–253Border Gateway Protocol (BGP), 377border gateways, 860border peers, 860border routers, 860BPDU (Bridge Protocol Data Unit)defined, 860STP, 507–508BPDUFilter, 523–524BPDUGuard, 523Break disabled bit, 255breaks, 259BRI (Basic Rate Interface), 860bridge groups, 861bridge priority, 861Bridge Protocol Data Unit (BPDU)defined, 860STP, 507–508bridges, 8, 9Data Link layer, 25–26defined, 861identifiers for, 507, 509–510, 861for network segmentation, 6STP, 506, 508–510, 532–534vs. switches, 8, 499before switching, 496bridging loops, 861bringing up router interfacesno shutdown command, 206–207steps, 175–179Broadband ISDN (BISDN), 860broadband transmissions, 861broadcast addresses, 93, 100–101broadcast and unknown servers (BUS), 862broadcast domains, 4–5, 53, 53breaking up, 6–8, 6defined, 861

10089bindex.fm  Page 916  Tuesday, July 24, 2007  9:38 AM




broadcast OSPF networks–Catalyst switch configuration917flat networks, 553, 553layer 2 switching, 499broadcast OSPF networks, 447broadcast stormsdefined, 861loop avoidance for, 504–505, 504broadcastsdefined, 861flat networks, 552–553IPv6, 742multimedia applications, 554routers, 23VLANs, 554brute force attacks, 613BSS (Basic Service Set), 713Buffer Full message, 87buffersconnection-oriented communication, 18–19defined, 861IP routing process, 333bundling links in EIGRP, 436bursting, 862bursty traffic, 800BUS (broadcast and unknown servers), 862bus topology, 862buses, 862BX.25 standard, 862bypass mode, 862bypass relays, 862byte-oriented protocolscontrol information in, 787defined, 862bytes, 27–29defined, 862in IP, 93

Ccable ranges, 862cablingCatalyst switches, 517description, 779Ethernet networking, 39–41, 39–42WANs, 779–782, 780–781, 785–786, 786CAC (Connection Admission Control), 862calendar command, 185call admission control, 862call establishment, 862call priority, 862call set-upconnection-oriented communication, 17time for, 862callback, PPP, 789Capability field, 285carets (^) as error indicators, 186Carrier Detect (CD) signal, 863Carrier Sense Multiple Access with Collision Detect (CSMA/CD)2.4 GHz wireless, 708–709, 708defined, 867operation, 31–32, 32Catalyst switch configuration, 514–517, 515–516BackboneFast, 524–525BPDUFilter, 523–524BPDUGuard, 523Core, 519–521EtherChannel, 526–527port security, 521–522PortFast, 522–523RSTP, 525–526S1, 517–518S2, 518–519

10089bindex.fm  Page 917  Tuesday, July 24, 2007  9:38 AM




918CBAC (Context-Based Access Control)–Cisco Discovery Protocol (CDP)trunking, 572UplinkFast, 524verifying, 528–534CBAC (Context-Based Access Control), 639–640, 639, 651CBR (Constant Bit Rate), 863CD (Carrier Detect) signal, 863cd command, 185, 266, 268CDP (Cisco Discovery Protocol), 283, 315–316defined, 863neighbor information, 284–289network topology, 292–294, 292, 294port and interface information, 290–291timers and holdtime information, 283–284traffic information, 289VLAN telephony, 586–588cdp enable command, 284, 290cdp holdtime command, 284, 863cdp timer command, 284, 863CDVT (Cell Delay Variation Tolerance), 863Cell Error Ratio (CER), 863Cell Loss Priority (CLP), 865Cell Loss Ratio (CLR), 865cell payload scrambling, 863cell relay, 863Cell Transfer Delay (CTD), 867–868cells, 863central office (CO)defined, 865WANs, 775Centrex service, 863CER (Cell Error Ratio), 863CGMP (Cisco Group Management Protocol), 863Challenge Handshake Authentication Protocol (CHAP)defined, 864PPP, 790–791, 793–794Channel Interface Processor (CIP), 864Channel Service Unit (CSU), 867Channel Service Unit/Data Service Unit (CSU/DSU)Physical layer, 30WANs, 786, 786channelized E1, 863channelized T1, 864CHAP (Challenge Handshake Authentication Protocol)defined, 864PPP, 790–791, 793–794Checksum field, 76checksums, 864choke packets, 864CIDR (Classless Interdomain Routing)defined, 864subnetting, 116–118CIP (Channel Interface Processor), 864CIR (Committed Information Rate)defined, 864Frame Relay, 799–800circuit switchingdefined, 864WANs, 776, 776Cisco Discovery Protocol (CDP), 283, 315–316defined, 863neighbor information, 284–289network topology, 292–294, 292, 294port and interface information, 290–291timers and holdtime information, 283–284traffic information, 289VLAN telephony, 586–588

10089bindex.fm  Page 918  Tuesday, July 24, 2007  9:38 AM




Cisco encapsulation–collision domains919Cisco encapsulation, 800, 806Cisco format in LMI, 804Cisco FRAD (Cisco Frame Relay Access Device), 864Cisco Group Management Protocol (CGMP), 863Cisco IOS. See IOS (Internetwork Operating System)Cisco Network Assistant (CNA)inter-VLAN routing configuration, 588–597, 589–596overview, 534–541, 535, 538–541CiscoFusion architecture, 864CiscoView software, 864Class A networks, 94–95, 94defined, 865format, 96–97reserved address space, 99subnetting, 115–116, 134–136Class B networks, 94–95, 94defined, 865format, 97–98reserved address space, 99subnetting, 115–116, 127–133Class C networks, 94–95, 94defined, 865format, 98reserved address space, 99subnetting, 115–116, 118–127, 120, 122Class D addresses, 94–96, 94Class E addresses, 94–96, 94class of service (CoS), 586–588classes of protocols, 378–379classful networks, 137, 137classful routing, 374–375defined, 865RIP, 383–384classical IP over ATM, 865Classless Interdomain Routing (CIDR)defined, 864subnetting, 116–118classless networks, 137–138classless protocols, 419classless routingdefined, 865RIP, 383clear command, 185clear counters command, 220clear ip nat translation command, 677clear line command, 299Clear To Send (CTS) signal, 708clearingcounters, 220Telnet connections, 299CLI. See command-line interface (CLI)client mode in VTP, 564clock command, 185–186clock rate command, 210–211, 222, 346clock set command, 186clocking, 210–211, 222closing Telnet sessions, 298–299CLP (Cell Loss Priority), 865CLR (Cell Loss Ratio), 865CNA (Cisco Network Assistant)inter-VLAN routing configuration, 588–597, 589–596overview, 534–541, 535, 538–541cns command, 185CO (central office)defined, 865WANs, 775Code bits field, 76collapsed backbonesdefined, 865before switching, 494–495, 495collision domains, 4, 6–11, 6defined, 865flat networks, 553identifying, 53, 53layer 2 switching, 499switches for, 26

10089bindex.fm  Page 919  Tuesday, July 24, 2007  9:38 AM




920collisions–configurationcollisions2.4 GHz wireless, 708–709CSMA/CD for, 31–32, 32defined, 865COM1 Properties dialog box, 41, 41command-line interface (CLI), 179–180for banners, 192–194for basic routing information, 189–191for configurationsdeleting, 214saving, 212–213, 237verifying, 214–223, 222–223viewing, 213–214defined, 865for descriptions, 201–203, 240do command, 203–204editing and help features, 185–189, 236–237for hostnames, 191–192, 239for logging onto routers, 235–236from non-ISR routers, 180–181for passwordsauxiliary, 195–196console, 196–197encrypting, 199–201setting, 194–195, 237–239Telnet, 197–198prompts, 182for interfaces, 182–183line commands, 183–184for routing protocol configurations, 184for subinterfaces, 183for router interfaces, 204–212, 209–210router modes, 181–182for SSH, 198–199comments for access lists, 638–639Committed Information Rate (CIR)defined, 864Frame Relay, 799–800Common Part Convergence Sublayer (CPCS), 867composite metricsdefined, 865IGRP, 392compressiondefined, 866LCP, 789confidentiality, ESP for, 827config-register command, 256, 259, 261configurationbacking upIOS, 212–213, 264–265, 314router, 237, 274–276, 314–315SDM, 280–283, 280–283Catalyst switches. See Catalyst switch configurationcommand-linedeleting, 214saving, 212–213, 237verifying, 214–223, 222–223viewing, 213–214copyingto NVRAM, 275–276to TFTP server, 276EIGRP, 426–429, 427Corp, 429discontiguous networks, 434–435R1, 429–430R2, 430R3, 430–432, 430summary routes, 474–476, 475–476verifying, 438–443erasing, 214, 277–278IFS for, 278–279IP address, 207–208

10089bindex.fm  Page 920  Tuesday, July 24, 2007  9:38 AM




configuration registers–congestion collapse921IP routing, 341–343, 341871W router, 359–3611242AP router, 361–3622621A router, 393–401, 399Corp router, 343–346R1 router, 346–349R2 router, 349–352R3 router, 352–359, 352–359verifying, 373–374, 393–401, 399IPv6 protocolautoconfiguration, 746–747, 747Corp, 756–758DHCPv6 servers, 747–749ICMPv6 servers, 749–750OSPFv3, 763–766R1, 758R2, 758–759R3, 759RIPng, 759–763routers, 747–748NAT, 679–684, 680dynamic, 675, 692–693overloading, 675–676, 694–695SDM for, 684–687, 685–686static, 674–675verifying, 676OSPF, 449871W, 457areas, 450–453, 452Corp, 453–454debugging, 462–464enabling, 449R1, 454R2, 454R3, 454–456, 455–456summary routes, 474–476, 475–476troubleshooting, 471–473, 472–474verifying, 457–462PPP, 791–792, 838–839, 838PPPoE, 796–797, 818–822, 819–822restoring, 276–277, 280–283, 280–283RIP, 405–406871W router, 387Corp router, 383–384example, 389–390, 389R1 router, 384–385R2 router, 385R3 router, 385–387, 386saving, 212–213, 237verifying. See verifyingviewing, 213–214VLANs, 568–570inter-VLAN routing, 575–580, 576–578, 580, 588–597switch port assignments, 570–571trunk ports, 571–574voice, 586–588VPNs, 828–836, 828–835VTP, 580–583wireless networks, 721–728, 722–728configuration registers, 253bits in, 254–255boot system commands, 261–262changing values, 256–257checking values, 256defined, 866for password recovery, 258–261configure command, 182, 185configure memory command, 181configure network command, 181configure terminal command, 181–182congestion, 866congestion avoidancedefined, 866Frame Relay, 804–805congestion collapse, 866

10089bindex.fm  Page 921  Tuesday, July 24, 2007  9:38 AM




922connect command–CPCS (Common Part Convergence Sublayer)connect command, 185Connect To dialog box, 41, 41Connection Admission Control (CAC), 862Connection Description dialog box, 41, 41connection IDs, 866connection-oriented communication, 75defined, 866Transport layer, 17–20, 18–19connectionless protocols, 78, 866connectionsconsole port, 173routers, 173–175, 174–175Telnet, 297WANs, 775–776, 776connectivity, 305debugging, 308–310ping command for, 305–306processes, 310–311SDM for, 306, 306traceroute command for, 307–308connectors, WANs, 785consoleconnections to, 173passwords for, 196–197port commands from, 196–197console command, 195Console line speed field, 255console ports, 173–174Catalyst switches, 516defined, 866Constant Bit Rate (CBR), 863Context-Based Access Control (CBAC), 639–640, 639, 651control direct VCC, 866control distribute VCC, 866convergencedefined, 866EIGRP, 422OSPF, 444STP, 511–512, 511conversions, number system, 26–30, 53–55copy command, 185, 266–267copy flash tftp command, 264–265copy running-config startup-config command, 212–213, 261, 275copy running-config tftp command, 275–276copy source-url destination-url command, 267copy startup-config running-config command, 261, 276–277copy startup-config tftp command, 275copy tftp flash command, 265–266copy tftp running-config command, 277copy tftp startup-config command, 277core layerdefined, 866internetworking, 47–48, 47Core switch configuration, 519–521Corp router configurationEIGRP, 429IP routing, 343–346IPv6, 756–758NAT, 680–681OSPF, 453–454RIP, 383–384static, 364–366CoS (class of service), 586–588costsdefined, 867OSPF, 448counters, clearing, 220counting to infinitydefined, 867from routing loops, 382CPCS (Common Part Convergence Sublayer), 867

10089bindex.fm  Page 922  Tuesday, July 24, 2007  9:38 AM




CPE (customer premises equipment)–Data field923CPE (customer premises equipment)defined, 867WANs, 775crankback technique, 867CRC (cyclic redundancy check)defined, 867Ethernet frames, 35–36, 46IP header, 85IP routing process, 332–336TCP segment, 76UDP segment, 78Create Connection screen, 723, 813Create Firewall screen, 643, 643, 648, 648Create NAT Configuration screen, 685, 685Create New Connection option, 353Create Site to Site VPN tab, 828, 828crossover cablesCatalyst switches, 517defined, 867Ethernet networking, 40, 40, 42crypto command, 185, 831crypto key generate rsa command, 198crypto pki command, 231CSMA/CD (Carrier Sense Multiple Access with Collision Detect)2.4 GHz wireless, 708–709, 708defined, 867operation, 31–32, 32CSU (Channel Service Unit), 867CSU/DSU (Channel Service Unit/Data Service Unit)Physical layer, 30WANs, 786, 786ct-isdn command, 185CTD (Cell Transfer Delay), 867–868Ctrl+A command, 187Ctrl+D command, 187Ctrl+E command, 187Ctrl+F command, 187Ctrl+R command, 187Ctrl+Shift+6 command, 297Ctrl+U command, 187Ctrl+W command, 187Ctrl+Z command, 187CTS (Clear To Send) signal, 708cumulative interface delay, 868cumulative line delay in EIGRP, 425customer premises equipment (CPE)defined, 867WANs, 775cut-through switching method, 868cyclic redundancy check (CRC)defined, 867Ethernet frames, 35–36, 46IP header, 85IP routing process, 332–336TCP segment, 76UDP segment, 78

DD (Data) channels, 869DA (Destination address) field, 36data circuit-terminating equipment, 868data communications equipment (DCE)defined, 869Physical layer, 30WANs, 786, 786Data Country Code (DCC), 868data direct VCC, 868data encapsulationdefined, 868Frame Relay, 800–801internetworking, 43–46, 43–44PPP, 792–795, 792–794Data Exchange Interface (DXI), 871Data fieldEthernet frames, 36IP header, 85

10089bindex.fm  Page 923  Tuesday, July 24, 2007  9:38 AM




924data frames–deleting configurationsTCP segment, 76UDP segment, 78data frames, 24, 868Data Link Connection Identifiers (DLCIs)defined, 870Frame Relay, 801–803, 802Data Link Control layer, 868Data Link layer, 24–25, 24defined, 868Ethernet networking at, 34–37, 34–35number system conversions, 26–30switches and bridges at, 25–26, 26Data Link Switching (DLSw), 870–871data over cable service interface specification (DOCSIS), 781, 781data packets, 43defined, 892Network layer, 22Data Service Units (DSUs), 871Data Set Ready (DSR) circuits, 871data terminal equipment (DTE)defined, 871Physical layer, 30WANs, 786, 786Data Terminal Ready (DTR) circuits, 871Datagram Delivery Protocol (DDP), 869datagrams, 43, 868DCC (Data Country Code), 868DCE (data communications equipment)defined, 869Physical layer, 30WANs, 786, 786DDP (Datagram Delivery Protocol), 869DDR (dial-on-demand routing), 869DE (Discard Eligibility) bitdefined, 869Frame Relay, 804de-encapsulationdefined, 869packets, 44Dead intervals in OSPF, 465debug command, 185, 308debug all command, 309debug eigrp command, 438, 442–443debug frame lmi command, 811debug ip eigrp command, 438, 442debug ip nat command, 676, 682–683debug ip ospf adj command, 464debug ip ospf hello command, 463–464debug ip ospf packet command, 463debug ip rip command, 309, 394–398debug ipv6 ospf hello command, 764debug ipv6 ospf packet command, 764debug ipv6 rip command, 762debug ppp authentication command, 793, 817debuggingconnectivity, 308–310OSPF, 462–464PPP, 793–796, 794–795decimal number conversions, 26–30, 53–55dedicated linesdefined, 869WANs, 776, 776defaultsadministrative distances, 378bandwidth, 219gateways, 331–332, 334routes, 96, 869routing, 374–377, 375–376delaydefined, 869EIGRP, 425IGRP, 393delete command, 185, 266, 268–269deleted state in LMI, 804deleting configurations, 214, 277–278

10089bindex.fm  Page 924  Tuesday, July 24, 2007  9:38 AM




demarcs–distribution layer925demarcsdefined, 869WANs, 775demodulation, 869demultiplexing, 869denial of service (DoS) attacksdetection and prevention, 615types, 612deny any any command, 635description command, 201–203descriptions, command-line interface for, 201–203, 240designated bridges, 869designated portsdefined, 869STP, 507designated routers (DRs)defined, 870elections, 465–466OSPF, 447desktop layerdefined, 870internetworking, 49Destination address (DA) field, 36destination addressesdefined, 870IP routing process, 333–340destination hosts, 333–334Destination IP address field, 85destination network parameter, 363Destination port field, 76, 78destination ports in TCP, 81–82Destination Service Access Points (DSAPs), 871Destination Unreachable message, 87destination URL policy management, 614Device ID field, 285Device Manager, 541DFS (Dynamic Frequency Selection), 710DHCP (Dynamic Host Configuration Protocol)defined, 870IP addresses, 73–74DHCPv6 server configuration, 747–749diagnostic addresses, 151dial backup, 870dial-on-demand routing (DDR), 869dialer pool command, 797Diffusing Update Algorithm (DUAL)defined, 871EIGRP, 421–422EIGRPv6, 751Digital, Intel, Xerox (DIX) group, 37digital subscriber line (DSL)description, 779WANs, 782–785, 782, 784Dijkstra algorithm, 444dir command, 266–267Direct Sequence Spread Spectrum (DSSS) technique, 709directed broadcasts, 870disable command, 180, 185disabled state in STP, 510Discard Eligibility (DE) bitdefined, 869Frame Relay, 804disconnect command, 185, 298discontiguous networksEIGRP, 423–424, 423, 434–435RIPv1 vs. RIPv2, 391Discover messages, 74discovery mode, 870Distance Vector Multicast Routing Protocol (DVMRP), 871distance-vector protocols, 378–382, 379–381distance-vector routing algorithm, 870distribute lists, 616distribution layerdefined, 870internetworking, 47, 48

10089bindex.fm  Page 925  Tuesday, July 24, 2007  9:38 AM




926distribution networks–dynamic routingdistribution networks, cable systems, 781DIX (Digital, Intel, Xerox) group, 37DLCIs (Data Link Connection Identifiers)defined, 870Frame Relay, 801–803, 802DLSw (Data Link Switching), 870–871DLSw+, 871DNS (Domain Name System)defined, 871name resolution, 5, 73, 302–304do command, 203–204DOCSIS (data over cable service interface specification), 781, 781DoD model and TCP/IP, 68–70, 69–70dollar signs ($) for scrolling, 188Domain Name System (DNS)defined, 871name resolution, 73, 302–304domainsbroadcast, 4–5, 53, 53breaking up, 6–8, 6defined, 861flat networks, 553, 553layer 2 switching, 499collision, 4, 6–11, 6defined, 865flat networks, 553identifying, 53, 53layer 2 switching, 499switches for, 26VTP, 584DoS (denial of service) attacksdetection and prevention, 615types, 612dotted-decimal notation, 93DRs (designated routers)defined, 870elections, 465–466OSPF, 447DSAPs (Destination Service Access Points), 871DSL (digital subscriber line)description, 779WANs, 782–785, 782, 784DSLAM switch, 783DSR (Data Set Ready) circuits, 871DSSS (Direct Sequence Spread Spectrum) technique, 709DSUs (Data Service Units), 871DTE (data terminal equipment)defined, 871Physical layer, 30WANs, 786, 786DTR (Data Terminal Ready) circuits, 871DUAL (Diffusing Update Algorithm)defined, 871EIGRP, 421–422EIGRPv6, 751dual stacking in IPv6 migration, 754duplex in Ethernet networking, 33–34DVMRP (Distance Vector Multicast Routing Protocol), 871DXI (Data Exchange Interface), 871dynamic access lists, 636–637dynamic command, 571–572dynamic entries, 871Dynamic Frequency Selection (DFS), 710Dynamic Host Configuration Protocol (DHCP)defined, 870IP addresses, 73–74dynamic NAT, 672, 675, 692–693Dynamic NAT-PT, 756dynamic port mapping, 615dynamic routing, 328defined, 871IGRP. See IGRP (Interior Gateway Routing Protocol)

10089bindex.fm  Page 926  Tuesday, July 24, 2007  9:38 AM




dynamic VLANs–EIGRPv6 protocol927IP, 377–379RIP. See RIP (Routing Information Protocol)dynamic VLANsbenefits, 559defined, 872

EE.164 standard, 872E channels, 872e-mailApplication layer for, 15SMTP for, 72E1 transmissions, 872EAP (Extensible Authentication Protocol), 721eBGP (External Border Gateway Protocol), 872edge devices, 872Edit Firewall Policy/ACL tab, 644, 644Edit Interface/Connection tab, 228, 228, 722, 722, 813, 816editing features in command-line interface, 185–189, 236–237EEPROM (electronically erasable programmable read-only memory)defined, 872loading from, 175EFCI (Explicit Forward Congestion Indication), 872EGP (Exterior Gateway Protocol), 377EIA/TIA-232-C standard, 788802.1 specification, 562–563, 879802.3 specification, 879802.5 specification, 879802.11 standards, 706–712, 712871W routerconfigurationIP routing, 359–361NAT, 681OSPF, 457RIP, 387static routing, 372redistribution, 432–4341841 router, 175, 175EIGRP (Enhanced IGRP), 418ASs in, 422–423configuration, 426–429, 427Corp, 429discontiguous networks, 434–435R1, 429–430R2, 430R3, 430–432, 430summary routes, 474–476, 475–476verifying, 438–443default ADs, 378defined, 873DUAL with, 421–422exam essentials, 476–477features and operation, 418–419hands-on labs, 478–483, 478, 482large network support, 422–426, 423–424load balancing, 435–438, 442maximum paths and hop counts, 425–426metrics, 425neighbor discovery, 419–421protocol-dependent modules, 419redistribution, 423, 432–434review questions, 484–490route discovery and maintenance, 424–425RTP with, 421summary, 476–477VLSM support and summarization, 418–419, 423–424, 423–424written lab, 477–478, 491EIGRPv6 protocol, 751–752

10089bindex.fm  Page 927  Tuesday, July 24, 2007  9:38 AM




928EIP (Ethernet Interface Processor)–exam essentialsEIP (Ethernet Interface Processor), 872–873ELANs (Emulated LANs), 872ELAP (EtherTalk Link Access Protocol), 872elections, DR and BDR, 465–466electronically erasable programmable read-only memory (EEPROM)defined, 872loading from, 175Emulated LANs (ELANs), 872enable command, 180, 185, 194–195, 296Enable diagnostic messages bit, 255enable passwordssecret, 195setting, 194–195show running-config for, 201Telnet, 296enablingOSPF configuration, 449RIPv2, 398–401, 399Encapsulating Security Payload (ESP), 827encapsulationdefined, 873Frame Relay, 800–801internetworking, 43–46, 43–44PPP, 792–795, 792–794VLANs, 575–576WANs, 777encapsulation command, 575, 777encapsulation frame-relay command, 800, 806–807, 812encryptiondefined, 873password, 199–201wireless networks, 721end-to-end VLANs, 873Enhanced IGRP. See EIGRP (Enhanced IGRP)enterprise networks, 873Erasable Programmable Read-Only Memory (EPROM), 873erase command, 266, 268erase start command, 346–347erase startup-config command, 214, 277, 343erasing configurations, 214, 277–278error checking and detectionEthernet frames, 35–36LCP, 789TCP, 75Esc+B command, 187Esc+F command, 187ESFs (Extended Superframes), 873ESP (Encapsulating Security Payload), 827ESS (Extended Service Set), 713EtherChannelCatalyst switch configuration, 526–527STP, 514Ethernet Interface Processor (EIP), 872–873Ethernet networking, 31–33, 32addressing in, 34–35, 34cabling, 39–41, 39–42at Data Link layer, 34–37, 34–35defined, 873frames, 35–37, 35half-duplex and full-duplex, 33–34at Physical layer, 37–39, 37Ethernet WAN Configuration Wizard, 819–820, 819–820EtherTalk Link Access Protocol (ELAP), 872EtherTalk product, 873exam essentialsaccess lists, 655EIGRP and OSPF, 476–477internetworking, 49–50

10089bindex.fm  Page 928  Tuesday, July 24, 2007  9:38 AM




excess burst size–FEIP (Fast Ethernet Interface Processor)929IOS, 233–234IP routing, 401–402IPv6 protocol, 767layer 2 switching and STP, 542management, 311–312NAT, 688subnetting, 158TCP/IP, 102VLANs, 598WANs, 836–837wireless networks, 729excess burst size, 873excess rate, 873exec banners, 193EXEC sessions, 173, 873exec-timeout command, 196–197exit commandfor logging out, 182Telnet, 297–298exit interface parameter, 363expansion, 874expedited delivery, 874Explicit Forward Congestion Indication (EFCI), 872explorer frames, 874explorer packets, 874exponents, 114Express Setup HTTP screen, 516–517, 516extended access listsIP, 626–632, 657–660, 874IPX, 874Extended Service Set (ESS), 713Extended Setup mode, 874Extended Superframes (ESFs), 873Extensible Authentication Protocol (EAP), 721Exterior Gateway Protocol (EGP), 377External Border Gateway Protocol (eBGP), 872external EIGRPdefault ADs, 378routes, 422, 874extranet VPNs, 826

Ffailure domains, 874fallback mechanism, 874Fast Ethernet Interface Processor (FEIP), 875Fast Ethernet technologydefined, 874speed of, 38Fast Serial Interface Processor (FSIP), 877fast switching, 874fault tolerancecore layer, 48defined, 875FCC (Federal Communications Commission), 705–706FCS (Frame Check Sequence) fieldEthernet frames, 36IP routing process, 333–336UDP segment, 78FDDI (Fiber Distributed Data Interface), 875FDM (Frequency-Division Multiplexing), 875feasible distances in EIGRP, 420, 441feasible successors in EIGRP, 420FECN (Forward-Explicit Congestion Notification) bitdefined, 875Frame Relay, 804Federal Communications Commission (FCC), 705–706FEIP (Fast Ethernet Interface Processor), 875

10089bindex.fm  Page 929  Tuesday, July 24, 2007  9:38 AM




930Fiber Distributed Data Interface (FDDI)–Frame RelayFiber Distributed Data Interface (FDDI), 875file prompt command, 267File Transfer Protocol (FTP)defined, 877for file transfer, 71–72files, transferring, 71–72filter tables, 497filteringdefined, 875frame, 502IOS firewall, 615firewalls, 610–611, 611creating, 647–654, 648–650, 653–654defined, 875IOS, 614–6155GHz wireless, 709–712, 710fixed configuration routers, 875Flags field, 85flappingdefined, 875preventing, 382flash memorydefined, 875loading, 175managing, 270–274, 271–274routers, 253verifying, 263–264flat networksdefined, 875structure of, 552–553, 553flexibility in VLANs, 555–558, 556–557floating routes, 875flooding, 875flow controldefined, 876Transport layer, 17format command, 266, 268Forward-Explicit Congestion Notification (FECN) bitdefined, 875Frame Relay, 804forward/filter decisions, 501–504, 502forward/filter tables, 499–504, 500–501forwarding ports in STP, 507forwarding state in STP, 510FQDNs (Fully Qualified Domain Names)defined, 876DNS, 73, 303FRADs (Frame Relay Access Devices), 876Fragment offset field, 85fragmentation, 876FragmentFree switching method, 876fragments, 876Frame Check Sequence (FCS) fieldEthernet frames, 36IP routing process, 333–336UDP segment, 78frame identification, 876Frame Relay, 798CIR in, 799–800congestion control, 804–805defined, 876description, 778DLCIs in, 801–803, 802encapsulation, 800–801LMI in, 803–804monitoring, 808–811overview, 798–799, 799SDM for, 822–825, 823–824single interfaces, 806subinterfaces, 806–808, 840–841, 840troubleshooting, 811–813, 812virtual circuits, 801

10089bindex.fm  Page 930  Tuesday, July 24, 2007  9:38 AM




Frame Relay Access Devices (FRADs)–half duplex931Frame Relay Access Devices (FRADs), 876Frame Relay Access Support (FRAS), 877frame relay bridging, 876frame-relay interface-dlci command, 802frame-relay lmi-type command, 803, 806frame-relay map command, 802, 812Frame Relay switching, 877frame taggingdefined, 876VLANs, 561–562framesData Link layer, 44defined, 876Ethernet, 35–37, 35filtering, 502framing, 877FRAS (Frame Relay Access Support), 877frequency, 877Frequency-Division Multiplexing (FDM), 875FSIP (Fast Serial Interface Processor), 877FTP (File Transfer Protocol)defined, 877for file transfer, 71–72full duplexdefined, 877in networking, 33–34Session layer, 16full mesh topology, 877Fully Qualified Domain Names (FQDNs)defined, 876DNS, 73, 303

GgatewaysIP routing, 331–332, 334of last resort, 375, 375Generic Routing Encapsulation (GRE)defined, 877–878IP header, 86VPNs, 826geographical distances, subnetting for, 113Get Nearest Server (GNS) requests, 877Gigabit Media Independent Interface (GMII), 38, 877global addresses in LMI, 803global commands, 184, 877global configuration mode, 184global NAT names, 672–673, 673–674global unicast addresses in IPv6, 744, 746GMII (Gigabit Media Independent Interface), 38, 877GNS (Get Nearest Server) requests, 877gossip protocol, 387grafting, 877GRE (Generic Routing Encapsulation)defined, 877–878IP header, 86VPNs, 826guard bands, 878guest-mode command, 351

HH channels, 878H field in EIGRP, 440half duplexdefined, 878Ethernet networking, 33–34Session layer, 16

10089bindex.fm  Page 931  Tuesday, July 24, 2007  9:38 AM




932hands-on labs–hostname commandhands-on labsaccess lists, 656–660, 656EIGRP and OSPF, 478–483, 478, 482IOS, 235–241IP routing, 403–406, 403management, 313–317NAT, 689–695, 690WANs, 838–841, 849handshakes, 17, 878hardware addresses, 84, 329Data Link layer, 34Ethernet addressing, 34IP routing process, 334–336HDLC (High-Level Data Link Control) protocoldefined, 878description, 778hands-on lab, 839operation, 787, 787headends, cable systems, 781Header checksum field, 85Header length fieldIP header, 85TCP segment, 76Hello messages in EIGRP, 419–420Hello protocol in OSPF, 446, 465help features in command-line interface, 185–189, 236–237helper address, 878hexadecimal numbering system, 36conversions, 26–30, 53–55IP addresses, 93notation for, 255HFC (hybrid fibre-coaxial)cable systems, 781, 781description, 779hierarchical addressing, 93–94, 878hierarchical internetworking model, 46–49, 47hierarchies, 878High-Level Data Link Control (HDLC) protocoldefined, 878description, 778hands-on lab, 839operation, 787, 787High-Speed Communication Interface (HSCI), 879High-Speed Serial Interface (HSSI), 879HIP (HSSI Interface Processor), 878history, command, 188–189hold time in EIGRP, 440holddown state, 878holddown timers, 383holddowns in IP routing, 382Holdtime field, 285holdtime information, 283–284hop countsdefined, 878EIGRP, 418, 425–426IGRP, 392–393maximum, 382RIPv1 vs. RIPv2, 391routing tables, 23hopsdefined, 878distance-vector protocols, 378–379Hops message, 87–88host addressesdefined, 878IP addresses, 94host tables, 300–302host-to-host layer, 69, 69–70, 74defined, 879key concepts of, 79port numbers in, 80–82, 80TCP, 75–77, 75UDP, 77–79, 78hostname command, 191, 791

10089bindex.fm  Page 932  Tuesday, July 24, 2007  9:38 AM




hostnames–incoming banners933hostnamescommand-line interface for, 191–192, 239host tables for, 300–302resolving, 73, 300–304, 316–317Hot Standby Router Protocol (HSRP), 879HSCI (High-Speed Communication Interface), 879HSRP (Hot Standby Router Protocol), 879HSSI (High-Speed Serial Interface), 879HSSI Interface Processor (HIP), 878hubs, 4, 4, 8–9, 9defined, 879at Physical layer, 30–31vs. switches, 26before switching, 495–496, 496hybrid fibre-coaxial (HFC)cable systems, 781, 781description, 779hybrid protocols, 379defined, 879EIGRP, 418HyperTerminal program, 41, 41, 259

IIARP (Inverse ARP)defined, 881for DLCIs, 801–802ICD (International Code Designator), 879ICMP (Internet Control Message Protocol), 87–90, 87, 89defined, 879IOS firewall, 614in IP header, 86in IP routing process, 331, 334, 336, 338–339, 338ICMPv6 server configuration, 749–750Identification field in IP header, 85identifying VLANs, 559–563, 561IEEE (Institute of Electrical and Electronics Engineers)defined, 879wireless standards, 705IEEE 802.1 specification, 562–563, 879IEEE 802.3 specification, 879IEEE 802.5 specification, 879IEEE 802.11 standards, 706–712, 712IETF (Internet Engineering Task Force) encapsulation, 800–801, 806IFS (IOS file system), 266–268for configuration, 278–279for IOS upgrades, 268–270IG address in NAT, 678IGMP (Internet Group Management Protocol), 880Ignore NVRAM contents bit, 255IGP (Interior Gateway Protocol)defined, 880function, 377IGRP (Interior Gateway Routing Protocol), 392–393default ADs, 378defined, 880enhanced. See EIGRP (Enhanced IGRP)in IP header, 86ILMI (Integrated Local Management Interface), 880implicit denies, 616–617, 630in-band management, 174, 880in-band signaling, 880inactive state in LMI, 804inactivity option with aging, 536inbound access lists, 617incoming banners, 193

10089bindex.fm  Page 933  Tuesday, July 24, 2007  9:38 AM




934Industrial, Scientific, and Medical (ISM) bands–Internet Control Message Industrial, Scientific, and Medical (ISM) bands, 706infinite networks, 382inside NAT networksdefined, 880names, 672–673, 673–674inspect command, 651–652Institute of Electrical and Electronics Engineers (IEEE)defined, 879wireless standards, 705insured bursts, 880Integrated Local Management Interface (ILMI), 880Integrated Services Digital Network (ISDN)defined, 882description, 778Inter-Switch Link (ISL) routingdefined, 882VLANs, 562inter-VLAN routing, 575–580, 576–578, 580, 588–597, 589–596interarea routing, 880interface command, 182, 193interface configuration modedefined, 880EIGRP, 426interface ethernet command, 204interface fastethernet command, 205–206interface loopback command, 467interface port-channel command, 526–527interface processors, 880interface range command, 513, 570–571interface serial command, 204–206interface tunnel command, 755–756interface type number sequence, 204–205interfaces, 182–183, 204–205bringing up, 175–179, 206–207CDP for, 290–291CLI prompts for, 182–183displaying, 221–223IP address configuration on, 207–208pipes, 208–209in routing tables, 23serial, 209–212, 209–210traffic information for, 289Interior Gateway Protocol (IGP)defined, 880function, 377Interior Routing Gateway Protocol, 392–393default ADs, 378defined, 880enhanced. See EIGRP (Enhanced IGRP)in IP header, 86Intermediate System-to-Intermediate System (IS-IS), 419, 880internal EIGRP routes, 422, 880internal router components, 252–253internal routers, 610–611, 611International Code Designator (ICD), 879International Organization for Standardization (ISO), 11, 14International Telecommunication Union-Telecommunication Standardization Sector (ITU-T), 882Internet, 880–881Internet Control Message Protocol (ICMP), 87–90, 87, 89defined, 879IOS firewall, 614

10089bindex.fm  Page 934  Tuesday, July 24, 2007  9:38 AM




Internet Engineering Task Force (IETF) encapsulation–IP access lists935in IP header, 86in IP routing process, 331, 334, 336, 338–339, 338Internet Engineering Task Force (IETF) encapsulation, 800–801, 806Internet Group Management Protocol (IGMP), 880Internet layer, 69, 69–70, 83ARP, 90–92, 90defined, 881ICMP, 87–90, 87, 89IP, 84–87, 84, 86RARP, 91, 92Internet Protocol (IP), 84–87, 84, 86, 881Internet protocolsdefined, 881IP addresses. See IP addressesTCP/IP. See TCP/IP (Transmission Control Protocol/Internet Protocol)Version 6. See IPv6 protocolinternets, 881Internetwork Operating System. See IOS (Internetwork Operating System)Internetwork Packet Exchange (IPX), 882internetworking, 3basics, 4–11, 4, 7, 9–10data encapsulation, 43–46, 43–44defined, 881Ethernet. See Ethernet networkingexam essentials, 49–50models, 11–13OSI reference model. See OSI (Open System Interconnection) reference modelreview questions, 56–61summary, 49three-layer hierarchical model, 46–49, 47written lab, 50–55, 53, 62–66internetworks, 881intra-area routing, 881intrusion detection, 614Inverse ARP (IARP)defined, 881for DLCIs, 801–802IOS (Internetwork Operating System), 172–173backing up, 264–265, 314for bringing up routers, 175–179command-line interface. See command-line interface (CLI)for connecting to routers, 173–175, 174–175defined, 865exam essentials, 233–234flash memorymanaging, 270–274, 271–274verifying, 263–264hands-on lab, 235–241IFS for, 266–270restoring, 265–266, 314review questions, 242–248summary, 232upgrading, 265–266, 268–270written labs, 234, 249IOS file system (IFS), 266–268for configuration, 278–279for IOS upgrades, 268–270IOS firewall, 614–615for Context-Based Access Control, 639–640, 639creating, 647–654, 648–650, 653–654IP (Internet Protocol), 84–87, 84, 86, 881ip access-group in command, 630ip access-group out command, 623, 630–631, 634ip access-list command, 633ip access-list standard command, 633–634IP access listsextended, 626–632, 657–660monitoring, 640–642

10089bindex.fm  Page 935  Tuesday, July 24, 2007  9:38 AM




936ip address command–IP routingstandard, 619–624, 622–624, 657for Telnet, 625–626wildcards with, 620–622ip address command, 207–208ip address negotiated command, 797IP Address screen, 815, 815IP addresses, 4–5, 84, 92–93broadcast, 100–101command-line interface for, 240configuring, 207–208defined, 881DHCP, 73–74hierarchical scheme, 93–94Class A addresses, 96–97Class B addresses, 97–98Class C addresses, 98network addressing, 94–98, 94in IP routing process, 333–340IPv6 protocol. See IPv6 protocolNAT. See Network Address Translation (NAT)private, 98–99terminology in, 93troubleshooting, 150–157, 150, 153–157IP broadcast with all zeros bit, 255IP broadcasts do not have net numbers bit, 255ip classless command, 374–375IP Control Program (IPCP), 881ip default-gateway command, 521ip default-network command, 375–377ip dhcp command, 231ip dhcp pool admin command, 351ip domain-lookup command, 303ip domain-name command, 303IP headers, 84–86, 84, 86ip host name command, 300ip http command, 225, 227ip inspect command, 639, 651–652IP multicasts, 881ip name-server command, 303ip nat inside source command, 674–676, 679ip nat outside source command, 675ip nat pool command, 675–676, 678–679ip nat translation max-entries command, 677ip nat translation timeout command, 678ip ospf cost command, 448ip route command, 363–364IP routing, 328basics, 329–331, 330configuration, 341–343, 341871W router, 359–3611242AP router, 361–3622621A router, 393–401, 399Corp router, 343–346R1 router, 346–349R2 router, 349–352R3 router, 352–359, 352–359verifying, 373–374, 393–401, 399default routing, 374–377, 375–376dynamic routing, 377–379exam essentials, 401–402examples, 336–341, 337–340hands-on lab, 403–406, 403holddowns, 382maximum hop counts, 382process, 331–336, 331protocolsadministrative distances in, 377–378classes, 378–379distance-vector, 378–382, 379–381IGRP, 392–393RIP, 383–392review questions, 407–414route poisoning, 382

10089bindex.fm  Page 936  Tuesday, July 24, 2007  9:38 AM




IP spoofing–isochronous transmission937routing loops, 380–382, 381split horizon, 382static routing, 363–364871W router, 372Corp router, 364–366R1 router, 366–368R2 router, 368–370R3 router, 370–372, 370–371summary, 401written lab, 402–403, 415IP spoofing, 612ip ssh command, 199ip subnet-zero command, 113, 142IP Unnumbered option, 356ipconfig command, 152, 781IPCP (IP Control Program), 881IPSecconfiguration, 828–836, 828–835security protocols, 827transforms, 826–827ipv6 address command, 747ipv6 dhcp pool command, 748–749ipv6 dhcp server command, 749ipv6 eigrp command, 752ipv6 enable command, 748ipv6 ospf command, 753, 763IPv6 protocol, 86, 740addressesspecial, 745structure, 742–743, 743types, 744–745benefits and uses, 741–742configurationautoconfiguration, 746–747, 747Corp, 756–758DHCPv6 servers, 747–749ICMPv6 servers, 749–750OSPFv3, 763–766R1, 758R2, 758–759R3, 759RIPng, 759–763routers, 747–748EIGRPv6, 751–752exam essentials, 767migrating to, 7536to4 tunneling, 754–755, 755dual stacking, 754NAT-PT, 755–756need for, 740–741OSPFv3, 752–753review questions, 768–771RIPng, 750–751shortened expressions, 743–744summary, 766written labs, 767, 772ipv6 rip command, 751, 759–760ipv6 router eigrp command, 752ipv6 router ospf command, 753ipv6 router rip command, 751ipv6 unicast-routing command, 747, 754IPX (Internetwork Packet Exchange), 882IPX Control Program (IPXCP), 882IPXCP (IPX Control Program), 882IPXWAN protocol, 882IS-IS (Intermediate System-to-Intermediate System), 419, 880ISDN (Integrated Services Digital Network)defined, 882description, 778ISL (Inter-Switch Link) routingdefined, 882VLANs, 562ISM (Industrial, Scientific, and Medical) bands, 706ISO (International Organization for Standardization), 11, 14isochronous transmission, 882

10089bindex.fm  Page 937  Tuesday, July 24, 2007  9:38 AM




938ITU-T–LCP (Link Control Protocol)ITU-T (International Telecommunication Union-Telecommunication Standardization Sector), 882ITU-T format, 804

Jjam signals, 32, 32Java applet blocking, 615

Kkeepalives, 803Kerberos authentication, 882keys, wireless networks, 720

LL2F (Layer 2 Forwarding), 826L2TP (Layer 2 Tunneling Protocol)defined, 826in IP header, 86LACP (Link Aggregation Control Protocol), 514LAN emulation (LANE), 882LAN Emulation Address Resolution Protocol (LE ARP), 883LAN Emulation Client (LEC), 883LAN Emulation Configuration Server (LECS), 883LAN Emulation Network-to-Network Interface (LNNI), 884LAN Emulation Server (LES), 884LAN Emulation User-to-Network Interface (LUNI), 885LAN switches, 882LANE (LAN emulation), 882LANs (Local Area Networks)defined, 882VLANs. See VLANs (virtual LANs)LAPB (Link Accessed Procedure, Balanced)defined, 883description, 778LAPD (Link Access Procedure, D channel)defined, 883description, 778large network support in EIGRP, 422–426, 423–424last-resort command, 194latencydefined, 883ports, 25Layer 2 broadcasts, 100Layer 2 Forwarding (L2F), 826layer 2 switching, 494address learning by, 499–501, 500–501benefits, 497–498vs. bridging, 499exam essentials, 542forward/filter decisions by, 501–504, 502limitations, 498–499loop avoidance, 504–505, 504–505review questions, 543–549STP in. See STP (Spanning Tree Protocol)summary, 541switching before, 494–497, 495–497written lab for, 542, 550Layer 2 Tunneling Protocol (L2TP)defined, 826in IP header, 86layered architecturedefined, 883in internetworking, 12layers, 883LCP (Link Control Protocol)defined, 883options, 789

10089bindex.fm  Page 938  Tuesday, July 24, 2007  9:38 AM




LE ARP–LocalTalk Link Access Protocol (LLAP)939LE ARP (LAN Emulation Address Resolution Protocol), 883leaky buckets, 883learning bridges, 883learning state in STP, 510leased linesdefined, 883WANs, 776LEC (LAN Emulation Client), 883LECS (LAN Emulation Configuration Server), 883Length field, 36Length of segment field, 78LES (LAN Emulation Server), 884licensing for wireless technologies, 705–706, 706Lightweight Access Point Protocol (LWAPP), 714–717, 717–718line command, 183–184, 195line aux command, 195line console command, 184, 196line delay in EIGRP, 425Line Printer Daemon (LPD)defined, 885function, 72line vty command, 197–198Link Access Procedure, D channel (LAPD)defined, 883description, 778Link Accessed Procedure, Balanced (LAPB)defined, 883description, 778Link Aggregation Control Protocol (LACP), 514Link Control Protocol (LCP)defined, 883options, 789link-establishment phase for PPP sessions, 790link-local addresses in IPv6, 744Link-State Advertisements (LSAs)defined, 885OSPF, 447link-state protocols, 378–379link-state routing algorithm, 884links, OSPF, 446listening state in STP, 510LLAP (LocalTalk Link Access Protocol), 884LLC (Logical Link Control) layerdefined, 884function, 25LMI (Local Management Interface)defined, 884Frame Relay, 803–804LNNI (LAN Emulation Network-to-Network Interface), 884load balancingdefined, 884EIGRP, 435–438, 442RIP, 379Local Area Network Emulation (LANE), 882Local Area Networks (LANs)defined, 882VLANs. See VLANs (virtual LANs)local explorer packets, 884Local Interface field, 285local loopsdefined, 885WANs, 775Local Management Interface (LMI)defined, 884Frame Relay, 803–804local NAT names, 672–673, 673–674locally unique addresses, 746LocalTalk protocol, 885LocalTalk Link Access Protocol (LLAP), 884

10089bindex.fm  Page 939  Tuesday, July 24, 2007  9:38 AM




940lock and key access lists–management of routerslock and key access lists, 636–637log command, 630logging into routers, 235–236logging synchronous command, 196–197logical addresses, 84, 329, 885Logical Link Control (LLC) layerdefined, 884function, 25login banners, 193–194login command, 195logout command, 181Long Range Ethernet (LRE), 784–785loopback addresses, 151loopback interfaces, 466–471loopback tests, 96loopsavoiding, 504–505, 504–505, 885routing, 380–382, 381LPD (Line Printer Daemon)defined, 885function, 72LRE (Long Range Ethernet), 784–785LSAs (Link-State Advertisements)defined, 885OSPF, 447LUNI (LAN Emulation User-to-Network Interface), 885LWAPP (Lightweight Access Point Protocol), 714–717, 717–718

Mmac access-list command, 635MAC (Media Access Control) addresses, 25Catalyst switches, 528–530Data Link layer, 34defined, 885Ethernet addressing, 4–5, 34–35, 34IPv6 autoconfiguration, 746RARP for, 91split-MAC architecture, 715–716, 715STP, 521–522VLANs, 559wireless network authentication, 719–720MAC forward/filter tables, 499–504, 500–501MAC frame format in Ethernet frames, 34–35MAC (Media Access Control) layerdefined, 885Ethernet, 38function, 25MacIP protocol, 885macro command, 536–537major commands, 184man-in-the-middle attacks, 613Management Information Base (MIB), 886management of routers, 252CDP for, 283–289, 315–316configuration backups and restorationbackups, 274–276erasing, 277–278IFS for, 278–279restoring, 276–277SDM for, 280–283, 280–283configuration registers, 254–262exam essentials, 311–312hands-on lab, 313–317hostname resolution, 300–304, 316–317and internal router components, 252–253IOS backups and restoration, 262–263, 263backups, 264–265flash memory management, 270–274, 271–274

10089bindex.fm  Page 940  Tuesday, July 24, 2007  9:38 AM




Manchester encoding–MII (Media Independent Interface)941flash memory verification, 263–264IFS for, 266–270restoring and upgrading, 265–266network connectivity, 305–311, 306review questions, 318–324and router boot sequence, 253–254subnetting for, 113summary, 311Telnet for, 295–299, 316written lab, 313, 325Manchester encoding, 885MANs (metropolitan area networks), 885map command, 802, 810–812MAPs (Mesh Access Points), 717mask parameter, 363masksOSPF, 450subnet, 115–116, 137–138Maximum Burst Size (MBS), 886maximum bursts, 886Maximum Cell Delay Variation (MCDV), 886Maximum Cell Loss Ratio (MCLR), 886Maximum Cell Transfer Delay (MCTD), 886maximum hop countsdefined, 886EIGRP, 425for routing loops, 382maximum-hops command, 425maximum-paths command, 425maximum paths in EIGRP, 425–426maximum rates, 886Maximum Transmission Units (MTUs)defined, 887displaying, 218–219EIGRP, 425Ethernet, 38ICMPv6 servers, 750IGRP, 392MBONEs (multicast backbones), 886MBS (Maximum Burst Size), 886MCDV (Maximum Cell Delay Variation), 886MCLR (Maximum Cell Loss Ratio), 886MCR (Minimum Cell Rate), 886MCTD (Maximum Cell Transfer Delay), 886Media Access Control (MAC) layerdefined, 885Ethernet, 38function, 25Media Access Control addresses. See MAC (Media Access Control) addressesMedia Independent Interface (MII)defined, 886throughput in, 38media translationdefined, 886LAN switching, 30Mesh Access Points (MAPs), 717Mesh wireless topology, 716–717, 717–718metricsdefined, 898EIGRP, 425IGRP, 392routing tables, 23metropolitan area networks (MANs), 885MIB (Management Information Base), 886migrating to IPv6 protocol, 7536to4 tunneling, 754–755, 755dual stacking, 754NAT-PT, 755–756MII (Media Independent Interface)defined, 886throughput in, 38

10089bindex.fm  Page 941  Tuesday, July 24, 2007  9:38 AM




942millions of instructions per second (mips)–NAT-PT (NAT protocol translation)millions of instructions per second (mips), 887MIMO (Multiple-Input Multiple Output), 711–712mini-OS component, 253Minimum Cell Rate (MCR), 886MIP (Multichannel Interface Processor), 887mips (millions of instructions per second), 887mkdir command, 266, 268MLP (Multilink PPP), 887mls qos command, 587–588MMP (Multichassis Multilink PPP), 887modem eliminators, 887modems, 887modulation, 887more command, 266–267MOSPF (Multicast OSPF), 887MOTD banners, 193–194MPLS (MiltiProtocol Label Switching), 779MPOA (Multiprotocol over ATM), 887MTUs (Maximum Transmission Units)defined, 887displaying, 218–219EIGRP, 425Ethernet, 38ICMPv6 servers, 750IGRP, 392multi-access networks, 447multicast addresses, 887multicast backbones (MBONEs), 886multicast group addresses, 101multicast groups, 887Multicast OSPF (MOSPF), 887multicast send VCC, 888multicasts, 100defined, 887IPv6, 742, 745layer 2 switching, 499LMI, 803multimedia applications, 554reliable, 421Multichannel Interface Processor (MIP), 887Multichassis Multilink PPP (MMP), 887multilayer switches, 888Multilink PPP (MLP), 887multilinksdefined, 888LCP, 789multimedia applications, 554multiple autonomous systems, 422–423multiple devices, Telnet with, 297Multiple-Input Multiple Output (MIMO), 711–712multiplexing, 888multipoint subinterfaces, 807MultiProtocol Label Switching (MPLS), 779Multiprotocol over ATM (MPOA), 887

NNAK (Negative Acknowledgment) responses, 888Name Binding Protocol (NBP), 888named access listsdefined, 888working with, 632–634namesNAT, 672, 673–674R3 router configuration, 353, 353NAPT-PT (Network Address Port Translation), 756NAT. See Network Address Translation (NAT)NAT-PT (NAT protocol translation), 755–756

10089bindex.fm  Page 942  Tuesday, July 24, 2007  9:38 AM




native VLANs–Network layer943native VLANs, 559defined, 888modifying, 573–574NBAR (Network Based Application Recognition), 834NBMA (non-broadcast multi-access) networks, 447NBP (Name Binding Protocol), 888NCP (Network Control Protocol), 788, 790Negative Acknowledgment (NAK) responses, 888neighbor databases, 446neighbor discovery, 419–421neighboring routers, 888neighborsCDP, 284–289defined, 888EIGRP, 419–421OSPF, 446, 465neighborship tablesdefined, 888EIGRP, 420, 425NetBEUI (NetBIOS Extended User Interface), 889NetBIOS (Network Basic Input/Output System), 889netmask command, 679NetView product, 889NetWare Link Services Protocol (NLSP), 890NetWare operating system, 889Network Access layerdefined, 889function, 69, 69–70Network Address Translation (NAT), 98–99, 670configuration, 679–684, 680dynamic, 675, 692–693overloading, 675–676, 694–695SDM for, 684–687, 685–686static, 674–675verifying, 676defined, 888exam essentials, 688hands-on labs, 689–695, 690names, 672, 673–674NAT-PT, 755–756operation, 673–674, 673–674review questions, 696–700summary, 688testing and troubleshooting, 677–679, 678–679types, 671–672uses, 670, 671written labs, 688–689, 701network addresses, 93defined, 889IP addressing, 94–98, 94routing tables, 22Network Based Application Recognition (NBAR), 834Network Basic Input/Output System (NetBIOS), 889network commandEIGRP, 426OSPF, 450RIP, 384network connectivity, 305ping command for, 305–306traceroute command for, 307–308Network Control Protocol (NCP), 788, 790Network File System (NFS)defined, 889file sharing, 71Network Interface Cards (NICs)defined, 890MAC addresses in, 34–35, 34Network layerdefined, 889encapsulation, 45–46OSI reference model, 22–23, 22–23

10089bindex.fm  Page 943  Tuesday, July 24, 2007  9:38 AM




944network-layer protocol phase in PPP sessions–OFDMnetwork-layer protocol phase in PPP sessions, 790Network Management Processor (NMP), 890network performance, subnetting for, 113network reconnaissance attacks, 613network segmentation, 6–8, 6network termination (NT) devicesNT1, 890NT2, 890Network Time Protocol (NTP), 637network traffic, subnetting for, 113networksclassless, 137–138topology documentation, 292–294, 292, 294next hop address parameter, 363Next Hop Resolution Protocol (NHRP), 889Next Hop Server (NHS), 889NFS (Network File System)defined, 889file sharing, 71NHRP (Next Hop Resolution Protocol), 889NHS (Next Hop Server), 889nibbles, 27–29defined, 890in MII, 38NICs (Network Interface Cards)defined, 890MAC addresses in, 34–35, 34NLSP (NetWare Link Services Protocol), 890NMP (Network Management Processor), 890no auto-summary command, 435no cdp enable command, 284, 290no cdp run command, 284, 290no ip domain-lookup command, 303no ip host command, 302no login command, 195, 295no shutdown command, 219, 579node addressesdefined, 890in IP addresses, 94non-broadcast multi-access (NBMA) networks, 447non-designated ports, 890non-stub areas, 890non-volatile RAM (NVRAM), 175copying configuration to, 275–276defined, 890displaying, 278for startup-config file, 253–254, 258nondesignated ports, 507nonegotiate command, 572nonroot bridges, 507NRZ (Nonreturn to Zero) encoding, 890NRZI (Nonreturn to Zero Inverted) encoding, 890NT (network termination) devicesNT1, 890NT2, 890NTP (Network Time Protocol), 637number system conversions, 26–30, 53–55NVRAM (non-volatile RAM), 175copying configuration to, 275–276defined, 890displaying, 278for startup-config file, 253–254, 258

Oo/r command, 259OC (Optical Carrier) protocols, 891octets, 93, 891OEM bit enabled bit, 255OFDM (Orthogonal Frequency Division Multiplexing) technique, 709

10089bindex.fm  Page 944  Tuesday, July 24, 2007  9:38 AM




100BaseFX technology–OUIs (Organizationally Unique Identifiers)945100BaseFX technology, 38100BaseT technology, 852100BaseTX technology, 38, 8521000BaseCX technology, 381000BaseLX technology, 391000BaseSX technology, 391000BaseT technology, 39ones density clocking, 891open-access mode in wireless networks, 719Open Shortest Path First protocol. See OSPF (Open Shortest Path First) protocolOpen System Interconnection (OSI), 891Optical Carrier (OC) protocols, 891optimized network performance, subnetting for, 113Options fieldIP header, 85TCP header, 76Organizationally Unique Identifiers (OUIs)defined, 891format of, 34Orthogonal Frequency Division Multiplexing (OFDM) technique, 709OSI (Open System Interconnection), 891OSI (Open System Interconnection) reference model, 11–15, 13–15, 50–52Application layer, 15–16Data Link layer, 24–30, 24, 26defined, 891Network layer, 22–23, 22–23Physical layer, 30–31Presentation layer, 16Session layer, 16Transport layer, 16–21, 18–21OSPF (Open Shortest Path First) protocoladjacencies, 465–466configuration, 449871W, 457areas, 450–453, 452Corp, 453–454debugging, 462–464enabling, 449R1, 454R2, 454R3, 454–456, 455–456summary routes, 474–476, 475–476troubleshooting, 471–473, 472–474verifying, 457–462default ADs, 378defined, 891DR and BDR elections, 465–466exam essentials, 476–477hands-on labs, 478–483, 478, 482as link-state protocol, 379loopback interfaces, 466–471neighbors, 465overview, 444–446, 445priorities, 469–471, 470review questions, 484–490vs. RIP, 444–445SPF tree calculation, 448–449summary, 476–477terminology, 446–448wildcards, 450–453, 452written lab, 477–478, 491OSPFv3 protocolIPv6 configuration, 763–766overview, 752–753OUIs (Organizationally Unique Identifiers)defined, 891format of, 34

10089bindex.fm  Page 945  Tuesday, July 24, 2007  9:38 AM




946out-of-band management–permit ip any commandout-of-band management, 174, 891out-of-band signaling, 892outbound access lists, 617outside NAT names, 672–673, 674overload command, 676overloading NAT, 672–674, 674configuration, 675–676, 694–695defined, 892

PPacket InterNet Group command. See Ping (Packet Internet Groper) commandPacket Level Protocol (PLP), 893packet sniffer attacks, 613packet switch exchange (PSE), 895packet-switched networks (PSNs), 895packet switches, 892packet switchingdefined, 892WANs, 776, 776packets, 43defined, 892Network layer, 22PAgP (Port Aggregation Protocol), 514PAP (Password Authentication Protocol)defined, 892PPP, 790–791parity checking, 892partial meshed networks, 892passive-interface commandEIGRP, 426–427RIP, 390passive interfaces, 386passive state in EIGRP, 441password attacks, 613Password Authentication Protocol (PAP)defined, 892PPP, 790–791password commandfor enable passwords, 194in PPP, 791passwordsauxiliary, 195–196console, 196–197enable. See enable passwordsencrypting, 199–201FTP, 71R3 router configuration, 353, 353recovering, 258–261SDM, 227setting, 194–195, 237–239Telnet, 197–198, 295–296PAT (Port Address Translation), 672–674, 674configuration, 675–676, 694–695defined, 892paths in EIGRP, 425–426PCM (pulse code modulation), 892PCR (peak cell rate), 892PDMs (protocol-dependent modules)defined, 895EIGRP, 419PDNs (public data networks), 892PDUs (Protocol Data Units)defined, 893in encapsulation, 43, 43–44peak cell rate (PCR), 892peer route authentication, 615per-user firewalls, 614performance, subnetting for, 113perimeters, 610–611, 611periodic command, 637permanent parameter, 364permanent virtual circuits (PVCs)defined, 895Frame Relay, 801permanent virtual paths (PVPs), 895permit ip any command, 646

10089bindex.fm  Page 946  Tuesday, July 24, 2007  9:38 AM




PGP (Pretty Good Privacy) encryption–ports and port numbers947PGP (Pretty Good Privacy) encryption, 893phantom routers, 893phone calls in VLANs, 586–588Physical layer, 30–31defined, 893Ethernet networking at, 37–39, 37PIM (Protocol Independent Multicast) protocol, 893PIM-DM (Protocol Independent Multicast Dense Mode), 893PIM-SM (Protocol Independent Multicast Sparse Mode), 893Ping (Packet Internet Groper) commanddefined, 893ICMP, 88IP addresses, 151–152for network connectivity, 305–306protocols with, 214–215TFTP, 265for verifying configuration, 373–374ping of death attacks, 612pinhole congestion, 380, 380, 893pipes, 208–209plain old telephone service (POTS), 894Platform field, 285pleisochronous transmissions, 893PLP (Packet Level Protocol), 893PNNI (Private Network-Network Interface), 893PoE (Power over Ethernet) light, 516point-to-multipoint connectionsdefined, 893OSPF, 448point-to-point connectionsdefined, 894OSPF, 448WANs, 776Point-to-Point Protocol. See PPP (Point-to-Point Protocol)Point-to-Point Protocol over Ethernet (PPPoE)for ADSL, 783–784, 784configuration, 796–797, 818–822, 819–822description, 778point-to-point subinterfaces, 807Point-to-Point Tunneling Protocol (PPTP), 826points of presence (POPs)defined, 894WANs, 775poison reverse updatesdefined, 894function, 382poisoning, route, 382policy-based, multi-interface filtering, 615polling access method, 894POP (Post Office Protocol), 894POPs (points of presence)defined, 894WANs, 775Port Address Translation (PAT), 672–674, 674configuration, 675–676, 694–695defined, 892Port Aggregation Protocol (PAgP), 514Port ID field, 285port redirection attacks, 613port-security aging command, 536port-security command, 503–504PortFast, Catalyst switch configuration, 522–523portfast command, 512–513, 522–523ports and port numbersaccess lists, 634–636Catalyst switches, 516, 521–522CDP for, 290–291CNA, 535–536, 535, 538, 538

10089bindex.fm  Page 947  Tuesday, July 24, 2007  9:38 AM




948positive acknowledgment with retransmission technique–propagations in RIPconsole commands for, 196–197defined, 894dynamic mapping, 615forward/filter decisions, 503–504host-to-host layer protocols, 80–82, 80NAPT-PT, 756security for, 503–504, 521–522, 894STP, 507–511TCP, 76, 80–82Transport layer, 45, 45UDP segment, 78VLANs, 570–574, 586positive acknowledgment with retransmission technique, 21, 894POST (power-on self test)in bringing up routers, 175–176as router component, 252–253Post Office Protocol (POP), 894POTS (plain old telephone service), 894Power over Ethernet (PoE) light, 516powers of 2, 114PPP (Point-to-Point Protocol), 788–789, 788authentication, 790–794, 813–818, 813–816configuring, 791–792, 838–839, 838debugging, 793–796, 794–795defined, 894description, 778encapsulation, 792–795, 792–794LCP in, 789sessions, 790ppp authentication command, 791, 797PPP callback, 789ppp chap command, 797PPP over ATM (PPPoA), 783PPPoE (Point-to-Point Protocol over Ethernet)for ADSL, 783–784, 784configuration, 796–797, 818–822, 819–822description, 778pppoe-client command, 797pppoe enable command, 796–797PPTP (Point-to-Point Tunneling Protocol), 826Pre-Shared Key (PSK), 720preambles in Ethernet frames, 36prefix-length command, 679prefix routing, 383, 894Presentation layerdefined, 894OSI reference model, 16Pretty Good Privacy (PGP) encryption, 893PRI (Primary Rate Interface), 894prioritiesdatagrams, 85OSPF, 469–471, 470STP, 508–510Priority field, 85priority queueing, 895private IP addresses, 98–99Private Network-Network Interface (PNNI), 893privileged modedefined, 184, 895entering, 180, 260Process/Application layer, 69, 69–70defined, 895protocols in, 70–74process switching, 895processes, connectivity, 310–311PROM (Programmable Read-Only Memory), 895prompts in command-line interface, 182for interfaces, 182–183line commands, 183–184for routing protocol configurations, 184for subinterfaces, 183propagation delay, 895propagations in RIP, 390

10089bindex.fm  Page 948  Tuesday, July 24, 2007  9:38 AM




Protocol Data Units (PDUs)–R3 router configuration949Protocol Data Units (PDUs)defined, 893in encapsulation, 43, 43–44protocol-dependent modules (PDMs)defined, 895EIGRP, 419Protocol field, 85–87Protocol Independent Multicast (PIM) protocol, 893Protocol Independent Multicast Dense Mode (PIM-DM), 893Protocol Independent Multicast Sparse Mode (PIM-SM), 893protocol stacks, 895protocols. See also specific protocols by nameadministrative distances in, 377–378classes, 378–379CLI prompts for, 184defined, 895Proxy ARP (Proxy Address Resolution Protocol)defined, 895operation, 91–92pruningdefined, 895VTP, 565–566PSE (packet switch exchange), 895PSK (Pre-Shared Key), 720PSNs (packet-switched networks), 895PSTNs (public switched telephone networks), 895public data networks (PDNs), 892pulse code modulation (PCM), 892PVCs (permanent virtual circuits)defined, 895Frame Relay, 801PVP tunneling, 896PVPs (permanent virtual paths), 895pwd command, 266, 268

QQoS (Quality of Service)defined, 896VLAN telephony, 586VPN tunnels, 832–836, 832–835QoS Policy Generation screen, 834, 834QoS Wizard, 833–835, 833–835question marks (?) for commands, 185–186queues, 896

RR reference point, 896R1 router configurationEIGRP, 429–430IP, 346–349IPv6, 758NAT, 681OSPF, 454RIP, 384–385static routing, 366–368R2 router configurationEIGRP, 430IP, 349–352IPv6, 758–759NAT, 681OSPF, 454RIP, 385static routing, 368–370R3 router configurationEIGRP, 430–432, 430IP, 352–359, 352–359IPv6, 759NAT, 681OSPF, 454–456, 455–456RIP, 385–387, 386static routing, 370–372, 370–371

10089bindex.fm  Page 949  Tuesday, July 24, 2007  9:38 AM




950RA (router advertisement) requests–remote VPNsRA (router advertisement) requestsDHCPv6 servers, 748ICMPv6 servers, 750IPv6 autoconfiguration, 746, 747radio frequencies (RF) for wireless technologies, 704RADIUS (Remote Authentication Dial-In User Service)defined, 896wireless networks, 721RAM (random access memory)defined, 896routers, 253range commandSTP, 512–513VLANs, 570–571Rapid Spanning Tree Protocol (RSTP)benefits, 513–514Catalyst switch configuration, 525–526RAPs (Route Access Points), 717–718RARP (Reverse Address Resolution Protocol)defined, 896operation, 91, 92RARP servers, 91, 896rate queues, 896RCP (Remote Copy Protocol), 896read-only memory (ROM)defined, 897routers, 253recovering passwords, 258–261redirection attacks, 613redistribute eigrp command, 433redistribute rip command, 432redistributionEIGRP, 423, 432–434OSPF, 444RIP, 391reduced network traffic, subnetting for, 113redundancy, 896refcount command, 677reference modelsdefined, 896OSI. See OSI (Open System Interconnection) reference modelreflexive access lists, 637registered jack (RJ) connectorsconsole ports, 173–174defined, 897Ethernet, 37, 41, 42registers, configuration, 253bits in, 254–255changing values, 256–257checking values, 256defined, 866for password recovery, 258–261Registry, hexadecimal addresses in, 93reliabilitydefined, 896displaying, 218EIGRP, 425reliable data delivery, 21, 21reliable multicasts, 421, 896–897reliable networking, 17Reliable Transport Protocol (RTP), 421reloadingdefined, 897routers, 260remark command, 638–639remarks in access lists, 638–639Remote Authentication Dial-In User Service (RADIUS)defined, 896wireless networks, 721remote client machines, Telnet for, 71Remote Copy Protocol (RCP), 896remote VPNs, 825

10089bindex.fm  Page 950  Tuesday, July 24, 2007  9:38 AM




repeaters–route invalid timers951repeaters, 30–31reported distances in EIGRP, 420“request timed out” message, 335Request To Send (RTS) signalCSMA/CD, 708defined, 898reserved addressesIP, 96, 99IPv6, 745Reserved field, 76Reset Router option, 352resolving host names, 73, 300–304, 316–317restorationconfiguration, 276–277, 280–283, 280–283IOS, 265–266, 314Retransmission Time Out (RTO) field, 440Reverse Address Resolution Protocol (RARP)defined, 896operation, 91, 92RF (radio frequencies) for wireless technologies, 704RFC1483 routing, 783RIDs (Router IDs)OSPF, 446, 466OSPFv3, 752RIF (Routing Information Field), 897ring stations, 897ring topology, 897RIP (Routing Information Protocol), 383configuration, 405–406871W router, 387Corp router, 383–384example, 389–390, 389R1 router, 384–385R2 router, 385R3 router, 385–387, 386default ADs, 378defined, 897load balancing, 379need for, 391vs. OSPF, 444–445propagations, 390RIPv2enabling, 398–401, 399vs. RIPv1, 390–392timers, 383verifying routing tables, 387–389RIPng protocolconfiguration, 759–763overview, 750–751RJ connectorsconsole ports, 173–174defined, 897Ethernet, 37, 41, 42rmdir command, 266, 268robbed-bit signaling, 897rolled cabledefined, 897Ethernet networking, 40–41, 40ROM (read-only memory)defined, 897routers, 253ROM monitor component, 252ROM monitor mode field, 255rommon 1 prompt, 259root bridgesdefined, 897STP, 506, 508–510, 532–534root ports in STP, 507round-robin load balancing, 379Route Access Points (RAPs), 717–718route aggregation, 147–150, 147, 149route discovery in EIGRP, 424–425route flaps, 897route flush timers, 383route invalid timers, 383

10089bindex.fm  Page 951  Tuesday, July 24, 2007  9:38 AM




952route poisoning–RSTP (Rapid Spanning Tree Protocol)route poisoningdefined, 897IP routing, 382Route Processors (RPs), 898route redistributionEIGRP, 423, 432–434OSPF, 444RIP, 391route summarizationdefined, 898EIGRP, 423–424, 424process, 147–150, 147, 149Route/Switch processors (RSPs), 898route update packets, 22route update timers, 383routed protocolsdefined, 898Network layer, 22router advertisement (RA) requestsDHCPv6 servers, 748ICMPv6 servers, 750IPv6 autoconfiguration, 746, 747router configuration mode, 426router eigrp command, 426, 429Router IDs (RIDs)OSPF, 446, 466OSPFv3, 752router ospf command, 449–450router rip command, 383–384router solicitation (RS) requestsDHCPv6 servers, 748ICMPv6 servers, 750IPv6 autoconfiguration, 746, 747routers, 8–11, 9–10, 23, 23boot sequence, 253–254, 259bringing up, 175–179configuring. See configurationconnecting to, 173–175, 174–175defined, 898interfaces for. See interfaceslogging into, 235–236managing. See management of routersNetwork layer, 22network segmentation, 6–7reloading, 260before switching, 494–495routingcommand-line interface for, 189–191defined, 898Internet layer protocols for, 83IP. See IP routingbetween VLANs, 567–568, 567–568routing by rumor, 379routing domains, 898Routing Information Field (RIF), 897Routing Information Protocol. See RIP (Routing Information Protocol)routing loops, 380–382, 381routing metricsdefined, 898EIGRP, 425IGRP, 392routing tables, 23routing protocols, 22. See also specific protocols by nameCLI prompts for, 184defined, 898routing tables, 22–23, 22defined, 898EIGRP, 425RIP, 387–389RPs (Route Processors), 898RS (router solicitation) requestsDHCPv6 servers, 748ICMPv6 servers, 750IPv6 autoconfiguration, 746, 747RSPs (Route/Switch processors), 898RSTP (Rapid Spanning Tree Protocol)benefits, 513–514Catalyst switch configuration, 525–526

10089bindex.fm  Page 952  Tuesday, July 24, 2007  9:38 AM




RTO (Retransmission Time Out) field–seed routers953RTO (Retransmission Time Out) field, 440RTP (Reliable Transport Protocol), 421RTS (Request To Send) signalCSMA/CD, 708defined, 898running-config file, 177for configuration, 213, 275for interface status, 207for IP access lists, 640–642for passwords, 199–200for PPP, 796RXBOOT component, 253

SS reference points, 899S1 Catalyst switch configuration, 517–518S2 Catalyst switch configuration, 518–519SA (Source Address) field, 36sampling rates, 899SAP (Service Access Point), 899SAP (Service Advertising Protocol), 899saving configurations, 212–213, 237scalabilityRIPv1 vs. RIPv2, 391VLANs, 555–558, 556–557SCR (sustainable cell rate), 899scrolling command lines, 188SDH (Synchronous Digital Hierarchy), 899SDLC (Synchronous Data Link Control), 899SDM (Security Device Manager), 175, 223–224access listscreating, 643–647, 643–646firewalls, 647–654, 648–650, 653–654backups and restores, 280–283, 280–283configuring, 225–232, 226–230, 241connectivity, 306, 306downloading, 224flash memory management, 270–274, 271–274Frame Relay configuration, 822–825, 823–824NAT configuration, 684–687, 685–686PPP authentication, 813–818, 813–816PPPoE configuration, 818–822, 819–822Telnet protocol, 299–300, 299–300VPN configuration, 828–836, 828–835wireless network configuration, 721–728, 722–728secondary command, 207secret command, 194Secure Shell (SSH), 198–199security, 610access lists. See access listsCatalyst switch configuration, 521–522devices for, 610–611, 611firewalls, 614–615, 647–654, 648–650, 653–654forward/filter decisions, 503–504IPSec, 827threats, 611–613VLANs, 555wireless networks, 718–721Security Device Manager. See SDM (Security Device Manager)seed routers, 899

10089bindex.fm  Page 953  Tuesday, July 24, 2007  9:38 AM




954segment format–show interface commandsegment formatTCP, 75–77, 75UDP, 78–79, 78segmentation, 6–8, 6Seq field in EIGRP, 440Sequence number fieldESP, 827TCP header, 76Sequenced Packet Exchange (SPX), 902sequencingdefined, 899TCP, 76serial interface commands, 209–212, 209–210Serial Line Internet Protocol (SLIP), 900serial transmissionsdefined, 899WANs, 785–786Serial Tunnel (STUN) technology, 904Serial WAN Connection Wizard, 814, 814server mode in VTP, 564servers, 899Service Access Point (SAP), 899Service Advertising Protocol (SAP), 899service password-encryption command, 200–201, 791Service Profile Identifiers (SPIDs), 902Service Set Identifiers (SSIDs), 713, 719–720Session layerdefined, 899OSI reference model, 16sessionsPPP, 790Telnet, closing, 298–299set-based routers, 899setup command, 178setup modesdefined, 899router, 178–179, 184SFD (Start Frame Delimiter)/Synch field, 36SFs (Super Frames), 900shared keys, 720shared trees, 900shielded twisted pair (STP) wiring, 904shortened expressions in IPv6, 743–744Shortest Path First (SPF) algorithmdefined, 902OSPF, 448–449shortest-path-first protocols, 378–379, 900show access-list command, 640–641show cdp command, 284show cdp entry * command, 287–288show cdp interface command, 289show cdp neighbors command, 284–285show cdp neighbors detail command, 286–289, 796show cdp traffic command, 289show commands, do for, 203–204show controllers command, 222–223, 222–223, 346show file command, 267, 269–270, 278show flash command, 257, 263–264, 269show frame command, 808show frame map command, 810–811show frame-relay lmi command, 809show frame-relay map command, 802, 812show frame-relay pvc command, 805, 809–810show history command, 188–189show hosts command, 301, 304show interface commandCatalyst switches, 528Frame Relay, 810interface status, 207PPP, 792–793, 796verifying configurations, 216–220

10089bindex.fm  Page 954  Tuesday, July 24, 2007  9:38 AM




show interface fastethernet command–Site to Site VPN screen955show interface fastethernet command, 217show interface serial command, 219show interface trunk command, 566show ip access-list command, 640show ip arp command, 152show ip eigrp command, 438, 440–441show ip eigrp topology command, 420show ip interface command, 220–221, 395, 640–642show ip interface brief command, 221show ip nat statistics command, 677, 683show ip nat translation command, 676, 682–683show ip ospf command, 458–459, 467–470show ip ospf database command, 459show ip ospf interface command, 460, 466, 471–472show ip ospf neighbor command, OSPF, 461show ip protocols command, 394–395, 461–462show ip route command, 330default routes, 376EIGRP, 438–439OSPF, 457–458RIP routes, 388–389routing tables, 345, 349static routes, 365–372show ipv6 ospf neighbor command, 764show ipv6 protocols command, 761, 764show ipv6 rip command, 761show ipv6 route command, 757–758, 760, 764show mac access-group command, 640, 642show mac address-table command, 502, 528show parser command, 537–538show processes command, 310–311show protocols command, 221show running-config commandCatalyst switches, 528configuration, 213, 275documentation, 292–294interface status, 207IP access lists, 640–642passwords, 201PPP, 796show sessions command, 297–298, 302show spanning-tree command, 509–510, 525, 530–533show startup-config command, 213–214, 275show terminal command, 188–189show users command, 299show version command, 189, 256–257show vlan command, 569–570, 583show vlan privileged command, 587show vtp password command, 581–582show vtp status command, 581–582, 584–585shutdown command, 206–207Signal Quality Error (SQE) messages, 902signaling packets, 900silicon switching, 900Silicon Switching Engine (SSE), 903Simple Mail Transfer Protocol (SMTP)defined, 900function, 72Simple Network Management Protocol (SNMP)defined, 901function, 72–73simplex modes, 16, 900simplified management, subnetting for, 113Site to Site VPN screen, 828, 828

10089bindex.fm  Page 955  Tuesday, July 24, 2007  9:38 AM




956site-to-site VPNs–SR/TLB (Source-Route Translational Bridging)site-to-site VPNs, 825, 828, 8286to4 tunneling, 754–755, 755sliding window method, 900SLIP (Serial Line Internet Protocol), 900Small Office/Home Office (SOHO)cable, 780defined, 901smart-serial cable, 785SmartPorts, 535, 535, 538, 538SMDS (Switched Multimegabit Data Service), 900SMTP (Simple Mail Transfer Protocol)defined, 900function, 72SNA (System Network Architecture), 900SNAP (Subnetwork Architecture Protocol), 900snapshot routing, 901SNMP (Simple Network Management Protocol)defined, 901function, 72–73sockets, 901software addresses, 84, 901SOHO (Small Office/Home Office)cable, 780defined, 901SONET (Synchronous Optical Network), 901Source Address (SA) field, 36Source IP address field, 85Source port fieldTCP segment, 76UDP segment, 78source ports in TCP, 80–81Source-Route Bridging (SRB), 902Source-Route Translational Bridging (SR/TLB), 903Source-Route Transparent Bridging (SRT), 903Source Service Access Points (SSAPs), 903source trees, 901SPAN (Switched Port Analyzer), 901spanning explorer packets, 901spanning-tree algorithm (STA), 506, 902spanning-tree backbonefast command, 524–525spanning-tree bpdufilter command, 524spanning-tree bpduguard command, 523–524spanning-tree mode command, 525spanning-tree portfast command, 512–513, 522–523Spanning-Tree Protocol. See STP (Spanning Tree Protocol)spanning-tree uplinkfast command, 524spanning-tree vlan command, 509spanning trees, 901spans, 901special purpose addressesIP, 96IPv6, 745specific configuration modes, 184SPF (Shortest Path First) algorithmdefined, 902OSPF, 448–449SPIDs (Service Profile Identifiers), 902split horizon protocolsdefined, 902IP routing, 382split-MAC architecture, 715–716, 715spoofing, 612, 902spoolers, 902SPs (switch processors), 901SPX (Sequenced Packet Exchange), 902SQE (Signal Quality Error) messages, 902SR/TLB (Source-Route Translational Bridging), 903

10089bindex.fm  Page 956  Tuesday, July 24, 2007  9:38 AM




SRB (Source-Route Bridging)–stub networks957SRB (Source-Route Bridging), 902SRT (Source-Route Transparent Bridging), 903SRTT field, 440SSAPs (Source Service Access Points), 903SSE (Silicon Switching Engine), 903SSH (Secure Shell), 198–199SSIDs (Service Set Identifiers), 713, 719–720STA (spanning-tree algorithm), 506, 902Stacheldraht attacks, 612standard access listsIP, 619–624, 622–624, 657, 903IPX, 903star topology, 903Start Frame Delimiter (SFD)/Synch field, 36startup-config file, 212–213deleting, 277displaying, 213–214, 275password recovery, 258router boot sequence, 254startup ranges, 903state transitionsdefined, 903Physical layer, 30Stateful IOS Firewall inspection engine, 614static NAT, 671, 674–675Static NAT-PT, 756static routing, 328, 363–364871W router, 372Corp router, 364–366default ADs, 378defined, 903hands-on lab, 404–405R1 router, 366–368R2 router, 368–370R3 router, 370–372, 370–371static VLANscharacteristics, 558–559defined, 903statistical multiplexing, 903statusinterface, 207virtual circuits, 803sticky command, 504, 522STM-1 (Synchronous Transport Module Level 1), 904store-and-forward switching method, 904STP (shielded twisted pair) wiring, 904STP (Spanning Tree Protocol), 505–506, 506BackboneFast, 513Catalyst switches. See Catalyst switch configurationCisco Network Assistant, 534–541, 535, 538–541convergence, 511–512, 511defined, 902, 904EtherChannel, 514exam essentials, 542operations, 507–508port states, 510–511PortFast, 512–513review questions, 543–549root bridge selection, 508–510, 532–534RSTP, 513–514summary, 541terminology, 506–507UplinkFast, 513written labs, 542, 550straight-through cabledefined, 904Ethernet networking, 39, 39stub areas, 904stub networks, 904

10089bindex.fm  Page 957  Tuesday, July 24, 2007  9:38 AM




958STUN (Serial Tunnel) technology–switchport trunk commandSTUN (Serial Tunnel) technology, 904subarea nodes, 904subareas, 904subchannels, 904subcommands, 184subinterfacesCLI prompts for, 183defined, 904Frame Relay, 806–808, 840–841, 840VLANs, 575subnet addresses, 904subnet masksdefined, 904need for, 115–116VLSMs, 137–138subnet-zero command, 113, 142subnets and subnetting, 112–113CIDR, 116–118Class A addresses, 134–136Class B addresses, 127–133Class C networks, 118–127, 120, 122creating, 114–115defined, 904–905exam essentials, 158ip subnet-zero, 113review questions, 161–167subnet masks for, 115–116summarization, 147–150, 147, 149summary, 157–158VLANs, 575written labs, 158–160, 168–169Subnetwork Architecture Protocol (SNAP), 900subscribers to group addresses, 101successor routes, 421summarizationdefined, 905EIGRP, 423–424, 424process, 147–150, 147, 149summary-address eigrp command, 475summary route configuration, 474–476, 475–476Super Frames (SFs), 900sustainable cell rate (SCR), 899SVCs (switched virtual circuits)defined, 905Frame Relay, 801switch blocks, 905switch fabrics, 905switch portsaccess lists, 634–636VLANs, 570–571switch processors (SPs), 901switched LANs, 905Switched Multimegabit Data Service (SMDS), 900Switched Port Analyzer (SPAN), 901switched virtual circuits (SVCs)defined, 905Frame Relay, 801switches, 10, 10vs. bridges, 8Catalyst. See Catalyst switch configurationData Link layer, 25–26, 26defined, 905vs. hubs, 26layer 2. See layer 2 switchingfor network segmentation, 6, 6switchport command, 570–571switchport access command, 571–572switchport mode command, 571–572switchport nonegotiate command, 527, 572switchport port-security command, 503–504switchport port-security aging command, 536switchport port-security mac-address command, 521switchport trunk command, 566

10089bindex.fm  Page 958  Tuesday, July 24, 2007  9:38 AM




switchport trunk allowed command–Telnet protocol959switchport trunk allowed command, 573switchport trunk encapsulation command, 572switchport trunk native command, 574switchport voice vlan command, 587symmetrical DSL, 782syn packet acknowledgments, 82Synchronous Data Link Control (SDLC), 899Synchronous Digital Hierarchy (SDH), 899Synchronous Optical Network (SONET), 901synchronous transmissions, 905Synchronous Transport Module Level 1 (STM-1), 904syslog protocol, 905system LED, 516–517, 516System Network Architecture (SNA), 900

TT-connectors, 38T reference points, 905T1 WANs, 905T3 WANs, 905Tab command, 187tables for VLSMs, 140–144, 141, 143, 145TACACS+ (Terminal Access Controller Access Control System), 905tagged traffic, 905TAs (terminal adapters), 906TCP (Transmission Control Protocol), 75–77defined, 906destination ports, 81–82key concepts, 79port numbers, 80–82, 80segment format, 75–77, 75source ports, 80–81syn packet acknowledgments, 82TCP/IP (Transmission Control Protocol/Internet Protocol)defined, 906and DoD model, 68–70, 69–70exam essentials, 102host-to-host layer protocolsTCP, 75–77, 75UDP, 77–79, 78Internet layer protocols, 83ARP, 90–92, 90ICMP, 87–90, 87, 89IP, 84–87, 84, 86RARP, 91, 92IP addresses. See IP addressesprocess/application layer protocols, 70–74review questions, 104–109summary, 101written labs, 102–103, 110TCP SYN flood attacks, 612TDM (Time Division Multiplexing), 906TE (terminal equipment) devicesdefined, 906TE1, 906TE2, 906telco abbreviation, 906telephony, VLANs, 586–588telnet command, 214–215Telnet protocol, 71, 295–296, 316closing sessions, 298–299for configuration information, 214–215connections, 297defined, 906IP access lists, 625–626with multiple devices, 297

10089bindex.fm  Page 959  Tuesday, July 24, 2007  9:38 AM




96010Base2 technology–Transmission Power Control (TPC)passwords, 197–198, 295–296for router connections, 174SDM for, 299–300, 299–300users, 297–29810Base2 technology, 3810Base5 technology, 3810BaseT technology, 38, 852Teredo, 755Terminal Access Controller Access Control System (TACACS+), 905terminal adapters (TAs), 906terminal emulationdefined, 906Telnet, 71terminal equipment (TE) devicesdefined, 906TE1, 906TE2, 906terminal history size command, 189terminal monitor command, 395testing NAT, 677–679, 678–679TFN (Tribe Flood Network) attacks, 612TFTP (Trivial File Transfer Protocol), 71copying with, 264–265, 275–276defined, 906TFTP hosts, 906tftp-server command, 266thicknet, 38, 906thin protocols, 77thinnet, 38, 906this network or segment address, 96thrashing of MAC tables, 505threats, security, 611–613three-layer hierarchical model, 46–49, 47three-way handshakes, 17, 906time-based access lists, 615, 637–638Time Division Multiplexing (TDM), 906time-range command, 637–638Time To Live (TTL)defined, 907IP header, 85timersCDP for, 283–284RIP, 383token buses, 907token passing access method, 907Token Ring Interface Processor (TRIP), 907Token Ring technology, 907tokens, 907toll networksdefined, 907WANs, 775topologydatabases, 446, 907documenting, 292–294, 292, 294EIGRP tables, 420, 425Topology View screen, 540, 540Total length field, 85TPC (Transmission Power Control), 710traceroute commanddefined, 907ICMP, 88, 152, 215for network connectivity, 307–308traffic flow, ESP for, 827traffic information, CDP for, 289transferring files, 71–72transforms, IPSec, 826–827translation timeout in NAT, 678Transmission Control Protocol. See TCP (Transmission Control Protocol)Transmission Control Protocol/Internet Protocol. See TCP/IP (Transmission Control Protocol/Internet Protocol)Transmission Power Control (TPC), 710

10089bindex.fm  Page 960  Tuesday, July 24, 2007  9:38 AM




transparent bridging–unified wireless solution961transparent bridgingdefined, 907operation, 25transparent mode in VTP, 565Transport layer, 16–17acknowledgments, 21, 21connection-oriented communication, 17–20, 18–19defined, 907flow control, 17port numbers, 45, 45windowing, 20–21, 20trapsdefined, 907SNMP, 73Tribe Flood Network (TFN) attacks, 612TRIP (Token Ring Interface Processor), 907Trivial File Transfer Protocol (TFTP), 71copying with, 264–265, 275–276defined, 907Trojan horse attacks, 613troubleshootingconnectivity, 308–310debug ip rip for, 397–398Frame Relay, 811–813, 812IP addresses, 150–157, 150, 153–157NAT, 677–679, 678–679OSPF, 471–473, 472–474show ip protocols for, 394–395VTP, 583–586trunk command, 571–572trunk linksdefined, 907VLANs, 560–561, 561trunk ports, 512, 571–574Trunk Up-Down (TUD) protocol, 907trust exploitation attacks, 613trusted networks, 611TTL (Time to Live)defined, 907IP header, 85TUD (Trunk Up-Down) protocol, 907tunneling, 35defined, 908IPv6 migration, 754–755, 755quality of service in, 832–836, 832–8352.4GHz wireless, 708–709, 708, 711–7121242AP router configuration, 361–3622500 routers configuration, 259–2602600 routersbringing up, 177–179configuration, 259interfaces and connections, 174, 1742800 routersbringing up, 175–177interfaces and connections, 174–175, 174Type field in Ethernet frames, 36Type of Service field, 85

UU reference points, 908UDP (User Datagram Protocol), 77–78defined, 908key concepts, 79port numbers, 80–82, 80segment format, 78–79, 78undebug all command, 309unicasts, 100defined, 908IPv6, 742, 744unidirectional shared trees, 908unified wireless solution, 712–714, 713AWPP, 718MESH and LWAPP, 716–717, 717–718

10089bindex.fm  Page 961  Tuesday, July 24, 2007  9:38 AM




962UNII (Unlicensed National Information Infrastructure)–VIP (Virtual IP) functionsecurity, 718–721split-MAC architecture, 715–716, 715UNII (Unlicensed National Information Infrastructure), 706, 706, 709–710, 710unique local addresses, 745universal bit, 35unnumbered frames, 908unreliable protocols, 77unshielded twisted-pair (UTP)defined, 908Ethernet, 37, 41, 42untrusted networks, 611updates with holddown timers, 382upgrading IOS, 265–266, 268–270, 314UplinkFast feature, 513, 524Urgent pointer field, 76URLs in IFS, 267use-tacacs command, 194User Datagram Protocol (UDP), 77–78defined, 908key concepts, 79port numbers, 80–82, 80segment format, 78–79, 78user EXEC mode, 184user mode, 180–181username command, 225–226, 791, 818usernamesFTP, 71PPP, 791SDM, 226–227WANs, 818users, Telnet, 297–298UTP (unshielded twisted-pair) wiringdefined, 908Ethernet, 37, 41, 42

VV.24 standard, 788V.35 standard, 788valid host IDsClass A addresses, 97Class B addresses, 98Class C addresses, 98variable bit rate (VBR) class, 908variable-length subnet masks. See VLSMs (variable-length subnet masks)variance command, 442VBR (variable bit rate) class, 908VCCs (virtual channel connections), 908VDSL (Very High Data Rate Digital Subscriber Line), 784verifyingconfigurationsCatalyst switches, 528–534EIGRP, 438–443IOS, 214–223, 222–223IP routing, 373–374, 393–398NAT, 676OSPF, 457–462OSPFv3, 763–766PPP encapsulation, 792–793, 792–793RIP routing tables, 387–389RIPng, 760–763router, 275, 373–374flash memory, 263–264Versatile Interface Processor (VIP), 908version 2 command, 391Version field, 85Very High Data Rate Digital Subscriber Line (VDSL), 784viewing configurations, 213–214violation command, 522, 536VIP (Versatile Interface Processor), 908VIP (Virtual IP) function, 908

10089bindex.fm  Page 962  Tuesday, July 24, 2007  9:38 AM




virtual channel connections (VCCs)–vtp mode client command963virtual channel connections (VCCs), 908virtual circuitsdefined, 908Frame Relay, 801port numbers, 80TCP, 75Virtual IP (VIP) function, 908virtual LANs. See VLANs (virtual LANs)virtual private networks (VPNs), 825–826configuration, 828–836, 828–835defined, 909IPSec for, 826–836quality of service across tunnels, 832–836, 832–835virtual rings, 908vlan command, 568–569, 585VLAN IDs, 909VLAN Management Policy Server (VMPS) service, 559, 909VLAN Trunk Protocol. See VTP (VLAN Trunk Protocol)VLANs (virtual LANs), 552broadcast control, 554configuration, 568–570inter-VLAN routing, 575–580, 576–578, 580, 588–597switch port assignments, 570–571trunk ports, 571–574voice, 586–588defined, 909dynamic, 559exam essentials, 598flexibility and scalability, 555–558, 556–557frame tagging, 561–562identifying, 559–563, 561ISL for, 562membership, 558–559operation, 552–554, 553review questions, 600–606routing between, 567–568, 567–568security, 555static, 558–559summary, 597–598trunk links, 560–561, 561VTP for. See VTP (VLAN Trunk Protocol)written lab, 599, 607VLSMs (variable-length subnet masks), 137–138, 137benefits, 139defined, 909designing, 138–139, 138EIGRP, 418–419, 423–424, 423–424implementing, 139–144, 141–147RIPv1 vs. RIPv2, 391–392VMPS (VLAN Management Policy Server) service, 559, 909voice configuration, 586–588voice traversal with firewalls, 614VPN Connection Information screen, 829, 829VPNs (virtual private networks), 825–826configuration, 828–836, 828–835defined, 909IPSec for, 826–836quality of service across tunnels, 832–836, 832–835VTP (VLAN Trunk Protocol), 563–564configuration, 580–583defined, 909importance, 565modes of operation, 564–565, 564pruning, 565–566troubleshooting, 583–586vtp domain command, 581vtp mode client command, 582

10089bindex.fm  Page 963  Tuesday, July 24, 2007  9:38 AM




964vtp mode server command–wireless networksvtp mode server command, 581, 585vtp password command, 581–582VTP transparent mode, 565, 909VTYaccess lists for, 625–626passwords for, 295vty command, 195

WWAN Wizard, 353–358, 353–358WANs (wide area networks), 774–775cabling, 779–782, 780–781, 785–786, 786connection types, 775–776, 776defined, 909DSL, 782–785, 782, 784DTE and DCE for, 786, 786exam essentials, 836–837Frame Relay. See Frame Relayhands-on lab, 838–841, 849HDLC for, 787, 787PPP for, 788–789, 788authentication, 790–794, 813–818, 813–816configuration, 791–792debugging, 793–796, 794–795encapsulation, 792–795, 792–794LCP options, 789sessions, 790PPPoE for, 796–797, 818–822, 819–822review questions, 842–848summary, 836support, 777–779terminology, 775written lab, 837WCS (Wireless Control System), 713–714well-known port numbers, 80WEP (Wired Equivalency Protocol), 719–720Wi-Fi Alliance, 706Wi-Fi Protected Access (WPA), 720wildcardsfor access lists, 620–622for default routes, 374defined, 909OSPF, 450–453, 452Window field, 76windowingdefined, 909Transport layer, 20–21, 20Windows Registry, hexadecimal addresses in, 93WINS (Windows Internet Name Service), 909WinSock interface, 909Wired Equivalency Protocol (WEP), 719–720Wireless Control System (WCS), 713–714Wireless Express Security screen, 724, 724Wireless Interfaces screen, 725–726, 725–726wireless networks, 704802.11 standards, 706–712, 708–710, 712AWPP, 718configuration, 721–728, 722–728exam essentials, 729MESH and LWAPP, 716–717, 717–718overview, 704–706, 706review questions, 731–736security, 718–721split-MAC architecture, 715–716, 715summary, 729

10089bindex.fm  Page 964  Tuesday, July 24, 2007  9:38 AM




Wireless Security setting–zones965unified solution, 712–714, 713written labs, 730, 737Wireless Security settings, 727, 727workgroup layer, 48workgroup layers, 909workgroup switching, 909WPA (Wi-Fi Protected Access), 720written labsaccess lists, 655–656, 667EIGRP and OSPF, 477–478, 491internetworking, 50–55, 53, 62–66IOS, 234, 249IP routing, 402–403, 415IPv6 protocol, 767, 772layer 2 switching and STP, 542, 550management, 313, 325NAT, 688–689, 701subnetting, 158–160, 168–169TCP/IP, 102–103, 110VLANs, 599, 607WANs, 837wireless networks, 730, 737

XX.25 standarddefined, 910for Frame Relay, 798X Window systemdefined, 910purpose, 72

ZZIP (Zone Information Protocol), 910ZIP storms, 910zones, 910

10089bindex.fm  Page 965  Tuesday, July 24, 2007  9:38 AM




 CCNA: Cisco Certified Network Associate Study Guide CCNA Exam 640-802 Objectives OBJECTIVECHAPTER

Describe How A Network Works

 Describe the purpose and functions of various network devices1Select the components required to meet a network specification1Use the OSI and TCP/IP models and their associated protocols to explain how data flows in a network 1Describe common networked applications including web applications 1Describe the purpose and basic operation of the protocols in the OSI and TCP models 1, 2Describe the impact of applications (Voice Over IP and Video Over IP) on a network 1, 9Interpret network diagrams 1, 3Determine the path between two hosts across a network 6Describe the components required for network and Internet communications 1Identify and correct common network problems at layers 1, 2, 3 and 7 using a layered model approach 1, 2Differentiate between LAN/WAN operation and features 1, 14Configure, verify and troubleshoot a switch with VLANs and interswitch communicationsSelect the appropriate media, cables, ports, and connectors to connect switches to other network devices and hosts 1, 8Explain the technology and media access control method for Ethernet networks Explain network segmentation and basic traffic management concepts 1, 8Explain basic switching concepts and the operation of Cisco switches 8Perform and verify initial switch configuration tasks including remote access management 8Verify network status and switch operation using basic utilities (including: ping, traceroute, telnet, SSH, arp, ipconfig), SHOW & DEBUG commands 8, 9Identify, prescribe, and resolve common switched network media issues, configuration issues, auto negotiation, and switch hardware failures 8, 9Describe enhanced switching technologies (including: VTP, RSTP, VLAN, PVSTP, 802.1q) 9

 10089bperfcard.fm  Page 1  Monday, July 23, 2007  7:16 PM




 Exam objectives are subject to change at any time without prior notice and at Cisco’s sole discretion. Please visit Cisco’s website ( www.cisco.com/web/learning ) for the most current listing of exam objectives.

 Describe how VLANs create logically separate networks and the need for routing between them 9Configure, verify, and troubleshoot VLANs 9Configure, verify, and troubleshoot trunking on Cisco switches 9Configure, verify, and troubleshoot interVLAN routing 9Configure, verify, and troubleshoot VTP 9Configure, verify, and troubleshoot RSTP operation 9Interpret the output of various show and debug commands to verify the operational status of a Cisco switched network 9Implement basic switch security (including: port security, trunk access, management vlan other than vlan1, etc.9Implement an IP addressing scheme and IP Services to meet network requirements in a medium-size Enterprise branch office networkDescribe the operation and benefits of using private and public IP addressing 2, 3Explain the operation and benefits of using DHCP and DNS 1Configure, verify and troubleshoot DHCP and DNS operation on a router (including: CLI/SDM) 4Implement static and dynamic addressing services for hosts in a LAN environment 3Calculate and apply an addressing scheme including VLSM IP addressing design to a network 3Determine the appropriate classless addressing scheme using VLSM and summarization to satisfy addressing requirements in a LAN/WAN environment 3Describe the technological requirements for running IPv6 in conjunction with IPv4 (including: protocols, dual stack, tunneling, etc) 13Describe IPv6 addresses 13Identify and correct common problems associated with IP addressing and host configurations 3Configure, verify, and troubleshoot basic router operation and routing on Cisco devicesDescribe basic routing concepts (including: packet forwarding, router lookup process6 OBJECTIVECHAPTER

 10089bperfcard.fm  Page 2  Monday, July 23, 2007  7:16 PM




 Describe the operation of Cisco routers (including: router bootup process, POST, router components) 4Select the appropriate media, cables, ports, and connectors to connect routers to other network devices and hosts1Configure, verify, and troubleshoot RIPv2 6Access and utilize the router to set basic parameters (including: CLI/SDM) 4, 6, 7Connect, configure, and verify operation status of a device interface 4, 6, 7Verify device configuration and network connectivity using ping, traceroute, telnet, SSH or other utilities 4, 6, 7Perform and verify routing configuration tasks for a static or default route given specific routing requirements 6, 7Manage IOS configuration files (including: save, edit, upgrade, restore) 5Manage Cisco IOS 5Compare and contrast methods of routing and routing protocols 6, 7Configure, verify, and troubleshoot OSPF 6, 7Configure, verify, and troubleshoot EIGRP 6, 7Verify network connectivity (including: using ping, traceroute, and telnet or SSH) 4, 5, 6, 7Troubleshoot routing issues 4, 6, 7Verify router hardware and software operation using SHOW & DEBUG commands 4, 6, 7Implement basic router security 6, 7, 10Explain and select the appropriate administrative tasks required for a WLANDescribe standards associated with wireless media (including: IEEE WI-FI Alliance, ITU/FCC) 12Identify and describe the purpose of the components in a small wireless network (Including: SSID, BSS, ESS) 12Identify the basic parameters to configure on a wireless network to ensure that devices connect to the correct access point 12Compare and contrast wireless security features and capabilities of WPA security (including: open, WEP, WPA-1/2) 12 OBJECTIVECHAPTER

 10089bperfcard.fm  Page 3  Monday, July 23, 2007  7:16 PM




 Exam objectives are subject to change at any time without prior notice and at Cisco’s sole discretion. Please visit Cisco’s website ( www.cisco.com/web/learning ) for the most current listing of exam objectives.

 Identify common issues with implementing wireless networks (Including: Interface, Miss configuration) 12Identify security threats to a network and describe general methods to mitigate those threatsDescribe today's increasing network security threats and explain the need to implement a comprehensive security policy to mitigate the threats 10Explain general methods to mitigate common security threats to network devices, hosts, and applications 10Describe the functions of common security appliances and applications 10Describe security recommended practices including initial steps to secure network devices10Implement, verify, and troubleshoot NAT and ACLs in a medium-size Enterprise branch office networkDescribe the purpose and types of ACLs 10Configure and apply ACLs based on network filtering requirements (including: CLI/SDM) 10Configure and apply an ACLs to limit telnet and SSH access to the router using (including: SDM/CLI) 10Verify and monitor ACLs in a network environment 10Troubleshoot ACL issues 10Explain the basic operation of NAT 11Configure NAT for given network requirements using (including: CLI/SDM) 11Troubleshoot NAT issues 11Implement and verify WAN linksDescribe different methods for connecting to a WAN 14Configure and verify a basic WAN serial connection 14Configure and verify Frame Relay on Cisco routers 14Troubleshoot WAN implementation issues 14Describe VPN technology (including: importance, benefits, role, impact, components) 14Configure and verify a PPP connection between Cisco routers 14 OBJECTIVECHAPTER

 10089bperfcard.fm  Page 4  Monday, July 23, 2007  7:16 PM




Use the electronic flashcards for PCs orPalm devices to jog your memory andprep last minute for the exam! Reinforce your understanding of key concepts with these hardcoreflashcard-style questions. Download the flashcards to your Palmdevice and go on the road. Now youcan study for the CCNA exam anytime, anywhere. Reinforce what you’ve learned withover an hour’s worth of useful audioand video files designed to enhanceyour learning experience.

TSearch through the complete book in PDF! Access the entire CCNA: Cisco CertifiedNetwork Associate Study Guide, com-plete with figures and tables, in elec-tronic format. Search the CCNA: Cisco Certified Net-work Associate Study Guide chapters tofind information on any topic in seconds.

Get ready for Cisco’s new CCNA exam (640-802) with the most comprehensive and challenging sample tests anywhere!The Sybex Test Engine features: All the review questions, as covered ineach chapter of the book Four full-length bonus exams with challenging questions representative of those you’ll find on the real exam,available only on the CD An Assessment Test to narrow your focusto certain objective groups

he Absolute Best CCNA Package on the Market!

10089bmedinst.qxd:Layout 1  7/23/07  7:18 PM  Page B




The complete CCNA

®

 

study solution from Sybex

®

CCNA: Cisco® Certified Network Associate Study Guide, 

Sixth Edition, Exam 640-802

978-0-470-11008-9 • US $49.99 

•  

In-depth coverage of every exam objective, expanded coverage on key topics in the 

current version of the exam, plus updates that reflect technology developments over 

the past year

•  

Enhanced CD contains over an hour of useful video and audio files, as well as the 

Sybex Test Engine, flashcards, and entire book in PDF format

CCNA: Cisco® Certified Network Associate Study Guide Deluxe, 

Fifth Edition, Exam 640-802

978-0-470-11009-6 • US $99.99 

•  

Bonus CD includes a fully functional version of the popular network simulator, 

CCNA Virtual Lab, Platinum Edition

, allowing the reader to perform numerous 

labs—a value of over $150 U.S.!

•  

Contains over an hour of video instruction from the author, as well as 30 minutes 

of audio, in addition to the Sybex Test Engine and flashcards

Todd Lammle’s CCNA IOS Commands Survival Guide 

978-0-470-17560-6 • US $29.99

•  

Highlights the hundreds of IOS commands needed to pass the exam and that Cisco 

networking professionals need to know to perform their jobs

•  

Detailed examples of how to use these commands provide a quick reference guide 

for CCNA candidates

Visit www.sybex.com

Wiley, Sybex, and related logos are registered trademarks of John Wiley & Sons, Inc. 

and/or its af? liates.  CCNA is a registered trademark of Cisco Systems, Inc.

CCNA: Cisco Certified Network Associate Fast Pass, 

Third Edition

978-0-470-18571-1 • US $29.99 

•  

Organized by objectives for quick review and reinforcement of key topics

•  

CD contains two bonus exams, handy flashcard questions, and a searchable PDF 

of Glossary of Terms




 Wiley Publishing, Inc.End-User License Agreement READ THIS.  You should carefully read these terms and conditions before opening the software packet(s) included with this book “Book”. This is a license agree-ment “Agreement” between you and Wiley Publishing, Inc. “WPI”. By opening the accompanying software packet(s), you acknowledge that you have read and accept the following terms and conditions. If you do not agree and do not want to be bound by such terms and conditions, promptly return the Book and the unopened software packet(s) to the place you obtained them for a full refund. 1. License Grant.  WPI grants to you (either an individ-ual or entity) a nonexclusive license to use one copy of the enclosed software program(s) (collectively, the “Software,” solely for your own personal or business purposes on a single computer (whether a standard computer or a workstation component of a multi-user network). The Software is in use on a computer when it is loaded into temporary memory (RAM) or installed into permanent memory (hard disk, CD-ROM, or other storage device). WPI reserves all rights not expressly granted herein. 2. Ownership.  WPI is the owner of all right, title, and interest, including copyright, in and to the compilation of the Software recorded on the physical packet included with this Book “Software Media”. Copyright to the individual programs recorded on the Software Media is owned by the author or other authorized copyright owner of each program. Ownership of the Software and all proprietary rights relating thereto remain with WPI and its licensers. 3. Restrictions On Use and Transfer.  (a) You may only (i) make one copy of the Software for backup or archival purposes, or (ii) transfer the Soft-ware to a single hard disk, provided that you keep the original for backup or archival purposes. You may not (i) rent or lease the Software, (ii) copy or reproduce the Software through a LAN or other network system or through any computer subscriber system or bulletin-board system, or (iii) modify, adapt, or create deriva-tive works based on the Software.(b) You may not reverse engineer, decompile, or disas-semble the Software. You may transfer the Software and user documentation on a permanent basis, pro-vided that the transferee agrees to accept the terms and conditions of this Agreement and you retain no copies. If the Software is an update or has been updated, any transfer must include the most recent update and all prior versions. 4. Restrictions on Use of Individual Programs.  You must follow the individual requirements and restric-tions detailed for each individual program in the About the CD-ROM appendix of this Book or on the Soft-ware Media. These limitations are also contained in the individual license agreements recorded on the Software Media. These limitations may include a requirement that after using the program for a specified period of time, the user must pay a registration fee or discontinue use. By opening the Software packet(s), you will be agreeing to abide by the licenses and restrictions for these individual programs that are detailed in the About the CD-ROM appendix and/or on the Software Media. None of the material on this Software Media or listed in this Book may ever be redistributed, in original or modified form, for commercial purposes. 5. Limited Warranty. (a) WPI warrants that the Software and Software Media are free from defects in materials and workman-ship under normal use for a period of sixty (60) days from the date of purchase of this Book. If WPI receives notification within the warranty period of defects in materials or workmanship, WPI will replace the defec-tive Software Media. (b) WPI AND THE AUTHOR(S) OF THE BOOK DISCLAIM ALL OTHER WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITA-TION IMPLIED WARRANTIES OF MERCHANT-ABILITY AND FITNESS FOR A PARTICULAR PURPOSE, WITH RESPECT TO THE SOFTWARE, THE PROGRAMS, THE SOURCE CODE CON-TAINED THEREIN, AND/OR THE TECHNIQUES DESCRIBED IN THIS BOOK. WPI DOES NOT WARRANT THAT THE FUNCTIONS CON-TAINED IN THE SOFTWARE WILL MEET YOUR REQUIREMENTS OR THAT THE OPERATION OF THE SOFTWARE WILL BE ERROR FREE. (c) This limited warranty gives you specific legal rights, and you may have other rights that vary from jurisdic-tion to jurisdiction. 6. Remedies.  (a) WPI’s entire liability and your exclusive remedy for defects in materials and workmanship shall be limited to replacement of the Software Media, which may be returned to WPI with a copy of your receipt at the fol-lowing address: Software Media Fulfillment Depart-ment, Attn.:  CCNA ® : Cisco ®  Certified Network Associate Study Guide , Wiley Publishing, Inc., 10475 Crosspoint Blvd., Indianapolis, IN 46256, or call 1-800-762-2974. Please allow four to six weeks for deliv-ery. This Limited Warranty is void if failure of the Soft-ware Media has resulted from accident, abuse, or misapplication. Any replacement Software Media will be warranted for the remainder of the original war-ranty period or thirty (30) days, whichever is longer. (b) In no event shall WPI or the author be liable for any damages whatsoever (including without limitation damages for loss of business profits, business interrup-tion, loss of business information, or any other pecuni-ary loss) arising from the use of or inability to use the Book or the Software, even if WPI has been advised of the possibility of such damages. (c) Because some jurisdictions do not allow the exclu-sion or limitation of liability for consequential or inci-dental damages, the above limitation or exclusion may not apply to you. 7. U.S. Government Restricted Rights.  Use, duplica-tion, or disclosure of the Software for or on behalf of the United States of America, its agencies and/or instru-mentalities “U.S. Government” is subject to restric-tions as stated in paragraph (c)(1)(ii) of the Rights in Technical Data and Computer Software clause of DFARS 252.227-7013, or subparagraphs (c) (1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, and in similar clauses in the NASA FAR supplement, as applicable. 8. General.  This Agreement constitutes the entire understanding of the parties and revokes and super-sedes all prior agreements, oral or written, between them and may not be modified or amended except in a writing signed by both parties hereto that specifically refers to this Agreement. This Agreement shall take precedence over any other documents that may be in conflict herewith. If any one or more provisions con-tained in this Agreement are held by any court or tribu-nal to be invalid, illegal, or otherwise unenforceable, each and every other provision shall remain in full force and effect.

 

10089bmeddis.fm  Page 1  Monday, July 23, 2007  7:15 PM




